#!/usr/bin/env python3
"""
Split llms-full.txt files into individual pages/documents.

Usage:
    uv run scripts-temp/split_llms_pages.py

Two splitting strategies:
1. Source URL pattern: ^# .*$\nSource: (for LangChain, Anthropic, etc.)
2. Header-only pattern: ^# (for PydanticAI - filters out code block false positives)

Outputs to data/interim/pages/{source}/ directory.
"""

import json
import re
from pathlib import Path

# Directories
RAW_DIR = Path(__file__).parent.parent / 'data' / 'raw'
OUTPUT_DIR = Path(__file__).parent.parent / 'data' / 'interim' / 'pages'

# Sources with Source: URL pattern
SOURCES_WITH_URL = [
    'LangChain',
    'Anthropic',
    'Prefect',
    'FastMCP',
    'McpProtocol',
]

# Sources with header-only pattern (no Source: line)
SOURCES_HEADER_ONLY = [
    'PydanticAI',
    'Zep',
]

# Regex patterns
# Pattern 1: # Title followed by Source: URL
PAGE_PATTERN_WITH_URL = re.compile(r'^# (.+)$\nSource: (https?://[^\n]+)', re.MULTILINE)

# Pattern 2: # Title at start of line (outside code blocks)
PAGE_PATTERN_HEADER_ONLY = re.compile(r'^# (.+)$', re.MULTILINE)


def neutralize_code_block_headers(content: str) -> str:
    """Convert # headers inside code blocks to ### to avoid false positive matches.

    This preserves the code block content while preventing ^# from matching
    Python comments or markdown headers inside code examples.
    """

    def replace_headers_in_block(match: re.Match) -> str:
        """Replace ^# with ^### inside a code block."""
        block = match.group(0)
        # Replace lines starting with "# " (Python comments) with "### "
        # This changes character count, so we can't use position-based extraction
        return re.sub(r'^# ', '### ', block, flags=re.MULTILINE)

    return re.sub(r'```[\s\S]*?```', replace_headers_in_block, content)


def split_with_url_pattern(content: str) -> list[dict]:
    """Split content using # Title + Source: URL pattern.

    Args:
        content: Full text content of llms-full.txt

    Returns:
        List of page dicts with title, source_url, and content
    """
    pages = []
    matches = list(PAGE_PATTERN_WITH_URL.finditer(content))

    for i, match in enumerate(matches):
        content_start = match.end()
        content_end = matches[i + 1].start() if i + 1 < len(matches) else len(content)
        page_content = content[content_start:content_end].strip()

        pages.append({
            'title': match.group(1).strip(),
            'source_url': match.group(2).strip(),
            'content': page_content,
            'content_length': len(page_content),
        })

    return pages


def split_with_header_pattern(content: str) -> list[dict]:
    """Split content using ^# Title pattern, filtering out code block headers.

    Strategy: Remove code blocks (preserving line count), find headers in cleaned
    content, then use those same positions in the original content since line
    counts are preserved.

    Args:
        content: Full text content of llms-full.txt

    Returns:
        List of page dicts with title, source_url (None), and content
    """
    # Neutralize headers inside code blocks to avoid false matches
    content_neutralized = neutralize_code_block_headers(content)

    pages = []
    matches = list(PAGE_PATTERN_HEADER_ONLY.finditer(content_neutralized))

    for i, match in enumerate(matches):
        # Extract from neutralized content (code blocks have ### instead of #)
        content_start = match.end()
        content_end = matches[i + 1].start() if i + 1 < len(matches) else len(content_neutralized)

        # Extract from neutralized content - code blocks will show ### for comments
        page_content = content_neutralized[content_start:content_end].strip()

        pages.append({
            'title': match.group(1).strip(),
            'source_url': None,  # No source URL in this format
            'content': page_content,
            'content_length': len(page_content),
        })

    return pages


def process_source(source_name: str, use_header_only: bool = False) -> dict:
    """Process a single source's llms-full.txt file.

    Args:
        source_name: Name of the source (directory name)
        use_header_only: If True, use header-only pattern instead of URL pattern

    Returns:
        Dict with processing results
    """
    input_path = RAW_DIR / source_name / 'llms-full.txt'
    output_dir = OUTPUT_DIR / source_name

    if not input_path.exists():
        return {
            'status': 'skipped',
            'error': f'File not found: {input_path}',
            'page_count': 0,
        }

    # Read content
    content = input_path.read_text(encoding='utf-8')

    # Split into pages using appropriate pattern
    if use_header_only:
        pages = split_with_header_pattern(content)
    else:
        pages = split_with_url_pattern(content)

    if not pages:
        return {
            'status': 'failed',
            'error': 'No pages found - check regex pattern',
            'page_count': 0,
        }

    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)

    # Write individual page files as JSON
    for i, page in enumerate(pages):
        # Create safe filename from title
        safe_title = re.sub(r'[^\w\s-]', '', page['title'])[:50].strip()
        safe_title = re.sub(r'\s+', '_', safe_title)
        filename = f'{i:04d}_{safe_title}.json'

        page_path = output_dir / filename
        page_path.write_text(json.dumps(page, indent=2, ensure_ascii=False), encoding='utf-8')

    # Write summary manifest
    manifest = {
        'source': source_name,
        'input_file': str(input_path),
        'pattern_type': 'header_only' if use_header_only else 'with_url',
        'page_count': len(pages),
        'total_content_chars': sum(p['content_length'] for p in pages),
        'avg_page_size': sum(p['content_length'] for p in pages) / len(pages),
        'pages': [
            {
                'index': i,
                'title': p['title'],
                'source_url': p['source_url'],
                'content_length': p['content_length'],
            }
            for i, p in enumerate(pages)
        ],
    }

    manifest_path = output_dir / 'manifest.json'
    manifest_path.write_text(json.dumps(manifest, indent=2, ensure_ascii=False), encoding='utf-8')

    return {
        'status': 'success',
        'page_count': len(pages),
        'avg_size_chars': manifest['avg_page_size'],
        'output_dir': str(output_dir),
    }


def main():
    """Process all sources and split into pages."""
    print('Splitting llms-full.txt files into pages')
    print(f'Raw directory: {RAW_DIR}')
    print(f'Output directory: {OUTPUT_DIR}')
    print()

    # Ensure output directory exists
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    results = {}
    total_pages = 0

    # Process sources with URL pattern
    print('=== Sources with Source: URL pattern ===')
    for source in SOURCES_WITH_URL:
        print(f'Processing {source}...', end=' ')
        result = process_source(source, use_header_only=False)
        results[source] = result

        if result['status'] == 'success':
            print(f'✓ {result["page_count"]} pages ({result["avg_size_chars"]:.0f} chars avg)')
            total_pages += result['page_count']
        else:
            print(f'✗ {result.get("error", "unknown error")}')

    # Process sources with header-only pattern
    print()
    print('=== Sources with header-only pattern ===')
    for source in SOURCES_HEADER_ONLY:
        print(f'Processing {source}...', end=' ')
        result = process_source(source, use_header_only=True)
        results[source] = result

        if result['status'] == 'success':
            print(f'✓ {result["page_count"]} pages ({result["avg_size_chars"]:.0f} chars avg)')
            total_pages += result['page_count']
        else:
            print(f'✗ {result.get("error", "unknown error")}')

    # Write overall manifest
    all_sources = SOURCES_WITH_URL + SOURCES_HEADER_ONLY
    overall_manifest = {
        'sources_processed': len([r for r in results.values() if r['status'] == 'success']),
        'total_pages': total_pages,
        'results': results,
    }

    overall_manifest_path = OUTPUT_DIR / 'manifest.json'
    overall_manifest_path.write_text(
        json.dumps(overall_manifest, indent=2, ensure_ascii=False), encoding='utf-8'
    )

    print()
    print(f'Summary: {total_pages} total pages from {len(all_sources)} sources')
    print(f'Manifest: {overall_manifest_path}')


if __name__ == '__main__':
    main()
