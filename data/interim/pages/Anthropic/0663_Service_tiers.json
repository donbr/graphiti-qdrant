{
  "title": "Service tiers",
  "source_url": "https://docs.claude.com/en/api/service-tiers",
  "content": "Different tiers of service allow you to balance availability, performance, and predictable costs based on your application's needs.\n\nWe offer three service tiers:\n\n* **Priority Tier:** Best for workflows deployed in production where time, availability, and predictable pricing are important\n* **Standard:** Default tier for both piloting and scaling everyday use cases\n* **Batch:** Best for asynchronous workflows which can wait or benefit from being outside your normal capacity\n\n## Standard Tier\n\nThe standard tier is the default service tier for all API requests. Requests in this tier are prioritized alongside all other requests and observe best-effort availability.\n\n## Priority Tier\n\nRequests in this tier are prioritized over all other requests to Anthropic. This prioritization helps minimize [\"server overloaded\" errors](/en/api/errors#http-errors), even during peak times.\n\nFor more information, see [Get started with Priority Tier](#get-started-with-priority-tier)\n\n## How requests get assigned tiers\n\nWhen handling a request, Anthropic decides to assign a request to Priority Tier in the following scenarios:\n\n* Your organization has sufficient priority tier capacity **input** tokens per minute\n* Your organization has sufficient priority tier capacity **output** tokens per minute\n\nAnthropic counts usage against Priority Tier capacity as follows:\n\n**Input Tokens**\n\n* Cache reads as 0.1 tokens per token read from the cache\n* Cache writes as 1.25 tokens per token written to the cache with a 5 minute TTL\n* Cache writes as 2.00 tokens per token written to the cache with a 1 hour TTL\n* For [long-context](/en/docs/build-with-claude/context-windows) (>200k input tokens) requests, input tokens are 2 tokens per token\n* All other input tokens are 1 token per token\n\n**Output Tokens**\n\n* For [long-context](/en/docs/build-with-claude/context-windows) (>200k input tokens) requests, output tokens are 1.5 tokens per token\n* All other output tokens are 1 token per token\n\nOtherwise, requests proceed at standard tier.\n\n<Note>\n  Requests assigned Priority Tier pull from both the Priority Tier capacity and the regular rate limits.\n  If servicing the request would exceed the rate limits, the request is declined.\n</Note>\n\n## Using service tiers\n\nYou can control which service tiers can be used for a request by setting the `service_tier` parameter:\n\n```python  theme={null}\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello, Claude!\"}],\n    service_tier=\"auto\"  # Automatically use Priority Tier when available, fallback to standard\n)\n```\n\nThe `service_tier` parameter accepts the following values:\n\n* `\"auto\"` (default) - Uses the Priority Tier capacity if available, falling back to your other capacity if not\n* `\"standard_only\"` - Only use standard tier capacity, useful if you don't want to use your Priority Tier capacity\n\nThe response `usage` object also includes the service tier assigned to the request:\n\n```json  theme={null}\n{\n  \"usage\": {\n    \"input_tokens\": 410,\n    \"cache_creation_input_tokens\": 0,\n    \"cache_read_input_tokens\": 0,\n    \"output_tokens\": 585,\n    \"service_tier\": \"priority\"\n  }\n}\n```\n\nThis allows you to determine which service tier was assigned to the request.\n\nWhen requesting `service_tier=\"auto\"` with a model with a Priority Tier commitment, these response headers provide insights:\n\n```\nanthropic-priority-input-tokens-limit: 10000\nanthropic-priority-input-tokens-remaining: 9618\nanthropic-priority-input-tokens-reset: 2025-01-12T23:11:59Z\nanthropic-priority-output-tokens-limit: 10000\nanthropic-priority-output-tokens-remaining: 6000\nanthropic-priority-output-tokens-reset: 2025-01-12T23:12:21Z\n```\n\nYou can use the presence of these headers to detect if your request was eligible for Priority Tier, even if it was over the limit.\n\n## Get started with Priority Tier\n\nYou may want to commit to Priority Tier capacity if you are interested in:\n\n* **Higher availability**: Target 99.5% uptime with prioritized computational resources\n* **Cost Control**: Predictable spend and discounts for longer commitments\n* **Flexible overflow**: Automatically falls back to standard tier when you exceed your committed capacity\n\nCommitting to Priority Tier will involve deciding:\n\n* A number of input tokens per minute\n* A number of output tokens per minute\n* A commitment duration (1, 3, 6, or 12 months)\n* A specific model version\n\n<Note>\n  The ratio of input to output tokens you purchase matters. Sizing your Priority Tier capacity to align with your actual traffic patterns helps you maximize utilization of your purchased tokens.\n</Note>\n\n### Supported models\n\nPriority Tier is supported by:\n\n* Claude Opus 4.1\n* Claude Opus 4\n* Claude Sonnet 4\n* Claude Sonnet 3.7\n* Claude Haiku 3.5\n\nCheck the [model overview page](/en/docs/about-claude/models/overview) for more details on our models.\n\n### How to access Priority Tier\n\nTo begin using Priority Tier:\n\n1. [Contact sales](https://claude.com/contact-sales/priority-tier) to complete provisioning\n2. (Optional) Update your API requests to optionally set the `service_tier` parameter to `auto`\n3. Monitor your usage through response headers and the Claude Console",
  "content_length": 5234
}