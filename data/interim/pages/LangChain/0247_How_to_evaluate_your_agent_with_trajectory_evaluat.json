{
  "title": "How to evaluate your agent with trajectory evaluations",
  "source_url": "https://docs.langchain.com/langsmith/trajectory-evals",
  "content": "Many agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain's [`agentevals`](https://github.com/langchain-ai/agentevals) package provides evaluators specifically designed for testing agent trajectories with live models.\n\n<Note>\n  This guide covers the open source [LangChain](/oss/python/langchain/overview) `agentevals` package, which integrates with LangSmith for trajectory evaluation.\n</Note>\n\nAgentEvals allows you to evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a *trajectory match* or by using an *LLM judge*:\n\n<Card title=\"Trajectory match\" icon=\"equals\" arrow=\"true\" href=\"#trajectory-match-evaluator\">\n  Hard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.\n\n  Ideal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn't require additional LLM calls.\n</Card>\n\n<Card title=\"LLM-as-judge\" icon=\"gavel\" arrow=\"true\" href=\"#llm-as-judge-evaluator\">\n  Use a LLM to qualitatively validate your agent's execution trajectory. The \"judge\" LLM reviews the agent's decisions against a prompt rubric (which can include a reference trajectory).\n\n  More flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent's trajectory without strict tool call or ordering requirements.\n</Card>\n\n## Installing AgentEvals\n\n<CodeGroup>\n  ```bash Python theme={null}\n  pip install agentevals\n  ```\n\n  ```bash TypeScript theme={null}\n  npm install agentevals @langchain/core\n  ```\n</CodeGroup>\n\nOr, clone the [AgentEvals repository](https://github.com/langchain-ai/agentevals) directly.\n\n## Trajectory match evaluator\n\nAgentEvals offers the `create_trajectory_match_evaluator` function in Python and `createTrajectoryMatchEvaluator` in TypeScript to match your agent's trajectory against a reference trajectory.\n\nYou can use the following modes:\n\n| Mode                                     | Description                                               | Use Case                                                              |\n| ---------------------------------------- | --------------------------------------------------------- | --------------------------------------------------------------------- |\n| [`strict`](#strict-match)                | Exact match of messages and tool calls in the same order  | Testing specific sequences (e.g., policy lookup before authorization) |\n| [`unordered`](#unordered-match)          | Same tool calls allowed in any order                      | Verifying information retrieval when order doesn't matter             |\n| [`subset`](#subset-and-superset-match)   | Agent calls only tools from reference (no extras)         | Ensuring agent doesn't exceed expected scope                          |\n| [`superset`](#subset-and-superset-match) | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken                          |\n\n### Strict match\n\nThe `strict` mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain.agents import create_agent\n  from langchain.tools import tool\n  from langchain.messages import HumanMessage, AIMessage, ToolMessage\n  from agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n  @tool\n  def get_weather(city: str):\n      \"\"\"Get weather information for a city.\"\"\"\n      return f\"It's 75 degrees and sunny in {city}.\"\n\n  agent = create_agent(\"gpt-4o\", tools=[get_weather])\n\n  evaluator = create_trajectory_match_evaluator(  # [!code highlight]\n      trajectory_match_mode=\"strict\",  # [!code highlight]\n  )  # [!code highlight]\n\n  def test_weather_tool_called_strict():\n      result = agent.invoke({\n          \"messages\": [HumanMessage(content=\"What's the weather in San Francisco?\")]\n      })\n\n      reference_trajectory = [\n          HumanMessage(content=\"What's the weather in San Francisco?\"),\n          AIMessage(content=\"\", tool_calls=[\n              {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"San Francisco\"}}\n          ]),\n          ToolMessage(content=\"It's 75 degrees and sunny in San Francisco.\", tool_call_id=\"call_1\"),\n          AIMessage(content=\"The weather in San Francisco is 75 degrees and sunny.\"),\n      ]\n\n      evaluation = evaluator(\n          outputs=result[\"messages\"],\n          reference_outputs=reference_trajectory\n      )\n      # {\n      #     'key': 'trajectory_strict_match',\n      #     'score': True,\n      #     'comment': None,\n      # }\n      assert evaluation[\"score\"] is True\n  ```\n\n  ```ts TypeScript theme={null}\n  import { createAgent, tool, HumanMessage, AIMessage, ToolMessage } from \"langchain\"\n  import { createTrajectoryMatchEvaluator } from \"agentevals\";\n  import * as z from \"zod\";\n\n  const getWeather = tool(\n    async ({ city }: { city: string }) => {\n      return `It's 75 degrees and sunny in ${city}.`;\n    },\n    {\n      name: \"get_weather\",\n      description: \"Get weather information for a city.\",\n      schema: z.object({\n        city: z.string(),\n      }),\n    }\n  );\n\n  const agent = createAgent({\n    model: \"gpt-4o\",\n    tools: [getWeather]\n  });\n\n  const evaluator = createTrajectoryMatchEvaluator({  // [!code highlight]\n    trajectoryMatchMode: \"strict\",  // [!code highlight]\n  });  // [!code highlight]\n\n  async function testWeatherToolCalledStrict() {\n    const result = await agent.invoke({\n      messages: [new HumanMessage(\"What's the weather in San Francisco?\")]\n    });\n\n    const referenceTrajectory = [\n      new HumanMessage(\"What's the weather in San Francisco?\"),\n      new AIMessage({\n        content: \"\",\n        tool_calls: [\n          { id: \"call_1\", name: \"get_weather\", args: { city: \"San Francisco\" } }\n        ]\n      }),\n      new ToolMessage({\n        content: \"It's 75 degrees and sunny in San Francisco.\",\n        tool_call_id: \"call_1\"\n      }),\n      new AIMessage(\"The weather in San Francisco is 75 degrees and sunny.\"),\n    ];\n\n    const evaluation = await evaluator({\n      outputs: result.messages,\n      referenceOutputs: referenceTrajectory\n    });\n    // {\n    //     'key': 'trajectory_strict_match',\n    //     'score': true,\n    //     'comment': null,\n    // }\n    expect(evaluation.score).toBe(true);\n  }\n  ```\n</CodeGroup>\n\n### Unordered match\n\nThe `unordered` mode allows the same tool calls in any order, which is helpful when you want to verify that the correct set of tools are being invoked but don't care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn't matter.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain.agents import create_agent\n  from langchain.tools import tool\n  from langchain.messages import HumanMessage, AIMessage, ToolMessage\n  from agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n  @tool\n  def get_weather(city: str):\n      \"\"\"Get weather information for a city.\"\"\"\n      return f\"It's 75 degrees and sunny in {city}.\"\n\n  @tool\n  def get_events(city: str):\n      \"\"\"Get events happening in a city.\"\"\"\n      return f\"Concert at the park in {city} tonight.\"\n\n  agent = create_agent(\"gpt-4o\", tools=[get_weather, get_events])\n\n  evaluator = create_trajectory_match_evaluator(  # [!code highlight]\n      trajectory_match_mode=\"unordered\",  # [!code highlight]\n  )  # [!code highlight]\n\n  def test_multiple_tools_any_order():\n      result = agent.invoke({\n          \"messages\": [HumanMessage(content=\"What's happening in SF today?\")]\n      })\n\n      # Reference shows tools called in different order than actual execution\n      reference_trajectory = [\n          HumanMessage(content=\"What's happening in SF today?\"),\n          AIMessage(content=\"\", tool_calls=[\n              {\"id\": \"call_1\", \"name\": \"get_events\", \"args\": {\"city\": \"SF\"}},\n              {\"id\": \"call_2\", \"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}},\n          ]),\n          ToolMessage(content=\"Concert at the park in SF tonight.\", tool_call_id=\"call_1\"),\n          ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool_call_id=\"call_2\"),\n          AIMessage(content=\"Today in SF: 75 degrees and sunny with a concert at the park tonight.\"),\n      ]\n\n      evaluation = evaluator(\n          outputs=result[\"messages\"],\n          reference_outputs=reference_trajectory,\n      )\n      # {\n      #     'key': 'trajectory_unordered_match',\n      #     'score': True,\n      # }\n      assert evaluation[\"score\"] is True\n  ```\n\n  ```ts TypeScript theme={null}\n  import { createAgent, tool, HumanMessage, AIMessage, ToolMessage } from \"langchain\"\n  import { createTrajectoryMatchEvaluator } from \"agentevals\";\n  import * as z from \"zod\";\n\n  const getWeather = tool(\n    async ({ city }: { city: string }) => {\n      return `It's 75 degrees and sunny in ${city}.`;\n    },\n    {\n      name: \"get_weather\",\n      description: \"Get weather information for a city.\",\n      schema: z.object({ city: z.string() }),\n    }\n  );\n\n  const getEvents = tool(\n    async ({ city }: { city: string }) => {\n      return `Concert at the park in ${city} tonight.`;\n    },\n    {\n      name: \"get_events\",\n      description: \"Get events happening in a city.\",\n      schema: z.object({ city: z.string() }),\n    }\n  );\n\n  const agent = createAgent({\n    model: \"gpt-4o\",\n    tools: [getWeather, getEvents]\n  });\n\n  const evaluator = createTrajectoryMatchEvaluator({  // [!code highlight]\n    trajectoryMatchMode: \"unordered\",  // [!code highlight]\n  });  // [!code highlight]\n\n  async function testMultipleToolsAnyOrder() {\n    const result = await agent.invoke({\n      messages: [new HumanMessage(\"What's happening in SF today?\")]\n    });\n\n    // Reference shows tools called in different order than actual execution\n    const referenceTrajectory = [\n      new HumanMessage(\"What's happening in SF today?\"),\n      new AIMessage({\n        content: \"\",\n        tool_calls: [\n          { id: \"call_1\", name: \"get_events\", args: { city: \"SF\" } },\n          { id: \"call_2\", name: \"get_weather\", args: { city: \"SF\" } },\n        ]\n      }),\n      new ToolMessage({\n        content: \"Concert at the park in SF tonight.\",\n        tool_call_id: \"call_1\"\n      }),\n      new ToolMessage({\n        content: \"It's 75 degrees and sunny in SF.\",\n        tool_call_id: \"call_2\"\n      }),\n      new AIMessage(\"Today in SF: 75 degrees and sunny with a concert at the park tonight.\"),\n    ];\n\n    const evaluation = await evaluator({\n      outputs: result.messages,\n      referenceOutputs: referenceTrajectory,\n    });\n    // {\n    //     'key': 'trajectory_unordered_match',\n    //     'score': true,\n    // }\n    expect(evaluation.score).toBe(true);\n  }\n  ```\n</CodeGroup>\n\n### Subset and superset match\n\nThe `superset` and `subset` modes focus on which tools are called rather than the order of tool calls, allowing you to control how strictly the agent's tool calls must align with the reference.\n\n* Use `superset` mode when you want to verify that a few key tools are called in the execution, but you're okay with the agent calling additional tools. The agent's trajectory must include at least all the tool calls in the reference trajectory, and may include additional tool calls beyond the reference.\n* Use `subset` mode to ensure agent efficiency by verifying that the agent did not call any irrelevant or unnecessary tools beyond those in the reference. The agent's trajectory must include only tool calls that appear in the reference trajectory.\n\nThe following example demonstrates `superset` mode, where the reference trajectory only requires the `get_weather` tool, but the agent can call additional tools:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain.agents import create_agent\n  from langchain.tools import tool\n  from langchain.messages import HumanMessage, AIMessage, ToolMessage\n  from agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n  @tool\n  def get_weather(city: str):\n      \"\"\"Get weather information for a city.\"\"\"\n      return f\"It's 75 degrees and sunny in {city}.\"\n\n  @tool\n  def get_detailed_forecast(city: str):\n      \"\"\"Get detailed weather forecast for a city.\"\"\"\n      return f\"Detailed forecast for {city}: sunny all week.\"\n\n  agent = create_agent(\"gpt-4o\", tools=[get_weather, get_detailed_forecast])\n\n  evaluator = create_trajectory_match_evaluator(  # [!code highlight]\n      trajectory_match_mode=\"superset\",  # [!code highlight]\n  )  # [!code highlight]\n\n  def test_agent_calls_required_tools_plus_extra():\n      result = agent.invoke({\n          \"messages\": [HumanMessage(content=\"What's the weather in Boston?\")]\n      })\n\n      # Reference only requires get_weather, but agent may call additional tools\n      reference_trajectory = [\n          HumanMessage(content=\"What's the weather in Boston?\"),\n          AIMessage(content=\"\", tool_calls=[\n              {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"Boston\"}},\n          ]),\n          ToolMessage(content=\"It's 75 degrees and sunny in Boston.\", tool_call_id=\"call_1\"),\n          AIMessage(content=\"The weather in Boston is 75 degrees and sunny.\"),\n      ]\n\n      evaluation = evaluator(\n          outputs=result[\"messages\"],\n          reference_outputs=reference_trajectory,\n      )\n      # {\n      #     'key': 'trajectory_superset_match',\n      #     'score': True,\n      #     'comment': None,\n      # }\n      assert evaluation[\"score\"] is True\n  ```\n\n  ```ts TypeScript theme={null}\n  import { createAgent } from \"langchain\"\n  import { tool } from \"@langchain/core/tools\";\n  import { HumanMessage, AIMessage, ToolMessage } from \"@langchain/core/messages\";\n  import { createTrajectoryMatchEvaluator } from \"agentevals\";\n  import * as z from \"zod\";\n\n  const getWeather = tool(\n    async ({ city }: { city: string }) => {\n      return `It's 75 degrees and sunny in ${city}.`;\n    },\n    {\n      name: \"get_weather\",\n      description: \"Get weather information for a city.\",\n      schema: z.object({ city: z.string() }),\n    }\n  );\n\n  const getDetailedForecast = tool(\n    async ({ city }: { city: string }) => {\n      return `Detailed forecast for ${city}: sunny all week.`;\n    },\n    {\n      name: \"get_detailed_forecast\",\n      description: \"Get detailed weather forecast for a city.\",\n      schema: z.object({ city: z.string() }),\n    }\n  );\n\n  const agent = createAgent({\n    model: \"gpt-4o\",\n    tools: [getWeather, getDetailedForecast]\n  });\n\n  const evaluator = createTrajectoryMatchEvaluator({  // [!code highlight]\n    trajectoryMatchMode: \"superset\",  // [!code highlight]\n  });  // [!code highlight]\n\n  async function testAgentCallsRequiredToolsPlusExtra() {\n    const result = await agent.invoke({\n      messages: [new HumanMessage(\"What's the weather in Boston?\")]\n    });\n\n    // Reference only requires getWeather, but agent may call additional tools\n    const referenceTrajectory = [\n      new HumanMessage(\"What's the weather in Boston?\"),\n      new AIMessage({\n        content: \"\",\n        tool_calls: [\n          { id: \"call_1\", name: \"get_weather\", args: { city: \"Boston\" } },\n        ]\n      }),\n      new ToolMessage({\n        content: \"It's 75 degrees and sunny in Boston.\",\n        tool_call_id: \"call_1\"\n      }),\n      new AIMessage(\"The weather in Boston is 75 degrees and sunny.\"),\n    ];\n\n    const evaluation = await evaluator({\n      outputs: result.messages,\n      referenceOutputs: referenceTrajectory,\n    });\n    // {\n    //     'key': 'trajectory_superset_match',\n    //     'score': true,\n    //     'comment': null,\n    // }\n    expect(evaluation.score).toBe(true);\n  }\n  ```\n</CodeGroup>\n\n<Info>\n  You can also customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference by setting the `tool_args_match_mode` (Python) or `toolArgsMatchMode` (TypeScript) property, as well as the `tool_args_match_overrides` (Python) or `toolArgsMatchOverrides` (TypeScript) property. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#tool-args-match-modes) for more details.\n</Info>\n\n## LLM-as-judge evaluator\n\n<Note>\n  This section covers the trajectory-specific LLM-as-a-judge evaluator from the `agentevals` package. For general-purpose LLM-as-a-judge evaluators in LangSmith, refer to the [LLM-as-a-judge evaluator](/langsmith/llm-as-judge).\n</Note>\n\nYou can also use an LLM to evaluate the agent's execution path. Unlike the trajectory match evaluators, it doesn't require a reference trajectory, but one can be provided if available.\n\n### Without reference trajectory\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain.agents import create_agent\n  from langchain.tools import tool\n  from langchain.messages import HumanMessage, AIMessage, ToolMessage\n  from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\n\n  @tool\n  def get_weather(city: str):\n      \"\"\"Get weather information for a city.\"\"\"\n      return f\"It's 75 degrees and sunny in {city}.\"\n\n  agent = create_agent(\"gpt-4o\", tools=[get_weather])\n\n  evaluator = create_trajectory_llm_as_judge(  # [!code highlight]\n      model=\"openai:o3-mini\",  # [!code highlight]\n      prompt=TRAJECTORY_ACCURACY_PROMPT,  # [!code highlight]\n  )  # [!code highlight]\n\n  def test_trajectory_quality():\n      result = agent.invoke({\n          \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")]\n      })\n\n      evaluation = evaluator(\n          outputs=result[\"messages\"],\n      )\n      # {\n      #     'key': 'trajectory_accuracy',\n      #     'score': True,\n      #     'comment': 'The provided agent trajectory is reasonable...'\n      # }\n      assert evaluation[\"score\"] is True\n  ```\n\n  ```ts TypeScript theme={null}\n  import { createAgent } from \"langchain\"\n  import { tool } from \"@langchain/core/tools\";\n  import { HumanMessage, AIMessage, ToolMessage } from \"@langchain/core/messages\";\n  import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from \"agentevals\";\n  import * as z from \"zod\";\n\n  const getWeather = tool(\n    async ({ city }: { city: string }) => {\n      return `It's 75 degrees and sunny in ${city}.`;\n    },\n    {\n      name: \"get_weather\",\n      description: \"Get weather information for a city.\",\n      schema: z.object({ city: z.string() }),\n    }\n  );\n\n  const agent = createAgent({\n    model: \"gpt-4o\",\n    tools: [getWeather]\n  });\n\n  const evaluator = createTrajectoryLLMAsJudge({  // [!code highlight]\n    model: \"openai:o3-mini\",  // [!code highlight]\n    prompt: TRAJECTORY_ACCURACY_PROMPT,  // [!code highlight]\n  });  // [!code highlight]\n\n  async function testTrajectoryQuality() {\n    const result = await agent.invoke({\n      messages: [new HumanMessage(\"What's the weather in Seattle?\")]\n    });\n\n    const evaluation = await evaluator({\n      outputs: result.messages,\n    });\n    // {\n    //     'key': 'trajectory_accuracy',\n    //     'score': true,\n    //     'comment': 'The provided agent trajectory is reasonable...'\n    // }\n    expect(evaluation.score).toBe(true);\n  }\n  ```\n</CodeGroup>\n\n### With reference trajectory\n\nIf you have a reference trajectory, you can add an extra variable to your prompt and pass in the reference trajectory. Below, we use the prebuilt `TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE` prompt and configure the `reference_outputs` variable:\n\n<CodeGroup>\n  ```python Python theme={null}\n  evaluator = create_trajectory_llm_as_judge(\n      model=\"openai:o3-mini\",\n      prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n  )\n  evaluation = judge_with_reference(\n      outputs=result[\"messages\"],\n      reference_outputs=reference_trajectory,\n  )\n  ```\n\n  ```ts TypeScript theme={null}\n  import { TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE } from \"agentevals\";\n\n  const evaluator = createTrajectoryLLMAsJudge({\n    model: \"openai:o3-mini\",\n    prompt: TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n  });\n\n  const evaluation = await evaluator({\n    outputs: result.messages,\n    referenceOutputs: referenceTrajectory,\n  });\n  ```\n</CodeGroup>\n\n<Info>\n  For more configurability over how the LLM evaluates the trajectory, visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#trajectory-llm-as-judge).\n</Info>\n\n## Async support (Python)\n\nAll `agentevals` evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding `async` after `create_` in the function name.\n\nHere's an example using the async judge and evaluator:\n\n```python  theme={null}\nfrom agentevals.trajectory.llm import create_async_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\nfrom agentevals.trajectory.match import create_async_trajectory_match_evaluator\n\nasync_judge = create_async_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT,\n)\n\nasync_evaluator = create_async_trajectory_match_evaluator(\n    trajectory_match_mode=\"strict\",\n)\n\nasync def test_async_evaluation():\n    result = await agent.ainvoke({\n        \"messages\": [HumanMessage(content=\"What's the weather?\")]\n    })\n\n    evaluation = await async_judge(outputs=result[\"messages\"])\n    assert evaluation[\"score\"] is True\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trajectory-evals.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 22259
}