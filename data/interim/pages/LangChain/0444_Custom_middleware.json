{
  "title": "Custom middleware",
  "source_url": "https://docs.langchain.com/oss/python/langchain/middleware/custom",
  "content": "Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\n\n## Hooks\n\nMiddleware provides two styles of hooks to intercept agent execution:\n\n<CardGroup cols={2}>\n  <Card title=\"Node-style hooks\" icon=\"share-nodes\" href=\"#node-style-hooks\">\n    Run sequentially at specific execution points.\n  </Card>\n\n  <Card title=\"Wrap-style hooks\" icon=\"container-storage\" href=\"#wrap-style-hooks\">\n    Run around each model or tool call.\n  </Card>\n</CardGroup>\n\n### Node-style hooks\n\nRun sequentially at specific execution points. Use for logging, validation, and state updates.\n\n**Available hooks:**\n\n* `before_agent` - Before agent starts (once per invocation)\n* `before_model` - Before each model call\n* `after_model` - After each model response\n* `after_agent` - After agent completes (once per invocation)\n\n**Example:**\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import before_model, after_model, AgentState\n    from langchain.messages import AIMessage\n    from langgraph.runtime import Runtime\n    from typing import Any\n\n\n    @before_model(can_jump_to=[\"end\"])\n    def check_message_limit(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        if len(state[\"messages\"]) >= 50:\n            return {\n                \"messages\": [AIMessage(\"Conversation limit reached.\")],\n                \"jump_to\": \"end\"\n            }\n        return None\n\n    @after_model\n    def log_response(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"Model returned: {state['messages'][-1].content}\")\n        return None\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\n    from langchain.messages import AIMessage\n    from langgraph.runtime import Runtime\n    from typing import Any\n\n    class MessageLimitMiddleware(AgentMiddleware):\n        def __init__(self, max_messages: int = 50):\n            super().__init__()\n            self.max_messages = max_messages\n\n        @hook_config(can_jump_to=[\"end\"])\n        def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n            if len(state[\"messages\"]) == self.max_messages:\n                return {\n                    \"messages\": [AIMessage(\"Conversation limit reached.\")],\n                    \"jump_to\": \"end\"\n                }\n            return None\n\n        def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n            print(f\"Model returned: {state['messages'][-1].content}\")\n            return None\n    ```\n  </Tab>\n</Tabs>\n\n### Wrap-style hooks\n\nIntercept execution and control when the handler is called. Use for retries, caching, and transformation.\n\nYou decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\n\n**Available hooks:**\n\n* `wrap_model_call` - Around each model call\n* `wrap_tool_call` - Around each tool call\n\n**Example:**\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n\n    @wrap_model_call\n    def retry_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        for attempt in range(3):\n            try:\n                return handler(request)\n            except Exception as e:\n                if attempt == 2:\n                    raise\n                print(f\"Retry {attempt + 1}/3 after error: {e}\")\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from typing import Callable\n\n    class RetryMiddleware(AgentMiddleware):\n        def __init__(self, max_retries: int = 3):\n            super().__init__()\n            self.max_retries = max_retries\n\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            for attempt in range(self.max_retries):\n                try:\n                    return handler(request)\n                except Exception as e:\n                    if attempt == self.max_retries - 1:\n                        raise\n                    print(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}\")\n    ```\n  </Tab>\n</Tabs>\n\n## Create middleware\n\nYou can create middleware in two ways:\n\n<CardGroup cols={2}>\n  <Card title=\"Decorator-based middleware\" icon=\"at\" href=\"#decorator-based-middleware\">\n    Quick and simple for single-hook middleware. Use decorators to wrap individual functions.\n  </Card>\n\n  <Card title=\"Class-based middleware\" icon=\"brackets-curly\" href=\"#class-based-middleware\">\n    More powerful for complex middleware with multiple hooks or configuration.\n  </Card>\n</CardGroup>\n\n### Decorator-based middleware\n\nQuick and simple for single-hook middleware. Use decorators to wrap individual functions.\n\n**Available decorators:**\n\n**Node-style:**\n\n* [`@before_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_agent) - Runs before agent starts (once per invocation)\n* [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) - Runs before each model call\n* [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model) - Runs after each model response\n* [`@after_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_agent) - Runs after agent completes (once per invocation)\n\n**Wrap-style:**\n\n* [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) - Wraps each model call with custom logic\n* [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call) - Wraps each tool call with custom logic\n\n**Convenience:**\n\n* [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) - Generates dynamic system prompts\n\n**Example:**\n\n```python  theme={null}\nfrom langchain.agents.middleware import before_model, wrap_model_call\nfrom langchain.agents.middleware import AgentState, ModelRequest, ModelResponse\nfrom langchain.agents import create_agent\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\n\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    print(f\"About to call model with {len(state['messages'])} messages\")\n    return None\n\n@wrap_model_call\ndef retry_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    for attempt in range(3):\n        try:\n            return handler(request)\n        except Exception as e:\n            if attempt == 2:\n                raise\n            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    middleware=[log_before_model, retry_model],\n    tools=[...],\n)\n```\n\n**When to use decorators:**\n\n* Single hook needed\n* No complex configuration\n* Quick prototyping\n\n### Class-based middleware\n\nMore powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.\n\n**Example:**\n\n```python  theme={null}\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, ModelRequest, ModelResponse\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\nclass LoggingMiddleware(AgentMiddleware):\n    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"About to call model with {len(state['messages'])} messages\")\n        return None\n\n    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"Model returned: {state['messages'][-1].content}\")\n        return None\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    middleware=[LoggingMiddleware()],\n    tools=[...],\n)\n```\n\n**When to use classes:**\n\n* Defining both sync and async implementations for the same hook\n* Multiple hooks needed in a single middleware\n* Complex configuration required (e.g., configurable thresholds, custom models)\n* Reuse across projects with init-time configuration\n\n## Custom state schema\n\nMiddleware can extend the agent's state with custom properties.\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.messages import HumanMessage\n    from langchain.agents.middleware import AgentState, before_model, after_model\n    from typing_extensions import NotRequired\n    from typing import Any\n    from langgraph.runtime import Runtime\n\n\n    class CustomState(AgentState):\n        model_call_count: NotRequired[int]\n        user_id: NotRequired[str]\n\n\n    @before_model(state_schema=CustomState, can_jump_to=[\"end\"])\n    def check_call_limit(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n        count = state.get(\"model_call_count\", 0)\n        if count > 10:\n            return {\"jump_to\": \"end\"}\n        return None\n\n\n    @after_model(state_schema=CustomState)\n    def increment_counter(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        middleware=[check_call_limit, increment_counter],\n        tools=[],\n    )\n\n    # Invoke with custom state\n    result = agent.invoke({\n        \"messages\": [HumanMessage(\"Hello\")],\n        \"model_call_count\": 0,\n        \"user_id\": \"user-123\",\n    })\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.messages import HumanMessage\n    from langchain.agents.middleware import AgentState, AgentMiddleware\n    from typing_extensions import NotRequired\n    from typing import Any\n\n\n    class CustomState(AgentState):\n        model_call_count: NotRequired[int]\n        user_id: NotRequired[str]\n\n\n    class CallCounterMiddleware(AgentMiddleware[CustomState]):\n        state_schema = CustomState\n\n        def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n            count = state.get(\"model_call_count\", 0)\n            if count > 10:\n                return {\"jump_to\": \"end\"}\n            return None\n\n        def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n            return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        middleware=[CallCounterMiddleware()],\n        tools=[],\n    )\n\n    # Invoke with custom state\n    result = agent.invoke({\n        \"messages\": [HumanMessage(\"Hello\")],\n        \"model_call_count\": 0,\n        \"user_id\": \"user-123\",\n    })\n    ```\n  </Tab>\n</Tabs>\n\n## Execution order\n\nWhen using multiple middleware, understand how they execute:\n\n```python  theme={null}\nagent = create_agent(\n    model=\"gpt-4o\",\n    middleware=[middleware1, middleware2, middleware3],\n    tools=[...],\n)\n```\n\n<Accordion title=\"Execution flow\">\n  **Before hooks run in order:**\n\n  1. `middleware1.before_agent()`\n  2. `middleware2.before_agent()`\n  3. `middleware3.before_agent()`\n\n  **Agent loop starts**\n\n  4. `middleware1.before_model()`\n  5. `middleware2.before_model()`\n  6. `middleware3.before_model()`\n\n  **Wrap hooks nest like function calls:**\n\n  7. `middleware1.wrap_model_call()` → `middleware2.wrap_model_call()` → `middleware3.wrap_model_call()` → model\n\n  **After hooks run in reverse order:**\n\n  8. `middleware3.after_model()`\n  9. `middleware2.after_model()`\n  10. `middleware1.after_model()`\n\n  **Agent loop ends**\n\n  11. `middleware3.after_agent()`\n  12. `middleware2.after_agent()`\n  13. `middleware1.after_agent()`\n</Accordion>\n\n**Key rules:**\n\n* `before_*` hooks: First to last\n* `after_*` hooks: Last to first (reverse)\n* `wrap_*` hooks: Nested (first middleware wraps all others)\n\n## Agent jumps\n\nTo exit early from middleware, return a dictionary with `jump_to`:\n\n**Available jump targets:**\n\n* `'end'`: Jump to the end of the agent execution (or the first `after_agent` hook)\n* `'tools'`: Jump to the tools node\n* `'model'`: Jump to the model node (or the first `before_model` hook)\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import after_model, hook_config, AgentState\n    from langchain.messages import AIMessage\n    from langgraph.runtime import Runtime\n    from typing import Any\n\n\n    @after_model\n    @hook_config(can_jump_to=[\"end\"])\n    def check_for_blocked(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        last_message = state[\"messages\"][-1]\n        if \"BLOCKED\" in last_message.content:\n            return {\n                \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n                \"jump_to\": \"end\"\n            }\n        return None\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, hook_config, AgentState\n    from langchain.messages import AIMessage\n    from langgraph.runtime import Runtime\n    from typing import Any\n\n    class BlockedContentMiddleware(AgentMiddleware):\n        @hook_config(can_jump_to=[\"end\"])\n        def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n            last_message = state[\"messages\"][-1]\n            if \"BLOCKED\" in last_message.content:\n                return {\n                    \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n                    \"jump_to\": \"end\"\n                }\n            return None\n    ```\n  </Tab>\n</Tabs>\n\n## Best practices\n\n1. Keep middleware focused - each should do one thing well\n2. Handle errors gracefully - don't let middleware errors crash the agent\n3. **Use appropriate hook types**:\n   * Node-style for sequential logic (logging, validation)\n   * Wrap-style for control flow (retry, fallback, caching)\n4. Clearly document any custom state properties\n5. Unit test middleware independently before integrating\n6. Consider execution order - place critical middleware first in the list\n7. Use built-in middleware when possible\n\n## Examples\n\n### Dynamic model selection\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n\n\n    complex_model = init_chat_model(\"gpt-4o\")\n    simple_model = init_chat_model(\"gpt-4o-mini\")\n\n    @wrap_model_call\n    def dynamic_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Use different model based on conversation length\n       if len(request.messages) > 10:\n           model = complex_model\n        else:\n           model = simple_model\n        return handler(request.override(model=model))\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n\n    complex_model = init_chat_model(\"gpt-4o\")\n    simple_model = init_chat_model(\"gpt-4o-mini\")\n\n    class DynamicModelMiddleware(AgentMiddleware):\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            # Use different model based on conversation length\n            if len(request.messages) > 10:\n               model = complex_model\n            else:\n               model = simple_model\n            return handler(request.override(model=model))\n    ```\n  </Tab>\n</Tabs>\n\n### Tool call monitoring\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_tool_call\n    from langchain.tools.tool_node import ToolCallRequest\n    from langchain.messages import ToolMessage\n    from langgraph.types import Command\n    from typing import Callable\n\n\n    @wrap_tool_call\n    def monitor_tool(\n        request: ToolCallRequest,\n        handler: Callable[[ToolCallRequest], ToolMessage | Command],\n    ) -> ToolMessage | Command:\n        print(f\"Executing tool: {request.tool_call['name']}\")\n        print(f\"Arguments: {request.tool_call['args']}\")\n        try:\n            result = handler(request)\n            print(f\"Tool completed successfully\")\n            return result\n        except Exception as e:\n            print(f\"Tool failed: {e}\")\n            raise\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.tools.tool_node import ToolCallRequest\n    from langchain.agents.middleware import AgentMiddleware\n    from langchain.messages import ToolMessage\n    from langgraph.types import Command\n    from typing import Callable\n\n    class ToolMonitoringMiddleware(AgentMiddleware):\n        def wrap_tool_call(\n            self,\n            request: ToolCallRequest,\n            handler: Callable[[ToolCallRequest], ToolMessage | Command],\n        ) -> ToolMessage | Command:\n            print(f\"Executing tool: {request.tool_call['name']}\")\n            print(f\"Arguments: {request.tool_call['args']}\")\n            try:\n                result = handler(request)\n                print(f\"Tool completed successfully\")\n                return result\n            except Exception as e:\n                print(f\"Tool failed: {e}\")\n                raise\n    ```\n  </Tab>\n</Tabs>\n\n### Dynamically selecting tools\n\nSelect relevant tools at runtime to improve performance and accuracy.\n\n**Benefits:**\n\n* **Shorter prompts** - Reduce complexity by exposing only relevant tools\n* **Better accuracy** - Models choose correctly from fewer options\n* **Permission control** - Dynamically filter tools based on user access\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n\n    @wrap_model_call\n    def select_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n        # Select a small, relevant subset of tools based on state/context\n        relevant_tools = select_relevant_tools(request.state, request.runtime)\n        return handler(request.override(tools=relevant_tools))\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=all_tools,  # All available tools need to be registered upfront\n        middleware=[select_tools],\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from typing import Callable\n\n\n    class ToolSelectorMiddleware(AgentMiddleware):\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n            # Select a small, relevant subset of tools based on state/context\n            relevant_tools = select_relevant_tools(request.state, request.runtime)\n            return handler(request.override(tools=relevant_tools))\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=all_tools,  # All available tools need to be registered upfront\n        middleware=[ToolSelectorMiddleware()],\n    )\n    ```\n  </Tab>\n</Tabs>\n\n### Working with system messages\n\nModify system messages in middleware using the `system_message` field on `ModelRequest`. The `system_message` field contains a [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) object (even if the agent was created with a string `system_prompt`).\n\n**Example: Adding context to system message**\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.messages import SystemMessage\n    from typing import Callable\n\n\n    @wrap_model_call\n    def add_context(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Always work with content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\"type\": \"text\", \"text\": \"Additional context.\"}\n        ]\n        new_system_message = SystemMessage(content=new_content)\n        return handler(request.override(system_message=new_system_message))\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from langchain.messages import SystemMessage\n    from typing import Callable\n\n\n    class ContextMiddleware(AgentMiddleware):\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            # Always work with content blocks\n            new_content = list(request.system_message.content_blocks) + [\n                {\"type\": \"text\", \"text\": \"Additional context.\"}\n            ]\n            new_system_message = SystemMessage(content=new_content)\n            return handler(request.override(system_message=new_system_message))\n    ```\n  </Tab>\n</Tabs>\n\n**Example: Working with cache control (Anthropic)**\n\nWhen working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:\n\n<Tabs>\n  <Tab title=\"Decorator\">\n    ```python  theme={null}\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.messages import SystemMessage\n    from typing import Callable\n\n\n    @wrap_model_call\n    def add_cached_context(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Always work with content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\n                \"type\": \"text\",\n                \"text\": \"Here is a large document to analyze:\\n\\n<document>...</document>\",\n                # content up until this point is cached\n                \"cache_control\": {\"type\": \"ephemeral\"}\n            }\n        ]\n\n        new_system_message = SystemMessage(content=new_content)\n        return handler(request.override(system_message=new_system_message))\n    ```\n  </Tab>\n\n  <Tab title=\"Class\">\n    ```python  theme={null}\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n    from langchain.messages import SystemMessage\n    from typing import Callable\n\n\n    class CachedContextMiddleware(AgentMiddleware):\n        def wrap_model_call(\n            self,\n            request: ModelRequest,\n            handler: Callable[[ModelRequest], ModelResponse],\n        ) -> ModelResponse:\n            # Always work with content blocks\n            new_content = list(request.system_message.content_blocks) + [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Here is a large document to analyze:\\n\\n<document>...</document>\",\n                    \"cache_control\": {\"type\": \"ephemeral\"}  # This content will be cached\n                }\n            ]\n\n            new_system_message = SystemMessage(content=new_content)\n            return handler(request.override(system_message=new_system_message))\n    ```\n  </Tab>\n</Tabs>\n\n**Notes:**\n\n* `ModelRequest.system_message` is always a [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) object, even if the agent was created with `system_prompt=\"string\"`\n* Use `SystemMessage.content_blocks` to access content as a list of blocks, regardless of whether the original content was a string or list\n* When modifying system messages, use `content_blocks` and append new blocks to preserve existing structure\n* You can pass [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) objects directly to `create_agent`'s `system_prompt` parameter for advanced use cases like cache control\n\n## Additional resources\n\n* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/)\n* [Built-in middleware](/oss/python/langchain/middleware/built-in)\n* [Testing agents](/oss/python/langchain/test)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/custom.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 25564
}