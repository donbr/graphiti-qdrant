{
  "title": "Beta LangSmith Collector-Proxy",
  "source_url": "https://docs.langchain.com/langsmith/collector-proxy",
  "content": "<Note>\n  This is a beta feature. The API may change in future releases.\n</Note>\n\nThe LangSmith Collector-Proxy is a lightweight, high-performance proxy server that sits between your application and the LangSmith backend. It batches and compresses trace data before sending it to LangSmith, reducing network overhead and improving performance.\n\n## When to Use the Collector-Proxy\n\nThe Collector-Proxy is particularly valuable when:\n\n* You're running multiple instances of your application in parallel and need to efficiently aggregate traces\n* You want more efficient tracing than direct OTEL API calls to LangSmith (the collector optimizes batching and compression)\n* You're using a language that doesn't have a native LangSmith SDK\n\n## Key Features\n\n* **Efficient Data Transfer** Batches multiple spans into fewer, larger uploads.\n* **Compression** Uses zstd to minimize payload size.\n* **OTLP Support** Accepts OTLP JSON and Protobuf over HTTP POST.\n* **Semantic Translation** Maps GenAI/OpenInference conventions to the LangSmith Run model.\n* **Flexible Batching** Flush by span count or time interval.\n\n## Configuration\n\nConfigure via environment variables:\n\n| Variable             | Description                       | Default                           |\n| -------------------- | --------------------------------- | --------------------------------- |\n| `HTTP_PORT`          | Port to run the proxy server      | `4318`                            |\n| `LANGSMITH_ENDPOINT` | LangSmith backend URL             | `https://api.smith.langchain.com` |\n| `LANGSMITH_API_KEY`  | API key for LangSmith             | **Required** (env var or header)  |\n| `LANGSMITH_PROJECT`  | Default tracing project           | Default project if not specified  |\n| `BATCH_SIZE`         | Spans per upload batch            | `100`                             |\n| `FLUSH_INTERVAL_MS`  | Flush interval in milliseconds    | `1000`                            |\n| `MAX_BUFFER_BYTES`   | Max uncompressed buffer size      | `10485760` (10 MB)                |\n| `MAX_BODY_BYTES`     | Max incoming request body size    | `209715200` (200 MB)              |\n| `MAX_RETRIES`        | Retry attempts for failed uploads | `3`                               |\n| `RETRY_BACKOFF_MS`   | Initial backoff in milliseconds   | `100`                             |\n\n### Project Configuration\n\nThe Collector-Proxy supports LangSmith project configuration with the following priority:\n\n1. If a project is specified in the request headers (`Langsmith-Project`), that project will be used\n2. If no project is specified in headers, it will use the project set in the `LANGSMITH_PROJECT` environment variable\n3. If neither is set, it will trace to the `default` project.\n\n### Authentication\n\nThe API key can be provided either:\n\n* As an environment variable (`LANGSMITH_API_KEY`)\n* In the request headers (`X-API-Key`)\n\n## Deployment (Docker)\n\nYou can deploy the Collector-Proxy with Docker:\n\n1. **Build the image**\n\n   ```bash  theme={null}\n   docker build \\\n     -t langsmith-collector-proxy:beta .\n   ```\n\n2. **Run the container**\n\n   ```bash  theme={null}\n   docker run -d \\\n     -p 4318:4318 \\\n     -e LANGSMITH_API_KEY=<your_api_key> \\\n     -e LANGSMITH_PROJECT=<your_project> \\\n     langsmith-collector-proxy:beta\n   ```\n\n## Usage\n\nPoint any OTLP-compatible client or the OpenTelemetry Collector exporter at:\n\n```bash  theme={null}\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://<host>:4318/v1/traces\nexport OTEL_EXPORTER_OTLP_HEADERS=\"X-API-Key=<your_api_key>,Langsmith-Project=<your_project>\"\n```\n\nSend a test trace:\n\n```bash  theme={null}\ncurl -X POST http://localhost:4318/v1/traces \\\n  -H \"Content-Type: application/json\" \\\n  --data '{\n    \"resourceSpans\": [\n      {\n        \"resource\": {\n          \"attributes\": [\n            {\n              \"key\": \"service.name\",\n              \"value\": { \"stringValue\": \"test-service\" }\n            }\n          ]\n        },\n        \"scopeSpans\": [\n          {\n            \"scope\": {\n              \"name\": \"example/instrumentation\",\n              \"version\": \"1.0.0\"\n            },\n            \"spans\": [\n              {\n                \"traceId\": \"T6nh/mMkIONaoHewS9UWIw==\",\n                \"spanId\": \"0tEqJwCpvU0=\",\n                \"name\": \"parent-span\",\n                \"kind\": \"SPAN_KIND_INTERNAL\",\n                \"startTimeUnixNano\": 1747675155185223936,\n                \"endTimeUnixNano\":   1747675156185223936,\n                \"attributes\": [\n                  {\n                    \"key\": \"gen_ai.prompt\",\n                    \"value\": {\n                      \"stringValue\": \"{\\\"text\\\":\\\"Hello, world!\\\"}\"\n                    }\n                  },\n                  {\n                    \"key\": \"gen_ai.usage.input_tokens\",\n                    \"value\": {\n                      \"intValue\": \"5\"\n                    }\n                  },\n                  {\n                    \"key\": \"gen_ai.completion\",\n                    \"value\": {\n                      \"stringValue\": \"{\\\"text\\\":\\\"Hi there!\\\"}\"\n                    }\n                  },\n                  {\n                    \"key\": \"gen_ai.usage.output_tokens\",\n                    \"value\": {\n                      \"intValue\": \"3\"\n                    }\n                  }\n                ],\n                \"droppedAttributesCount\": 0,\n                \"events\": [],\n                \"links\": [],\n                \"status\": {}\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }'\n```\n\n## Health & Scaling\n\n* **Liveness**: `GET /live` → 200\n* **Readiness**: `GET /ready` → 200\n\n## Horizontal Scaling\n\nTo ensure full traces are batched correctly, route spans with the same trace ID to the same instance (e.g., via consistent hashing).\n\n## Fork & Extend\n\nFork the [Collector-Proxy repo on GitHub](https://github.com/langchain-ai/langsmith-collector-proxy) and implement your own converter:\n\n* Create a custom `GenAiConverter` or modify the existing one in `internal/translator/otel_converter.go`\n* Register the custom converter in `internal/translator/translator.go`\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/collector-proxy.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 6400
}