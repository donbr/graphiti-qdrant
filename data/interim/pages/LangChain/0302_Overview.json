{
  "title": "Overview",
  "source_url": "https://docs.langchain.com/oss/javascript/integrations/providers/google",
  "content": "Functionality related to [Google Cloud Platform](https://cloud.google.com/)\nand [AI Studio](https://aistudio.google.com/)\n\n## Chat models\n\n### Gemini Models\n\nAccess Gemini models such as `gemini-2.5-pro` and `gemini-2.0-flex` through the [`ChatGoogleGenerativeAI`](/oss/javascript/integrations/chat/google_generative_ai),\nor if using VertexAI, via the [`ChatVertexAI`](/oss/javascript/integrations/chat/google_vertex_ai) class.\n\n<Tip>\n  See [this section for general instructions on installing LangChain packages](/oss/javascript/langchain/install).\n</Tip>\n\n<Tabs>\n  <Tab title=\"GenAI\">\n    ```bash npm theme={null}\n    npm install @langchain/google-genai @langchain/core\n    ```\n\n    Configure your API key.\n\n    ```\n    export GOOGLE_API_KEY=your-api-key\n    ```\n\n    ```typescript  theme={null}\n    import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\n    const model = new ChatGoogleGenerativeAI({\n      model: \"gemini-pro\",\n      maxOutputTokens: 2048,\n    });\n\n    // Batch and stream are also supported\n    const res = await model.invoke([\n      [\n        \"human\",\n        \"What would be a good company name for a company that makes colorful socks?\",\n      ],\n    ]);\n    ```\n\n    More recent Gemini models support image inputs:\n\n    ```typescript  theme={null}\n    const visionModel = new ChatGoogleGenerativeAI({\n      model: \"gemini-2.5-flash-lite\",\n      maxOutputTokens: 2048,\n    });\n    const image = fs.readFileSync(\"./hotdog.jpg\").toString(\"base64\");\n    const input2 = [\n      new HumanMessage({\n        content: [\n          {\n            type: \"text\",\n            text: \"Describe the following image.\",\n          },\n          {\n            type: \"image_url\",\n            image_url: `data:image/png;base64,${image}`,\n          },\n        ],\n      }),\n    ];\n\n    const res = await visionModel.invoke(input2);\n    ```\n\n    <Tip>\n      **Click [here](/oss/javascript/integrations/chat/google_generative_ai) for the `@langchain/google-genai` specific integration docs**\n    </Tip>\n  </Tab>\n\n  <Tab title=\"VertexAI\">\n    ```bash npm theme={null}\n    npm install @langchain/google-vertexai @langchain/core\n    ```\n\n    Then, you'll need to add your service account credentials, either directly as a `GOOGLE_VERTEX_AI_WEB_CREDENTIALS` environment variable:\n\n    ```\n    GOOGLE_VERTEX_AI_WEB_CREDENTIALS={\"type\":\"service_account\",\"project_id\":\"YOUR_PROJECT-12345\",...}\n    ```\n\n    or as a file path:\n\n    ```\n    GOOGLE_VERTEX_AI_WEB_CREDENTIALS_FILE=/path/to/your/credentials.json\n    ```\n\n    ```typescript  theme={null}\n    import { ChatVertexAI } from \"@langchain/google-vertexai\";\n    // Or, if using the web entrypoint:\n    // import { ChatVertexAI } from \"@langchain/google-vertexai-web\";\n\n    const model = new ChatVertexAI({\n      model: \"gemini-2.5-pro\",\n      maxOutputTokens: 2048,\n    });\n\n    // Batch and stream are also supported\n    const res = await model.invoke([\n      [\n        \"human\",\n        \"What would be a good company name for a company that makes colorful socks?\",\n      ],\n    ]);\n    ```\n\n    Gemini vision models support image inputs when providing a single human message. For example:\n\n    ```typescript  theme={null}\n    const visionModel = new ChatVertexAI({\n      model: \"gemini-pro-vision\",\n      maxOutputTokens: 2048,\n    });\n    const image = fs.readFileSync(\"./hotdog.png\").toString(\"base64\");\n    const input2 = [\n      new HumanMessage({\n        content: [\n          {\n            type: \"text\",\n            text: \"Describe the following image.\",\n          },\n          {\n            type: \"image_url\",\n            image_url: `data:image/png;base64,${image}`,\n          },\n        ],\n      }),\n    ];\n\n    const res = await visionModel.invoke(input2);\n    ```\n\n    <Tip>\n      Click [here](/oss/javascript/integrations/chat/google_vertex_ai) for the `@langchain/google-vertexai` specific integration docs\n    </Tip>\n  </Tab>\n</Tabs>\n\nThe value of `image_url` must be a base64 encoded image (e.g., `data:image/png;base64,abcd124`).\n\n### Gemma\n\nAccess the `gemma-3-27b-it` model through AI Studio using the `ChatGoogle` class.\n(This class is a superclass of the [`ChatVertexAI`](/oss/javascript/integrations/chat/google_vertex_ai)\nclass that works with both Vertex AI and the AI Studio APIs.)\n\n<Tip>\n  **Since Gemma is an open model, it may also be available from other platforms** including [Ollama](/oss/javascript/integrations/chat/ollama/).\n</Tip>\n\n```bash npm theme={null}\nnpm install @langchain/google-gauth @langchain/core\n```\n\nConfigure your API key.\n\n```\nexport GOOGLE_API_KEY=your-api-key\n```\n\n```typescript  theme={null}\nimport { ChatGoogle } from \"@langchain/google-gauth\";\n\nconst model = new ChatGoogle({\n  model: \"gemma-3-27b-it\",\n});\n\nconst res = await model.invoke([\n  {\n    role: \"user\",\n    content:\n      \"What would be a good company name for a company that makes colorful socks?\",\n  },\n]);\n```\n\n### Third Party Models\n\nSee above for setting up authentication through Vertex AI to use these models.\n\n[Anthropic](/oss/javascript/integrations/chat/anthropic) Claude models are also available through\nthe [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude)\nplatform. See [here](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude)\nfor more information about enabling access to the models and the model names to use.\n\nPaLM models are no longer supported.\n\n## Vector Store\n\n### Vertex AI Vector Search\n\n> [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/matching-engine/overview),\n> formerly known as Vertex AI Matching Engine, provides the industry's leading high-scale\n> low latency vector database. These vector databases are commonly\n> referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.\n\n```typescript  theme={null}\nimport { MatchingEngine } from \"@langchain/community/vectorstores/googlevertexai\";\n```\n\n### Postgres Vector Store\n\nThe [PostgresVectorStore](/oss/javascript/integrations/vectorstores/google_cloudsql_pg) module from the\n[`@langchain/google-cloud-sql-pg`](https://www.npmjs.com/package/@langchain/google-cloud-sql-pg) package provides a way to use the CloudSQL for PostgresSQL to store\nvector embeddings using the class.\n\n```bash  theme={null}\n$ yarn add @langchain/google-cloud-sql-pg\n```\n\nSet your environment variables:\n\n```bash  theme={null}\nPROJECT_ID=\"your-project-id\"\nREGION=\"your-project-region\"\nINSTANCE_NAME=\"your-instance\"\nDB_NAME=\"your-database-name\"\nDB_USER=\"your-database-user\"\nPASSWORD=\"your-database-password\"\n```\n\nCreate a DB connection through the PostgresEngine class:\n\n```typescript  theme={null}\nconst engine: PostgresEngine = await PostgresEngine.fromInstance(\n  process.env.PROJECT_ID ?? \"\",\n  process.env.REGION ?? \"\",\n  process.env.INSTANCE_NAME ?? \"\",\n  process.env.DB_NAME ?? \"\",\n  peArgs\n);\n```\n\nInitialize the vector store table:\n\n```typescript  theme={null}\nawait engine.initVectorstoreTable(\n  \"my_vector_store_table\",\n  768,\n  vectorStoreArgs\n);\n```\n\nCreate a vector store instance:\n\n```typescript  theme={null}\nconst vectorStore = await PostgresVectorStore.initialize(\n  engine,\n  embeddingService,\n  \"my_vector_store_table\",\n  pvectorArgs\n);\n```\n\n## Tools\n\n### Google Search\n\n* Set up a Custom Search Engine, following [these instructions](https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search)\n* Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables `GOOGLE_API_KEY` and `GOOGLE_CSE_ID` respectively\n\nThere exists a `GoogleCustomSearch` utility which wraps this API. To import this utility:\n\n```typescript  theme={null}\nimport { GoogleCustomSearch } from \"@langchain/community/tools/google_custom_search\";\n```\n\nWe can easily load this wrapper as a Tool (to use with an Agent). We can do this with:\n\n```typescript  theme={null}\nconst tools = [new GoogleCustomSearch({})];\n// Pass this variable into your agent.\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/providers/google.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 8356
}