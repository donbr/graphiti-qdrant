{
  "title": "Quickstart",
  "source_url": "https://docs.langchain.com/oss/javascript/langgraph/quickstart",
  "content": "This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\n\n* [Use the Graph API](#use-the-graph-api) if you prefer to define your agent as a graph of nodes and edges.\n* [Use the Functional API](#use-the-functional-api) if you prefer to define your agent as a single function.\n\nFor conceptual information, see [Graph API overview](/oss/javascript/langgraph/graph-api) and [Functional API overview](/oss/javascript/langgraph/functional-api).\n\n<Info>\n  For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.\n</Info>\n\n<Tabs>\n  <Tab title=\"Use the Graph API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```typescript  theme={null}\n    import { ChatAnthropic } from \"@langchain/anthropic\";\n    import { tool } from \"@langchain/core/tools\";\n    import * as z from \"zod\";\n\n    const model = new ChatAnthropic({\n      model: \"claude-sonnet-4-5-20250929\",\n      temperature: 0,\n    });\n\n    // Define tools\n    const add = tool(({ a, b }) => a + b, {\n      name: \"add\",\n      description: \"Add two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const multiply = tool(({ a, b }) => a * b, {\n      name: \"multiply\",\n      description: \"Multiply two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const divide = tool(({ a, b }) => a / b, {\n      name: \"divide\",\n      description: \"Divide two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    // Augment the LLM with tools\n    const toolsByName = {\n      [add.name]: add,\n      [multiply.name]: multiply,\n      [divide.name]: divide,\n    };\n    const tools = Object.values(toolsByName);\n    const modelWithTools = model.bindTools(tools);\n    ```\n\n    ## 2. Define state\n\n    The graph's state is used to store the messages and the number of LLM calls.\n\n    <Tip>\n      State in LangGraph persists throughout the agent's execution.\n\n      The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.\n    </Tip>\n\n    ```typescript  theme={null}\n    import { StateGraph, START, END } from \"@langchain/langgraph\";\n    import { MessagesZodMeta } from \"@langchain/langgraph\";\n    import { registry } from \"@langchain/langgraph/zod\";\n    import { type BaseMessage } from \"@langchain/core/messages\";\n\n    const MessagesState = z.object({\n      messages: z\n        .array(z.custom<BaseMessage>())\n        .register(registry, MessagesZodMeta),\n      llmCalls: z.number().optional(),\n    });\n    ```\n\n    ## 3. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    ```typescript  theme={null}\n    import { SystemMessage } from \"@langchain/core/messages\";\n    async function llmCall(state: z.infer<typeof MessagesState>) {\n      return {\n        messages: await modelWithTools.invoke([\n          new SystemMessage(\n            \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n          ),\n          ...state.messages,\n        ]),\n        llmCalls: (state.llmCalls ?? 0) + 1,\n      };\n    }\n    ```\n\n    ## 4. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```typescript  theme={null}\n    import { isAIMessage, ToolMessage } from \"@langchain/core/messages\";\n    async function toolNode(state: z.infer<typeof MessagesState>) {\n      const lastMessage = state.messages.at(-1);\n\n      if (lastMessage == null || !isAIMessage(lastMessage)) {\n        return { messages: [] };\n      }\n\n      const result: ToolMessage[] = [];\n      for (const toolCall of lastMessage.tool_calls ?? []) {\n        const tool = toolsByName[toolCall.name];\n        const observation = await tool.invoke(toolCall);\n        result.push(observation);\n      }\n\n      return { messages: result };\n    }\n    ```\n\n    ## 5. Define end logic\n\n    The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.\n\n    ```typescript  theme={null}\n    async function shouldContinue(state: z.infer<typeof MessagesState>) {\n      const lastMessage = state.messages.at(-1);\n      if (lastMessage == null || !isAIMessage(lastMessage)) return END;\n\n      // If the LLM makes a tool call, then perform an action\n      if (lastMessage.tool_calls?.length) {\n        return \"toolNode\";\n      }\n\n      // Otherwise, we stop (reply to the user)\n      return END;\n    }\n    ```\n\n    ## 6. Build and compile the agent\n\n    The agent is built using the [`StateGraph`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html) class and compiled using the [`compile`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html#compile) method.\n\n    ```typescript  theme={null}\n    const agent = new StateGraph(MessagesState)\n      .addNode(\"llmCall\", llmCall)\n      .addNode(\"toolNode\", toolNode)\n      .addEdge(START, \"llmCall\")\n      .addConditionalEdges(\"llmCall\", shouldContinue, [\"toolNode\", END])\n      .addEdge(\"toolNode\", \"llmCall\")\n      .compile();\n\n    // Invoke\n    import { HumanMessage } from \"@langchain/core/messages\";\n    const result = await agent.invoke({\n      messages: [new HumanMessage(\"Add 3 and 4.\")],\n    });\n\n    for (const message of result.messages) {\n      console.log(`[${message.getType()}]: ${message.text}`);\n    }\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Graph API.\n\n    <Accordion title=\"Full code example\">\n      ```typescript  theme={null}\n      // Step 1: Define tools and model\n\n      import { ChatAnthropic } from \"@langchain/anthropic\";\n      import { tool } from \"@langchain/core/tools\";\n      import * as z from \"zod\";\n\n      const model = new ChatAnthropic({\n        model: \"claude-sonnet-4-5-20250929\",\n        temperature: 0,\n      });\n\n      // Define tools\n      const add = tool(({ a, b }) => a + b, {\n        name: \"add\",\n        description: \"Add two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const multiply = tool(({ a, b }) => a * b, {\n        name: \"multiply\",\n        description: \"Multiply two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const divide = tool(({ a, b }) => a / b, {\n        name: \"divide\",\n        description: \"Divide two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      // Augment the LLM with tools\n      const toolsByName = {\n        [add.name]: add,\n        [multiply.name]: multiply,\n        [divide.name]: divide,\n      };\n      const tools = Object.values(toolsByName);\n      const modelWithTools = model.bindTools(tools);\n\n      // Step 2: Define state\n\n      import { StateGraph, START, END } from \"@langchain/langgraph\";\n      import { MessagesZodMeta } from \"@langchain/langgraph\";\n      import { registry } from \"@langchain/langgraph/zod\";\n      import { type BaseMessage } from \"@langchain/core/messages\";\n\n      const MessagesState = z.object({\n        messages: z\n          .array(z.custom<BaseMessage>())\n          .register(registry, MessagesZodMeta),\n        llmCalls: z.number().optional(),\n      });\n\n      // Step 3: Define model node\n\n      import { SystemMessage } from \"@langchain/core/messages\";\n      async function llmCall(state: z.infer<typeof MessagesState>) {\n        return {\n          messages: await modelWithTools.invoke([\n            new SystemMessage(\n              \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n            ),\n            ...state.messages,\n          ]),\n          llmCalls: (state.llmCalls ?? 0) + 1,\n        };\n      }\n\n      // Step 4: Define tool node\n\n      import { isAIMessage, ToolMessage } from \"@langchain/core/messages\";\n      async function toolNode(state: z.infer<typeof MessagesState>) {\n        const lastMessage = state.messages.at(-1);\n\n        if (lastMessage == null || !isAIMessage(lastMessage)) {\n          return { messages: [] };\n        }\n\n        const result: ToolMessage[] = [];\n        for (const toolCall of lastMessage.tool_calls ?? []) {\n          const tool = toolsByName[toolCall.name];\n          const observation = await tool.invoke(toolCall);\n          result.push(observation);\n        }\n\n        return { messages: result };\n      }\n\n      // Step 5: Define logic to determine whether to end\n\n      async function shouldContinue(state: z.infer<typeof MessagesState>) {\n        const lastMessage = state.messages.at(-1);\n        if (lastMessage == null || !isAIMessage(lastMessage)) return END;\n\n        // If the LLM makes a tool call, then perform an action\n        if (lastMessage.tool_calls?.length) {\n          return \"toolNode\";\n        }\n\n        // Otherwise, we stop (reply to the user)\n        return END;\n      }\n\n      // Step 6: Build and compile the agent\n\n      const agent = new StateGraph(MessagesState)\n        .addNode(\"llmCall\", llmCall)\n        .addNode(\"toolNode\", toolNode)\n        .addEdge(START, \"llmCall\")\n        .addConditionalEdges(\"llmCall\", shouldContinue, [\"toolNode\", END])\n        .addEdge(\"toolNode\", \"llmCall\")\n        .compile();\n\n      // Invoke\n      import { HumanMessage } from \"@langchain/core/messages\";\n      const result = await agent.invoke({\n        messages: [new HumanMessage(\"Add 3 and 4.\")],\n      });\n\n      for (const message of result.messages) {\n        console.log(`[${message.getType()}]: ${message.text}`);\n      }\n      ```\n    </Accordion>\n  </Tab>\n\n  <Tab title=\"Use the Functional API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```typescript  theme={null}\n    import { ChatAnthropic } from \"@langchain/anthropic\";\n    import { tool } from \"@langchain/core/tools\";\n    import * as z from \"zod\";\n\n    const model = new ChatAnthropic({\n      model: \"claude-sonnet-4-5-20250929\",\n      temperature: 0,\n    });\n\n    // Define tools\n    const add = tool(({ a, b }) => a + b, {\n      name: \"add\",\n      description: \"Add two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const multiply = tool(({ a, b }) => a * b, {\n      name: \"multiply\",\n      description: \"Multiply two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    const divide = tool(({ a, b }) => a / b, {\n      name: \"divide\",\n      description: \"Divide two numbers\",\n      schema: z.object({\n        a: z.number().describe(\"First number\"),\n        b: z.number().describe(\"Second number\"),\n      }),\n    });\n\n    // Augment the LLM with tools\n    const toolsByName = {\n      [add.name]: add,\n      [multiply.name]: multiply,\n      [divide.name]: divide,\n    };\n    const tools = Object.values(toolsByName);\n    const modelWithTools = model.bindTools(tools);\n\n    ```\n\n    ## 2. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    ```typescript  theme={null}\n    import { task, entrypoint } from \"@langchain/langgraph\";\n    import { SystemMessage } from \"@langchain/core/messages\";\n    const callLlm = task({ name: \"callLlm\" }, async (messages: BaseMessage[]) => {\n      return modelWithTools.invoke([\n        new SystemMessage(\n          \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n        ),\n        ...messages,\n      ]);\n    });\n    ```\n\n    ## 3. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```typescript  theme={null}\n    import type { ToolCall } from \"@langchain/core/messages/tool\";\n    const callTool = task({ name: \"callTool\" }, async (toolCall: ToolCall) => {\n      const tool = toolsByName[toolCall.name];\n      return tool.invoke(toolCall);\n    });\n    ```\n\n    ## 4. Define agent\n\n    ```typescript  theme={null}\n    import { addMessages } from \"@langchain/langgraph\";\n    import { type BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n\n    const agent = entrypoint({ name: \"agent\" }, async (messages: BaseMessage[]) => {\n      let modelResponse = await callLlm(messages);\n\n      while (true) {\n        if (!modelResponse.tool_calls?.length) {\n          break;\n        }\n\n        // Execute tools\n        const toolResults = await Promise.all(\n          modelResponse.tool_calls.map((toolCall) => callTool(toolCall))\n        );\n        messages = addMessages(messages, [modelResponse, ...toolResults]);\n        modelResponse = await callLlm(messages);\n      }\n\n      return messages;\n    });\n\n    // Invoke\n    import { HumanMessage } from \"@langchain/core/messages\";\n\n    const result = await agent.invoke([new HumanMessage(\"Add 3 and 4.\")]);\n\n    for (const message of result) {\n      console.log(`[${message.getType()}]: ${message.text}`);\n    }\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Functional API.\n\n    <Accordion title=\"Full code example\" icon=\"code\">\n      ```typescript  theme={null}\n      // Step 1: Define tools and model\n\n      import { ChatAnthropic } from \"@langchain/anthropic\";\n      import { tool } from \"@langchain/core/tools\";\n      import * as z from \"zod\";\n\n      const model = new ChatAnthropic({\n        model: \"claude-sonnet-4-5-20250929\",\n        temperature: 0,\n      });\n\n      // Define tools\n      const add = tool(({ a, b }) => a + b, {\n        name: \"add\",\n        description: \"Add two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const multiply = tool(({ a, b }) => a * b, {\n        name: \"multiply\",\n        description: \"Multiply two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      const divide = tool(({ a, b }) => a / b, {\n        name: \"divide\",\n        description: \"Divide two numbers\",\n        schema: z.object({\n          a: z.number().describe(\"First number\"),\n          b: z.number().describe(\"Second number\"),\n        }),\n      });\n\n      // Augment the LLM with tools\n      const toolsByName = {\n        [add.name]: add,\n        [multiply.name]: multiply,\n        [divide.name]: divide,\n      };\n      const tools = Object.values(toolsByName);\n      const modelWithTools = model.bindTools(tools);\n\n      // Step 2: Define model node\n\n      import { task, entrypoint } from \"@langchain/langgraph\";\n      import { SystemMessage } from \"@langchain/core/messages\";\n      const callLlm = task({ name: \"callLlm\" }, async (messages: BaseMessage[]) => {\n        return modelWithTools.invoke([\n          new SystemMessage(\n            \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n          ),\n          ...messages,\n        ]);\n      });\n\n      // Step 3: Define tool node\n\n      import type { ToolCall } from \"@langchain/core/messages/tool\";\n      const callTool = task({ name: \"callTool\" }, async (toolCall: ToolCall) => {\n        const tool = toolsByName[toolCall.name];\n        return tool.invoke(toolCall);\n      });\n\n      // Step 4: Define agent\n      import { addMessages } from \"@langchain/langgraph\";\n      import { type BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n      const agent = entrypoint({ name: \"agent\" }, async (messages: BaseMessage[]) => {\n        let modelResponse = await callLlm(messages);\n\n        while (true) {\n          if (!modelResponse.tool_calls?.length) {\n            break;\n          }\n\n          // Execute tools\n          const toolResults = await Promise.all(\n            modelResponse.tool_calls.map((toolCall) => callTool(toolCall))\n          );\n          messages = addMessages(messages, [modelResponse, ...toolResults]);\n          modelResponse = await callLlm(messages);\n        }\n\n        return messages;\n      });\n\n      // Invoke\n      import { HumanMessage } from \"@langchain/core/messages\";\n      const result = await agent.invoke([new HumanMessage(\"Add 3 and 4.\")]);\n\n      for (const message of result) {\n        console.log(`[${message.getType()}]: ${message.text}`);\n      }\n      ```\n    </Accordion>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/quickstart.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 17740
}