{
  "title": "Interrupts",
  "source_url": "https://docs.langchain.com/oss/javascript/langgraph/interrupts",
  "content": "Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its [persistence](/oss/javascript/langgraph/persistence) layer and waits indefinitely until you resume execution.\n\nInterrupts work by calling the `interrupt()` function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you're ready to continue, you resume execution by re-invoking the graph using `Command`, which then becomes the return value of the `interrupt()` call from inside the node.\n\nUnlike static breakpoints (which pause before or after specific nodes), interrupts are **dynamic**‚Äîthey can be placed anywhere in your code and can be conditional based on your application logic.\n\n* **Checkpointing keeps your place:** the checkpointer writes the exact graph state so you can resume later, even when in an error state.\n* **`thread_id` is your pointer:** use `{ configurable: { thread_id: ... } }` as options to the `invoke` method to tell the checkpointer which state to load.\n* **Interrupt payloads surface as `__interrupt__`:** the values you pass to `interrupt()` return to the caller in the `__interrupt__` field so you know what the graph is waiting on.\n\nThe `thread_id` you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.\n\n## Pause using `interrupt`\n\nThe [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) function pauses graph execution and returns a value to the caller. When you call [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) within a node, LangGraph saves the current graph state and waits for you to resume execution with input.\n\nTo use [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html), you need:\n\n1. A **checkpointer** to persist the graph state (use a durable checkpointer in production)\n2. A **thread ID** in your config so the runtime knows which state to resume from\n3. To call `interrupt()` where you want to pause (payload must be JSON-serializable)\n\n```typescript  theme={null}\nimport { interrupt } from \"@langchain/langgraph\";\n\nasync function approvalNode(state: State) {\n    // Pause and ask for approval\n    const approved = interrupt(\"Do you approve this action?\");\n\n    // Command({ resume: ... }) provides the value returned into this variable\n    return { approved };\n}\n```\n\nWhen you call [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html), here's what happens:\n\n1. **Graph execution gets suspended** at the exact point where [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) is called\n2. **State is saved** using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)\n3. **Value is returned** to the caller under `__interrupt__`; it can be any JSON-serializable value (string, object, array, etc.)\n4. **Graph waits indefinitely** until you resume execution with a response\n5. **Response is passed back** into the node when you resume, becoming the return value of the `interrupt()` call\n\n## Resuming interrupts\n\nAfter an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed back to the `interrupt` call, allowing the node to continue execution with the external input.\n\n```typescript  theme={null}\nimport { Command } from \"@langchain/langgraph\";\n\n// Initial run - hits the interrupt and pauses\n// thread_id is the durable pointer back to the saved checkpoint\nconst config = { configurable: { thread_id: \"thread-1\" } };\nconst result = await graph.invoke({ input: \"data\" }, config);\n\n// Check what was interrupted\n// __interrupt__ mirrors every payload you passed to interrupt()\nconsole.log(result.__interrupt__);\n// [{ value: 'Do you approve this action?', ... }]\n\n// Resume with the human's response\n// Command({ resume }) returns that value from interrupt() in the node\nawait graph.invoke(new Command({ resume: true }), config);\n```\n\n**Key points about resuming:**\n\n* You must use the **same thread ID** when resuming that was used when the interrupt occurred\n* The value passed to `Command(resume=...)` becomes the return value of the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) call\n* The node restarts from the beginning of the node where the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) was called when resumed, so any code before the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) runs again\n* You can pass any JSON-serializable value as the resume value\n\n## Common patterns\n\nThe key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:\n\n* <Icon icon=\"check-circle\" /> [Approval workflows](#approve-or-reject): Pause before executing critical actions (API calls, database changes, financial transactions)\n* <Icon icon=\"pencil\" /> [Review and edit](#review-and-edit-state): Let humans review and modify LLM outputs or tool calls before continuing\n* <Icon icon=\"wrench\" /> [Interrupting tool calls](#interrupts-in-tools): Pause before executing tool calls to review and edit the tool call before execution\n* <Icon icon=\"shield-check\" /> [Validating human input](#validating-human-input): Pause before proceeding to the next step to validate human input\n\n### Approve or reject\n\nOne of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.\n\n```typescript  theme={null}\nimport { interrupt, Command } from \"@langchain/langgraph\";\n\nfunction approvalNode(state: State): Command {\n  // Pause execution; payload surfaces in result.__interrupt__\n  const isApproved = interrupt({\n    question: \"Do you want to proceed?\",\n    details: state.actionDetails\n  });\n\n  // Route based on the response\n  if (isApproved) {\n    return new Command({ goto: \"proceed\" }); // Runs after the resume payload is provided\n  } else {\n    return new Command({ goto: \"cancel\" });\n  }\n}\n```\n\nWhen you resume the graph, pass `true` to approve or `false` to reject:\n\n```typescript  theme={null}\n// To approve\nawait graph.invoke(new Command({ resume: true }), config);\n\n// To reject\nawait graph.invoke(new Command({ resume: false }), config);\n```\n\n<Accordion title=\"Full example\">\n  ```typescript  theme={null}\n  import {\n    Command,\n    MemorySaver,\n    START,\n    END,\n    StateGraph,\n    interrupt,\n  } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  const State = z.object({\n    actionDetails: z.string(),\n    status: z.enum([\"pending\", \"approved\", \"rejected\"]).nullable(),\n  });\n\n  const graphBuilder = new StateGraph(State)\n    .addNode(\"approval\", async (state) => {\n      // Expose details so the caller can render them in a UI\n      const decision = interrupt({\n        question: \"Approve this action?\",\n        details: state.actionDetails,\n      });\n      return new Command({ goto: decision ? \"proceed\" : \"cancel\" });\n    }, { ends: ['proceed', 'cancel'] })\n    .addNode(\"proceed\", () => ({ status: \"approved\" }))\n    .addNode(\"cancel\", () => ({ status: \"rejected\" }))\n    .addEdge(START, \"approval\")\n    .addEdge(\"proceed\", END)\n    .addEdge(\"cancel\", END);\n\n  // Use a more durable checkpointer in production\n  const checkpointer = new MemorySaver();\n  const graph = graphBuilder.compile({ checkpointer });\n\n  const config = { configurable: { thread_id: \"approval-123\" } };\n  const initial = await graph.invoke(\n    { actionDetails: \"Transfer $500\", status: \"pending\" },\n    config,\n  );\n  console.log(initial.__interrupt__);\n  // [{ value: { question: ..., details: ... } }]\n\n  // Resume with the decision; true routes to proceed, false to cancel\n  const resumed = await graph.invoke(new Command({ resume: true }), config);\n  console.log(resumed.status); // -> \"approved\"\n  ```\n</Accordion>\n\n### Review and edit state\n\nSometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.\n\n```typescript  theme={null}\nimport { interrupt } from \"@langchain/langgraph\";\n\nfunction reviewNode(state: State) {\n  // Pause and show the current content for review (surfaces in result.__interrupt__)\n  const editedContent = interrupt({\n    instruction: \"Review and edit this content\",\n    content: state.generatedText\n  });\n\n  // Update the state with the edited version\n  return { generatedText: editedContent };\n}\n```\n\nWhen resuming, provide the edited content:\n\n```typescript  theme={null}\nawait graph.invoke(\n  new Command({ resume: \"The edited and improved text\" }), // Value becomes the return from interrupt()\n  config\n);\n```\n\n<Accordion title=\"Full example\">\n  ```typescript  theme={null}\n  import {\n    Command,\n    MemorySaver,\n    START,\n    END,\n    StateGraph,\n    interrupt,\n  } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  const State = z.object({\n    generatedText: z.string(),\n  });\n\n  const builder = new StateGraph(State)\n    .addNode(\"review\", async (state) => {\n      // Ask a reviewer to edit the generated content\n      const updated = interrupt({\n        instruction: \"Review and edit this content\",\n        content: state.generatedText,\n      });\n      return { generatedText: updated };\n    })\n    .addEdge(START, \"review\")\n    .addEdge(\"review\", END);\n\n  const checkpointer = new MemorySaver();\n  const graph = builder.compile({ checkpointer });\n\n  const config = { configurable: { thread_id: \"review-42\" } };\n  const initial = await graph.invoke({ generatedText: \"Initial draft\" }, config);\n  console.log(initial.__interrupt__);\n  // [{ value: { instruction: ..., content: ... } }]\n\n  // Resume with the edited text from the reviewer\n  const finalState = await graph.invoke(\n    new Command({ resume: \"Improved draft after review\" }),\n    config,\n  );\n  console.log(finalState.generatedText); // -> \"Improved draft after review\"\n  ```\n</Accordion>\n\n### Interrupts in tools\n\nYou can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it's called, and allows for human review and editing of the tool call before it is executed.\n\nFirst, define a tool that uses [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html):\n\n```typescript  theme={null}\nimport { tool } from \"@langchain/core/tools\";\nimport { interrupt } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst sendEmailTool = tool(\n  async ({ to, subject, body }) => {\n    // Pause before sending; payload surfaces in result.__interrupt__\n    const response = interrupt({\n      action: \"send_email\",\n      to,\n      subject,\n      body,\n      message: \"Approve sending this email?\",\n    });\n\n    if (response?.action === \"approve\") {\n      // Resume value can override inputs before executing\n      const finalTo = response.to ?? to;\n      const finalSubject = response.subject ?? subject;\n      const finalBody = response.body ?? body;\n      return `Email sent to ${finalTo} with subject '${finalSubject}'`;\n    }\n    return \"Email cancelled by user\";\n  },\n  {\n    name: \"send_email\",\n    description: \"Send an email to a recipient\",\n    schema: z.object({\n      to: z.string(),\n      subject: z.string(),\n      body: z.string(),\n    }),\n  },\n);\n```\n\nThis approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.\n\n<Accordion title=\"Full example\">\n  ```typescript  theme={null}\n  import { tool } from \"@langchain/core/tools\";\n  import { ChatAnthropic } from \"@langchain/anthropic\";\n  import {\n    Command,\n    MemorySaver,\n    START,\n    END,\n    StateGraph,\n    interrupt,\n  } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  const sendEmailTool = tool(\n    async ({ to, subject, body }) => {\n      // Pause before sending; payload surfaces in result.__interrupt__\n      const response = interrupt({\n        action: \"send_email\",\n        to,\n        subject,\n        body,\n        message: \"Approve sending this email?\",\n      });\n\n      if (response?.action === \"approve\") {\n        const finalTo = response.to ?? to;\n        const finalSubject = response.subject ?? subject;\n        const finalBody = response.body ?? body;\n        console.log(\"[sendEmailTool]\", finalTo, finalSubject, finalBody);\n        return `Email sent to ${finalTo}`;\n      }\n      return \"Email cancelled by user\";\n    },\n    {\n      name: \"send_email\",\n      description: \"Send an email to a recipient\",\n      schema: z.object({\n        to: z.string(),\n        subject: z.string(),\n        body: z.string(),\n      }),\n    },\n  );\n\n  const model = new ChatAnthropic({ model: \"claude-sonnet-4-5-20250929\" }).bindTools([sendEmailTool]);\n\n  const Message = z.object({\n    role: z.enum([\"user\", \"assistant\", \"tool\"]),\n    content: z.string(),\n  });\n\n  const State = z.object({\n    messages: z.array(Message),\n  });\n\n  const graphBuilder = new StateGraph(State)\n    .addNode(\"agent\", async (state) => {\n      // LLM may decide to call the tool; interrupt pauses before sending\n      const response = await model.invoke(state.messages);\n      return { messages: [...state.messages, response] };\n    })\n    .addEdge(START, \"agent\")\n    .addEdge(\"agent\", END);\n\n  const checkpointer = new MemorySaver();\n  const graph = graphBuilder.compile({ checkpointer });\n\n  const config = { configurable: { thread_id: \"email-workflow\" } };\n  const initial = await graph.invoke(\n    {\n      messages: [\n        { role: \"user\", content: \"Send an email to alice@example.com about the meeting\" },\n      ],\n    },\n    config,\n  );\n  console.log(initial.__interrupt__); // -> [{ value: { action: 'send_email', ... } }]\n\n  // Resume with approval and optionally edited arguments\n  const resumed = await graph.invoke(\n    new Command({\n      resume: { action: \"approve\", subject: \"Updated subject\" },\n    }),\n    config,\n  );\n  console.log(resumed.messages.at(-1)); // -> Tool result returned by send_email\n  ```\n</Accordion>\n\n### Validating human input\n\nSometimes you need to validate input from humans and ask again if it's invalid. You can do this using multiple [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) calls in a loop.\n\n```typescript  theme={null}\nimport { interrupt } from \"@langchain/langgraph\";\n\nfunction getAgeNode(state: State) {\n  let prompt = \"What is your age?\";\n\n  while (true) {\n    const answer = interrupt(prompt); // payload surfaces in result.__interrupt__\n\n    // Validate the input\n    if (typeof answer === \"number\" && answer > 0) {\n      // Valid input - continue\n      return { age: answer };\n    } else {\n      // Invalid input - ask again with a more specific prompt\n      prompt = `'${answer}' is not a valid age. Please enter a positive number.`;\n    }\n  }\n}\n```\n\nEach time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.\n\n<Accordion title=\"Full example\">\n  ```typescript  theme={null}\n  import {\n    Command,\n    MemorySaver,\n    START,\n    END,\n    StateGraph,\n    interrupt,\n  } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  const State = z.object({\n    age: z.number().nullable(),\n  });\n\n  const builder = new StateGraph(State)\n    .addNode(\"collectAge\", (state) => {\n      let prompt = \"What is your age?\";\n\n      while (true) {\n        const answer = interrupt(prompt); // payload surfaces in result.__interrupt__\n\n        if (typeof answer === \"number\" && answer > 0) {\n          return { age: answer };\n        }\n\n        prompt = `'${answer}' is not a valid age. Please enter a positive number.`;\n      }\n    })\n    .addEdge(START, \"collectAge\")\n    .addEdge(\"collectAge\", END);\n\n  const checkpointer = new MemorySaver();\n  const graph = builder.compile({ checkpointer });\n\n  const config = { configurable: { thread_id: \"form-1\" } };\n  const first = await graph.invoke({ age: null }, config);\n  console.log(first.__interrupt__); // -> [{ value: \"What is your age?\", ... }]\n\n  // Provide invalid data; the node re-prompts\n  const retry = await graph.invoke(new Command({ resume: \"thirty\" }), config);\n  console.log(retry.__interrupt__); // -> [{ value: \"'thirty' is not a valid age...\", ... }]\n\n  // Provide valid data; loop exits and state updates\n  const final = await graph.invoke(new Command({ resume: 30 }), config);\n  console.log(final.age); // -> 30\n  ```\n</Accordion>\n\n## Rules of interrupts\n\nWhen you call [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.\n\nWhen execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning‚Äîit does not resume from the exact line where [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) was called. This means any code that ran before the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) will execute again. Because of this, there's a few important rules to follow when working with interrupts to ensure they behave as expected.\n\n### Do not wrap `interrupt` calls in try/catch\n\nThe way that [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) pauses execution at the point of the call is by throwing a special exception. If you wrap the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) call in a try/catch block, you will catch this exception and the interrupt will not be passed back to the graph.\n\n* ‚úÖ Separate [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) calls from error-prone code\n* ‚úÖ Conditionally catch errors if needed\n\n<CodeGroup>\n  ```typescript Separating logic theme={null}\n  async function nodeA(state: State) {\n      // ‚úÖ Good: interrupting first, then handling error conditions separately\n      const name = interrupt(\"What's your name?\");\n      try {\n          await fetchData(); // This can fail\n      } catch (err) {\n          console.error(error);\n      }\n      return state;\n  }\n  ```\n\n  ```typescript Conditionally handling errors theme={null}\n  async function nodeA(state: State) {\n      // ‚úÖ Good: re-throwing the exception will\n      // allow the interrupt to be passed back to\n      // the graph\n      try {\n          const name = interrupt(\"What's your name?\");\n          await fetchData(); // This can fail\n      } catch (err) {\n          if (error instanceof NetworkError) {\n              console.error(error);\n          }\n          throw error;\n      }\n      return state;\n  }\n  ```\n</CodeGroup>\n\n* üî¥ Do not wrap [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) calls in bare try/catch blocks\n\n```typescript  theme={null}\nasync function nodeA(state: State) {\n    // ‚ùå Bad: wrapping interrupt in bare try/catch will catch the interrupt exception\n    try {\n        const name = interrupt(\"What's your name?\");\n    } catch (err) {\n        console.error(error);\n    }\n    return state;\n}\n```\n\n### Do not reorder `interrupt` calls within a node\n\nIt's common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.\n\nWhen a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task's resume list. Matching is **strictly index-based**, so the order of interrupt calls within the node is important.\n\n* ‚úÖ Keep [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) calls consistent across node executions\n\n```typescript  theme={null}\nasync function nodeA(state: State) {\n    // ‚úÖ Good: interrupt calls happen in the same order every time\n    const name = interrupt(\"What's your name?\");\n    const age = interrupt(\"What's your age?\");\n    const city = interrupt(\"What's your city?\");\n\n    return {\n        name,\n        age,\n        city\n    };\n}\n```\n\n* üî¥ Do not conditionally skip [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) calls within a node\n* üî¥ Do not loop [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) calls using logic that isn't deterministic across executions\n\n<CodeGroup>\n  ```typescript Skipping interrupts theme={null}\n  async function nodeA(state: State) {\n      // ‚ùå Bad: conditionally skipping interrupts changes the order\n      const name = interrupt(\"What's your name?\");\n\n      // On first run, this might skip the interrupt\n      // On resume, it might not skip it - causing index mismatch\n      if (state.needsAge) {\n          const age = interrupt(\"What's your age?\");\n      }\n\n      const city = interrupt(\"What's your city?\");\n\n      return { name, city };\n  }\n  ```\n\n  ```typescript Looping interrupts theme={null}\n  async function nodeA(state: State) {\n      // ‚ùå Bad: looping based on non-deterministic data\n      // The number of interrupts changes between executions\n      const results = [];\n      for (const item of state.dynamicList || []) {  // List might change between runs\n          const result = interrupt(`Approve ${item}?`);\n          results.push(result);\n      }\n\n      return { results };\n  }\n  ```\n</CodeGroup>\n\n### Do not return complex values in `interrupt` calls\n\nDepending on which checkpointer is used, complex values may not be serializable (e.g. you can't serialize a function). To make your graphs adaptable to any deployment, it's best practice to only use values that can be reasonably serialized.\n\n* ‚úÖ Pass simple, JSON-serializable types to [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html)\n* ‚úÖ Pass dictionaries/objects with simple values\n\n<CodeGroup>\n  ```typescript Simple values theme={null}\n  async function nodeA(state: State) {\n      // ‚úÖ Good: passing simple types that are serializable\n      const name = interrupt(\"What's your name?\");\n      const count = interrupt(42);\n      const approved = interrupt(true);\n\n      return { name, count, approved };\n  }\n  ```\n\n  ```typescript Structured data theme={null}\n  async function nodeA(state: State) {\n      // ‚úÖ Good: passing objects with simple values\n      const response = interrupt({\n          question: \"Enter user details\",\n          fields: [\"name\", \"email\", \"age\"],\n          currentValues: state.user || {}\n      });\n\n      return { user: response };\n  }\n  ```\n</CodeGroup>\n\n* üî¥ Do not pass functions, class instances, or other complex objects to [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html)\n\n<CodeGroup>\n  ```typescript Functions theme={null}\n  function validateInput(value: string): boolean {\n      return value.length > 0;\n  }\n\n  async function nodeA(state: State) {\n      // ‚ùå Bad: passing a function to interrupt\n      // The function cannot be serialized\n      const response = interrupt({\n          question: \"What's your name?\",\n          validator: validateInput  // This will fail\n      });\n      return { name: response };\n  }\n  ```\n\n  ```typescript Class instances theme={null}\n  class DataProcessor {\n      constructor(private config: any) {}\n  }\n\n  async function nodeA(state: State) {\n      const processor = new DataProcessor({ mode: \"strict\" });\n\n      // ‚ùå Bad: passing a class instance to interrupt\n      // The instance cannot be serialized\n      const response = interrupt({\n          question: \"Enter data to process\",\n          processor: processor  // This will fail\n      });\n      return { result: response };\n  }\n  ```\n</CodeGroup>\n\n### Side effects called before `interrupt` must be idempotent\n\nBecause interrupts work by re-running the nodes they were called from, side effects called before [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.\n\nAs an example, you might have an API call to update a record inside of a node. If [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.\n\n* ‚úÖ Use idempotent operations before [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html)\n* ‚úÖ Place side effects after [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) calls\n* ‚úÖ Separate side effects into separate nodes when possible\n\n<CodeGroup>\n  ```typescript Idempotent operations theme={null}\n  async function nodeA(state: State) {\n      // ‚úÖ Good: using upsert operation which is idempotent\n      // Running this multiple times will have the same result\n      await db.upsertUser({\n          userId: state.userId,\n          status: \"pending_approval\"\n      });\n\n      const approved = interrupt(\"Approve this change?\");\n\n      return { approved };\n  }\n  ```\n\n  ```typescript Side effects after interrupt theme={null}\n  async function nodeA(state: State) {\n      // ‚úÖ Good: placing side effect after the interrupt\n      // This ensures it only runs once after approval is received\n      const approved = interrupt(\"Approve this change?\");\n\n      if (approved) {\n          await db.createAuditLog({\n              userId: state.userId,\n              action: \"approved\"\n          });\n      }\n\n      return { approved };\n  }\n  ```\n\n  ```typescript Separating into different nodes theme={null}\n  async function approvalNode(state: State) {\n      // ‚úÖ Good: only handling the interrupt in this node\n      const approved = interrupt(\"Approve this change?\");\n\n      return { approved };\n  }\n\n  async function notificationNode(state: State) {\n      // ‚úÖ Good: side effect happens in a separate node\n      // This runs after approval, so it only executes once\n      if (state.approved) {\n          await sendNotification({\n              userId: state.userId,\n              status: \"approved\"\n          });\n      }\n\n      return state;\n  }\n  ```\n</CodeGroup>\n\n* üî¥ Do not perform non-idempotent operations before [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html)\n* üî¥ Do not create new records without checking if they exist\n\n<CodeGroup>\n  ```typescript Creating records theme={null}\n  async function nodeA(state: State) {\n      // ‚ùå Bad: creating a new record before interrupt\n      // This will create duplicate records on each resume\n      const auditId = await db.createAuditLog({\n          userId: state.userId,\n          action: \"pending_approval\",\n          timestamp: new Date()\n      });\n\n      const approved = interrupt(\"Approve this change?\");\n\n      return { approved, auditId };\n  }\n  ```\n\n  ```typescript Appending to arrays theme={null}\n  async function nodeA(state: State) {\n      // ‚ùå Bad: appending to an array before interrupt\n      // This will add duplicate entries on each resume\n      await db.appendToHistory(state.userId, \"approval_requested\");\n\n      const approved = interrupt(\"Approve this change?\");\n\n      return { approved };\n  }\n  ```\n</CodeGroup>\n\n## Using with subgraphs called as functions\n\nWhen invoking a subgraph within a node, the parent graph will resume execution from the **beginning of the node** where the subgraph was invoked and the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) was triggered. Similarly, the **subgraph** will also resume from the beginning of the node where [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) was called.\n\n```typescript  theme={null}\nasync function nodeInParentGraph(state: State) {\n    someCode(); // <-- This will re-execute when resumed\n    // Invoke a subgraph as a function.\n    // The subgraph contains an `interrupt` call.\n    const subgraphResult = await subgraph.invoke(someInput);\n    // ...\n}\n\nasync function nodeInSubgraph(state: State) {\n    someOtherCode(); // <-- This will also re-execute when resumed\n    const result = interrupt(\"What's your name?\");\n    // ...\n}\n```\n\n## Debugging with interrupts\n\nTo debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying `interruptBefore` and `interruptAfter` when compiling the graph.\n\n<Note>\n  Static interrupts are **not** recommended for human-in-the-loop workflows. Use the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) method instead.\n</Note>\n\n<Tabs>\n  <Tab title=\"At compile time\">\n    ```typescript  theme={null}\n    const graph = builder.compile({\n        interruptBefore: [\"node_a\"],  // [!code highlight]\n        interruptAfter: [\"node_b\", \"node_c\"],  // [!code highlight]\n        checkpointer,\n    });\n\n    // Pass a thread ID to the graph\n    const config = {\n        configurable: {\n            thread_id: \"some_thread\"\n        }\n    };\n\n    // Run the graph until the breakpoint\n    await graph.invoke(inputs, config);# [!code highlight]\n\n    await graph.invoke(null, config);  # [!code highlight]\n    ```\n\n    1. The breakpoints are set during `compile` time.\n    2. `interruptBefore` specifies the nodes where execution should pause before the node is executed.\n    3. `interruptAfter` specifies the nodes where execution should pause after the node is executed.\n    4. A checkpointer is required to enable breakpoints.\n    5. The graph is run until the first breakpoint is hit.\n    6. The graph is resumed by passing in `null` for the input. This will run the graph until the next breakpoint is hit.\n  </Tab>\n\n  <Tab title=\"At run time\">\n    ```typescript  theme={null}\n    // Run the graph until the breakpoint\n    graph.invoke(inputs, {\n        interruptBefore: [\"node_a\"],  // [!code highlight]\n        interruptAfter: [\"node_b\", \"node_c\"],  // [!code highlight]\n        configurable: {\n            thread_id: \"some_thread\"\n        }\n    });\n\n    // Resume the graph\n    await graph.invoke(null, config);  // [!code highlight]\n    ```\n\n    1. `graph.invoke` is called with the `interruptBefore` and `interruptAfter` parameters. This is a run-time configuration and can be changed for every invocation.\n    2. `interruptBefore` specifies the nodes where execution should pause before the node is executed.\n    3. `interruptAfter` specifies the nodes where execution should pause after the node is executed.\n    4. The graph is run until the first breakpoint is hit.\n    5. The graph is resumed by passing in `null` for the input. This will run the graph until the next breakpoint is hit.\n  </Tab>\n</Tabs>\n\n### Using LangGraph Studio\n\nYou can use [LangGraph Studio](/langsmith/studio) to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5aa4e7cea2ab147cef5b4e210dd6c4a1\" alt=\"image\" data-og-width=\"1252\" width=\"1252\" data-og-height=\"1040\" height=\"1040\" data-path=\"oss/images/static-interrupt.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=52d02b507d0a6a879f7fb88d9c6767d0 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e363cd4980edff9bab422f4f1c0ee3c8 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=49d26a3641953c23ef3fbc51e828c305 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=2dba15683b3baa1a61bc3bcada35ae1e 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=9f9a2c0f2631c0e69cd248f6319933fe 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5a46b765b436ab5d0dc2f41c01ffad80 2500w\" />\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/interrupts.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 34658
}