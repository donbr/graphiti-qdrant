{
  "title": "Run backtests on a new version of an agent",
  "source_url": "https://docs.langchain.com/langsmith/run-backtests-new-agent",
  "content": "Deploying your application is just the beginning of a continuous improvement process. After you deploy to production, you'll want to refine your system by enhancing prompts, language models, tools, and architectures. Backtesting involves assessing new versions of your application using historical data and comparing the new outputs to the original ones. Compared to evaluations using pre-production datasets, backtesting offers a clearer indication of whether the new version of your application is an improvement over the current deployment.\n\nHere are the basic steps for backtesting:\n\n1. Select sample runs from your production tracing project to test against.\n2. Transform the run inputs into a dataset and record the run outputs as an initial experiment against that dataset.\n3. Execute your new system on the new dataset and compare the results of the experiments.\n\nThis process will provide you with a new dataset of representative inputs, which you can version and use for backtesting your models.\n\n<Info>\n  Often, you won't have definitive \"ground truth\" answers available. In such cases, you can manually label the outputs or use evaluators that don't rely on reference data. If your application allows for capturing ground-truth labels, for example by allowing users to leave feedback, we strongly recommend doing so.\n</Info>\n\n## Setup\n\n### Configure the environment\n\nInstall and set environment variables. This guide requires `langsmith>=0.2.4`.\n\n<Info>\n  For convenience we'll use the LangChain OSS framework in this tutorial, but the LangSmith functionality shown is framework-agnostic.\n</Info>\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langsmith langchain langchain-anthropic langchainhub emoji\n  ```\n\n  ```bash uv theme={null}\n  uv add langsmith langchain langchain-anthropic langchainhub emoji\n  ```\n</CodeGroup>\n\n```python  theme={null}\nimport getpass\nimport os\n\n# Set the project name to whichever project you'd like to be testing against\nproject_name = \"Tweet Writing Task\"\nos.environ[\"LANGSMITH_PROJECT\"] = project_name\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\n\nif not os.environ.get(\"LANGSMITH_API_KEY\"):\n    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"YOUR API KEY\")\n\n# Optional. You can swap OpenAI for any other tool-calling chat model.\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"\n\n# Optional. You can swap Tavily for the free DuckDuckGo search tool if preferred.\n# Get Tavily API key: https://tavily.com\nos.environ[\"TAVILY_API_KEY\"] = \"YOUR TAVILY API KEY\"\n```\n\n### Define the application\n\nFor this example lets create a simple Tweet-writing application that has access to some internet search tools:\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\nfrom langchain.agents import create_agent\nfrom langchain_community.tools import DuckDuckGoSearchRun, TavilySearchResults\nfrom langchain_core.rate_limiters import InMemoryRateLimiter\n\n\n# We will use GPT-3.5 Turbo as the baseline and compare against GPT-4o\ngpt_3_5_turbo = init_chat_model(\n    \"gpt-3.5-turbo\",\n    temperature=1,\n    configurable_fields=(\"model\", \"model_provider\"),\n)\n\n# The instrucitons are passed as a system message to the agent\ninstructions = \"\"\"You are a tweet writing assistant. Given a topic, do some research and write a relevant and engaging tweet about it.\n- Use at least 3 emojis in each tweet\n- The tweet should be no longer than 280 characters\n- Always use the search tool to gather recent information on the tweet topic\n- Write the tweet only based on the search content. Do not rely on your internal knowledge\n- When relevant, link to your sources\n- Make your tweet as engaging as possible\"\"\"\n\n# Define the tools our agent can use\n# If you have a higher tiered Tavily API plan you can increase this\nrate_limiter = InMemoryRateLimiter(requests_per_second=0.08)\n\n# Use DuckDuckGo if you don't have a Tavily API key:\n# tools = [DuckDuckGoSearchRun(rate_limiter=rate_limiter)]\ntools = [TavilySearchResults(max_results=5, rate_limiter=rate_limiter)]\n\nagent = create_agent(gpt_3_5_turbo, tools=tools, system_prompt=instructions)\n```\n\n### Simulate production data\n\nNow lets simulate some production data:\n\n```python  theme={null}\nfake_production_inputs = [\n    \"Alan turing's early childhood\",\n    \"Economic impacts of the European Union\",\n    \"Underrated philosophers\",\n    \"History of the Roxie theater in San Francisco\",\n    \"ELI5: gravitational waves\",\n    \"The arguments for and against a parliamentary system\",\n    \"Pivotal moments in music history\",\n    \"Big ideas in programming languages\",\n    \"Big questions in biology\",\n    \"The relationship between math and reality\",\n    \"What makes someone funny\",\n]\n\nagent.batch(\n    [{\"messages\": [{\"role\": \"user\", \"content\": content}]} for content in fake_production_inputs],\n)\n```\n\n## Convert Production Traces to Experiment\n\nThe first step is to generate a dataset based on the production *inputs*. Then copy over all the traces to serve as a baseline experiment.\n\n### Select runs to backtest on\n\nYou can select the runs to backtest on using the `filter` argument of `list_runs`. The `filter` argument uses the LangSmith [trace query syntax](/langsmith/trace-query-syntax) to select runs.\n\n```python  theme={null}\nfrom datetime import datetime, timedelta, timezone\nfrom uuid import uuid4\nfrom langsmith import Client\nfrom langsmith.beta import convert_runs_to_test\n\n# Fetch the runs we want to convert to a dataset/experiment\nclient = Client()\n\n# How we are sampling runs to include in our dataset\nend_time = datetime.now(tz=timezone.utc)\nstart_time = end_time - timedelta(days=1)\nrun_filter = f'and(gt(start_time, \"{start_time.isoformat()}\"), lt(end_time, \"{end_time.isoformat()}\"))'\nprod_runs = list(\n    client.list_runs(\n        project_name=project_name,\n        is_root=True,\n        filter=run_filter,\n    )\n)\n```\n\n### Convert runs to experiment\n\n`convert_runs_to_test` is a function which takes some runs and does the following:\n\n1. The inputs, and optionally the outputs, are saved to a dataset as Examples.\n2. The inputs and outputs are stored as an experiment, as if you had run the `evaluate` function and received those outputs.\n\n```python  theme={null}\n# Name of the dataset we want to create\ndataset_name = f'{project_name}-backtesting {start_time.strftime(\"%Y-%m-%d\")}-{end_time.strftime(\"%Y-%m-%d\")}'\n# Name of the experiment we want to create from the historical runs\nbaseline_experiment_name = f\"prod-baseline-gpt-3.5-turbo-{str(uuid4())[:4]}\"\n\n# This converts the runs to a dataset + experiment\nconvert_runs_to_test(\n    prod_runs,\n    # Name of the resulting dataset\n    dataset_name=dataset_name,\n    # Whether to include the run outputs as reference/ground truth\n    include_outputs=False,\n    # Whether to include the full traces in the resulting experiment\n    # (default is to just include the root run)\n    load_child_runs=True,\n    # Name of the experiment so we can apply evalautors to it after\n    test_project_name=baseline_experiment_name\n)\n```\n\nOnce this step is complete, you should see a new dataset in your LangSmith project called \"Tweet Writing Task-backtesting TODAYS DATE\", with a single experiment like so:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=73b60a75d6b33f2830f5ed68464c586b\" alt=\"\" data-og-width=\"3456\" width=\"3456\" data-og-height=\"1852\" height=\"1852\" data-path=\"langsmith/images/baseline-experiment.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e459a884bbec6e3741617830b9e70848 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6116dd6057709be29d84f0cbad32e7a1 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a86ab5cffeac0ceace81adbc59dba649 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e0058d68f4061df5fe4249c9e3954c38 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=270a7eb33f39fd612d1732aadaa0b373 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fe9dc037876fcf1c4eb22317d7bb3f45 2500w\" />\n\n## Benchmark against new system\n\nNow we can start the process of benchmarking our production runs against a new system.\n\n### Define evaluators\n\nFirst let's define the evaluators we will use to compare the two systems. Note that we have no reference outputs, so we'll need to come up with evaluation metrics that only require the actual outputs.\n\n```python  theme={null}\nimport emoji\nfrom pydantic import BaseModel, Field\nfrom langchain_core.messages import convert_to_openai_messages\n\nclass Grade(BaseModel):\n    \"\"\"Grade whether a response is supported by some context.\"\"\"\n    grounded: bool = Field(..., description=\"Is the majority of the response supported by the retrieved context?\")\n\ngrounded_instructions = f\"\"\"You have given somebody some contextual information and asked them to write a statement grounded in that context.\n\nGrade whether their response is fully supported by the context you have provided. \\\nIf any meaningful part of their statement is not backed up directly by the context you provided, then their response is not grounded. \\\nOtherwise it is grounded.\"\"\"\ngrounded_model = init_chat_model(model=\"gpt-4o\").with_structured_output(Grade)\n\ndef lt_280_chars(outputs: dict) -> bool:\n    messages = convert_to_openai_messages(outputs[\"messages\"])\n    return len(messages[-1]['content']) <= 280\n\ndef gte_3_emojis(outputs: dict) -> bool:\n    messages = convert_to_openai_messages(outputs[\"messages\"])\n    return len(emoji.emoji_list(messages[-1]['content'])) >= 3\n\nasync def is_grounded(outputs: dict) -> bool:\n    context = \"\"\n    messages = convert_to_openai_messages(outputs[\"messages\"])\n    for message in messages:\n        if message[\"role\"] == \"tool\":\n            # Tool message outputs are the results returned from the Tavily/DuckDuckGo tool\n            context += \"\\n\\n\" + message[\"content\"]\n    tweet = messages[-1][\"content\"]\n    user = f\"\"\"CONTEXT PROVIDED:\n    {context}\n\n    RESPONSE GIVEN:\n    {tweet}\"\"\"\n    grade = await grounded_model.ainvoke([\n        {\"role\": \"system\", \"content\": grounded_instructions},\n        {\"role\": \"user\", \"content\": user}\n    ])\n    return grade.grounded\n```\n\n### Evaluate baseline\n\nNow, let's run our evaluators against the baseline experiment.\n\n```python  theme={null}\nbaseline_results = await client.aevaluate(\n    baseline_experiment_name,\n    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],\n)\n# If you have pandas installed can easily explore results as df:\n# baseline_results.to_pandas()\n```\n\n### Define and evaluate new system\n\nNow, let's define and evaluate our new system. In this example our new system will be the same as the old system, but will use GPT-4o instead of GPT-3.5. Since we've made our model configurable we can just update the default config passed to our agent:\n\n```python  theme={null}\ncandidate_results = await client.aevaluate(\n    agent.with_config(model=\"gpt-4o\"),\n    data=dataset_name,\n    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],\n    experiment_prefix=\"candidate-gpt-4o\",\n)\n# If you have pandas installed can easily explore results as df:\n# candidate_results.to_pandas()\n```\n\n## Comparing the results\n\nAfter running both experiments, you can view them in your dataset:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1c5d4f1cf212e2c38917319c7bbf7f99\" alt=\"\" data-og-width=\"3022\" width=\"3022\" data-og-height=\"1536\" height=\"1536\" data-path=\"langsmith/images/dataset-page.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=01ebc5373bb6428b614eeade16aeb606 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b2308ef5ed76bb80f111a89000457424 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f2c76f9b1194a1a56efaa97d88b885f0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=beb69429d2a2e208b75924593a9a10c1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0fa47df443b9695d1e6863a57ca3a016 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d257f3444d8357fb0b0340d09c16e127 2500w\" />\n\nThe results reveal an interesting tradeoff between the two models:\n\n1. GPT-4o shows improved performance in following formatting rules, consistently including the requested number of emojis\n2. However, GPT-4o is less reliable at staying grounded in the provided search results\n\nTo illustrate the grounding issue: in [this example run](https://smith.langchain.com/public/be060e19-0bc0-4798-94f5-c3d35719a5f6/r/07d43e7a-8632-479d-ae28-c7eac6e54da4), GPT-4o included facts about Ab큰 Bakr Muhammad ibn Zakariyy훮 al-R훮z카's medical contributions that weren't present in the search results. This demonstrates how it's pulling from its internal knowledge rather than strictly using the provided information.\n\nThis backtesting exercise revealed that while GPT-4o is generally considered a more capable model, simply upgrading to it wouldn't improve our tweet-writer. To effectively use GPT-4o, we would need to:\n\n* Refine our prompts to more strongly emphasize using only provided information\n* Or modify our system architecture to better constrain the model's outputs\n\nThis insight demonstrates the value of backtesting - it helped us identify potential issues before deployment.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a8ab311399f3d0e69554a62f939fd475\" alt=\"\" data-og-width=\"3018\" width=\"3018\" data-og-height=\"1532\" height=\"1532\" data-path=\"langsmith/images/tutorial-comparison-view.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=92dca1af013a79d9ce2ee944a17e23a9 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=16ac3bc225307b5408a49c225646a99e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=517037b9b2a372dd5d4b2dc4e41eac6a 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d1b1d90b0838a53d1c693617fad61eb4 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ecbe835c06c9451cac57d7cf0a16d0b9 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=77d1da832c66587699653956fc15ccb6 2500w\" />\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-backtests-new-agent.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 16354
}