{
  "title": "Build a custom SQL agent",
  "source_url": "https://docs.langchain.com/oss/python/langgraph/sql-agent",
  "content": "In this tutorial we will build a custom agent that can answer questions about a SQL database using LangGraph.\n\nLangChain offers built-in [agent](/oss/python/langchain/agents) implementations, implemented using [LangGraph](/oss/python/langgraph/overview) primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a SQL agent. You can find a tutorial building a SQL agent using higher-level LangChain abstractions [here](/oss/python/langchain/sql-agent).\n\n<Warning>\n  Building Q\\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\n</Warning>\n\nThe [prebuilt agent](/oss/python/langchain/sql-agent) lets us get started quickly, but we relied on the system prompt to constrain its behaviorâ€” for example, we instructed the agent to always start with the \"list tables\" tool, and to always run a query-checker tool before executing the query.\n\nWe can enforce a higher degree of control in LangGraph by customizing the agent. Here, we implement a simple ReAct-agent setup, with dedicated nodes for specific tool-calls. We will use the same \\[state] as the pre-built agent.\n\n### Concepts\n\nWe will cover the following concepts:\n\n* [Tools](/oss/python/langchain/tools) for reading from SQL databases\n* The LangGraph [Graph API](/oss/python/langgraph/graph-api), including state, nodes, edges, and conditional edges.\n* [Human-in-the-loop](/oss/python/langgraph/interrupts) processes\n\n## Setup\n\n### Installation\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain  langgraph  langchain-community\n  ```\n</CodeGroup>\n\n### LangSmith\n\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:\n\n```shell  theme={null}\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n## 1. Select an LLM\n\nSelect a model that supports [tool-calling](/oss/python/integrations/providers/overview):\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    ðŸ‘‰ Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    ðŸ‘‰ Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    ðŸ‘‰ Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    ðŸ‘‰ Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"AWS Bedrock\">\n    ðŸ‘‰ Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    </CodeGroup>\n\n    <Tab title=\"HuggingFace\">\n      ðŸ‘‰ Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)\n\n      ```shell  theme={null}\n      pip install -U \"langchain[huggingface]\"\n      ```\n\n      <CodeGroup>\n        ```python init_chat_model theme={null}\n        import os\n        from langchain.chat_models import init_chat_model\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        model = init_chat_model(\n            \"microsoft/Phi-3-mini-4k-instruct\",\n            model_provider=\"huggingface\",\n            temperature=0.7,\n            max_tokens=1024,\n        )\n        ```\n\n        ```python Model Class theme={null}\n        import os\n        from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        llm = HuggingFaceEndpoint(\n            repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n            temperature=0.7,\n            max_length=1024,\n        )\n        model = ChatHuggingFace(llm=llm)\n        ```\n      </CodeGroup>\n    </Tab>\n  </Tab>\n</Tabs>\n\nThe output shown in the examples below used OpenAI.\n\n## 2. Configure the database\n\nYou will be creating a [SQLite database](https://www.sqlitetutorial.net/sqlite-sample-database/) for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store.\n\nFor convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket.\n\n```python  theme={null}\nimport requests, pathlib\n\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\nlocal_path = pathlib.Path(\"Chinook.db\")\n\nif local_path.exists():\n    print(f\"{local_path} already exists, skipping download.\")\nelse:\n    response = requests.get(url)\n    if response.status_code == 200:\n        local_path.write_bytes(response.content)\n        print(f\"File downloaded and saved as {local_path}\")\n    else:\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\n```\n\nWe will use a handy SQL database wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```python  theme={null}\nfrom langchain_community.utilities import SQLDatabase\n\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\nprint(f\"Dialect: {db.dialect}\")\nprint(f\"Available tables: {db.get_usable_table_names()}\")\nprint(f'Sample output: {db.run(\"SELECT * FROM Artist LIMIT 5;\")}')\n```\n\n```\nDialect: sqlite\nAvailable tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\nSample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\n```\n\n## 3. Add tools for database interactions\n\nUse the `SQLDatabase` wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```python  theme={null}\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\n\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\n\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(f\"{tool.name}: {tool.description}\\n\")\n```\n\n```\nsql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\n\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\n\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\n\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n```\n\n## 4. Define application steps\n\nWe construct dedicated nodes for the following steps:\n\n* Listing DB tables\n* Calling the \"get schema\" tool\n* Generating a query\n* Checking the query\n\nPutting these steps in dedicated nodes lets us (1) force tool-calls when needed, and (2) customize the prompts associated with each step.\n\n```python  theme={null}\nfrom typing import Literal\n\nfrom langchain.messages import AIMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import END, START, MessagesState, StateGraph\nfrom langgraph.prebuilt import ToolNode\n\n\nget_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\nget_schema_node = ToolNode([get_schema_tool], name=\"get_schema\")\n\nrun_query_tool = next(tool for tool in tools if tool.name == \"sql_db_query\")\nrun_query_node = ToolNode([run_query_tool], name=\"run_query\")\n\n\n# Example: create a predetermined tool call\ndef list_tables(state: MessagesState):\n    tool_call = {\n        \"name\": \"sql_db_list_tables\",\n        \"args\": {},\n        \"id\": \"abc123\",\n        \"type\": \"tool_call\",\n    }\n    tool_call_message = AIMessage(content=\"\", tool_calls=[tool_call])\n\n    list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n    tool_message = list_tables_tool.invoke(tool_call)\n    response = AIMessage(f\"Available tables: {tool_message.content}\")\n\n    return {\"messages\": [tool_call_message, tool_message, response]}\n\n\n# Example: force a model to create a tool call\ndef call_get_schema(state: MessagesState):\n    # Note that LangChain enforces that all models accept `tool_choice=\"any\"`\n    # as well as `tool_choice=<string name of tool>`.\n    llm_with_tools = model.bind_tools([get_schema_tool], tool_choice=\"any\")\n    response = llm_with_tools.invoke(state[\"messages\"])\n\n    return {\"messages\": [response]}\n\n\ngenerate_query_system_prompt = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n\n\ndef generate_query(state: MessagesState):\n    system_message = {\n        \"role\": \"system\",\n        \"content\": generate_query_system_prompt,\n    }\n    # We do not force a tool call here, to allow the model to\n    # respond naturally when it obtains the solution.\n    llm_with_tools = model.bind_tools([run_query_tool])\n    response = llm_with_tools.invoke([system_message] + state[\"messages\"])\n\n    return {\"messages\": [response]}\n\n\ncheck_query_system_prompt = \"\"\"\nYou are a SQL expert with a strong attention to detail.\nDouble check the {dialect} query for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\n\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes,\njust reproduce the original query.\n\nYou will call the appropriate tool to execute the query after running this check.\n\"\"\".format(dialect=db.dialect)\n\n\ndef check_query(state: MessagesState):\n    system_message = {\n        \"role\": \"system\",\n        \"content\": check_query_system_prompt,\n    }\n\n    # Generate an artificial user message to check\n    tool_call = state[\"messages\"][-1].tool_calls[0]\n    user_message = {\"role\": \"user\", \"content\": tool_call[\"args\"][\"query\"]}\n    llm_with_tools = model.bind_tools([run_query_tool], tool_choice=\"any\")\n    response = llm_with_tools.invoke([system_message, user_message])\n    response.id = state[\"messages\"][-1].id\n\n    return {\"messages\": [response]}\n```\n\n## 5. Implement the agent\n\nWe can now assemble these steps into a workflow using the [Graph API](/oss/python/langgraph/graph-api). We define a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) at the query generation step that will route to the query checker if a query is generated, or end if there are no tool calls present, such that the LLM has delivered a response to the query.\n\n```python  theme={null}\ndef should_continue(state: MessagesState) -> Literal[END, \"check_query\"]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if not last_message.tool_calls:\n        return END\n    else:\n        return \"check_query\"\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(list_tables)\nbuilder.add_node(call_get_schema)\nbuilder.add_node(get_schema_node, \"get_schema\")\nbuilder.add_node(generate_query)\nbuilder.add_node(check_query)\nbuilder.add_node(run_query_node, \"run_query\")\n\nbuilder.add_edge(START, \"list_tables\")\nbuilder.add_edge(\"list_tables\", \"call_get_schema\")\nbuilder.add_edge(\"call_get_schema\", \"get_schema\")\nbuilder.add_edge(\"get_schema\", \"generate_query\")\nbuilder.add_conditional_edges(\n    \"generate_query\",\n    should_continue,\n)\nbuilder.add_edge(\"check_query\", \"run_query\")\nbuilder.add_edge(\"run_query\", \"generate_query\")\n\nagent = builder.compile()\n```\n\nWe visualize the application below:\n\n```python  theme={null}\nfrom IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\ndisplay(Image(agent.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=1ddd4aae369fb8c143edaccb0a09c81f\" alt=\"SQL agent graph\" style={{ height: \"800px\" }} data-og-width=\"308\" width=\"308\" data-og-height=\"645\" height=\"645\" data-path=\"oss/images/sql-agent-langgraph.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=280&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=e5d3e67f17d65e438370f7d771e3ba7d 280w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=560&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=dbcb80fdb2d00a6dc33dc90f05d100b5 560w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=840&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=72be69a1e7ac39afad3d0aa03ecffffa 840w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=1100&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=5ad351b8b6641defe17882f5e102cab0 1100w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=1650&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=8a5cefc8ac6938d0b4b0946e0522ffaa 1650w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=2500&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=0b5b7711b4b2ece3a3ccb10a2b012166 2500w\" />\n\nWe can now invoke the graph:\n\n```python  theme={null}\nquestion = \"Which genre on average has the longest tracks?\"\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================ Human Message =================================\n\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\n\nAvailable tables: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_schema (call_yzje0tj7JK3TEzDx4QnRR3lL)\n Call ID: call_yzje0tj7JK3TEzDx4QnRR3lL\n  Args:\n    table_names: Genre, Track\n================================= Tool Message =================================\nName: sql_db_schema\n\n\nCREATE TABLE \"Genre\" (\n\t\"GenreId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(120),\n\tPRIMARY KEY (\"GenreId\")\n)\n\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\n\n\nCREATE TABLE \"Track\" (\n\t\"TrackId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(200) NOT NULL,\n\t\"AlbumId\" INTEGER,\n\t\"MediaTypeId\" INTEGER NOT NULL,\n\t\"GenreId\" INTEGER,\n\t\"Composer\" NVARCHAR(220),\n\t\"Milliseconds\" INTEGER NOT NULL,\n\t\"Bytes\" INTEGER,\n\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\n\tPRIMARY KEY (\"TrackId\"),\n\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\n\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\n\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_cb9ApLfZLSq7CWg6jd0im90b)\n Call ID: call_cb9ApLfZLSq7CWg6jd0im90b\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_DMVALfnQ4kJsuF3Yl6jxbeAU)\n Call ID: call_DMVALfnQ4kJsuF3Yl6jxbeAU\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nThe genre with the longest tracks on average is \"Sci Fi & Fantasy,\" with an average track length of approximately 2,911,783 milliseconds. Other genres with relatively long tracks include \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\n```\n\n<Tip>\n  See [LangSmith trace](https://smith.langchain.com/public/94b8c9ac-12f7-4692-8706-836a1f30f1ea/r) for the above run.\n</Tip>\n\n## 6. Implement human-in-the-loop review\n\nIt can be prudent to check the agent's SQL queries before they are executed for any unintended actions or inefficiencies.\n\nHere we leverage LangGraph's [human-in-the-loop](/oss/python/langgraph/interrupts) features to pause the run before executing a SQL query and wait for human review. Using LangGraph's [persistence layer](/oss/python/langgraph/persistence), we can pause the run indefinitely (or at least as long as the persistence layer is alive).\n\nLet's wrap the `sql_db_query` tool in a node that receives human input. We can implement this using the [interrupt](/oss/python/langgraph/interrupts) function. Below, we allow for input to approve the tool call, edit its arguments, or provide user feedback.\n\n```python  theme={null}\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.tools import tool\nfrom langgraph.types import interrupt\n\n@tool(\n    run_query_tool.name,\n    description=run_query_tool.description,\n    args_schema=run_query_tool.args_schema\n)\ndef run_query_tool_with_interrupt(config: RunnableConfig, **tool_input):\n    request = {\n        \"action\": run_query_tool.name,\n        \"args\": tool_input,\n        \"description\": \"Please review the tool call\"\n    }\n    response = interrupt([request]) # [!code highlight]\n    # approve the tool call\n    if response[\"type\"] == \"accept\":\n        tool_response = run_query_tool.invoke(tool_input, config)\n    # update tool call args\n    elif response[\"type\"] == \"edit\":\n        tool_input = response[\"args\"][\"args\"]\n        tool_response = run_query_tool.invoke(tool_input, config)\n    # respond to the LLM with user feedback\n    elif response[\"type\"] == \"response\":\n        user_feedback = response[\"args\"]\n        tool_response = user_feedback\n    else:\n        raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n\n    return tool_response\n\n# Redefine the tool node to use the interrupt version\nrun_query_node = ToolNode([run_query_tool_with_interrupt], name=\"run_query\") # [!code highlight]\n```\n\n<Note>\n  The above implementation follows the [tool interrupt example](/oss/python/langgraph/interrupts#configuring-interrupts) in the broader [human-in-the-loop](/oss/python/langgraph/interrupts) guide. Refer to that guide for details and alternatives.\n</Note>\n\nLet's now re-assemble our graph. We will replace the programmatic check with human review. Note that we now include a [checkpointer](/oss/python/langgraph/persistence); this is required to pause and resume the run.\n\n```python  theme={null}\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ndef should_continue(state: MessagesState) -> Literal[END, \"run_query\"]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if not last_message.tool_calls:\n        return END\n    else:\n        return \"run_query\"\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(list_tables)\nbuilder.add_node(call_get_schema)\nbuilder.add_node(get_schema_node, \"get_schema\")\nbuilder.add_node(generate_query)\nbuilder.add_node(run_query_node, \"run_query\")\n\nbuilder.add_edge(START, \"list_tables\")\nbuilder.add_edge(\"list_tables\", \"call_get_schema\")\nbuilder.add_edge(\"call_get_schema\", \"get_schema\")\nbuilder.add_edge(\"get_schema\", \"generate_query\")\nbuilder.add_conditional_edges(\n    \"generate_query\",\n    should_continue,\n)\nbuilder.add_edge(\"run_query\", \"generate_query\")\n\ncheckpointer = InMemorySaver() # [!code highlight]\nagent = builder.compile(checkpointer=checkpointer) # [!code highlight]\n```\n\nWe can invoke the graph as before. This time, execution is interrupted:\n\n```python  theme={null}\nimport json\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nquestion = \"Which genre on average has the longest tracks?\"\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    config,\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step:\n        action = step[\"__interrupt__\"][0]\n        print(\"INTERRUPTED:\")\n        for request in action.value:\n            print(json.dumps(request, indent=2))\n    else:\n        pass\n```\n\n```\n...\n\nINTERRUPTED:\n{\n  \"action\": \"sql_db_query\",\n  \"args\": {\n    \"query\": \"SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;\"\n  },\n  \"description\": \"Please review the tool call\"\n}\n```\n\nWe can accept or edit the tool call using [Command](/oss/python/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command):\n\n```python  theme={null}\nfrom langgraph.types import Command\n\n\nfor step in agent.stream(\n    Command(resume={\"type\": \"accept\"}),\n    # Command(resume={\"type\": \"edit\", \"args\": {\"query\": \"...\"}}),\n    config,\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step:\n        action = step[\"__interrupt__\"][0]\n        print(\"INTERRUPTED:\")\n        for request in action.value:\n            print(json.dumps(request, indent=2))\n    else:\n        pass\n```\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_t4yXkD6shwdTPuelXEmY3sAY)\n Call ID: call_t4yXkD6shwdTPuelXEmY3sAY\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average length of about 2,911,783 milliseconds. Other genres with long average track lengths include \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\n```\n\nRefer to the [human-in-the-loop guide](/oss/python/langgraph/interrupts) for details.\n\n## Next steps\n\nCheck out the [Evaluate a graph](/langsmith/evaluate-graph) guide for evaluating LangGraph applications, including SQL agents like this one, using LangSmith.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/sql-agent.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 27975
}