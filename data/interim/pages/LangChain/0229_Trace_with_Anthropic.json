{
  "title": "Trace with Anthropic",
  "source_url": "https://docs.langchain.com/langsmith/trace-anthropic",
  "content": "The `wrap_anthropic` methods in Python allows you to wrap your Anthropic client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. The wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.\n\n<Note>\n  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_anthropic`. This allows you to toggle tracing on and off without changing your code.\n\n  Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).\n\n  If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.\n\n  By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).\n</Note>\n\n```python  theme={null}\nimport anthropic\nfrom langsmith import traceable\nfrom langsmith.wrappers import wrap_anthropic\n\nclient = wrap_anthropic(anthropic.Anthropic())\n\n# You can also wrap the async client as well\n# async_client = wrap_anthropic(anthropic.AsyncAnthropic())\n\n@traceable(run_type=\"tool\", name=\"Retrieve Context\")\ndef my_tool(question: str) -> str:\n    return \"During this morning's meeting, we solved all world conflict.\"\n\n@traceable(name=\"Chat Pipeline\")\ndef chat_pipeline(question: str):\n    context = my_tool(question)\n    messages = [\n        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n    ]\n    messages = client.messages.create(\n      model=\"claude-sonnet-4-5-20250929\",\n      messages=messages,\n      max_tokens=1024,\n      system=\"You are a helpful assistant. Please respond to the user's request only based on the given context.\"\n    )\n    return messages\n\nchat_pipeline(\"Can you summarize this morning's meetings?\")\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-anthropic.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 2452
}