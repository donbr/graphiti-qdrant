{
  "title": "Configure LangSmith Agent Server for scale",
  "source_url": "https://docs.langchain.com/langsmith/agent-server-scale",
  "content": "The default configuration for LangSmith Agent Server is designed to handle substantial read and write load across a variety of different workloads. By following the best practices outlined below, you can tune your Agent Server to perform optimally for your specific workload. This page describes scaling considerations for the Agent Server and provides examples to help configure your deployment.\n\nFor some example self-hosted configurations, refer to the [Example Agent Server configurations for scale](#example-agent-server-configurations-for-scale) section.\n\n## Scaling for write load\n\nWrite load is primarily driven by the following factors:\n\n* Creation of new [runs](/langsmith/background-run)\n* Creation of new checkpoints during run execution\n* Writing to long term memory\n* Creation of new [threads](/langsmith/use-threads)\n* Creation of new [assistants](/langsmith/assistants)\n* Deletion of runs, checkpoints, threads, assistants and cron jobs\n\nThe following components are primarily responsible for handling write load:\n\n* API server: Handles initial request and persistence of data to the database.\n* Queue worker: Handles the execution of runs.\n* Redis: Handles the storage of ephemeral data about on-going runs.\n* Postgres: Handles the storage of all data, including run, thread, assistant, cron job, checkpointing and long term memory.\n\n### Best practices for scaling the write path\n\n#### Change `N_JOBS_PER_WORKER` based on assistant characteristics\n\nThe default value of [`N_JOBS_PER_WORKER`](/langsmith/env-var#n-jobs-per-worker) is 10. You can change this value to scale the maximum number of runs that can be executed at a time by a single queue worker based on the characteristics of your assistant.\n\nSome general guidelines for changing `N_JOBS_PER_WORKER`:\n\n* If your assistant is CPU bounded, the default value of 10 is likely sufficient. You might lower `N_JOBS_PER_WORKER` if you notice excessive CPU usage on queue workers or delays in run execution.\n* If your assistant is IO bounded, increase `N_JOBS_PER_WORKER` to handle more concurrent runs per worker.\n\nThere is no upper limit to `N_JOBS_PER_WORKER`. However, queue workers are greedy when fetching new runs, which means they will try to pick up as many runs as they have available jobs and begin executing them immediately. Setting `N_JOBS_PER_WORKER` too high in environments with bursty traffic can lead to uneven worker utilization and increased run execution times.\n\n#### Avoid synchronous blocking operations\n\nAvoid synchronous blocking operations in your code and prefer asynchronous operations. Long synchronous operations can block the main event loop, causing longer request and run execution times and potential timeouts.\n\nFor example, consider an application that needs to sleep for 1 second. Instead of using synchronous code like this:\n\n```python  theme={null}\nimport time\n\ndef my_function():\n    time.sleep(1)\n```\n\nPrefer asynchronous code like this:\n\n```python  theme={null}\nimport asyncio\n\nasync def my_function():\n    await asyncio.sleep(1)\n```\n\nIf an assistant requires synchronous blocking operations, set [`BG_JOB_ISOLATED_LOOPS`](/langsmith/env-var#bg-job-isolated-loops) to `True` to execute each run in a separate event loop.\n\n#### Minimize redundant checkpointing\n\nMinimize redundant checkpointing by setting [`durability`](/oss/python/langgraph/durable-execution#durability-modes) to the minimum value necessary to ensure your data is durable.\n\nThe default durability mode is `\"async\", meaning checkpoints are written after each step asynchronously. If an assistant needs to persist only the final state of the run, `durability`can be set to`\"exit\"\\`, storing only the final state of the run. This can be set when creating the run:\n\n```python  theme={null}\nfrom langgraph_sdk import get_client\n\nclient = get_client(url=<DEPLOYMENT_URL>)\nthread = await client.threads.create()\nrun = await client.runs.create(\n    thread_id=thread[\"thread_id\"],\n    assistant_id=\"agent\",\n    durability=\"exit\"\n)\n```\n\n#### Self-hosted\n\n<Note>\n  These settings are only required for [self-hosted](/langsmith/self-hosted) deployments. By default, [cloud](/langsmith/cloud) deployments already have these best practices enabled.\n</Note>\n\n##### Enable the use of queue workers\n\nBy default, the API server manages the queue and does not use queue workers. You can enable the use of queue workers by setting the `queue.enabled` configuration to `true`.\n\n```yaml  theme={null}\nqueue:\n  enabled: true\n```\n\nThis will allow the API server to offload the queue management to the queue workers, significantly reducing the load on the API server and allowing it to focus on handling requests.\n\n##### Support a number of jobs equal to expected throughput\n\nThe more runs you execute in parallel, the more jobs you will need to handle the load. There are two main parameters to scale the available jobs:\n\n* `number_of_queue_workers`: The number of queue workers provisioned.\n* `N_JOBS_PER_WORKER`: The number of runs that a single queue work can execute at a time. Defaults to 10.\n\nYou can calculate the available jobs with the following equation:\n\n```\navailable_jobs = number_of_queue_workers * `N_JOBS_PER_WORKER`\n```\n\nThroughput is then the number of runs that can be executed per second by the available jobs:\n\n```\nthroughput_per_second = available_jobs / average_run_execution_time_seconds\n```\n\nTherefore, the minimum number of queue workers you should provision to support your expected steady state throughput is:\n\n```\nnumber_of_queue_workers = throughput_per_second * average_run_execution_time_seconds / `N_JOBS_PER_WORKER`\n```\n\n##### Configure autoscaling for bursty workloads\n\nAutoscaling is disabled by default, but should be configured for bursty workloads. Using the same calculations as the [previous section](#support-a-number-of-jobs-equal-to-expected-throughput), you can determine the maximum number of queue workers you should allow the autoscaler to scale to based on maximum expected throughput.\n\n## Scaling for read load\n\nRead load is primarily driven by the following factors:\n\n* Getting the results of a [run](/langsmith/background-run)\n* Getting the state of a [thread](/langsmith/use-threads)\n* Searching for [runs](/langsmith/background-run), [threads](/langsmith/use-threads), [cron jobs](/langsmith/cron-jobs) and [assistants](/langsmith/assistants)\n* Retrieving checkpoints and long term memory\n\nThe following components are primarily responsible for handling read load:\n\n* API server: Handles the request and direct retrieval of data from the database.\n* Postgres: Handles the storage of all data, including run, thread, assistant, cron job, checkpointing and long term memory.\n* Redis: Handles the storage of ephemeral data about on-going runs, including streaming messages from queue workers to api servers.\n\n### Best practices for scaling the read path\n\n#### Use filtering to reduce the number of resources returned per request\n\n[Agent Server](/langsmith/agent-server) provides a search API for each resource type. These APIs implement pagination by default and offer many filtering options. Use filtering to reduce the number of resources returned per request and improve performance.\n\n#### Set a TTLs to automatically delete old data\n\nSet a [TTL on threads](/langsmith/configure-ttl) to automatically clean up old data. Runs and checkpoints are automatically deleted when the associated thread is deleted.\n\n#### Avoid polling and use /join to monitor the state of a run\n\nAvoid polling the state of a run by using the `/join` API endpoint. This method returns the final state of the run once the run is complete.\n\nIf you need to monitor the output of a run in real-time, use the `/stream` API endpoint. This method streams the run output including the final state of the run.\n\n#### Self-hosted\n\n<Note>\n  These settings are only required for [self-hosted](/langsmith/self-hosted) deployments. By default, [cloud](/langsmith/cloud) deployments already have these best practices enabled.\n</Note>\n\n##### Configure autoscaling for bursty workloads\n\nAutoscaling is disabled by default, but should be configured for bursty workloads. You can determine the maximum number of api servers you should allow the autoscaler to scale to based on maximum expected throughput. The default for [cloud](/langsmith/cloud) deployments is a maximum of 10 API servers.\n\n## Example self-hosted Agent Server configurations\n\n<Note>\n  The exact optimal configuration depends on your application complexity, request patterns, and data requirements. Use the following examples in combination with the information in the previous sections and your specific usage to update your deployment configuration as needed. If you have any questions, reach out to the LangChain team at [support@langchain.dev](mailto:support@langchain.dev).\n</Note>\n\nThe following table provides an overview comparing different LangSmith Agent Server configurations for various load patterns (read requests per second / write requests per second) and standard assistant characteristics (average run execution time of 1 second, moderate CPU and memory usage):\n\n|                                                                                                                          | **[Low / low](#low-reads-low-writes)** | **[Low / high](#low-reads-high-writes)** | **[High / low](#high-reads-low-writes)** | [Medium / medium](#medium-reads-medium-writes) | [High / high](#high-reads-high-writes) |\n| :----------------------------------------------------------------------------------------------------------------------- | :------------------------------------- | :--------------------------------------- | :--------------------------------------- | :--------------------------------------------- | :------------------------------------- |\n| <Tooltip tip=\"Number of write requests being processed by the deployment per second\">Write requests per second</Tooltip> | 5                                      | 5                                        | 500                                      | 50                                             | 500                                    |\n| <Tooltip tip=\"Number of read requests being processed by the deployment per second\">Read requests per second</Tooltip>   | 5                                      | 500                                      | 5                                        | 50                                             | 500                                    |\n| **API servers**<br />(1 CPU, 2Gi per server)                                                                             | 1 (default)                            | 6                                        | 10                                       | 3                                              | 15                                     |\n| **Queue workers**<br />(1 CPU, 2Gi per worker)                                                                           | 1 (default)                            | 10                                       | 1 (default)                              | 5                                              | 10                                     |\n| **`N_JOBS_PER_WORKER`**                                                                                                  | 10 (default)                           | 50                                       | 10                                       | 10                                             | 50                                     |\n| **Redis resources**                                                                                                      | 2 Gi (default)                         | 2 Gi (default)                           | 2 Gi (default)                           | 2 Gi (default)                                 | 2 Gi (default)                         |\n| **Postgres resources**                                                                                                   | 2 CPU<br />8 Gi (default)              | 4 CPU<br />16 Gi memory                  | 4 CPU<br />16 Gi                         | 4 CPU<br />16 Gi memory                        | 8 CPU<br />32 Gi memory                |\n\nThe following sample configurations enable each of these setups. Load levels are defined as:\n\n* Low means approximately 5 requests per second\n* Medium means approximately 50 requests per second\n* High means approximately 500 requests per second\n\n### Low reads, low writes <a name=\"low-reads-low-writes\" />\n\nThe default [LangSmith Deployment](/langsmith/deployments) configuration will handle this load. No custom resource configuration is needed here.\n\n### Low reads, high writes <a name=\"low-reads-high-writes\" />\n\nYou have a high volume of write requests (500 per second) being processed by your deployment, but relatively few read requests (5 per second).\n\nFor this, we recommend a configuration like this:\n\n```yaml  theme={null}\n# Example configuration for low reads, high writes (5 read/500 write requests per second)\napi:\n  replicas: 6\n  resources:\n    requests:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    limits:\n      cpu: \"2\"\n      memory: \"4Gi\"\n\nqueue:\n  replicas: 10\n  resources:\n    requests:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    limits:\n      cpu: \"2\"\n      memory: \"4Gi\"\n\nconfig:\n  numberOfJobsPerWorker: 50\n\nredis:\n  resources:\n    requests:\n      memory: \"2Gi\"\n    limits:\n      memory: \"2Gi\"\n\npostgres:\n  resources:\n    requests:\n      cpu: \"4\"\n      memory: \"16Gi\"\n    limits:\n      cpu: \"8\"\n      memory: \"32Gi\"\n```\n\n### High reads, low writes <a name=\"high-reads-low-writes\" />\n\nYou have a high volume of read requests (500 per second) but relatively few write requests (5 per second).\n\nFor this, we recommend a configuration like this:\n\n```yaml  theme={null}\n# Example configuration for high reads, low writes (500 read/5 write requests per second)\napi:\n  replicas: 10\n  resources:\n    requests:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    limits:\n      cpu: \"2\"\n      memory: \"4Gi\"\n\nqueue:\n  replicas: 1  # Default, minimal write load\n  resources:\n    requests:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    limits:\n      cpu: \"2\"\n      memory: \"4Gi\"\n\nredis:\n  resources:\n    requests:\n      memory: \"2Gi\"\n    limits:\n      memory: \"2Gi\"\n\npostgres:\n  resources:\n    requests:\n      cpu: \"4\"\n      memory: \"16Gi\"\n    limits:\n      cpu: \"8\"\n      memory: \"32Gi\"\n  # Consider read replicas for high read scenarios\n  readReplicas: 2\n```\n\n### Medium reads, medium writes <a name=\"medium-reads-medium-writes\" />\n\nThis is a balanced configuration that should handle moderate read and write loads (50 read/50 write requests per second).\n\nFor this, we recommend a configuration like this:\n\n```yaml  theme={null}\n# Example configuration for medium reads, medium writes (50 read/50 write requests per second)\napi:\n  replicas: 3\n  resources:\n    requests:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    limits:\n      cpu: \"2\"\n      memory: \"4Gi\"\n\nqueue:\n  replicas: 5\n  resources:\n    requests:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    limits:\n      cpu: \"2\"\n      memory: \"4Gi\"\n\nredis:\n  resources:\n    requests:\n      memory: \"2Gi\"\n    limits:\n      memory: \"2Gi\"\n\npostgres:\n  resources:\n    requests:\n      cpu: \"4\"\n      memory: \"16Gi\"\n    limits:\n      cpu: \"8\"\n      memory: \"32Gi\"\n```\n\n### High reads, high writes <a name=\"high-reads-high-writes\" />\n\nYou have high volumes of both read and write requests (500 read/500 write requests per second).\n\nFor this, we recommend a configuration like this:\n\n```yaml  theme={null}\n# Example configuration for high reads, high writes (500 read/500 write requests per second)\napi:\n  replicas: 15\n  resources:\n    requests:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    limits:\n      cpu: \"2\"\n      memory: \"4Gi\"\n\nqueue:\n  replicas: 10\n  resources:\n    requests:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    limits:\n      cpu: \"2\"\n      memory: \"4Gi\"\n\nconfig:\n  numberOfJobsPerWorker: 50\n\nredis:\n  resources:\n    requests:\n      memory: \"2Gi\"\n    limits:\n      memory: \"2Gi\"\n\npostgres:\n  resources:\n    requests:\n      cpu: \"8\"\n      memory: \"32Gi\"\n    limits:\n      cpu: \"16\"\n      memory: \"64Gi\"\n```\n\n### Autoscaling\n\nIf your deployment experiences bursty traffic, you can enable autoscaling to scale the number of API servers and queue workers to handle the load.\n\nHere is a sample configuration for autoscaling for high reads and high writes:\n\n```yaml  theme={null}\napi:\n  autoscaling:\n    enabled: true\n    minReplicas: 15\n    maxReplicas: 25\n\nqueue:\n  autoscaling:\n    enabled: true\n    minReplicas: 10\n    maxReplicas: 20\n```\n\n<Note>\n  Ensure that your deployment environment has sufficient resources to scale to the recommended size. Monitor your applications and infrastructure to ensure optimal performance. Consider implementing monitoring and alerting to track resource usage and application performance.\n</Note>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-server-scale.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 17193
}