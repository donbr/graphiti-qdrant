{
  "title": "Export LangSmith telemetry to your observability backend",
  "source_url": "https://docs.langchain.com/langsmith/export-backend",
  "content": "<Warning>\n  **This section is only applicable for Kubernetes deployments.**\n</Warning>\n\nSelf-Hosted LangSmith instances produce telemetry data in the form of logs, metrics and traces. This section will show you how to access and export that data to an observability collector or backend.\n\nThis section assumes that you have monitoring infrastructure set up already, or you will set up this infrastructure and want to know how to configure LangSmith to collect data from it.\n\nInfrastructure refers to:\n\n* Collectors, such as [OpenTelemetry](https://opentelemetry.io/docs/collector/), [FluentBit](https://docs.fluentbit.io/manual) or [Prometheus](https://prometheus.io/).\n* Observability backends, such as [Datadog](https://www.datadoghq.com/) or the [Grafana](https://grafana.com/) ecosystem.\n\n# Logs: [OTel Example](/langsmith/langsmith-collector#logs)\n\nAll services that are part of the LangSmith self-hosted deployment write logs to their node's filesystem and to stdout. In order to access these logs, you need to set up your collector to read from either the filesystem or stdout. Most popular collectors support reading logs from filesystems.\n\n* **OpenTelemetry**: [File Log Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver)\n* **FluentBit**: [Tail Input](https://docs.fluentbit.io/manual/pipeline/inputs/tail)\n* **Datadog**: [Kubernetes Log Collection](https://docs.datadoghq.com/containers/kubernetes/log/?tab=datadogoperator)\n\n# Metrics: [OTel Example](/langsmith/langsmith-collector#metrics)\n\n## LangSmith Services\n\nThe following LangSmith services expose metrics at an endpoint, in the Prometheus metrics format. The frontend does not currently expose metrics.\n\n* **Backend**: `http://<langsmith_release_name>-backend.<namespace>.svc.cluster.local:1984/metrics`\n* **Platform Backend**: `http://<langsmith_release_name>-platform-backend.<namespace>.svc.cluster.local:1986/metrics`\n* **Playground**: `http://<langsmith_release_name>-playground.<namespace>.svc.cluster.local:1988/metrics`\n* **(LangSmith Control Plane only) Host Backend**: `http://<langsmith_release_name>-host-backend.<namespace>.svc.cluster.local:1985/metrics`\n\nYou can use a [Prometheus](https://prometheus.io/docs/prometheus/latest/getting_started/#configure-prometheus-to-monitor-the-sample-targets) or [OpenTelemetry](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver) collector to scrape the endpoints, and export metrics to the backend of your choice.\n\n## Frontend Nginx\n\nThe frontend service exposes its Nginx metrics at the following endpoint: `langsmith-frontend.langsmith.svc.cluster.local:80/nginx_status`. You can either scrape them yourself, or bring up a Prometheus Nginx exporter using the [LangSmith Observability Helm Chart](/langsmith/observability-stack)\n\n<Warning>\n  **The following sections apply for in-cluster databases only. If you are using external databases, you will need to configure exposing and fetching metrics.**\n</Warning>\n\n## Postgres + Redis\n\nIf you are using in-cluster Postgres/Redis instances, you can use a Prometheus exporter to expose metrics from your instance. You can deploy your own, or if you would like, you can use the [LangSmith Observability Helm Chart](/langsmith/observability-stack) to deploy an exporter for you.\n\n## Clickhouse\n\nThe in-cluster Clickhouse is configured to expose metrics without the need for an exporter. You can use your collector to scrape metrics at `http://<langsmith_release_name>-clickhouse.<namespace>.svc.cluster.local:9363/metrics`\n\n# Traces: [OTel Example](/langsmith/langsmith-collector#traces)\n\nThe LangSmith Backend, Platform Backend, Playground and LangSmith Queue deployments have been instrumented to emit [Otel](https://opentelemetry.io/do/langsmith/observability-concepts/signals/traces/) traces. Tracing is toggled off by default, and can be enabled for all LangSmith services with the following in your `langsmith_config.yaml` (or equivalent) file:\n\n```yaml  theme={null}\nconfig:\n  tracing:\n    enabled: true\n    endpoint: \"<your_collector_endpoint>\"\n    useTls: true # / false\n    env: \"ls_self_hosted\" # This value will be set as an \"env\" attribute in your spans\n    exporter: \"http\" # must be either http or grpc\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/export-backend.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 4658
}