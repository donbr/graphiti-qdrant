{
  "title": "Trace with LangChain (Python and JS/TS)",
  "source_url": "https://docs.langchain.com/langsmith/trace-with-langchain",
  "content": "LangSmith integrates seamlessly with LangChain (Python and JavaScript), the popular open-source framework for building LLM applications.\n\n## Installation\n\nInstall the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).\n\nFor a full list of packages available, see the [LangChain docs](/oss/python/integrations/providers/overview).\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain_openai langchain_core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/openai @langchain/core\n  ```\n\n  ```bash npm theme={null}\n  npm install @langchain/openai @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/openai @langchain/core\n  ```\n</CodeGroup>\n\n## Quick start\n\n### 1. Configure your environment\n\n```bash wrap theme={null}\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n# This example uses OpenAI, but you can use any LLM provider of choice\nexport OPENAI_API_KEY=<your-openai-api-key>\n# For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.\nexport LANGSMITH_WORKSPACE_ID=<your-workspace-id>\n```\n\n<Info>\n  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`\n\n  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`\n</Info>\n\n### 2. Log a trace\n\nNo extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain_openai import ChatOpenAI\n  from langchain_core.prompts import ChatPromptTemplate\n  from langchain_core.output_parsers import StrOutputParser\n\n  prompt = ChatPromptTemplate.from_messages([\n      (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),\n      (\"user\", \"Question: {question}\\nContext: {context}\")\n  ])\n\n  model = ChatOpenAI(model=\"gpt-4o-mini\")\n  output_parser = StrOutputParser()\n  chain = prompt | model | output_parser\n\n  question = \"Can you summarize this morning's meetings?\"\n  context = \"During this morning's meeting, we solved all world conflict.\"\n\n  chain.invoke({\"question\": question, \"context\": context})\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n  import { StringOutputParser } from \"@langchain/core/output_parsers\";\n\n  const prompt = ChatPromptTemplate.fromMessages([\n    [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],\n    [\"user\", \"Question: {question}\\nContext: {context}\"],\n  ]);\n\n  const model = new ChatOpenAI({ modelName: \"gpt-4o-mini\" });\n  const outputParser = new StringOutputParser();\n  const chain = prompt.pipe(model).pipe(outputParser);\n\n  const question = \"Can you summarize this morning's meetings?\"\n  const context = \"During this morning's meeting, we solved all world conflict.\"\n\n  await chain.invoke({ question: question, context: context });\n  ```\n</CodeGroup>\n\n### 3. View your trace\n\nBy default, the trace will be logged to the project with the name `default`. An example of a trace logged using the above code is made public and can be viewed [here](https://smith.langchain.com/public/e6a46eb2-d785-4804-a1e3-23f167a04300/r).\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a116791e1efb842afdde7b478309b79f\" alt=\"\" data-og-width=\"1382\" width=\"1382\" data-og-height=\"557\" height=\"557\" data-path=\"langsmith/images/langchain-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ebfccf4a9da8876f8beee530a1037555 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fee08e1117028d5082ef7cca4c304394 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6ac2c67d187c2cf922c5e033183bc566 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ad262f1d5eb9c2a7cf44f2d15560c286 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4b12c56205f13ae2a01be3824dd8f624 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b3f2763df61045f2db2618991a82775b 2500w\" />\n\n## Trace selectively\n\nThe [previous section](#quick-start) showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application.\n\nThere are two ways to do this in Python: by manually passing in a `LangChainTracer` instance as a [callback](https://reference.langchain.com/python/langchain_core/callbacks/), or by using the [`tracing_context` context manager](https://reference.langchain.com/python/langsmith/observability/sdk/run_helpers/#langsmith.run_helpers.tracing_context).\n\nIn JS/TS, you can pass a [`LangChainTracer`](https://reference.langchain.com/javascript/classes/_langchain_core.tracers_tracer_langchain.LangChainTracer.html) instance as a callback.\n\n<CodeGroup>\n  ```python Python theme={null}\n  # You can opt-in to specific invocations..\n  import langsmith as ls\n\n  with ls.tracing_context(enabled=True):\n      chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"})\n\n  # This will NOT be traced (assuming LANGSMITH_TRACING is not set)\n  chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})\n\n  # This would not be traced, even if LANGSMITH_TRACING=true\n  with ls.tracing_context(enabled=False):\n      chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})\n  ```\n\n  ```typescript TypeScript theme={null}\n  // You can configure a LangChainTracer instance to trace a specific invocation.\n  import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";\n\n  const tracer = new LangChainTracer();\n  await chain.invoke(\n    {\n      question: \"Am I using a callback?\",\n      context: \"I'm using a callback\"\n    },\n    { callbacks: [tracer] }\n  );\n  ```\n</CodeGroup>\n\n## Log to a specific project\n\n### Statically\n\nAs mentioned in the [tracing conceptual guide](/langsmith/observability-concepts) LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the `LANGSMITH_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\n\n```bash  theme={null}\nexport LANGSMITH_PROJECT=my-project\n```\n\n<Warning>\n  The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.\n</Warning>\n\n### Dynamically\n\nThis largely builds off of the [previous section](#trace-selectively) and allows you to set the project name for a specific `LangChainTracer` instance or as parameters to the `tracing_context` context manager in Python.\n\n<CodeGroup>\n  ```python Python theme={null}\n  # You can set the project name using the project_name parameter.\n  import langsmith as ls\n\n  with ls.tracing_context(project_name=\"My Project\", enabled=True):\n      chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})\n  ```\n\n  ```typescript TypeScript theme={null}\n  // You can set the project name for a specific tracer instance:\n  import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";\n\n  const tracer = new LangChainTracer({ projectName: \"My Project\" });\n  await chain.invoke(\n    {\n      question: \"Am I using a callback?\",\n      context: \"I'm using a callback\"\n    },\n    { callbacks: [tracer] }\n  );\n  ```\n</CodeGroup>\n\n## Add metadata and tags to traces\n\nYou can annotate your traces with arbitrary metadata and tags by providing them in the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/?h=runnablecon#langchain_core.runnables.RunnableConfig). This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it. For information on how to query traces and runs by metadata and tags, see [this guide](/langsmith/export-traces)\n\n<Note>\n  When you attach metadata or tags to a runnable (either through the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) or at runtime with invocation params), they are inherited by all child runnables of that runnable.\n</Note>\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain_openai import ChatOpenAI\n  from langchain_core.prompts import ChatPromptTemplate\n  from langchain_core.output_parsers import StrOutputParser\n\n  prompt = ChatPromptTemplate.from_messages([\n      (\"system\", \"You are a helpful AI.\"),\n      (\"user\", \"{input}\")\n  ])\n\n  # The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run only\n  chat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})\n  output_parser = StrOutputParser()\n\n  # Tags and metadata can be configured with RunnableConfig\n  chain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})\n\n  # Tags and metadata can also be passed at runtime\n  chain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n  import { StringOutputParser } from \"@langchain/core/output_parsers\";\n\n  const prompt = ChatPromptTemplate.fromMessages([\n      [\"system\", \"You are a helpful AI.\"],\n      [\"user\", \"{input}\"]\n  ])\n\n  // The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run only\n  const model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });\n  const outputParser = new StringOutputParser();\n\n  // Tags and metadata can be configured with RunnableConfig\n  const chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});\n\n  // Tags and metadata can also be passed at runtime\n  await chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})\n  ```\n</CodeGroup>\n\n## Customize run name\n\nYou can customize the name of a given run when invoking or streaming your LangChain code by providing it in the [Config](https://reference.langchain.com/python/langchain_core/runnables/?h=runnablecon#langchain_core.runnables.RunnableConfig). This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI. This can be done by setting a `run_name` in the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) object at construction or by passing a `run_name` in the invocation parameters in JS/TS.\n\n<CodeGroup>\n  ```python Python theme={null}\n  # When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').\n  configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})\n  configured_chain.invoke({\"input\": \"What is the meaning of life?\"})\n\n  # You can also configure the run name at invocation time, like below\n  chain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})\n  ```\n\n  ```typescript TypeScript theme={null}\n  // When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').\n  const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });\n  await configuredChain.invoke({ input: \"What is the meaning of life?\" });\n\n  // You can also configure the run name at invocation time, like below\n  await chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})\n  ```\n</CodeGroup>\n\n<Note>\n  The `run_name` parameter only changes the name of the runnable you invoke (e.g., a chain, function). It does not rename the nested run automatically created when you invoke an LLM object like [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI) (`gpt-4o-mini`). In the example, the enclosing run will appear in LangSmith as `MyCustomChain`, while the nested LLM run still shows the modelâ€™s default name.\n\n  To give the LLM run a more meaningful name, you can either:\n\n  * Wrap the model in another runnable and assign a `run_name` to that step.\n  * Use a tracing decorator or helper (e.g., `@traceable` in Python, or `traceable` from `langsmith` in JS/TS) to create a custom run around the model call.\n</Note>\n\n## Customize run ID\n\nYou can customize the ID of a given run when invoking or streaming your LangChain code by providing it in the [Config](https://reference.langchain.com/python/langchain_core/runnables/?h=runnablecon#langchain_core.runnables.RunnableConfig). This ID is used to uniquely identify the run in LangSmith and can be used to query specific runs. The ID can be useful for linking runs across different systems or for implementing custom tracking logic. This can be done by setting a `run_id` in the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) object at construction or by passing a `run_id` in the invocation parameters.\n\n<Note>\n  This feature is not currently supported directly for LLM objects.\n</Note>\n\n<CodeGroup>\n  ```python Python theme={null}\n  import uuid\n\n  my_uuid = uuid.uuid4()\n\n  # You can configure the run ID at invocation time:\n  chain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_id\": my_uuid})\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { v4 as uuidv4 } from 'uuid';\n\n  const myUuid = uuidv4();\n\n  // You can configure the run ID at invocation time, like below\n  await chain.invoke({ input: \"What is the meaning of life?\" }, { runId: myUuid });\n  ```\n</CodeGroup>\n\nNote that if you do this at the **root** of a trace (i.e., the top-level run, that run ID will be used as the `trace_id`).\n\n## Access run (span) ID for LangChain invocations\n\nWhen you invoke a LangChain object, you can manually specify the run ID of the invocation. This run ID can be used to query the run in LangSmith.\n\nIn JS/TS, you can use a `RunCollectorCallbackHandler` instance to access the run ID.\n\n<CodeGroup>\n  ```python Python theme={null}\n  import uuid\n\n  from langchain_openai import ChatOpenAI\n  from langchain_core.prompts import ChatPromptTemplate\n  from langchain_core.output_parsers import StrOutputParser\n\n  prompt = ChatPromptTemplate.from_messages([\n      (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),\n      (\"user\", \"Question: {question}\\n\\nContext: {context}\")\n  ])\n  model = ChatOpenAI(model=\"gpt-4o-mini\")\n  output_parser = StrOutputParser()\n\n  chain = prompt | model | output_parser\n\n  question = \"Can you summarize this morning's meetings?\"\n  context = \"During this morning's meeting, we solved all world conflict.\"\n  my_uuid = uuid.uuid4()\n  result = chain.invoke({\"question\": question, \"context\": context}, {\"run_id\": my_uuid})\n  print(my_uuid)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n  import { StringOutputParser } from \"@langchain/core/output_parsers\";\n  import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";\n\n  const prompt = ChatPromptTemplate.fromMessages([\n    [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],\n    [\"user\", \"Question: {question}\\n\\nContext: {context}\"],\n  ]);\n  const model = new ChatOpenAI({ modelName: \"gpt-4o-mini\" });\n  const outputParser = new StringOutputParser();\n\n  const chain = prompt.pipe(model).pipe(outputParser);\n  const runCollector = new RunCollectorCallbackHandler();\n\n  const question = \"Can you summarize this morning's meetings?\"\n  const context = \"During this morning's meeting, we solved all world conflict.\"\n  await chain.invoke(\n      { question: question, context: context },\n      { callbacks: [runCollector] }\n  );\n  const runId = runCollector.tracedRuns[0].id;\n  console.log(runId);\n  ```\n</CodeGroup>\n\n## Ensure all traces are submitted before exiting\n\nIn LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.\n\nYou can make callbacks synchronous by setting the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to `\"false\"`.\n\nFor both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application. Below is an example:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain_openai import ChatOpenAI\n  from langchain_core.tracers.langchain import wait_for_all_tracers\n\n  llm = ChatOpenAI()\n\n  try:\n    llm.invoke(\"Hello, World!\")\n  finally:\n    wait_for_all_tracers()\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";\n\n  try {\n      const llm = new ChatOpenAI();\n      const response = await llm.invoke(\"Hello, World!\");\n  } catch (e) {\n      // handle error\n  } finally {\n      await awaitAllCallbacks();\n  }\n  ```\n</CodeGroup>\n\n## Trace without setting environment variables\n\nAs mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:\n\n* `LANGSMITH_TRACING`\n* `LANGSMITH_API_KEY`\n* `LANGSMITH_ENDPOINT`\n* `LANGSMITH_PROJECT`\n\nHowever, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\n\nThis largely builds off of the [previous section](#trace-selectively).\n\n<CodeGroup>\n  ```python Python theme={null}\n  import langsmith as ls\n\n  # You can create a client instance with an api key and api url\n  client = ls.Client(\n      api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager\n      api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region\n  )\n\n  # You can pass the client and project_name to the tracing_context\n  with ls.tracing_context(client=client, project_name=\"test-no-env\", enabled=True):\n      chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"})\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";\n  import { Client } from \"langsmith\";\n\n  // You can create a client instance with an api key and api url\n  const client = new Client(\n      {\n          apiKey: \"YOUR_API_KEY\",\n          apiUrl: \"https://api.smith.langchain.com\", // Update appropriately for self-hosted installations or the EU region\n      }\n  );\n\n  // You can pass the client and project_name to the LangChainTracer instance\n  const tracer = new LangChainTracer({client, projectName: \"test-no-env\"});\n  await chain.invoke(\n    {\n      question: \"Am I using a callback?\",\n      context: \"I'm using a callback\",\n    },\n    { callbacks: [tracer] }\n  );\n  ```\n</CodeGroup>\n\n## Distributed tracing with LangChain (Python)\n\nLangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications. The principles are similar to the [distributed tracing guide](/langsmith/distributed-tracing) for the LangSmith SDK.\n\n```python  theme={null}\nimport langsmith\nfrom langchain_core.runnables import chain\nfrom langsmith.run_helpers import get_current_run_tree\n\n# -- This code should be in a separate file or service --\n@chain\ndef child_chain(inputs):\n    return inputs[\"test\"] + 1\n\ndef child_wrapper(x, headers):\n    with langsmith.tracing_context(parent=headers):\n        child_chain.invoke({\"test\": x})\n\n# -- This code should be in a separate file or service --\n@chain\ndef parent_chain(inputs):\n    rt = get_current_run_tree()\n    headers = rt.to_headers()\n    # ... make a request to another service with the headers\n    # The headers should be passed to the other service, eventually to the child_wrapper function\n\nparent_chain.invoke({\"test\": 1})\n```\n\n## Interoperability between LangChain (Python) and LangSmith SDK\n\nIf you are using LangChain for part of your application and the LangSmith SDK (see [this guide](/langsmith/annotate-code)) for other parts, you can still trace the entire application seamlessly.\n\nLangChain objects will be traced when invoked within a `traceable` function and be bound as a child run of the `traceable` function.\n\n```python  theme={null}\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langsmith import traceable\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),\n    (\"user\", \"Question: {question}\\nContext: {context}\")\n])\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\n\n# The above chain will be traced as a child run of the traceable function\n@traceable(\n    tags=[\"openai\", \"chat\"],\n    metadata={\"foo\": \"bar\"}\n)\ndef invoke_runnnable(question, context):\n    result = chain.invoke({\"question\": question, \"context\": context})\n    return \"The response is: \" + result\n\ninvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\")\n```\n\nThis will produce the following trace tree: <img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=52c64fd784522c4b2d75886ae76f8c18\" alt=\"\" data-og-width=\"1334\" width=\"1334\" data-og-height=\"734\" height=\"734\" data-path=\"langsmith/images/trace-tree-python-interop.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=21a424e2326767bb66a6b5a207390bec 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=72f1854193fd30317d1b69d8de433d73 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=34fa74aff75c11172b350d38319bf276 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4ebfb4252764af54033e62ad088f60b1 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2d783eecf72ba0680e78a0f122bd4411 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9d977793e5281e5d255d962224bd70df 2500w\" />\n\n## Interoperability between LangChain.JS and LangSmith SDK\n\n### Tracing LangChain objects inside `traceable` (JS only)\n\nStarting with `langchain@0.2.x`, LangChain objects are traced automatically when used inside `@traceable` functions, inheriting the client, tags, metadata and project name of the traceable function.\n\nFor older versions of LangChain below `0.2.x`, you will need to manually pass an instance `LangChainTracer` created from the tracing context found in `@traceable`.\n\n```typescript  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { getLangchainCallbacks } from \"langsmith/langchain\";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",\n  ],\n  [\"user\", \"Question: {question}\\nContext: {context}\"],\n]);\n\nconst model = new ChatOpenAI({ modelName: \"gpt-4o-mini\" });\nconst outputParser = new StringOutputParser();\nconst chain = prompt.pipe(model).pipe(outputParser);\n\nconst main = traceable(\n  async (input: { question: string; context: string }) => {\n    const callbacks = await getLangchainCallbacks();\n    const response = await chain.invoke(input, { callbacks });\n    return response;\n  },\n  { name: \"main\" }\n);\n```\n\n### Tracing LangChain child runs via `traceable` / RunTree API (JS only)\n\n<Note>\n  We're working on improving the interoperability between `traceable` and LangChain. The following limitations are present when using combining LangChain with `traceable`:\n\n  1. Mutating RunTree obtained from `getCurrentRunTree()` of the RunnableLambda context will result in a no-op.\n  2. It's discouraged to traverse the RunTree obtained from RunnableLambda via `getCurrentRunTree()` as it may not contain all the RunTree nodes.\n  3. Different child runs may have the same `execution_order` and `child_execution_order` value. Thus in extreme circumstances, some runs may end up in a different order, depending on the `start_time`.\n</Note>\n\nIn some uses cases, you might want to run `traceable` functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the `RunTree` API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke `traceable`-wrapped functions within RunnableLambda.\n\n```typescript  theme={null}\nimport { traceable } from \"langsmith/traceable\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\nimport { RunnableConfig } from \"@langchain/core/runnables\";\n\nconst tracedChild = traceable((input: string) => `Child Run: ${input}`, {\n  name: \"Child Run\",\n});\n\nconst parrot = new RunnableLambda({\n  func: async (input: { text: string }, config?: RunnableConfig) => {\n    return await tracedChild(input.text);\n  },\n});\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7b117d3aa9b419fe2a314ec6d9cc7c16\" alt=\"Trace Tree\" data-og-width=\"2564\" width=\"2564\" data-og-height=\"1530\" height=\"1530\" data-path=\"langsmith/images/trace-tree-manual-tracing.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=517f8a525908d5241c0d635726bf2da7 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2772298bff6569c12537d8b31cc90e78 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4b02aa33df7ef1d18a33bb36c3e2edfe 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e9d4021c62cf3bad95e01c8aa2895d44 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=02ef49a98151ad9fc63c742241542d94 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=765dabfe056ce5f28b1b09ca7eb735d7 2500w\" />\n\nAlternatively, you can convert LangChain's [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) to a equivalent RunTree object by using `RunTree.fromRunnableConfig` or pass the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) as the first argument of `traceable`-wrapped function.\n\n<CodeGroup>\n  ```typescript Traceable theme={null}\n  import { traceable } from \"langsmith/traceable\";\n  import { RunnableLambda } from \"@langchain/core/runnables\";\n  import { RunnableConfig } from \"@langchain/core/runnables\";\n\n  const tracedChild = traceable((input: string) => `Child Run: ${input}`, {\n    name: \"Child Run\",\n  });\n\n  const parrot = new RunnableLambda({\n    func: async (input: { text: string }, config?: RunnableConfig) => {\n      // Pass the config to existing traceable function\n      await tracedChild(config, input.text);\n      return input.text;\n    },\n  });\n  ```\n\n  ```typescript Run Tree theme={null}\n  import { RunTree } from \"langsmith/run_trees\";\n  import { RunnableLambda } from \"@langchain/core/runnables\";\n  import { RunnableConfig } from \"@langchain/core/runnables\";\n\n  const parrot = new RunnableLambda({\n    func: async (input: { text: string }, config?: RunnableConfig) => {\n      // create the RunTree from the RunnableConfig of the RunnableLambda\n      const childRunTree = RunTree.fromRunnableConfig(config, {\n        name: \"Child Run\",\n      });\n\n      childRunTree.inputs = { input: input.text };\n      await childRunTree.postRun();\n\n      childRunTree.outputs = { output: `Child Run: ${input.text}` };\n      await childRunTree.patchRun();\n\n      return input.text;\n    },\n  });\n  ```\n</CodeGroup>\n\nIf you prefer a video tutorial, check out the [Alternative Ways to Trace video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langchain.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 31376
}