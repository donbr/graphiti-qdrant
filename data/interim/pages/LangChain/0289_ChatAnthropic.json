{
  "title": "ChatAnthropic",
  "source_url": "https://docs.langchain.com/oss/javascript/integrations/chat/anthropic",
  "content": "[Anthropic](https://www.anthropic.com/) is an AI safety and research company. They are the creator of Claude.\n\nThis will help you getting started with Anthropic [chat models](/oss/javascript/langchain/models). For detailed documentation of all `ChatAnthropic` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html).\n\n## Overview\n\n### Integration details\n\n| Class                                                                                        | Package                                                                      | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/anthropic/) |                                               Downloads                                              |                                              Version                                              |\n| :------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------- | :---: | :----------: | :--------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: |\n| [ChatAnthropic](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html) | [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic) |   ❌   |       ✅      |                                       ✅                                      | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/anthropic?style=flat-square\\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/anthropic?style=flat-square\\&label=%20&) |\n\n### Model features\n\nSee the links in the table headers below for guides on how to use specific features.\n\n| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |\n| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |\n|                        ✅                        |                                 ✅                                |     ❌     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ❌                               |\n\n## Setup\n\nYou'll need to sign up and obtain an [Anthropic API key](https://www.anthropic.com/), and install the `@langchain/anthropic` integration package.\n\n### Credentials\n\nHead to [Anthropic's website](https://www.anthropic.com/) to sign up to Anthropic and generate an API key. Once you've done this set the `ANTHROPIC_API_KEY` environment variable:\n\n```bash  theme={null}\nexport ANTHROPIC_API_KEY=\"your-api-key\"\n```\n\nIf you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:\n\n```bash  theme={null}\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n### Installation\n\nThe LangChain `ChatAnthropic` integration lives in the `@langchain/anthropic` package:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/anthropic @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/anthropic @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/anthropic @langchain/core\n  ```\n</CodeGroup>\n\n## Instantiation\n\nNow we can instantiate our model object and generate chat completions:\n\n```typescript  theme={null}\nimport { ChatAnthropic } from \"@langchain/anthropic\"\n\nconst llm = new ChatAnthropic({\n    model: \"claude-haiku-4-5-20251001\",\n    temperature: 0,\n    maxTokens: undefined,\n    maxRetries: 2,\n    // other params...\n});\n```\n\n## Invocation\n\n```typescript  theme={null}\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n```output  theme={null}\nAIMessage {\n  \"id\": \"msg_013WBXXiggy6gMbAUY6NpsuU\",\n  \"content\": \"Voici la traduction en français :\\n\\nJ'adore la programmation.\",\n  \"additional_kwargs\": {\n    \"id\": \"msg_013WBXXiggy6gMbAUY6NpsuU\",\n    \"type\": \"message\",\n    \"role\": \"assistant\",\n    \"model\": \"claude-haiku-4-5-20251001\",\n    \"stop_reason\": \"end_turn\",\n    \"stop_sequence\": null,\n    \"usage\": {\n      \"input_tokens\": 29,\n      \"output_tokens\": 20\n    }\n  },\n  \"response_metadata\": {\n    \"id\": \"msg_013WBXXiggy6gMbAUY6NpsuU\",\n    \"model\": \"claude-haiku-4-5-20251001\",\n    \"stop_reason\": \"end_turn\",\n    \"stop_sequence\": null,\n    \"usage\": {\n      \"input_tokens\": 29,\n      \"output_tokens\": 20\n    },\n    \"type\": \"message\",\n    \"role\": \"assistant\"\n  },\n  \"tool_calls\": [],\n  \"invalid_tool_calls\": [],\n  \"usage_metadata\": {\n    \"input_tokens\": 29,\n    \"output_tokens\": 20,\n    \"total_tokens\": 49\n  }\n}\n```\n\n```typescript  theme={null}\nconsole.log(aiMsg.content)\n```\n\n```output  theme={null}\nVoici la traduction en français :\n\nJ'adore la programmation.\n```\n\n## Content blocks\n\nOne key difference to note between Anthropic models and most others is that the contents of a single Anthropic [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html) can either be a single string or a **list of content blocks**. For example when an Anthropic model [calls a tool](/oss/javascript/langchain/tools), the tool invocation is part of the message content (as well as being exposed in the standardized `AIMessage.tool_calls` field):\n\n```typescript  theme={null}\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport * as z from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\nconst calculatorSchema = z.object({\n  operation: z\n    .enum([\"add\", \"subtract\", \"multiply\", \"divide\"])\n    .describe(\"The type of operation to execute.\"),\n  number1: z.number().describe(\"The first number to operate on.\"),\n  number2: z.number().describe(\"The second number to operate on.\"),\n});\n\nconst calculatorTool = {\n  name: \"calculator\",\n  description: \"A simple calculator tool\",\n  input_schema: zodToJsonSchema(calculatorSchema),\n};\n\nconst toolCallingLlm = new ChatAnthropic({\n  model: \"claude-haiku-4-5-20251001\",\n}).bindTools([calculatorTool]);\n\nconst toolPrompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"You are a helpful assistant who always needs to use a calculator.\",\n  ],\n  [\"human\", \"{input}\"],\n]);\n\n// Chain your prompt and model together\nconst toolCallChain = toolPrompt.pipe(toolCallingLlm);\n\nawait toolCallChain.invoke({\n  input: \"What is 2 + 2?\",\n});\n```\n\n```output  theme={null}\nAIMessage {\n  \"id\": \"msg_01DZGs9DyuashaYxJ4WWpWUP\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Here is the calculation for 2 + 2:\"\n    },\n    {\n      \"type\": \"tool_use\",\n      \"id\": \"toolu_01SQXBamkBr6K6NdHE7GWwF8\",\n      \"name\": \"calculator\",\n      \"input\": {\n        \"number1\": 2,\n        \"number2\": 2,\n        \"operation\": \"add\"\n      }\n    }\n  ],\n  \"additional_kwargs\": {\n    \"id\": \"msg_01DZGs9DyuashaYxJ4WWpWUP\",\n    \"type\": \"message\",\n    \"role\": \"assistant\",\n    \"model\": \"claude-haiku-4-5-20251001\",\n    \"stop_reason\": \"tool_use\",\n    \"stop_sequence\": null,\n    \"usage\": {\n      \"input_tokens\": 449,\n      \"output_tokens\": 100\n    }\n  },\n  \"response_metadata\": {\n    \"id\": \"msg_01DZGs9DyuashaYxJ4WWpWUP\",\n    \"model\": \"claude-haiku-4-5-20251001\",\n    \"stop_reason\": \"tool_use\",\n    \"stop_sequence\": null,\n    \"usage\": {\n      \"input_tokens\": 449,\n      \"output_tokens\": 100\n    },\n    \"type\": \"message\",\n    \"role\": \"assistant\"\n  },\n  \"tool_calls\": [\n    {\n      \"name\": \"calculator\",\n      \"args\": {\n        \"number1\": 2,\n        \"number2\": 2,\n        \"operation\": \"add\"\n      },\n      \"id\": \"toolu_01SQXBamkBr6K6NdHE7GWwF8\",\n      \"type\": \"tool_call\"\n    }\n  ],\n  \"invalid_tool_calls\": [],\n  \"usage_metadata\": {\n    \"input_tokens\": 449,\n    \"output_tokens\": 100,\n    \"total_tokens\": 549\n  }\n}\n```\n\n## Custom headers\n\nYou can pass custom headers in your requests like this:\n\n```typescript  theme={null}\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst llmWithCustomHeaders = new ChatAnthropic({\n  model: \"claude-sonnet-4-5-20250929\",\n  maxTokens: 1024,\n  clientOptions: {\n    defaultHeaders: {\n      \"X-Api-Key\": process.env.ANTHROPIC_API_KEY,\n    },\n  },\n});\n\nawait llmWithCustomHeaders.invoke(\"Why is the sky blue?\");\n```\n\n```output  theme={null}\nAIMessage {\n  \"id\": \"msg_019z4nWpShzsrbSHTWXWQh6z\",\n  \"content\": \"The sky appears blue due to a phenomenon called Rayleigh scattering. Here's a brief explanation:\\n\\n1) Sunlight is made up of different wavelengths of visible light, including all the colors of the rainbow.\\n\\n2) As sunlight passes through the atmosphere, the gases (mostly nitrogen and oxygen) cause the shorter wavelengths of light, such as violet and blue, to be scattered more easily than the longer wavelengths like red and orange.\\n\\n3) This scattering of the shorter blue wavelengths occurs in all directions by the gas molecules in the atmosphere.\\n\\n4) Our eyes are more sensitive to the scattered blue light than the scattered violet light, so we perceive the sky as having a blue color.\\n\\n5) The scattering is more pronounced for light traveling over longer distances through the atmosphere. This is why the sky appears even darker blue when looking towards the horizon.\\n\\nSo in essence, the selective scattering of the shorter blue wavelengths of sunlight by the gases in the atmosphere is what causes the sky to appear blue to our eyes during the daytime.\",\n  \"additional_kwargs\": {\n    \"id\": \"msg_019z4nWpShzsrbSHTWXWQh6z\",\n    \"type\": \"message\",\n    \"role\": \"assistant\",\n    \"model\": \"claude-3-sonnet-20240229\",\n    \"stop_reason\": \"end_turn\",\n    \"stop_sequence\": null,\n    \"usage\": {\n      \"input_tokens\": 13,\n      \"output_tokens\": 236\n    }\n  },\n  \"response_metadata\": {\n    \"id\": \"msg_019z4nWpShzsrbSHTWXWQh6z\",\n    \"model\": \"claude-3-sonnet-20240229\",\n    \"stop_reason\": \"end_turn\",\n    \"stop_sequence\": null,\n    \"usage\": {\n      \"input_tokens\": 13,\n      \"output_tokens\": 236\n    },\n    \"type\": \"message\",\n    \"role\": \"assistant\"\n  },\n  \"tool_calls\": [],\n  \"invalid_tool_calls\": [],\n  \"usage_metadata\": {\n    \"input_tokens\": 13,\n    \"output_tokens\": 236,\n    \"total_tokens\": 249\n  }\n}\n```\n\n## Prompt caching\n\n<Warning>\n  **Compatibility**: This feature is currently in beta.\n</Warning>\n\nAnthropic supports [caching parts of your prompt](https://platform.claude.com/docs/en/build-with-claude/prompt-caching) in order to reduce costs for use-cases that require long context. You can cache tools and both entire messages and individual blocks.\n\nThe initial request containing one or more blocks or tool definitions with a `\"cache_control\": { \"type\": \"ephemeral\" }` field will automatically cache that part of the prompt. This initial caching step will cost extra, but subsequent requests will be billed at a reduced rate. The cache has a lifetime of 5 minutes, but this is refereshed each time the cache is hit.\n\nThere is also currently a minimum cacheable prompt length, which varies according to model. You can see this information [here](https://platform.claude.com/docs/en/build-with-claude/prompt-caching#structuring-your-prompt).\n\nThis currently requires you to initialize your model with a beta header. Here's an example of caching part of a system message that contains the LangChain [conceptual docs](/oss/javascript/concepts/):\n\n```typescript  theme={null}\nlet CACHED_TEXT = \"...\";\n```\n\n```typescript  theme={null}\n// @lc-docs-hide-cell\n\nCACHED_TEXT = `## Components\n\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.\nSome components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\n\n### Chat models\n\n<span data-heading-keywords=\"chat model,chat models\"></span>\n\nLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).\nThese are generally newer models (older models are generally \\`LLMs\\`, see below).\nChat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.\n\nAlthough the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input.\nThis gives them the same interface as LLMs (and simpler to use).\nWhen a string is passed in as input, it will be converted to a \\`HumanMessage\\` under the hood before being passed to the underlying model.\n\nLangChain does not host any Chat Models, rather we rely on third party integrations.\n\nWe have some standardized parameters when constructing ChatModels:\n\n- \\`model\\`: the name of the model\n\nChat Models also accept other parameters that are specific to that integration.\n\n<Warning>\n**Some chat models have been fine-tuned for **tool calling** and provide a dedicated API for it.**\n\nGenerally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.\nPlease see the [tool calling section](/oss/javascript/langchain/tools) for more information.\n</Warning>\n\nFor specifics on how to use chat models, see the [relevant how-to guides here](/oss/javascript/langchain/models).\n\n#### Multimodality\n\nSome chat models are multimodal, accepting images, audio and even video as inputs.\nThese are still less common, meaning model providers haven't standardized on the \"best\" way to define the API.\nMultimodal outputs are even less common. As such, we've kept our multimodal abstractions fairly light weight\nand plan to further solidify the multimodal APIs and interaction patterns as the field matures.\n\nIn LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format.\nSo far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.\n\nFor specifics on how to use multimodal models, see the [relevant how-to guides here](/oss/javascript/how-to/#multimodal).\n\n### LLMs\n\n<span data-heading-keywords=\"llm,llms\"></span>\n\n<Warning>\n**Pure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as [chat completion models](/oss/javascript/langchain/models),**\n\neven for non-chat use cases.\n\nYou are probably looking for [the section above instead](/oss/javascript/langchain/models).\n</Warning>\n\nLanguage models that takes a string as input and returns a string.\nThese are traditionally older models (newer models generally are [Chat Models](/oss/javascript/langchain/models), see above).\n\nAlthough the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.\nThis gives them the same interface as [Chat Models](/oss/javascript/langchain/models).\nWhen messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.\n\nLangChain does not host any LLMs, rather we rely on third party integrations.\n\nFor specifics on how to use LLMs, see the [relevant how-to guides here](/oss/javascript/langchain/models).\n\n### Message types\n\nSome language models take an array of messages as input and return a message.\nThere are a few different types of messages.\nAll messages have a \\`role\\`, \\`content\\`, and \\`response_metadata\\` property.\n\nThe \\`role\\` describes WHO is saying the message.\nLangChain has different message classes for different roles.\n\nThe \\`content\\` property describes the content of the message.\nThis can be a few different things:\n\n- A string (most models deal this type of content)\n- A List of objects (this is used for multi-modal input, where the object contains information about that input type and that input location)\n\n#### HumanMessage\n\nThis represents a message from the user.\n\n#### AIMessage\n\nThis represents a message from the model. In addition to the \\`content\\` property, these messages also have:\n\n**\\`response_metadata\\`**\n\nThe \\`response_metadata\\` property contains additional metadata about the response. The data here is often specific to each model provider.\nThis is where information like log-probs and token usage may be stored.\n\n**\\`tool_calls\\`**\n\nThese represent a decision from an language model to call a tool. They are included as part of an \\`AIMessage\\` output.\nThey can be accessed from there with the \\`.tool_calls\\` property.\n\nThis property returns a list of \\`ToolCall\\`s. A \\`ToolCall\\` is an object with the following arguments:\n\n- \\`name\\`: The name of the tool that should be called.\n- \\`args\\`: The arguments to that tool.\n- \\`id\\`: The id of that tool call.\n\n#### SystemMessage\n\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\n\n#### ToolMessage\n\nThis represents the result of a tool call. In addition to \\`role\\` and \\`content\\`, this message has:\n\n- a \\`tool_call_id\\` field which conveys the id of the call to the tool that was called to produce this result.\n- an \\`artifact\\` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\n\n#### (Legacy) FunctionMessage\n\nThis is a legacy message type, corresponding to OpenAI's legacy function-calling API. \\`ToolMessage\\` should be used instead to correspond to the updated tool-calling API.\n\nThis represents the result of a function call. In addition to \\`role\\` and \\`content\\`, this message has a \\`name\\` parameter which conveys the name of the function that was called to produce this result.\n\n### Prompt templates\n\n<span data-heading-keywords=\"prompt,prompttemplate,chatprompttemplate\"></span>\n\nPrompt templates help to translate user input and parameters into instructions for a language model.\nThis can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n\nPrompt Templates take as input an object, where each key represents a variable in the prompt template to fill in.\n\nPrompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or an array of messages.\nThe reason this PromptValue exists is to make it easy to switch between strings and messages.\n\nThere are a few different types of prompt templates:\n\n#### String PromptTemplates\n\nThese prompt templates are used to format a single string, and generally are used for simpler inputs.\nFor example, a common way to construct and use a PromptTemplate is as follows:\n\n\\`\\`\\`typescript\nimport { PromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\n\nawait promptTemplate.invoke({ topic: \"cats\" });\n\\`\\`\\`\n\n#### ChatPromptTemplates\n\nThese prompt templates are used to format an array of messages. These \"templates\" consist of an array of templates themselves.\nFor example, a common way to construct and use a ChatPromptTemplate is as follows:\n\n\\`\\`\\`typescript\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"user\", \"Tell me a joke about {topic}\"],\n]);\n\nawait promptTemplate.invoke({ topic: \"cats\" });\n\\`\\`\\`\n\nIn the above example, this ChatPromptTemplate will construct two messages when called.\nThe first is a system message, that has no variables to format.\nThe second is a HumanMessage, and will be formatted by the \\`topic\\` variable the user passes in.\n\n#### MessagesPlaceholder\n\n<span data-heading-keywords=\"messagesplaceholder\"></span>\n\nThis prompt template is responsible for adding an array of messages in a particular place.\nIn the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\nBut what if we wanted the user to pass in an array of messages that we would slot into a particular spot?\nThis is how you use MessagesPlaceholder.\n\n\\`\\`\\`typescript\nimport {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from \"@langchain/core/prompts\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  new MessagesPlaceholder(\"msgs\"),\n]);\n\npromptTemplate.invoke({ msgs: [new HumanMessage({ content: \"hi!\" })] });\n\\`\\`\\`\n\nThis will produce an array of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\nIf we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\nThis is useful for letting an array of messages be slotted into a particular spot.\n\nAn alternative way to accomplish the same thing without using the \\`MessagesPlaceholder\\` class explicitly is:\n\n\\`\\`\\`typescript\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"placeholder\", \"{msgs}\"], // <-- This is the changed part\n]);\n\\`\\`\\`\n\nFor specifics on how to use prompt templates, see the [relevant how-to guides here](/oss/javascript/how-to/#prompt-templates).\n\n### Example Selectors\n\nOne common prompting technique for achieving better performance is to include examples as part of the prompt.\nThis gives the language model concrete examples of how it should behave.\nSometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\nExample Selectors are classes responsible for selecting and then formatting examples into prompts.\n\nFor specifics on how to use example selectors, see the [relevant how-to guides here](/oss/javascript/how-to/#example-selectors).\n\n### Output parsers\n\n<span data-heading-keywords=\"output parser\"></span>\n\n<Note>\n**The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.**\n\nMore and more models are supporting function (or tool) calling, which handles this automatically.\nIt is recommended to use function/tool calling rather than output parsing.\nSee documentation for that [here](/oss/javascript/langchain/tools).\n\n</Note>\n\nResponsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\nUseful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\n\nThere are two main methods an output parser must implement:\n\n- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n\nAnd then one optional one:\n\n- \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n\nOutput parsers accept a string or \\`BaseMessage\\` as input and can return an arbitrary type.\n\nLangChain has many different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:\n\n**Name**: The name of the output parser\n\n**Supports Streaming**: Whether the output parser supports streaming.\n\n**Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific arguments.\n\n**Output Type**: The output type of the object returned by the parser.\n\n**Description**: Our commentary on this output parser and when to use it.\n\nThe current date is ${new Date().toISOString()}`;\n\n// Noop statement to hide output\nvoid 0;\n```\n\n```typescript  theme={null}\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst modelWithCaching = new ChatAnthropic({\n  model: \"claude-haiku-4-5-20251001\",\n  clientOptions: {\n    defaultHeaders: {\n      \"anthropic-beta\": \"prompt-caching-2024-07-31\",\n    },\n  },\n});\n\nconst LONG_TEXT = `You are a pirate. Always respond in pirate dialect.\n\nUse the following as context when answering questions:\n\n${CACHED_TEXT}`;\n\nconst messages = [\n  {\n    role: \"system\",\n    content: [\n      {\n        type: \"text\",\n        text: LONG_TEXT,\n        // Tell Anthropic to cache this block\n        cache_control: { type: \"ephemeral\" },\n      },\n    ],\n  },\n  {\n    role: \"user\",\n    content: \"What types of messages are supported in LangChain?\",\n  },\n];\n\nconst res = await modelWithCaching.invoke(messages);\n\nconsole.log(\"USAGE:\", res.response_metadata.usage);\n```\n\n```output  theme={null}\nUSAGE: {\n  input_tokens: 19,\n  cache_creation_input_tokens: 2921,\n  cache_read_input_tokens: 0,\n  output_tokens: 355\n}\n```\n\nWe can see that there's a new field called `cache_creation_input_tokens` in the raw usage field returned from Anthropic.\n\nIf we use the same messages again, we can see that the long text's input tokens are read from the cache:\n\n```typescript  theme={null}\nconst res2 = await modelWithCaching.invoke(messages);\n\nconsole.log(\"USAGE:\", res2.response_metadata.usage);\n```\n\n```output  theme={null}\nUSAGE: {\n  input_tokens: 19,\n  cache_creation_input_tokens: 0,\n  cache_read_input_tokens: 2921,\n  output_tokens: 357\n}\n```\n\n### Tool caching\n\nYou can also cache tools by setting the same `\"cache_control\": { \"type\": \"ephemeral\" }` within a tool definition. This currently requires you to bind a tool in [Anthropic's raw tool format](https://platform.claude.com/docs/en/agents-and-tools/tool-use/overview) Here's an example:\n\n```typescript  theme={null}\nconst SOME_LONG_DESCRIPTION = \"...\";\n\n// Tool in Anthropic format\nconst anthropicTools = [{\n  name: \"get_weather\",\n  description: SOME_LONG_DESCRIPTION,\n  input_schema: {\n    type: \"object\",\n    properties: {\n      location: {\n        type: \"string\",\n        description: \"Location to get the weather for\",\n      },\n      unit: {\n        type: \"string\",\n        description: \"Temperature unit to return\",\n      },\n    },\n    required: [\"location\"],\n  },\n  // Tell Anthropic to cache this tool\n  cache_control: { type: \"ephemeral\" },\n}]\n\nconst modelWithCachedTools = modelWithCaching.bindTools(anthropicTools);\n\nawait modelWithCachedTools.invoke(\"what is the weather in SF?\");\n```\n\nFor more on how prompt caching works, see [Anthropic's docs](https://platform.claude.com/docs/en/build-with-claude/prompt-caching#how-prompt-caching-works).\n\n## Custom clients\n\nAnthropic models [may be hosted on cloud services such as Google Vertex](https://platform.claude.com/docs/en/build-with-claude/claude-on-vertex-ai) that rely on a different underlying client with the same interface as the primary Anthropic client. You can access these services by providing a `createClient` method that returns an initialized instance of an Anthropic client. Here's an example:\n\n```typescript  theme={null}\nimport { AnthropicVertex } from \"@anthropic-ai/vertex-sdk\";\n\nconst customClient = new AnthropicVertex();\n\nconst modelWithCustomClient = new ChatAnthropic({\n  modelName: \"claude-3-sonnet@20240229\",\n  maxRetries: 0,\n  createClient: () => customClient,\n});\n\nawait modelWithCustomClient.invoke([{ role: \"user\", content: \"Hello!\" }]);\n```\n\n## Citations\n\nAnthropic supports a [citations](https://platform.claude.com/docs/en/build-with-claude/citations) feature that lets Claude attach context to its answers based on source material supplied by the user. This source material can be provided either as [document content blocks](https://platform.claude.com/docs/en/build-with-claude/citations#document-types), which describe full documents, or as [search results](https://platform.claude.com/docs/en/build-with-claude/search-results), which describe relevant passages or snippets returned from a retrieval system. When `\"citations\": { \"enabled\": true }` is included in a query, Claude may generate direct citations to the provided material in its response.\n\n### Document example\n\nIn this example we pass a [plain text document](https://platform.claude.com/docs/en/build-with-claude/citations#plain-text-documents). In the background, Claude [automatically chunks](https://platform.claude.com/docs/en/build-with-claude/citations#plain-text-documents) the input text into sentences, which are used when generating citations.\n\n```typescript  theme={null}\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst citationsModel = new ChatAnthropic({\n  model: \"claude-haiku-4-5-20251001\",\n});\n\nconst messagesWithCitations = [\n  {\n    role: \"user\",\n    content: [\n      {\n        type: \"document\",\n        source: {\n          type: \"text\",\n          media_type: \"text/plain\",\n          data: \"The grass is green. The sky is blue.\",\n        },\n        title: \"My Document\",\n        context: \"This is a trustworthy document.\",\n        citations: {\n          enabled: true,\n        },\n      },\n      {\n        type: \"text\",\n        text: \"What color is the grass and sky?\",\n      },\n    ],\n  }\n];\n\nconst responseWithCitations = await citationsModel.invoke(messagesWithCitations);\n\nconsole.log(JSON.stringify(responseWithCitations.content, null, 2));\n```\n\n```output  theme={null}\n[\n  {\n    \"type\": \"text\",\n    \"text\": \"Based on the document, I can tell you that:\\n\\n- \"\n  },\n  {\n    \"type\": \"text\",\n    \"text\": \"The grass is green\",\n    \"citations\": [\n      {\n        \"type\": \"char_location\",\n        \"cited_text\": \"The grass is green. \",\n        \"document_index\": 0,\n        \"document_title\": \"My Document\",\n        \"start_char_index\": 0,\n        \"end_char_index\": 20\n      }\n    ]\n  },\n  {\n    \"type\": \"text\",\n    \"text\": \"\\n- \"\n  },\n  {\n    \"type\": \"text\",\n    \"text\": \"The sky is blue\",\n    \"citations\": [\n      {\n        \"type\": \"char_location\",\n        \"cited_text\": \"The sky is blue.\",\n        \"document_index\": 0,\n        \"document_title\": \"My Document\",\n        \"start_char_index\": 20,\n        \"end_char_index\": 36\n      }\n    ]\n  }\n]\n```\n\n### Search results example\n\nIn this example, we pass in [search results](https://platform.claude.com/docs/en/build-with-claude/search-results) as part of our message content. This allows Claude to cite specific passages or snippets from your own retrieval system in its response.\n\nThis approach is helpful when you want Claude to cite information from a specific set of knowledge, but you want to bring your own pre-fetched/cached content directly rather than having the model search or retrieve them automatically.\n\n```typescript  theme={null}\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst citationsModel = new ChatAnthropic({\n  model: \"claude-haiku-4-5-20251001\",\n});\n\nconst messagesWithCitations = [\n  {\n    type: \"user\",\n    content: [\n      {\n        type: \"search_result\",\n        title: \"History of France\",\n        source: \"https://some-uri.com\",\n        citations: { enabled: true },\n        content: [\n          {\n            type: \"text\",\n            text: \"The capital of France is Paris.\",\n          },\n          {\n            type: \"text\",\n            text: \"The old capital of France was Lyon.\",\n          },\n        ],\n      },\n      {\n        type: \"text\",\n        text: \"What is the capital of France?\",\n      },\n    ],\n  },\n];\n\nconst responseWithCitations = await citationsModel.invoke(messagesWithCitations);\n\nconsole.log(JSON.stringify(responseWithCitations.content, null, 2));\n```\n\n#### Search results from a tool\n\nYou can also use a tool to provide search results that the model can cite in its responses. This is well suited for RAG (or [Retrieval-Augmented Generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)) workflows where Claude can decide when and where to retrieve information from. When returning this information as [search results](https://platform.claude.com/docs/en/build-with-claude/search-results), it gives Claude the ability to create citations from the material returned from the tool.\n\nHere's how you can create a tool that returns search results in the format expected by Anthropic's citations API:\n\n```typescript  theme={null}\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { tool } from \"@langchain/core/tools\";\n\n// Create a tool that returns search results\nconst ragTool = tool(\n  () => [\n    {\n      type: \"search_result\",\n      title: \"History of France\",\n      source: \"https://some-uri.com\",\n      citations: { enabled: true },\n      content: [\n        {\n          type: \"text\",\n          text: \"The capital of France is Paris.\",\n        },\n        {\n          type: \"text\",\n          text: \"The old capital of France was Lyon.\",\n        },\n      ],\n    },\n    {\n      type: \"search_result\",\n      title: \"Geography of France\",\n      source: \"https://some-uri.com\",\n      citations: { enabled: true },\n      content: [\n        {\n          type: \"text\",\n          text: \"France is a country in Europe.\",\n        },\n        {\n          type: \"text\",\n          text: \"The capital of France is Paris.\",\n        },\n      ],\n    },\n  ],\n  {\n    name: \"my_rag_tool\",\n    description: \"Retrieval system that accesses my knowledge base.\",\n    schema: z.object({\n      query: z.string().describe(\"query to search in the knowledge base\"),\n    }),\n  }\n);\n\n// Create model with search results beta header\nconst model = new ChatAnthropic({\n  model: \"claude-haiku-4-5-20251001\",\n}).bindTools([ragTool]);\n\nconst result = await model.invoke([\n  {\n    role: \"user\",\n    content: \"What is the capital of France?\",\n  },\n]);\n\nconsole.log(JSON.stringify(result.content, null, 2));\n\n```\n\nLearn more about how RAG works in LangChain [here](https://js.langchain.com/docs/concepts/rag/)\n\nLearn more about tool calling [here](https://js.langchain.com/docs/how_to/tool_calling/)\n\n### Using with text splitters\n\nAnthropic also lets you specify your own splits using [custom document](https://platform.claude.com/docs/en/build-with-claude/citations#custom-content-documents) types. LangChain text splitters can be used to generate meaningful splits for this purpose. See the below example, where we split the LangChain.js README (a markdown document) and pass it to Claude as context:\n\n```typescript  theme={null}\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { MarkdownTextSplitter } from \"@langchain/classic/text_splitter\";\n\nfunction formatToAnthropicDocuments(documents: string[]) {\n  return {\n    type: \"document\",\n    source: {\n      type: \"content\",\n      content: documents.map((document) => ({ type: \"text\", text: document })),\n    },\n    citations: { enabled: true },\n  };\n}\n\n// Pull readme\nconst readmeResponse = await fetch(\n  \"https://raw.githubusercontent.com/langchain-ai/langchainjs/master/README.md\"\n);\n\nconst readme = await readmeResponse.text();\n\n// Split into chunks\nconst splitter = new MarkdownTextSplitter({\n  chunkOverlap: 0,\n  chunkSize: 50,\n});\nconst documents = await splitter.splitText(readme);\n\n// Construct message\nconst messageWithSplitDocuments = {\n  role: \"user\",\n  content: [\n    formatToAnthropicDocuments(documents),\n    { type: \"text\", text: \"Give me a link to LangChain's tutorials. Cite your sources\" },\n  ],\n};\n\n// Query LLM\nconst citationsModelWithSplits = new ChatAnthropic({\n  model: \"claude-sonnet-4-5-20250929\",\n});\nconst resWithSplits = await citationsModelWithSplits.invoke([messageWithSplitDocuments]);\n\nconsole.log(JSON.stringify(resWithSplits.content, null, 2));\n```\n\n```output  theme={null}\n[\n  {\n    \"type\": \"text\",\n    \"text\": \"Based on the documentation, I can provide you with a link to LangChain's tutorials:\\n\\n\"\n  },\n  {\n    \"type\": \"text\",\n    \"text\": \"The tutorials can be found at: https://js.langchain.com/docs/tutorials/\",\n    \"citations\": [\n      {\n        \"type\": \"content_block_location\",\n        \"cited_text\": \"[Tutorial](https://js.langchain.com/docs/tutorials/) walkthroughs\",\n        \"document_index\": 0,\n        \"document_title\": null,\n        \"start_block_index\": 191,\n        \"end_block_index\": 194\n      }\n    ]\n  }\n]\n```\n\n## Context management\n\nAnthropic supports a context editing feature that will automatically manage the model's context window (e.g., by clearing tool results).\n\nSee [Anthropic documentation](https://platform.claude.com/docs/en/build-with-claude/context-editing) for details and configuration options.\n\n<Info>\n  **Context management is supported since `@langchain/anthropic@0.3.29`**\n</Info>\n\n```typescript  theme={null}\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst llm = new ChatAnthropic({\n  model: \"claude-sonnet-4-5-20250929\",\n  clientOptions: {\n    defaultHeaders: {\n      \"anthropic-beta\": \"context-management-2025-06-27\",\n    },\n  },\n  contextManagement: { edits: [{ type: \"clear_tool_uses_20250919\" }] },\n)\nconst llmWithTools = llm.bindTools([{ type: \"web_search_20250305\", name: \"web_search\" }]);\nconst response = await llmWithTools.invoke(\"Search for recent developments in AI\");\n```\n\n***\n\n## API reference\n\nFor detailed documentation of all ChatAnthropic features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/chat/anthropic.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 38674
}