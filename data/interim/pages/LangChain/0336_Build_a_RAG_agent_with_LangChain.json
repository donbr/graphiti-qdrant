{
  "title": "Build a RAG agent with LangChain",
  "source_url": "https://docs.langchain.com/oss/javascript/langchain/rag",
  "content": "## Overview\n\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q\\&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](/oss/javascript/langchain/retrieval/).\n\nThis tutorial will show how to build a simple Q\\&A application over an unstructured text data source. We will demonstrate:\n\n1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.\n2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n### Concepts\n\nWe will cover the following concepts:\n\n* **Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens in a separate process.*\n\n* **Retrieval and generation**: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nOnce we've indexed our data, we will use an [agent](/oss/javascript/langchain/agents) as our orchestration framework to implement the retrieval and generation steps.\n\n<Note>\n  The indexing portion of this tutorial will largely follow the [semantic search tutorial](/oss/javascript/langchain/knowledge-base).\n\n  If your data is already available for search (i.e., you have a function to execute a search), or you're comfortable with the content from that tutorial, feel free to skip to the section on [retrieval and generation](#2-retrieval-and-generation)\n</Note>\n\n### Preview\n\nIn this guide we'll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n\nWe can create a simple indexing pipeline and RAG chain to do this in \\~40 lines of code. See below for the full code snippet:\n\n<Accordion title=\"Expand for full code snippet\">\n  ```typescript  theme={null}\n  import \"cheerio\";\n  import { createAgent, tool } from \"langchain\";\n  import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n  import { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n  import * as z from \"zod\";\n\n  // Load and chunk contents of blog\n  const pTagSelector = \"p\";\n  const cheerioLoader = new CheerioWebBaseLoader(\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    {\n      selector: pTagSelector\n    }\n  );\n\n  const docs = await cheerioLoader.load();\n\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: 1000,\n    chunkOverlap: 200\n  });\n  const allSplits = await splitter.splitDocuments(docs);\n\n  // Index chunks\n  await vectorStore.addDocuments(allSplits)\n\n  // Construct a tool for retrieving context\n  const retrieveSchema = z.object({ query: z.string() });\n\n  const retrieve = tool(\n    async ({ query }) => {\n      const retrievedDocs = await vectorStore.similaritySearch(query, 2);\n      const serialized = retrievedDocs\n        .map(\n          (doc) => `Source: ${doc.metadata.source}\\nContent: ${doc.pageContent}`\n        )\n        .join(\"\\n\");\n      return [serialized, retrievedDocs];\n    },\n    {\n      name: \"retrieve\",\n      description: \"Retrieve information related to a query.\",\n      schema: retrieveSchema,\n      responseFormat: \"content_and_artifact\",\n    }\n  );\n\n  const agent = createAgent({ model: \"gpt-5\", tools: [retrieve] });\n  ```\n\n  ```typescript  theme={null}\n  let inputMessage = `What is Task Decomposition?`;\n\n  let agentInputs = { messages: [{ role: \"user\", content: inputMessage }] };\n\n  for await (const step of await agent.stream(agentInputs, {\n    streamMode: \"values\",\n  })) {\n    const lastMessage = step.messages[step.messages.length - 1];\n    prettyPrint(lastMessage);\n    console.log(\"-----\\n\");\n  }\n  ```\n\n  Check out the [LangSmith trace](https://smith.langchain.com/public/a117a1f8-c96c-4c16-a285-00b85646118e/r).\n</Accordion>\n\n## Setup\n\n### Installation\n\nThis tutorial requires these langchain dependencies:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm i langchain @langchain/community @langchain/textsplitters\n  ```\n\n  ```bash yarn theme={null}\n  yarn add langchain @langchain/community @langchain/textsplitters\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add langchain @langchain/community @langchain/textsplitters\n  ```\n</CodeGroup>\n\nFor more details, see our [Installation guide](/oss/javascript/langchain/install).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```shell  theme={null}\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n### Components\n\nWe will need to select three components from LangChain's suite of integrations.\n\nSelect a chat model:\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    ðŸ‘‰ Read the [OpenAI chat model integration docs](/oss/javascript/integrations/chat/openai/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/openai\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/openai\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/openai\n      ```\n\n      ```bash bun theme={null} theme={null}\n      bun add @langchain/openai\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      process.env.OPENAI_API_KEY = \"your-api-key\";\n\n      const model = await initChatModel(\"gpt-4.1\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { ChatOpenAI } from \"@langchain/openai\";\n\n      const model = new ChatOpenAI({\n        model: \"gpt-4.1\",\n        apiKey: \"your-api-key\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    ðŸ‘‰ Read the [Anthropic chat model integration docs](/oss/javascript/integrations/chat/anthropic/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/anthropic\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/anthropic\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/anthropic\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/anthropic\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      process.env.ANTHROPIC_API_KEY = \"your-api-key\";\n\n      const model = await initChatModel(\"claude-sonnet-4-5-20250929\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { ChatAnthropic } from \"@langchain/anthropic\";\n\n      const model = new ChatAnthropic({\n        model: \"claude-sonnet-4-5-20250929\",\n        apiKey: \"your-api-key\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    ðŸ‘‰ Read the [Azure chat model integration docs](/oss/javascript/integrations/chat/azure/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/azure\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/azure\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/azure\n      ```\n\n      ```bash bun theme={null} theme={null}\n      bun add @langchain/azure\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      process.env.AZURE_OPENAI_API_KEY = \"your-api-key\";\n      process.env.AZURE_OPENAI_ENDPOINT = \"your-endpoint\";\n      process.env.OPENAI_API_VERSION = \"your-api-version\";\n\n      const model = await initChatModel(\"azure_openai:gpt-4.1\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { AzureChatOpenAI } from \"@langchain/openai\";\n\n      const model = new AzureChatOpenAI({\n        model: \"gpt-4.1\",\n        azureOpenAIApiKey: \"your-api-key\",\n        azureOpenAIApiEndpoint: \"your-endpoint\",\n        azureOpenAIApiVersion: \"your-api-version\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    ðŸ‘‰ Read the [Google GenAI chat model integration docs](/oss/javascript/integrations/chat/google_generative_ai/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/google-genai\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/google-genai\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/google-genai\n      ```\n\n      ```bash bun theme={null} theme={null}\n      bun add @langchain/google-genai\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      process.env.GOOGLE_API_KEY = \"your-api-key\";\n\n      const model = await initChatModel(\"google-genai:gemini-2.5-flash-lite\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\n      const model = new ChatGoogleGenerativeAI({\n        model: \"gemini-2.5-flash-lite\",\n        apiKey: \"your-api-key\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Bedrock Converse\">\n    ðŸ‘‰ Read the [AWS Bedrock chat model integration docs](/oss/javascript/integrations/chat/bedrock_converse/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/aws\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/aws\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/aws\n      ```\n\n      ```bash bun theme={null} theme={null}\n      bun add @langchain/aws\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      // Follow the steps here to configure your credentials:\n      // https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      const model = await initChatModel(\"bedrock:gpt-4.1\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { ChatBedrockConverse } from \"@langchain/aws\";\n\n      // Follow the steps here to configure your credentials:\n      // https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      const model = new ChatBedrockConverse({\n        model: \"gpt-4.1\",\n        region: \"us-east-2\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n</Tabs>\n\nSelect an embeddings model:\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/openai\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/openai\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/openai\n      ```\n    </CodeGroup>\n\n    ```typescript theme={null} theme={null}\n    import { OpenAIEmbeddings } from \"@langchain/openai\";\n\n    const embeddings = new OpenAIEmbeddings({\n      model: \"text-embedding-3-large\"\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"Azure\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/openai\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/openai\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/openai\n      ```\n    </CodeGroup>\n\n    ```bash theme={null} theme={null}\n    AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>\n    AZURE_OPENAI_API_KEY=<YOUR_KEY>\n    AZURE_OPENAI_API_VERSION=\"2024-02-01\"\n    ```\n\n    ```typescript theme={null} theme={null}\n    import { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n\n    const embeddings = new AzureOpenAIEmbeddings({\n      azureOpenAIApiEmbeddingsDeploymentName: \"text-embedding-ada-002\"\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"AWS\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/aws\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/aws\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/aws\n      ```\n    </CodeGroup>\n\n    ```bash theme={null} theme={null}\n    BEDROCK_AWS_REGION=your-region\n    ```\n\n    ```typescript theme={null} theme={null}\n    import { BedrockEmbeddings } from \"@langchain/aws\";\n\n    const embeddings = new BedrockEmbeddings({\n      model: \"amazon.titan-embed-text-v1\"\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"VertexAI\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/google-vertexai\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/google-vertexai\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/google-vertexai\n      ```\n    </CodeGroup>\n\n    ```bash theme={null} theme={null}\n    GOOGLE_APPLICATION_CREDENTIALS=credentials.json\n    ```\n\n    ```typescript theme={null} theme={null}\n    import { VertexAIEmbeddings } from \"@langchain/google-vertexai\";\n\n    const embeddings = new VertexAIEmbeddings({\n      model: \"gemini-embedding-001\"\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"MistralAI\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/mistralai\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/mistralai\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/mistralai\n      ```\n    </CodeGroup>\n\n    ```bash theme={null} theme={null}\n    MISTRAL_API_KEY=your-api-key\n    ```\n\n    ```typescript theme={null} theme={null}\n    import { MistralAIEmbeddings } from \"@langchain/mistralai\";\n\n    const embeddings = new MistralAIEmbeddings({\n      model: \"mistral-embed\"\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"Cohere\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/cohere\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/cohere\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/cohere\n      ```\n    </CodeGroup>\n\n    ```bash theme={null} theme={null}\n    COHERE_API_KEY=your-api-key\n    ```\n\n    ```typescript theme={null} theme={null}\n    import { CohereEmbeddings } from \"@langchain/cohere\";\n\n    const embeddings = new CohereEmbeddings({\n      model: \"embed-english-v3.0\"\n    });\n    ```\n  </Tab>\n</Tabs>\n\nSelect a vector store:\n\n<Tabs>\n  <Tab title=\"Memory\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/classic\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/classic\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/classic\n      ```\n    </CodeGroup>\n\n    ```typescript theme={null} theme={null}\n    import { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";\n\n    const vectorStore = new MemoryVectorStore(embeddings);\n    ```\n  </Tab>\n\n  <Tab title=\"Chroma\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/community\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/community\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/community\n      ```\n    </CodeGroup>\n\n    ```typescript theme={null} theme={null}\n    import { Chroma } from \"@langchain/community/vectorstores/chroma\";\n\n    const vectorStore = new Chroma(embeddings, {\n      collectionName: \"a-test-collection\",\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"FAISS\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/community\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/community\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/community\n      ```\n    </CodeGroup>\n\n    ```typescript theme={null} theme={null}\n    import { FaissStore } from \"@langchain/community/vectorstores/faiss\";\n\n    const vectorStore = new FaissStore(embeddings, {});\n    ```\n  </Tab>\n\n  <Tab title=\"MongoDB\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/mongodb\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/mongodb\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/mongodb\n      ```\n    </CodeGroup>\n\n    ```typescript theme={null} theme={null}\n    import { MongoDBAtlasVectorSearch } from \"@langchain/mongodb\"\n    import { MongoClient } from \"mongodb\";\n\n    const client = new MongoClient(process.env.MONGODB_ATLAS_URI || \"\");\n    const collection = client\n      .db(process.env.MONGODB_ATLAS_DB_NAME)\n      .collection(process.env.MONGODB_ATLAS_COLLECTION_NAME);\n\n    const vectorStore = new MongoDBAtlasVectorSearch(embeddings, {\n      collection: collection,\n      indexName: \"vector_index\",\n      textKey: \"text\",\n      embeddingKey: \"embedding\",\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"PGVector\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/community\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/community\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/community\n      ```\n    </CodeGroup>\n\n    ```typescript theme={null} theme={null}\n    import { PGVectorStore } from \"@langchain/community/vectorstores/pgvector\";\n\n    const vectorStore = await PGVectorStore.initialize(embeddings, {})\n    ```\n  </Tab>\n\n  <Tab title=\"Pinecone\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/pinecone\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/pinecone\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/pinecone\n      ```\n    </CodeGroup>\n\n    ```typescript theme={null} theme={null}\n    import { PineconeStore } from \"@langchain/pinecone\";\n    import { Pinecone as PineconeClient } from \"@pinecone-database/pinecone\";\n\n    const pinecone = new PineconeClient({\n      apiKey: process.env.PINECONE_API_KEY,\n    });\n    const pineconeIndex = pinecone.Index(\"your-index-name\");\n\n    const vectorStore = new PineconeStore(embeddings, {\n      pineconeIndex,\n      maxConcurrency: 5,\n    });\n    ```\n  </Tab>\n\n  <Tab title=\"Qdrant\">\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm i @langchain/qdrant\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/qdrant\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/qdrant\n      ```\n    </CodeGroup>\n\n    ```typescript theme={null} theme={null}\n    import { QdrantVectorStore } from \"@langchain/qdrant\";\n\n    const vectorStore = await QdrantVectorStore.fromExistingCollection(embeddings, {\n      url: process.env.QDRANT_URL,\n      collectionName: \"langchainjs-testing\",\n    });\n    ```\n  </Tab>\n</Tabs>\n\n## 1. Indexing\n\n<Note>\n  **This section is an abbreviated version of the content in the [semantic search tutorial](/oss/javascript/langchain/knowledge-base).**\n\n  If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you're comfortable with [document loaders](/oss/javascript/langchain/retrieval#document_loaders), [embeddings](/oss/javascript/langchain/retrieval#embedding_models), and [vector stores](/oss/javascript/langchain/retrieval#vectorstores), feel free to skip to the next section on [retrieval and generation](/oss/javascript/langchain/rag#2-retrieval-and-generation).\n</Note>\n\nIndexing commonly works as follows:\n\n1. **Load**: First we need to load our data. This is done with [Document Loaders](/oss/javascript/langchain/retrieval#document_loaders).\n2. **Split**: [Text splitters](/oss/javascript/langchain/retrieval#text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](/oss/javascript/langchain/retrieval#vectorstores) and [Embeddings](/oss/javascript/langchain/retrieval#embedding_models) model.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=21403ce0d0c772da84dcc5b75cff4451\" alt=\"index_diagram\" data-og-width=\"2583\" width=\"2583\" data-og-height=\"1299\" height=\"1299\" data-path=\"images/rag_indexing.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=bf4eb8255b82a809dbbd2bc2a96d2ed7 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4ebc538b2c4765b609f416025e4dbbda 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=1838328a870c7353c42bf1cc2290a779 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=675f55e100bab5e2904d27db01775ccc 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4b9e544a7a3ec168651558bce854eb60 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=f5aeaaaea103128f374c03b05a317263 2500w\" />\n\n### Loading documents\n\nWe need to first load the blog post contents. We can use [DocumentLoaders](/oss/javascript/langchain/retrieval#document_loaders) for this, which are objects that load in data from a source and return a list of [Document](https://reference.langchain.com/javascript/classes/_langchain_core.documents.Document.html) objects.\n\n```typescript  theme={null}\nimport \"cheerio\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n\nconst pTagSelector = \"p\";\nconst cheerioLoader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n  {\n    selector: pTagSelector,\n  }\n);\n\nconst docs = await cheerioLoader.load();\n\nconsole.assert(docs.length === 1);\nconsole.log(`Total characters: ${docs[0].pageContent.length}`);\n```\n\n```\nTotal characters: 22360\n```\n\n```typescript  theme={null}\nconsole.log(docs[0].pageContent.slice(0, 500));\n```\n\n```\nBuilding agents with LLM (large language model) as its core controller is...\n```\n\n**Go deeper**\n\n`DocumentLoader`: Object that loads data from a source as list of `Documents`.\n\n* [Integrations](/oss/javascript/integrations/document_loaders/): 160+ integrations to choose from.\n* [`BaseLoader`](https://reference.langchain.com/javascript/classes/_langchain_core.document_loaders_base.BaseDocumentLoader.html): API reference for the base interface.\n\n### Splitting documents\n\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n\nTo handle this we'll split the [`Document`](https://reference.langchain.com/javascript/classes/_langchain_core.documents.Document.html) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n\nAs in the [semantic search tutorial](/oss/javascript/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\n```typescript  theme={null}\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\nconst allSplits = await splitter.splitDocuments(docs);\nconsole.log(`Split blog post into ${allSplits.length} sub-documents.`);\n```\n\n```\nSplit blog post into 29 sub-documents.\n```\n\n### Storing documents\n\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the [semantic search tutorial](/oss/javascript/langchain/knowledge-base), our approach is to [embed](/oss/javascript/langchain/retrieval#embedding_models/) the contents of each document split and insert these embeddings into a [vector store](/oss/javascript/langchain/retrieval#vectorstores/). Given an input query, we can then use vector search to retrieve relevant documents.\n\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the [start of the tutorial](/oss/javascript/langchain/rag#components).\n\n```typescript  theme={null}\nawait vectorStore.addDocuments(allSplits);\n```\n\n**Go deeper**\n\n`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings.\n\n* [Integrations](/oss/javascript/integrations/text_embedding/): 30+ integrations to choose from.\n* [Interface](https://reference.langchain.com/javascript/classes/_langchain_core.embeddings.Embeddings.html): API reference for the base interface.\n\n`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings.\n\n* [Integrations](/oss/javascript/integrations/vectorstores/): 40+ integrations to choose from.\n* [Interface](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html): API reference for the base interface.\n\nThis completes the **Indexing** portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\n## 2. Retrieval and Generation\n\nRAG applications commonly work as follows:\n\n1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/oss/javascript/langchain/retrieval#retrievers).\n2. **Generate**: A [model](/oss/javascript/langchain/models) produces an answer using a prompt that includes both the question with the retrieved data\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=994c3585cece93c80873d369960afd44\" alt=\"retrieval_diagram\" data-og-width=\"2532\" width=\"2532\" data-og-height=\"1299\" height=\"1299\" data-path=\"images/rag_retrieval_generation.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=3bd28b3662e08c8364b60b74f510751e 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=43484903ca631a47a54e86191eb5ba22 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=67fe2302e241fc24238a5df1cf56573d 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=d390a6a758e688ec36352d30b22249b0 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=59729377317a0631598b6a4a2a7d8c92 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=c07711c71153c3b2dfd5b0104ad3e324 2500w\" />\n\nNow let's write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n\nWe will demonstrate:\n\n1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.\n2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n### RAG agents\n\nOne formulation of a RAG application is as a simple [agent](/oss/javascript/langchain/agents) with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a [tool](/oss/javascript/langchain/tools) that wraps our vector store:\n\n```typescript  theme={null}\nimport * as z from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst retrieveSchema = z.object({ query: z.string() });\n\nconst retrieve = tool(\n  async ({ query }) => {\n    const retrievedDocs = await vectorStore.similaritySearch(query, 2);\n    const serialized = retrievedDocs\n      .map(\n        (doc) => `Source: ${doc.metadata.source}\\nContent: ${doc.pageContent}`\n      )\n      .join(\"\\n\");\n    return [serialized, retrievedDocs];\n  },\n  {\n    name: \"retrieve\",\n    description: \"Retrieve information related to a query.\",\n    schema: retrieveSchema,\n    responseFormat: \"content_and_artifact\",\n  }\n);\n```\n\n<Tip>\n  Here we specify the `responseFormat` to `content_and_artifact` to confiugre the tool to attach raw documents as [artifacts](/oss/javascript/langchain/messages#param-artifact) to each [ToolMessage](/oss/javascript/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n</Tip>\n\nGiven our tool, we can construct the agent:\n\n```typescript  theme={null}\nimport { createAgent } from \"langchain\";\n\nconst tools = [retrieve];\nconst systemPrompt = new SystemMessage(\n    \"You have access to a tool that retrieves context from a blog post. \" +\n    \"Use the tool to help answer user queries.\"\n)\n\nconst agent = createAgent({ model: \"gpt-5\", tools, systemPrompt });\n```\n\nLet's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\n\n```typescript  theme={null}\nlet inputMessage = `What is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.`;\n\nlet agentInputs = { messages: [{ role: \"user\", content: inputMessage }] };\n\nconst stream = await agent.stream(agentInputs, {\n  streamMode: \"values\",\n});\nfor await (const step of stream) {\n  const lastMessage = step.messages[step.messages.length - 1];\n  console.log(`[${lastMessage.role}]: ${lastMessage.content}`);\n  console.log(\"-----\\n\");\n}\n```\n\n```\n[human]: What is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n-----\n\n[ai]:\nTools:\n- retrieve({\"query\":\"standard method for Task Decomposition\"})\n-----\n\n[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: hard tasks into smaller and simpler steps...\nSource: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: System message:Think step by step and reason yourself...\n-----\n\n[ai]:\nTools:\n- retrieve({\"query\":\"common extensions of Task Decomposition method\"})\n-----\n\n[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: hard tasks into smaller and simpler steps...\nSource: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: be provided by other developers (as in Plugins) or self-defined...\n-----\n\n[ai]: ### Standard Method for Task Decomposition\n\nThe standard method for task decomposition involves...\n-----\n```\n\nNote that the agent:\n\n1. Generates a query to search for a standard method for task decomposition;\n2. Receiving the answer, generates a second query to search for common extensions of it;\n3. Having received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/7b42d478-33d2-4631-90a4-7cb731681e88/r).\n\n<Tip>\n  You can add a deeper level of control and customization using the [LangGraph](/oss/javascript/langgraph/overview) framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph's [Agentic RAG tutorial](/oss/javascript/langgraph/agentic-rag) for more advanced formulations.\n</Tip>\n\n### RAG chains\n\nIn the above [agentic RAG](#rag-agents) formulation we allow the LLM to use its discretion in generating a [tool call](/oss/javascript/langchain/models#tool-calling) to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\n| âœ… Benefits                                                                                                                                                 | âš ï¸ Drawbacks                                                                                                                                |\n| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Search only when needed** â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.                        | **Two inference calls** â€“ When a search is performed, it requires one call to generate the query and another to produce the final response. |\n| **Contextual search queries** â€“ By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.                    |\n| **Multiple searches allowed** â€“ The LLM can execute several searches in support of a single user query.                                                    |                                                                                                                                             |\n\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\n\nIn this approach we no longer call the model in a loop, but instead make a single pass.\n\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\n\n```typescript  theme={null}\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\nimport { SystemMessage } from \"@langchain/core/messages\";\n\nconst agent = createAgent({\n  model,\n  tools: [],\n  middleware: [\n    dynamicSystemPromptMiddleware(async (state) => {\n        const lastQuery = state.messages[state.messages.length - 1].content;\n\n        const retrievedDocs = await vectorStore.similaritySearch(lastQuery, 2);\n\n        const docsContent = retrievedDocs\n        .map((doc) => doc.pageContent)\n        .join(\"\\n\\n\");\n\n        // Build system message\n        const systemMessage = new SystemMessage(\n        `You are a helpful assistant. Use the following context in your response:\\n\\n${docsContent}`\n        );\n\n        // Return system + existing messages\n        return [systemMessage, ...state.messages];\n    })\n  ]\n});\n```\n\nLet's try this out:\n\n```typescript  theme={null}\nlet inputMessage = `What is Task Decomposition?`;\n\nlet chainInputs = { messages: [{ role: \"user\", content: inputMessage }] };\n\nconst stream = await agent.stream(chainInputs, {\n  streamMode: \"values\",\n})\nfor await (const step of stream) {\n  const lastMessage = step.messages[step.messages.length - 1];\n  prettyPrint(lastMessage);\n  console.log(\"-----\\n\");\n}\n```\n\nIn the [LangSmith trace](https://smith.langchain.com/public/0322904b-bc4c-4433-a568-54c6b31bbef4/r/9ef1c23e-380e-46bf-94b3-d8bb33df440c) we can see the retrieved context incorporated into the model prompt.\n\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\n\n<Accordion title=\"Returning source documents\">\n  The above [RAG chain](#rag-chains) incorporates retrieved context into a single system message for that run.\n\n  As in the [agentic RAG](#rag-agents) formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\n\n  1. Adding a key to the state to store the retrieved documents\n  2. Adding a new node via a [pre-model hook](/oss/javascript/langchain/agents#pre-model-hook) to populate that key (as well as inject the context).\n\n  ```typescript  theme={null}\n  import { createMiddleware, Document, createAgent } from \"langchain\";\n  import { MessagesZodSchema } from \"@langchain/langgraph\";\n\n  const StateSchema = z.object({\n    messages: MessagesZodSchema,\n    context: z.array(z.custom<Document>()),\n  })\n\n  const retrieveDocumentsMiddleware = createMiddleware({\n    stateSchema: StateSchema,\n    beforeModel: async (state) => {\n      const lastMessage = state.messages[state.messages.length - 1].content;\n      const retrievedDocs = await vectorStore.similaritySearch(lastMessage, 2);\n\n      const docsContent = retrievedDocs\n        .map((doc) => doc.pageContent)\n        .join(\"\\n\\n\");\n\n      const augmentedMessageContent = [\n          ...lastMessage.content,\n          { type: \"text\", text: `Use the following context to answer the query:\\n\\n${docsContent}` }\n      ]\n\n      // Below we augment each input message with context, but we could also\n      // modify just the system message, as before.\n      return {\n        messages: [{\n          ...lastMessage,\n          content: augmentedMessageContent,\n        }]\n        context: retrievedDocs,\n      }\n    },\n  });\n\n  const agent = createAgent({\n    model,\n    tools: [],\n    middleware: [retrieveDocumentsMiddleware],\n  });\n  ```\n</Accordion>\n\n## Next steps\n\nNow that we've implemented a simple RAG application via [`createAgent`](https://reference.langchain.com/javascript/functions/langchain.index.createAgent.html), we can easily incorporate new features and go deeper:\n\n* [Stream](/oss/javascript/langchain/streaming) tokens and other information for responsive user experiences\n* Add [conversational memory](/oss/javascript/langchain/short-term-memory) to support multi-turn interactions\n* Add [long-term memory](/oss/javascript/langchain/long-term-memory) to support memory across conversational threads\n* Add [structured responses](/oss/javascript/langchain/structured-output)\n* Deploy your application with [LangSmith Deployments](/langsmith/deployments)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/rag.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 39827
}