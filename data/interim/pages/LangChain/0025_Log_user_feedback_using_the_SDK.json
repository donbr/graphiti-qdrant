{
  "title": "Log user feedback using the SDK",
  "source_url": "https://docs.langchain.com/langsmith/attach-user-feedback",
  "content": "<Tip>\n  **Key concepts**\n\n  * [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)\n  * [Reference guide on feedback data format](/langsmith/feedback-data-format)\n</Tip>\n\nLangSmith makes it easy to attach feedback to traces.\nThis feedback can come from users, annotators, automated evaluators, etc., and is crucial for monitoring and evaluating applications.\n\n## Use [create\\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback) / [createFeedback()](https://docs.smith.langchain.com/reference/js/classes/client.Client#createfeedback)\n\nHere we'll walk through how to log feedback using the SDK.\n\n<Info>\n  **Child runs**\n  You can attach user feedback to ANY child run of a trace, not just the trace (root run) itself.\n  This is useful for critiquing specific steps of the LLM application, such as the retrieval step or generation step of a RAG pipeline.\n</Info>\n\n<Tip>\n  **Non-blocking creation (Python only)**\n  The Python client will automatically background feedback creation if you pass `trace_id=` to [create\\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback).\n  This is essential for low-latency environments, where you want to make sure your application isn't blocked on feedback creation.\n</Tip>\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import trace, traceable, Client\n\n      @traceable\n      def foo(x):\n          return {\"y\": x * 2}\n\n      @traceable\n      def bar(y):\n          return {\"z\": y - 1}\n\n      client = Client()\n\n      inputs = {\"x\": 1}\n      with trace(name=\"foobar\", inputs=inputs) as root_run:\n          result = foo(**inputs)\n          result = bar(**result)\n          root_run.outputs = result\n          trace_id = root_run.id\n          child_runs = root_run.child_runs\n\n      # Provide feedback for a trace (a.k.a. a root run)\n      client.create_feedback(\n          key=\"user_feedback\",\n          score=1,\n          trace_id=trace_id,\n          comment=\"the user said that ...\"\n      )\n\n  # Provide feedback for a child run\n  foo_run_id = [run for run in child_runs if run.name == \"foo\"][0].id\n  client.create_feedback(\n      key=\"correctness\",\n      score=0,\n      run_id=foo_run_id,\n      # trace_id= is optional but recommended to enable batched and backgrounded\n      # feedback ingestion.\n      trace_id=trace_id,\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n  const client = new Client();\n\n      // ... Run your application and get the run_id...\n      // This information can be the result of a user-facing feedback form\n\n  await client.createFeedback(\n      runId,\n      \"feedback-key\",\n      {\n          score: 1.0,\n          comment: \"comment\",\n      }\n  );\n  ```\n</CodeGroup>\n\nYou can even log feedback for in-progress runs using `create_feedback() / createFeedback()`. See [this guide](/langsmith/access-current-span) for how to get the run ID of an in-progress run.\n\nTo learn more about how to filter traces based on various attributes, including user feedback, see [this guide](/langsmith/filter-traces-in-application).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/attach-user-feedback.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 3577
}