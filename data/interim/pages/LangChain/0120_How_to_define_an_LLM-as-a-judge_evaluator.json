{
  "title": "How to define an LLM-as-a-judge evaluator",
  "source_url": "https://docs.langchain.com/langsmith/llm-as-judge",
  "content": "<Info>\n  * [LLM-as-a-judge evaluator](/langsmith/evaluation-concepts#llm-as-judge)\n</Info>\n\nLLM applications can be challenging to evaluate since they often generate conversational text with no single correct answer.\n\nThis guide shows you how to define an LLM-as-a-judge evaluator for [offline evaluation](/langsmith/evaluation-concepts#offline-evaluation) using either the LangSmith SDK or the UI. Note: To run evaluations in real-time on your production traces, refer to [setting up online evaluations](/langsmith/online-evaluations#configure-llm-as-judge-evaluators).\n\n## SDK\n\n### Pre-built evaluators\n\nPre-built evaluators are a useful starting point for setting up evaluations. Refer to [pre-built evaluators](/langsmith/prebuilt-evaluators) for how to use pre-built evaluators with LangSmith.\n\n### Create your own LLM-as-a-judge evaluator\n\nFor complete control of evaluator logic, create your own LLM-as-a-judge evaluator and run it using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) / [TypeScript](https://docs.smith.langchain.com/reference/js)).\n\nRequires `langsmith>=0.2.0`\n\n```python  theme={null}\nfrom langsmith import evaluate, traceable, wrappers, Client\nfrom openai import OpenAI\n# Assumes you've installed pydantic\nfrom pydantic import BaseModel\n\n# Optionally wrap the OpenAI client to trace all model calls.\noai_client = wrappers.wrap_openai(OpenAI())\n\ndef valid_reasoning(inputs: dict, outputs: dict) -> bool:\n    \"\"\"Use an LLM to judge if the reasoning and the answer are consistent.\"\"\"\n    instructions = \"\"\"\nGiven the following question, answer, and reasoning, determine if the reasoning\nfor the answer is logically valid and consistent with the question and the answer.\"\"\"\n\n    class Response(BaseModel):\n        reasoning_is_valid: bool\n\n    msg = f\"Question: {inputs['question']}\\nAnswer: {outputs['answer']}\\nReasoning: {outputs['reasoning']}\"\n    response = oai_client.beta.chat.completions.parse(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"system\", \"content\": instructions,}, {\"role\": \"user\", \"content\": msg}],\n        response_format=Response\n    )\n    return response.choices[0].message.parsed.reasoning_is_valid\n\n# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.\n@traceable\ndef dummy_app(inputs: dict) -> dict:\n    return {\"answer\": \"hmm i'm not sure\", \"reasoning\": \"i didn't understand the question\"}\n\nls_client = Client()\ndataset = ls_client.create_dataset(\"big questions\")\nexamples = [\n    {\"inputs\": {\"question\": \"how will the universe end\"}},\n    {\"inputs\": {\"question\": \"are we alone\"}},\n]\nls_client.create_examples(dataset_id=dataset.id, examples=examples)\n\nresults = evaluate(\n    dummy_app,\n    data=dataset,\n    evaluators=[valid_reasoning]\n)\n```\n\nSee [here](/langsmith/code-evaluator) for more on how to write a custom evaluator.\n\n## UI\n\n### Pre-built evaluators\n\nPre-built evaluators are a useful starting point when setting up evaluations. The LangSmith UI supports the following pre-built evaluators:\n\n* **Hallucination**: Detect factually incorrect outputs. Requires a reference output.\n* **Correctness**: Check semantic similarity to a reference.\n* **Conciseness**: Evaluate whether an answer is a concise response to a question.\n* **Code checker**: Verify correctness of code answers.\n\nYou can configure these evaluators::\n\n* When running an evaluation using the [playground](/langsmith/observability-concepts#prompt-playground)\n* As part of a dataset to [automatically run evaluations on experiments](/langsmith/bind-evaluator-to-dataset)\n* When running an [online evaluation](/langsmith/online-evaluations#configure-llm-as-judge-evaluators)\n\n## Customize your LLM-as-a-judge evaluator\n\nAdd specific instructions for your LLM-as-a-judge evalutor prompt and configure which parts of the input/output/reference output should be passed to the evaluator.\n\n### Select/create the evaluator\n\n* In the playground or from a dataset: Select the **+Evaluator** button\n* From a tracing project: Select **Add rules**, configure your rule and select **Apply evaluator**\n\nSelect the **Create your own evaluator option**. Alternatively, you may start by selecting a pre-built evaluator and editing it.\n\n### Configure the evaluator\n\n#### Prompt\n\nCreate a new prompt, or choose an existing prompt from the [prompt hub](/langsmith/prompt-engineering-quickstart).\n\n* **Create your own prompt**: Create a custom prompt inline.\n\n* **Pull a prompt from the prompt hub**: Use the **Select a prompt** dropdown to select from an existing prompt. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses. To make changes, edit the prompt in the playground and commit the version, and then pull in your new prompt in the evaluator.\n\n#### Model\n\nSelect the desired model from the provided options.\n\n#### Mapping variables\n\nUse variable mapping to indicate the variables that are passed into your evaluator prompt from your run or example. To aid with variable mapping, an example (or run) is provided for reference. Click on the the variables in your prompt and use the dropdown to map them to the relevant parts of the input, output, or reference output.\n\nTo add prompt variables type the variable with double curly brackets `{{prompt_var}}` if using mustache formatting (the default) or single curly brackets `{prompt_var}` if using f-string formatting.\n\nYou may remove variables as needed. For example if you are evaluating a metric such as conciseness, you typically don't need a reference output so you may remove that variable.\n\n#### Preview\n\nPreviewing the prompt will show you of what the formatted prompt will look like using the reference run and dataset example shown on the right.\n\n#### Improve your evaluator with few-shot examples\n\nTo better align the LLM-as-a-judge evaluator to human preferences, LangSmith allows you to collect [human corrections](/langsmith/create-few-shot-evaluators#make-corrections) on evaluator scores. With this selection enabled, corrections are then inserted automatically as few-shot examples into your prompt.\n\nLearn [how to set up few-shot examples and make corrections](/langsmith/create-few-shot-evaluators).\n\n#### Feedback configuration\n\nFeedback configuration is the scoring criteria that your LLM-as-a-judge evaluator will use. Think of this as the rubric that your evaluator will grade based on. Scores will be added as [feedback](/langsmith/observability-concepts#feedback) to a run or example. Defining feedback for your evaluator:\n\n1. **Name the feedback key**: This is the name that will appear when viewing evaluation results. Names should be unique across experiments.\n\n2. **Add a description**: Describe what the feedback represents.\n\n3. **Choose a feedback type**:\n\n* **Boolean**: True/false feedback.\n* **Categorical**: Select from predefined categories.\n* **Continuous**: Numerical scoring within a specified range.\n\nBehind the scenes, feedback configuration is added as [structured output](https://python.langchain.com/docs/concepts/structured_outputs/) to the LLM-as-a-judge prompt. If you're using an existing prompt from the hub, you must add an output schema to the prompt before configuring an evaluator to use it. Each top-level key in the output schema will be treated as a separate piece of feedback.\n\n### Save the evaluator\n\nOnce your are finished configuring, save your changes.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/llm-as-judge.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 7773
}