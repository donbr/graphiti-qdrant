{
  "title": "ChatGoogleGenerativeAI",
  "source_url": "https://docs.langchain.com/oss/javascript/integrations/chat/google_generative_ai",
  "content": "[Google AI](https://ai.google.dev/) offers a number of different chat models, including the powerful Gemini series. For information on the latest models, their features, context windows, etc. head to the [Google AI docs](https://ai.google.dev/gemini-api/docs/models/gemini).\n\nThis will help you getting started with `ChatGoogleGenerativeAI` [chat models](/oss/javascript/langchain/models). For detailed documentation of all `ChatGoogleGenerativeAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html).\n\n## Overview\n\n### Integration details\n\n| Class                                                                                                             | Package                                                                                     | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/google_generative_ai) |                                                Downloads                                                |                                                Version                                               |\n| :---------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------ | :---: | :----------: | :------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: |\n| [ChatGoogleGenerativeAI](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html) | [@langchain/google-genai](https://api.js.langchain.com/modules/langchain_google_genai.html) |   ❌   |       ✅      |                                            ✅                                           | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-genai?style=flat-square\\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-genai?style=flat-square\\&label=%20&) |\n\n### Model features\n\nSee the links in the table headers below for guides on how to use specific features.\n\n| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |\n| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |\n|                        ✅                        |                                 ✅                                |     ❌     |                               ✅                              |      ✅      |      ✅      |                               ✅                               |                              ✅                              |                                ❌                               |\n\n## Setup\n\nYou can access Google's `gemini` and `gemini-vision` models, as well as other\ngenerative models in LangChain through `ChatGoogleGenerativeAI` class in the\n`@langchain/google-genai` integration package.\n\n<Tip>\n  You can also access Google's `gemini` family of models via the LangChain VertexAI and VertexAI-web integrations. Click [here](/oss/javascript/integrations/chat/google_vertex_ai) to read the docs.\n</Tip>\n\n### Credentials\n\nGet an API key here: [https://ai.google.dev/tutorials/setup](https://ai.google.dev/tutorials/setup)\n\nThen set the `GOOGLE_API_KEY` environment variable:\n\n```bash  theme={null}\nexport GOOGLE_API_KEY=\"your-api-key\"\n```\n\nIf you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:\n\n```bash  theme={null}\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n### Installation\n\nThe LangChain `ChatGoogleGenerativeAI` integration lives in the `@langchain/google-genai` package:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/google-genai @langchain/core\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/google-genai @langchain/core\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/google-genai @langchain/core\n  ```\n</CodeGroup>\n\n## Instantiation\n\nNow we can instantiate our model object and generate chat completions:\n\n```typescript  theme={null}\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\"\n\nconst llm = new ChatGoogleGenerativeAI({\n    model: \"gemini-2.5-pro\",\n    temperature: 0,\n    maxRetries: 2,\n    // other params...\n})\n```\n\n## Invocation\n\n```typescript  theme={null}\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n```output  theme={null}\nAIMessage {\n  \"content\": \"J'adore programmer. \\n\",\n  \"additional_kwargs\": {\n    \"finishReason\": \"STOP\",\n    \"index\": 0,\n    \"safetyRatings\": [\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      }\n    ]\n  },\n  \"response_metadata\": {\n    \"finishReason\": \"STOP\",\n    \"index\": 0,\n    \"safetyRatings\": [\n      {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      },\n      {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"probability\": \"NEGLIGIBLE\"\n      }\n    ]\n  },\n  \"tool_calls\": [],\n  \"invalid_tool_calls\": [],\n  \"usage_metadata\": {\n    \"input_tokens\": 21,\n    \"output_tokens\": 5,\n    \"total_tokens\": 26\n  }\n}\n```\n\n```typescript  theme={null}\nconsole.log(aiMsg.content)\n```\n\n```output  theme={null}\nJ'adore programmer.\n```\n\n## Safety settings\n\nGemini models have default safety settings that can be overridden. If you are receiving lots of \"Safety Warnings\" from your models, you can try tweaking the safety\\_settings attribute of the model. For example, to turn off safety blocking for dangerous content, you can import enums from the `@google/generative-ai` package, then construct your LLM as follows:\n\n```typescript  theme={null}\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\nimport { HarmBlockThreshold, HarmCategory } from \"@google/generative-ai\";\n\nconst llmWithSafetySettings = new ChatGoogleGenerativeAI({\n  model: \"gemini-2.5-pro\",\n  temperature: 0,\n  safetySettings: [\n    {\n      category: HarmCategory.HARM_CATEGORY_HARASSMENT,\n      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    },\n  ],\n  // other params...\n});\n```\n\n## Tool calling\n\nTool calling with Google AI is mostly the same [as tool calling with other models](/oss/javascript/langchain/tools), but has a few restrictions on schema.\n\nThe Google AI API does not allow tool schemas to contain an object with unknown properties. For example, the following Zod schemas will throw an error:\n\n`const invalidSchema = z.object({ properties: z.record(z.unknown()) });`\n\nand\n\n`const invalidSchema2 = z.record(z.unknown());`\n\nInstead, you should explicitly define the properties of the object field. Here's an example:\n\n```typescript  theme={null}\nimport { tool } from \"@langchain/core/tools\";\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\nimport * as z from \"zod\";\n\n// Define your tool\nconst fakeBrowserTool = tool((_) => {\n  return \"The search result is xyz...\"\n}, {\n  name: \"browser_tool\",\n  description: \"Useful for when you need to find something on the web or summarize a webpage.\",\n  schema: z.object({\n    url: z.string().describe(\"The URL of the webpage to search.\"),\n    query: z.string().optional().describe(\"An optional search query to use.\"),\n  }),\n})\n\nconst llmWithTool = new ChatGoogleGenerativeAI({\n  model: \"gemini-pro\",\n}).bindTools([fakeBrowserTool]) // Bind your tools to the model\n\nconst toolRes = await llmWithTool.invoke([\n  [\n    \"human\",\n    \"Search the web and tell me what the weather will be like tonight in new york. use a popular weather website\",\n  ],\n]);\n\nconsole.log(toolRes.tool_calls);\n```\n\n```output  theme={null}\n[\n  {\n    name: 'browser_tool',\n    args: {\n      url: 'https://www.weather.com',\n      query: 'weather tonight in new york'\n    },\n    type: 'tool_call'\n  }\n]\n```\n\n### Built in Google Search Retrieval\n\nGoogle also offers a built in search tool which you can use to ground content generation in real-world information. Here's an example of how to use it:\n\n```typescript  theme={null}\nimport { DynamicRetrievalMode, GoogleSearchRetrievalTool } from \"@google/generative-ai\";\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\nconst searchRetrievalTool: GoogleSearchRetrievalTool = {\n  googleSearchRetrieval: {\n    dynamicRetrievalConfig: {\n      mode: DynamicRetrievalMode.MODE_DYNAMIC,\n      dynamicThreshold: 0.7, // default is 0.7\n    }\n  }\n};\nconst searchRetrievalModel = new ChatGoogleGenerativeAI({\n  model: \"gemini-2.5-pro\",\n  temperature: 0,\n  maxRetries: 0,\n}).bindTools([searchRetrievalTool]);\n\nconst searchRetrievalResult = await searchRetrievalModel.invoke(\"Who won the 2024 MLB World Series?\");\n\nconsole.log(searchRetrievalResult.content);\n```\n\n```output  theme={null}\nThe Los Angeles Dodgers won the 2024 World Series, defeating the New York Yankees in Game 5 on October 30, 2024, by a score of 7-6. This victory marks the Dodgers' eighth World Series title and their first in a full season since 1988.  They achieved this win by overcoming a 5-0 deficit, making them the first team in World Series history to win a clinching game after being behind by such a margin.  The Dodgers also became the first team in MLB postseason history to overcome a five-run deficit, fall behind again, and still win.  Walker Buehler earned the save in the final game, securing the championship for the Dodgers.\n```\n\nThe response also includes metadata about the search result:\n\n```typescript  theme={null}\nconsole.dir(searchRetrievalResult.response_metadata?.groundingMetadata, { depth: null });\n```\n\n```output  theme={null}\n{\n  searchEntryPoint: {\n    renderedContent: '<style>\\n' +\n      '.container {\\n' +\n      '  align-items: center;\\n' +\n      '  border-radius: 8px;\\n' +\n      '  display: flex;\\n' +\n      '  font-family: Google Sans, Roboto, sans-serif;\\n' +\n      '  font-size: 14px;\\n' +\n      '  line-height: 20px;\\n' +\n      '  padding: 8px 12px;\\n' +\n      '}\\n' +\n      '.chip {\\n' +\n      '  display: inline-block;\\n' +\n      '  border: solid 1px;\\n' +\n      '  border-radius: 16px;\\n' +\n      '  min-width: 14px;\\n' +\n      '  padding: 5px 16px;\\n' +\n      '  text-align: center;\\n' +\n      '  user-select: none;\\n' +\n      '  margin: 0 8px;\\n' +\n      '  -webkit-tap-highlight-color: transparent;\\n' +\n      '}\\n' +\n      '.carousel {\\n' +\n      '  overflow: auto;\\n' +\n      '  scrollbar-width: none;\\n' +\n      '  white-space: nowrap;\\n' +\n      '  margin-right: -12px;\\n' +\n      '}\\n' +\n      '.headline {\\n' +\n      '  display: flex;\\n' +\n      '  margin-right: 4px;\\n' +\n      '}\\n' +\n      '.gradient-container {\\n' +\n      '  position: relative;\\n' +\n      '}\\n' +\n      '.gradient {\\n' +\n      '  position: absolute;\\n' +\n      '  transform: translate(3px, -9px);\\n' +\n      '  height: 36px;\\n' +\n      '  width: 9px;\\n' +\n      '}\\n' +\n      '@media (prefers-color-scheme: light) {\\n' +\n      '  .container {\\n' +\n      '    background-color: #fafafa;\\n' +\n      '    box-shadow: 0 0 0 1px #0000000f;\\n' +\n      '  }\\n' +\n      '  .headline-label {\\n' +\n      '    color: #1f1f1f;\\n' +\n      '  }\\n' +\n      '  .chip {\\n' +\n      '    background-color: #ffffff;\\n' +\n      '    border-color: #d2d2d2;\\n' +\n      '    color: #5e5e5e;\\n' +\n      '    text-decoration: none;\\n' +\n      '  }\\n' +\n      '  .chip:hover {\\n' +\n      '    background-color: #f2f2f2;\\n' +\n      '  }\\n' +\n      '  .chip:focus {\\n' +\n      '    background-color: #f2f2f2;\\n' +\n      '  }\\n' +\n      '  .chip:active {\\n' +\n      '    background-color: #d8d8d8;\\n' +\n      '    border-color: #b6b6b6;\\n' +\n      '  }\\n' +\n      '  .logo-dark {\\n' +\n      '    display: none;\\n' +\n      '  }\\n' +\n      '  .gradient {\\n' +\n      '    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n' +\n      '  }\\n' +\n      '}\\n' +\n      '@media (prefers-color-scheme: dark) {\\n' +\n      '  .container {\\n' +\n      '    background-color: #1f1f1f;\\n' +\n      '    box-shadow: 0 0 0 1px #ffffff26;\\n' +\n      '  }\\n' +\n      '  .headline-label {\\n' +\n      '    color: #fff;\\n' +\n      '  }\\n' +\n      '  .chip {\\n' +\n      '    background-color: #2c2c2c;\\n' +\n      '    border-color: #3c4043;\\n' +\n      '    color: #fff;\\n' +\n      '    text-decoration: none;\\n' +\n      '  }\\n' +\n      '  .chip:hover {\\n' +\n      '    background-color: #353536;\\n' +\n      '  }\\n' +\n      '  .chip:focus {\\n' +\n      '    background-color: #353536;\\n' +\n      '  }\\n' +\n      '  .chip:active {\\n' +\n      '    background-color: #464849;\\n' +\n      '    border-color: #53575b;\\n' +\n      '  }\\n' +\n      '  .logo-light {\\n' +\n      '    display: none;\\n' +\n      '  }\\n' +\n      '  .gradient {\\n' +\n      '    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n' +\n      '  }\\n' +\n      '}\\n' +\n      '</style>\\n' +\n      '<div class=\"container\">\\n' +\n      '  <div class=\"headline\">\\n' +\n      '    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n' +\n      '      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n' +\n      '      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n' +\n      '      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n' +\n      '      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n' +\n      '    </svg>\\n' +\n      '    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n' +\n      '      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n' +\n      '      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n' +\n      '      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n' +\n      '      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n' +\n      '      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n' +\n      '    </svg>\\n' +\n      '    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n' +\n      '  </div>\\n' +\n      '  <div class=\"carousel\">\\n' +\n      '    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AZnLMfyXqJN3K4FKueRIZDY2Owjs5Rw4dqgDOc6ZjYKsFo4GgENxLktR2sPHtNUuEBIUeqmUYc3jz9pLRq2cgSpc-4EoGBwQSTTpKk71CX7revnXUa54r9LxcxKgYxrUNBm5HpEm6JDNeJykc6NacPYv43M2wgkrhHCHCzHRyjEP2YR0Pxq4JQMUuOrLeTAYWB9oUb87FE5ksfuB6gimqO5-6uS3psR6\">who won the 2024 mlb world series</a>\\n' +\n      '  </div>\\n' +\n      '</div>\\n'\n  },\n  groundingChunks: [\n    {\n      web: {\n        uri: 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AZnLMfwvs0gpiM4BbIcNXZnnp4d4ED_rLnIYz2ZwM-lwFnoUxXNlKzy7ZSbbs_E27yhARG6Gx2AuW7DsoqkWPfDFMqPdXfvG3n0qFOQxQ4MBQ9Ox9mTk3KH5KPRJ79m8V118RQRyhi6oK5qg5-fLQunXUVn_a42K7eMk7Kjb8VpZ4onl8Glv1lQQsAK7YWyYkQ7WkTHDHVGB-vrL2U2yRQ==',\n        title: 'foxsports.com'\n      }\n    },\n    {\n      web: {\n        uri: 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AZnLMfwxwBq8VYgKAhf3UC8U6U5D-i0lK4TwP-2Jf8ClqB-sI0iptm9GxgeaH1iHFbSi-j_C3UqYj8Ok0YDTyvg87S7JamU48pndrd467ZQbI2sI0yWxsCCZ_dosXHwemBHFL5TW2hbAqasq93CfJ09cp1jU',\n        title: 'mlb.com'\n      }\n    }\n  ],\n  groundingSupports: [\n    {\n      segment: {\n        endIndex: 131,\n        text: 'The Los Angeles Dodgers won the 2024 World Series, defeating the New York Yankees in Game 5 on October 30, 2024, by a score of 7-6.'\n      },\n      groundingChunkIndices: [ 0, 1 ],\n      confidenceScores: [ 0.7652759, 0.7652759 ]\n    },\n    {\n      segment: {\n        startIndex: 401,\n        endIndex: 531,\n        text: 'The Dodgers also became the first team in MLB postseason history to overcome a five-run deficit, fall behind again, and still win.'\n      },\n      groundingChunkIndices: [ 1 ],\n      confidenceScores: [ 0.8487609 ]\n    }\n  ],\n  retrievalMetadata: { googleSearchDynamicRetrievalScore: 0.93359375 },\n  webSearchQueries: [ 'who won the 2024 mlb world series' ]\n}\n```\n\n### Code execution\n\nGoogle Generative AI also supports code execution. Using the built in `CodeExecutionTool`, you can make the model generate code, execute it, and use the results in a final completion:\n\n```typescript  theme={null}\nimport { CodeExecutionTool } from \"@google/generative-ai\";\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\nconst codeExecutionTool: CodeExecutionTool = {\n  codeExecution: {}, // Simply pass an empty object to enable it.\n};\nconst codeExecutionModel = new ChatGoogleGenerativeAI({\n  model: \"gemini-2.5-pro\",\n  temperature: 0,\n  maxRetries: 0,\n}).bindTools([codeExecutionTool]);\n\nconst codeExecutionResult = await codeExecutionModel.invoke(\"Use code execution to find the sum of the first and last 3 numbers in the following list: [1, 2, 3, 72638, 8, 727, 4, 5, 6]\");\n\nconsole.dir(codeExecutionResult.content, { depth: null });\n```\n\n```output  theme={null}\n[\n  {\n    type: 'text',\n    text: \"Here's how to find the sum of the first and last three numbers in the given list using Python:\\n\" +\n      '\\n'\n  },\n  {\n    type: 'executableCode',\n    executableCode: {\n      language: 'PYTHON',\n      code: '\\n' +\n        'my_list = [1, 2, 3, 72638, 8, 727, 4, 5, 6]\\n' +\n        '\\n' +\n        'first_three_sum = sum(my_list[:3])\\n' +\n        'last_three_sum = sum(my_list[-3:])\\n' +\n        'total_sum = first_three_sum + last_three_sum\\n' +\n        '\\n' +\n        'print(f\"{first_three_sum=}\")\\n' +\n        'print(f\"{last_three_sum=}\")\\n' +\n        'print(f\"{total_sum=}\")\\n' +\n        '\\n'\n    }\n  },\n  {\n    type: 'codeExecutionResult',\n    codeExecutionResult: {\n      outcome: 'OUTCOME_OK',\n      output: 'first_three_sum=6\\nlast_three_sum=15\\ntotal_sum=21\\n'\n    }\n  },\n  {\n    type: 'text',\n    text: 'Therefore, the sum of the first three numbers (1, 2, 3) is 6, the sum of the last three numbers (4, 5, 6) is 15, and their total sum is 21.\\n'\n  }\n]\n```\n\nYou can also pass this generation back to the model as chat history:\n\n```typescript  theme={null}\nconst codeExecutionExplanation = await codeExecutionModel.invoke([\n  codeExecutionResult,\n  {\n    role: \"user\",\n    content: \"Please explain the question I asked, the code you wrote, and the answer you got.\",\n  }\n])\n\nconsole.log(codeExecutionExplanation.content);\n```\n\n```output  theme={null}\nYou asked for the sum of the first three and the last three numbers in the list `[1, 2, 3, 72638, 8, 727, 4, 5, 6]`.\n\nHere's a breakdown of the code:\n\n1. **`my_list = [1, 2, 3, 72638, 8, 727, 4, 5, 6]`**: This line defines the list of numbers you provided.\n\n2. **`first_three_sum = sum(my_list[:3])`**: This calculates the sum of the first three numbers.  `my_list[:3]` is a slice of the list that takes elements from the beginning up to (but not including) the index 3.  So, it takes elements at indices 0, 1, and 2, which are 1, 2, and 3. The `sum()` function then adds these numbers together.\n\n3. **`last_three_sum = sum(my_list[-3:])`**: This calculates the sum of the last three numbers. `my_list[-3:]` is a slice that takes elements starting from the third element from the end and goes to the end of the list. So it takes elements at indices -3, -2, and -1 which correspond to 4, 5, and 6. The `sum()` function adds these numbers.\n\n4. **`total_sum = first_three_sum + last_three_sum`**: This adds the sum of the first three numbers and the sum of the last three numbers to get the final result.\n\n5. **`print(f\"{first_three_sum=}\")`**, **`print(f\"{last_three_sum=}\")`**, and **`print(f\"{total_sum=}\")`**: These lines print the calculated sums in a clear and readable format.\n\n\nThe output of the code was:\n\n* `first_three_sum=6`\n* `last_three_sum=15`\n* `total_sum=21`\n\nTherefore, the answer to your question is 21.\n```\n\n## Context caching\n\nContext caching allows you to pass some content to the model once, cache the input tokens, and then refer to the cached tokens for subsequent requests to reduce cost. You can create a `CachedContent` object using `GoogleAICacheManager` class and then pass the `CachedContent` object to your `ChatGoogleGenerativeAIModel` with `enableCachedContent()` method.\n\n```typescript  theme={null}\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\nimport {\n  GoogleAICacheManager,\n  GoogleAIFileManager,\n} from \"@google/generative-ai/server\";\n\nconst fileManager = new GoogleAIFileManager(process.env.GOOGLE_API_KEY);\nconst cacheManager = new GoogleAICacheManager(process.env.GOOGLE_API_KEY);\n\n// uploads file for caching\nconst pathToVideoFile = \"/path/to/video/file\";\nconst displayName = \"example-video\";\nconst fileResult = await fileManager.uploadFile(pathToVideoFile, {\n    displayName,\n    mimeType: \"video/mp4\",\n});\n\n// creates cached content AFTER uploading is finished\nconst cachedContent = await cacheManager.create({\n    model: \"models/gemini-2.5-flash\",\n    displayName: displayName,\n    systemInstruction: \"You are an expert video analyzer, and your job is to answer \" +\n      \"the user's query based on the video file you have access to.\",\n    contents: [\n        {\n            role: \"user\",\n            parts: [\n                {\n                    fileData: {\n                        mimeType: fileResult.file.mimeType,\n                        fileUri: fileResult.file.uri,\n                    },\n                },\n            ],\n        },\n    ],\n    ttlSeconds: 300,\n});\n\n// passes cached video to model\nconst model = new ChatGoogleGenerativeAI({});\nmodel.useCachedContent(cachedContent);\n\n// invokes model with cached video\nawait model.invoke(\"Summarize the video\");\n```\n\n**Note**\n\n* The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model.\n\n## Gemini prompting FAQs\n\nAs of the time this doc was written (2023/12/12), Gemini has some restrictions on the types and structure of prompts it accepts. Specifically:\n\n1. When providing multimodal (image) inputs, you are restricted to at most 1 message of \"human\" (user) type. You cannot pass multiple messages (though the single human message may have multiple content entries)\n2. System messages are not natively supported, and will be merged with the first human message if present.\n3. For regular chat conversations, messages must follow the human/ai/human/ai alternating pattern. You may not provide 2 AI or human messages in sequence.\n4. Message may be blocked if they violate the safety checks of the LLM. In this case, the model will return an empty response.\n\n***\n\n## API reference\n\nFor detailed documentation of all `ChatGoogleGenerativeAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/chat/google_generative_ai.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 25847
}