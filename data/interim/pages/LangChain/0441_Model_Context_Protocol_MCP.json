{
  "title": "Model Context Protocol (MCP)",
  "source_url": "https://docs.langchain.com/oss/python/langchain/mcp",
  "content": "[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library.\n\n## Install\n\nInstall the `langchain-mcp-adapters` library to use MCP tools in LangGraph:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-mcp-adapters\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-mcp-adapters\n  ```\n</CodeGroup>\n\n## Transport types\n\nMCP supports different transport mechanisms for client-server communication:\n\n* **stdio** – Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\n* **Streamable HTTP** – Server runs as an independent process handling HTTP requests. Supports remote connections and multiple clients.\n* **Server-Sent Events (SSE)** – a variant of streamable HTTP optimized for real-time streaming communication.\n\n## Use MCP tools\n\n`langchain-mcp-adapters` enables agents to use tools defined across one or more MCP server.\n\n```python Accessing multiple MCP servers icon=\"server\" theme={null}\nfrom langchain_mcp_adapters.client import MultiServerMCPClient  # [!code highlight]\nfrom langchain.agents import create_agent\n\n\nclient = MultiServerMCPClient(  # [!code highlight]\n    {\n        \"math\": {\n            \"transport\": \"stdio\",  # Local subprocess communication\n            \"command\": \"python\",\n            # Absolute path to your math_server.py file\n            \"args\": [\"/path/to/math_server.py\"],\n        },\n        \"weather\": {\n            \"transport\": \"streamable_http\",  # HTTP-based remote server\n            # Ensure you start your weather server on port 8000\n            \"url\": \"http://localhost:8000/mcp\",\n        }\n    }\n)\n\ntools = await client.get_tools()  # [!code highlight]\nagent = create_agent(\n    \"claude-sonnet-4-5-20250929\",\n    tools  # [!code highlight]\n)\nmath_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n)\nweather_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n)\n```\n\n<Note>\n  `MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up.\n</Note>\n\n## Custom MCP servers\n\nTo create your own MCP servers, you can use the `mcp` library. This library provides a simple way to define [tools](https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions) and run them as servers.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install mcp\n  ```\n\n  ```bash uv theme={null}\n  uv add mcp\n  ```\n</CodeGroup>\n\nUse the following reference implementations to test your agent with MCP tool servers.\n\n```python title=\"Math server (stdio transport)\" icon=\"floppy-disk\" theme={null}\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Math\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n```\n\n```python title=\"Weather server (streamable HTTP transport)\" icon=\"wifi\" theme={null}\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Weather\")\n\n@mcp.tool()\nasync def get_weather(location: str) -> str:\n    \"\"\"Get weather for location.\"\"\"\n    return \"It's always sunny in New York\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n```\n\n## Stateful tool usage\n\nFor stateful servers that maintain context between tool calls, use `client.session()` to create a persistent `ClientSession`.\n\n```python Using MCP ClientSession for stateful tool usage theme={null}\nfrom langchain_mcp_adapters.tools import load_mcp_tools\n\nclient = MultiServerMCPClient({...})\nasync with client.session(\"math\") as session:\n    tools = await load_mcp_tools(session)\n```\n\n## Additional resources\n\n* [MCP documentation](https://modelcontextprotocol.io/introduction)\n* [MCP Transport documentation](https://modelcontextprotocol.io/docs/concepts/transports)\n* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/mcp.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 4660
}