{
  "title": "Short-term memory",
  "source_url": "https://docs.langchain.com/oss/javascript/langchain/short-term-memory",
  "content": "## Overview\n\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n\nShort term memory lets your application remember previous interactions within a single thread or conversation.\n\n<Note>\n  A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n</Note>\n\nConversation history is the most common form of short-term memory. Long conversations pose a challenge to today's LLMs; a full history may not fit inside an LLM's context window, resulting in an context loss or errors.\n\nEven if your model supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs.\n\nChat models accept context using [messages](/oss/javascript/langchain/messages), which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or \"forget\" stale information.\n\n## Usage\n\nTo add short-term memory (thread-level persistence) to an agent, you need to specify a `checkpointer` when creating an agent.\n\n<Info>\n  LangChain's agent manages short-term memory as a part of your agent's state.\n\n  By storing these in the graph's state, the agent can access the full context for a given conversation while maintaining separation between different threads.\n\n  State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\n\n  Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\n</Info>\n\n```ts {highlight={2,4, 9,14}} theme={null}\nimport { createAgent } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools: [],\n    checkpointer,\n});\n\nawait agent.invoke(\n    { messages: [{ role: \"user\", content: \"hi! i am Bob\" }] },\n    { configurable: { thread_id: \"1\" } }\n);\n```\n\n### In production\n\nIn production, use a checkpointer backed by a database:\n\n```ts  theme={null}\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\n```\n\n## Customizing agent memory\n\nYou can extend the agent state by creating custom middleware with a state schema. Custom state schemas can be passed using the `stateSchema` parameter in middleware.\n\n```typescript  theme={null}\nimport * as z from \"zod\";\nimport { createAgent, createMiddleware } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst customStateSchema = z.object({  // [!code highlight]\n    userId: z.string(),  // [!code highlight]\n    preferences: z.record(z.string(), z.any()),  // [!code highlight]\n});  // [!code highlight]\n\nconst stateExtensionMiddleware = createMiddleware({\n    name: \"StateExtension\",\n    stateSchema: customStateSchema,  // [!code highlight]\n});\n\nconst checkpointer = new MemorySaver();\nconst agent = createAgent({\n    model: \"gpt-5\",\n    tools: [],\n    middleware: [stateExtensionMiddleware],  // [!code highlight]\n    checkpointer,\n});\n\n// Custom state can be passed in invoke\nconst result = await agent.invoke({\n    messages: [{ role: \"user\", content: \"Hello\" }],\n    userId: \"user_123\",  // [!code highlight]\n    preferences: { theme: \"dark\" },  // [!code highlight]\n});\n```\n\n## Common patterns\n\nWith [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:\n\n<CardGroup cols={2}>\n  <Card title=\"Trim messages\" icon=\"scissors\" href=\"#trim-messages\" arrow>\n    Remove first or last N messages (before calling LLM)\n  </Card>\n\n  <Card title=\"Delete messages\" icon=\"trash\" href=\"#delete-messages\" arrow>\n    Delete messages from LangGraph state permanently\n  </Card>\n\n  <Card title=\"Summarize messages\" icon=\"layer-group\" href=\"#summarize-messages\" arrow>\n    Summarize earlier messages in the history and replace them with a summary\n  </Card>\n\n  <Card title=\"Custom strategies\" icon=\"gears\">\n    Custom strategies (e.g., message filtering, etc.)\n  </Card>\n</CardGroup>\n\nThis allows the agent to keep track of the conversation without exceeding the LLM's context window.\n\n### Trim messages\n\nMost LLMs have a maximum supported context window (denominated in tokens).\n\nOne way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `maxTokens`) to use for handling the boundary.\n\nTo trim message history in an agent, use `stateModifier` with the [`trimMessages`](https://js.langchain.com/docs/how_to/trim_messages/) function:\n\n```typescript  theme={null}\nimport {\n    createAgent,\n    trimMessages,\n    type AgentState,\n} from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\n// This function will be called every time before the node that calls LLM\nconst stateModifier = async (state: AgentState) => {\n    return {\n        messages: await trimMessages(state.messages, {\n        strategy: \"last\",\n        maxTokens: 384,\n        startOn: \"human\",\n        endOn: [\"human\", \"tool\"],\n        tokenCounter: (msgs) => msgs.length,\n        }),\n    };\n};\n\nconst checkpointer = new MemorySaver();\nconst agent = createAgent({\n    model: \"gpt-5\",\n    tools: [],\n    preModelHook: stateModifier,\n    checkpointer,\n});\n```\n\n### Delete messages\n\nYou can delete messages from the graph state to manage the message history.\n\nThis is useful when you want to remove specific messages or clear the entire message history.\n\nTo delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with [`messagesStateReducer`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.messagesStateReducer.html) [reducer](/oss/javascript/langgraph/graph-api#reducers), like `MessagesZodState`.\n\nTo remove specific messages:\n\n```typescript  theme={null}\nimport { RemoveMessage } from \"@langchain/core/messages\";\n\nconst deleteMessages = (state) => {\n    const messages = state.messages;\n    if (messages.length > 2) {\n        // remove the earliest two messages\n        return {\n        messages: messages\n            .slice(0, 2)\n            .map((m) => new RemoveMessage({ id: m.id })),\n        };\n    }\n};\n```\n\n<Warning>\n  When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:\n\n  * Some providers expect message history to start with a `user` message\n  * Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.\n</Warning>\n\n```typescript  theme={null}\nimport { RemoveMessage } from \"@langchain/core/messages\";\nimport { AgentState, createAgent } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst deleteMessages = (state: AgentState) => {\n    const messages = state.messages;\n    if (messages.length > 2) {\n        // remove the earliest two messages\n        return {\n        messages: messages\n            .slice(0, 2)\n            .map((m) => new RemoveMessage({ id: m.id! })),\n        };\n    }\n    return {};\n};\n\nconst agent = createAgent({\n    model: \"gpt-5-nano\",\n    tools: [],\n    prompt: \"Please be concise and to the point.\",\n    postModelHook: deleteMessages,\n    checkpointer: new MemorySaver(),\n});\n\nconst config = { configurable: { thread_id: \"1\" } };\n\nconst streamA = await agent.stream(\n    { messages: [{ role: \"user\", content: \"hi! I'm bob\" }] },\n    { ...config, streamMode: \"values\" }\n);\nfor await (const event of streamA) {\n    const messageDetails = event.messages.map((message) => [\n        message.getType(),\n        message.content,\n    ]);\n    console.log(messageDetails);\n}\n\nconst streamB = await agent.stream(\n    {\n        messages: [{ role: \"user\", content: \"what's my name?\" }],\n    },\n    { ...config, streamMode: \"values\" }\n);\nfor await (const event of streamB) {\n    const messageDetails = event.messages.map((message) => [\n        message.getType(),\n        message.content,\n    ]);\n    console.log(messageDetails);\n}\n```\n\n```\n[['human', \"hi! I'm bob\"]]\n[['human', \"hi! I'm bob\"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?']]\n[['human', \"hi! I'm bob\"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'], ['human', \"what's my name?\"]]\n[['human', \"hi! I'm bob\"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'], ['human', \"what's my name?\"], ['ai', 'Your name is Bob.']]\n[['human', \"what's my name?\"], ['ai', 'Your name is Bob.']]\n```\n\n### Summarize messages\n\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.\nBecause of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c8ed3facdccd4ef5c7e52902c72ba938\" alt=\"\" data-og-width=\"609\" width=\"609\" data-og-height=\"242\" height=\"242\" data-path=\"oss/images/summary.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4208b9b0cc9f459f3dc4e5219918471b 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=7acb77c081545f57042368f4e9d0c8cb 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2fcfdb0c481d2e1d361e76db763a41e5 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4abdac693a562788aa0db8681bef8ea7 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=40acfefa91dcb11b247a6e4a7705f22b 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d765aaf7551e8b0fc2720de7d2ac2a8 2500w\" />\n\nTo summarize message history in an agent, use the built-in [`summarizationMiddleware`](/oss/javascript/langchain/middleware#summarization):\n\n```typescript  theme={null}\nimport { createAgent, summarizationMiddleware } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4o-mini\",\n      trigger: { tokens: 4000 },\n      keep: { messages: 20 },\n    }),\n  ],\n  checkpointer,\n});\n\nconst config = { configurable: { thread_id: \"1\" } };\nawait agent.invoke({ messages: \"hi, my name is bob\" }, config);\nawait agent.invoke({ messages: \"write a short poem about cats\" }, config);\nawait agent.invoke({ messages: \"now do the same but for dogs\" }, config);\nconst finalResponse = await agent.invoke({ messages: \"what's my name?\" }, config);\n\nconsole.log(finalResponse.messages.at(-1)?.content);\n// Your name is Bob!\n```\n\nSee [`summarizationMiddleware`](/oss/javascript/langchain/middleware#summarization) for more configuration options.\n\n## Access memory\n\nYou can access and modify the short-term memory (state) of an agent in several ways:\n\n### Tools\n\n#### Read short-term memory in a tool\n\nAccess short term memory (state) in a tool using the `ToolRuntime` parameter.\n\nThe `tool_runtime` parameter is hidden from the tool signature (so the model doesn't see it), but the tool can access the state through it.\n\n```typescript  theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst stateSchema = z.object({\n    userId: z.string(),\n});\n\nconst getUserInfo = tool(\n    async (_, config) => {\n        const userId = config.context?.userId;\n        return { userId };\n    },\n    {\n        name: \"get_user_info\",\n        description: \"Get user info\",\n        schema: z.object({}),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-5-nano\",\n    tools: [getUserInfo],\n    stateSchema,\n});\n\nconst result = await agent.invoke(\n    {\n        messages: [{ role: \"user\", content: \"what's my name?\" }],\n    },\n    {\n        context: {\n        userId: \"user_123\",\n        },\n    }\n);\n\nconsole.log(result.messages.at(-1)?.content);\n// Outputs: \"User is John Smith.\"\n```\n\n#### Write short-term memory from tools\n\nTo modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools.\n\nThis is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\n\n```typescript  theme={null}\nimport * as z from \"zod\";\nimport { tool, createAgent } from \"langchain\";\nimport { MessagesZodState, Command } from \"@langchain/langgraph\";\n\nconst CustomState = z.object({\n    messages: MessagesZodState.shape.messages,\n    userName: z.string().optional(),\n});\n\nconst updateUserInfo = tool(\n    async (_, config) => {\n        const userId = config.context?.userId;\n        const name = userId === \"user_123\" ? \"John Smith\" : \"Unknown user\";\n        return new Command({\n        update: {\n            userName: name,\n            // update the message history\n            messages: [\n            {\n                role: \"tool\",\n                content: \"Successfully looked up user information\",\n                tool_call_id: config.toolCall?.id,\n            },\n            ],\n        },\n        });\n    },\n    {\n        name: \"update_user_info\",\n        description: \"Look up and update user info.\",\n        schema: z.object({}),\n    }\n);\n\nconst greet = tool(\n    async (_, config) => {\n        const userName = config.context?.userName;\n        return `Hello ${userName}!`;\n    },\n    {\n        name: \"greet\",\n        description: \"Use this to greet the user once you found their info.\",\n        schema: z.object({}),\n    }\n);\n\nconst agent = createAgent({\n    model,\n    tools: [updateUserInfo, greet],\n    stateSchema: CustomState,\n});\n\nawait agent.invoke(\n    { messages: [{ role: \"user\", content: \"greet the user\" }] },\n    { context: { userId: \"user_123\" } }\n);\n```\n\n### Prompt\n\nAccess short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.\n\n```typescript  theme={null}\nimport * as z from \"zod\";\nimport { createAgent, tool, SystemMessage } from \"langchain\";\n\nconst contextSchema = z.object({\n    userName: z.string(),\n});\n\nconst getWeather = tool(\n    async ({ city }, config) => {\n        return `The weather in ${city} is always sunny!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get user info\",\n        schema: z.object({\n        city: z.string(),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-5-nano\",\n    tools: [getWeather],\n    contextSchema,\n    prompt: (state, config) => {\n        return [\n        new SystemMessage(\n            `You are a helpful assistant. Address the user as ${config.context?.userName}.`\n        ),\n        ...state.messages,\n    },\n});\n\nconst result = await agent.invoke(\n    {\n        messages: [{ role: \"user\", content: \"What is the weather in SF?\" }],\n    },\n    {\n        context: {\n        userName: \"John Smith\",\n        },\n    }\n);\n\nfor (const message of result.messages) {\n    console.log(message);\n}\n/**\n * HumanMessage {\n *   \"content\": \"What is the weather in SF?\",\n *   // ...\n * }\n * AIMessage {\n *   // ...\n *   \"tool_calls\": [\n *     {\n *       \"name\": \"get_weather\",\n *       \"args\": {\n *         \"city\": \"San Francisco\"\n *       },\n *       \"type\": \"tool_call\",\n *       \"id\": \"call_tCidbv0apTpQpEWb3O2zQ4Yx\"\n *     }\n *   ],\n *   // ...\n * }\n * ToolMessage {\n *   \"content\": \"The weather in San Francisco is always sunny!\",\n *   \"tool_call_id\": \"call_tCidbv0apTpQpEWb3O2zQ4Yx\"\n *   // ...\n * }\n * AIMessage {\n *   \"content\": \"John Smith, here's the latest: The weather in San Francisco is always sunny!\\n\\nIf you'd like more details (temperature, wind, humidity) or a forecast for the next few days, I can pull that up. What would you like?\",\n *   // ...\n * }\n */\n```\n\n### Before model\n\n```mermaid  theme={null}\n%%{\n    init: {\n        \"fontFamily\": \"monospace\",\n        \"flowchart\": {\n        \"curve\": \"basis\"\n        },\n        \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n    }\n}%%\ngraph TD\n    S([\"\\_\\_start\\_\\_\"])\n    PRE(before_model)\n    MODEL(model)\n    TOOLS(tools)\n    END([\"\\_\\_end\\_\\_\"])\n    S --> PRE\n    PRE --> MODEL\n    MODEL -.-> TOOLS\n    MODEL -.-> END\n    TOOLS --> PRE\n    classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n    class S blueHighlight;\n    class END blueHighlight;\n```\n\n```typescript  theme={null}\nimport { RemoveMessage } from \"@langchain/core/messages\";\nimport { createAgent, createMiddleware, trimMessages, type AgentState } from \"langchain\";\n\nconst trimMessageHistory = createMiddleware({\n  name: \"TrimMessages\",\n  beforeModel: async (state) => {\n    const trimmed = await trimMessages(state.messages, {\n      maxTokens: 384,\n      strategy: \"last\",\n      startOn: \"human\",\n      endOn: [\"human\", \"tool\"],\n      tokenCounter: (msgs) => msgs.length,\n    });\n    return { messages: trimmed };\n  },\n});\n\nconst agent = createAgent({\n    model: \"gpt-5-nano\",\n    tools: [],\n    middleware: [trimMessageHistory],\n});\n```\n\n### After model\n\n```mermaid  theme={null}\n%%{\n    init: {\n        \"fontFamily\": \"monospace\",\n        \"flowchart\": {\n        \"curve\": \"basis\"\n        },\n        \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n    }\n}%%\ngraph TD\n    S([\"\\_\\_start\\_\\_\"])\n    MODEL(model)\n    POST(after_model)\n    TOOLS(tools)\n    END([\"\\_\\_end\\_\\_\"])\n    S --> MODEL\n    MODEL --> POST\n    POST -.-> END\n    POST -.-> TOOLS\n    TOOLS --> MODEL\n    classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n    class S blueHighlight;\n    class END blueHighlight;\n    class POST greenHighlight;\n```\n\n```typescript  theme={null}\nimport { RemoveMessage } from \"@langchain/core/messages\";\nimport { createAgent, createMiddleware, type AgentState } from \"langchain\";\n\nconst validateResponse = createMiddleware({\n  name: \"ValidateResponse\",\n  afterModel: (state) => {\n    const lastMessage = state.messages.at(-1)?.content;\n    if (typeof lastMessage === \"string\" && lastMessage.toLowerCase().includes(\"confidential\")) {\n      return {\n        messages: [new RemoveMessage({ id: \"all\" }), ...state.messages],\n      };\n    }\n    return;\n  },\n});\n\nconst agent = createAgent({\n    model: \"gpt-5-nano\",\n    tools: [],\n    middleware: [validateResponse],\n});\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/short-term-memory.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 19962
}