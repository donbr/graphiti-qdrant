{
  "title": "Configure threads",
  "source_url": "https://docs.langchain.com/langsmith/threads",
  "content": "Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the `Threads` feature in LangSmith.\n\n## Group traces into threads\n\nA `Thread` is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\n\nTo associate traces together, you need to pass in a special `metadata` key where the value is the unique identifier for that thread. The key name should be one of:\n\n* `session_id`\n* `thread_id`\n* `conversation_id`.\n\nThe value can be any string you want, but we recommend using UUIDs, such as `f47ac10b-58cc-4372-a567-0e02b2c3d479`. Check out [this guide](./add-metadata-tags) for instructions on adding metadata to your traces.\n\n### Example\n\nThis example demonstrates how to log and retrieve conversation history using a structured message format to maintain long-running chats.\n\n<CodeGroup>\n  ```python Python theme={null}\n  import os\n  from typing import List, Dict, Any, Optional\n\n  import openai\n  from langsmith import traceable, Client\n  import langsmith as ls\n  from langsmith.wrappers import wrap_openai\n\n  # Initialize clients\n  client = wrap_openai(openai.Client())\n  langsmith_client = Client()\n\n  # Configuration\n  LANGSMITH_PROJECT = \"project-with-threads\"\n  THREAD_ID = \"thread-id-1\"\n  langsmith_extra={\"project_name\": LANGSMITH_PROJECT, \"metadata\":{\"session_id\": THREAD_ID}}\n\n  # gets a history of all LLM calls in the thread to construct conversation history\n  def get_thread_history(thread_id: str, project_name: str):\n      # Filter runs by the specific thread and project\n      filter_string = f'and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"{thread_id}\"))'\n      # Only grab the LLM runs\n      runs = [r for r in langsmith_client.list_runs(project_name=project_name, filter=filter_string, run_type=\"llm\")]\n\n      # Sort by start time to get the most recent interaction\n      runs = sorted(runs, key=lambda run: run.start_time, reverse=True)\n\n      # Reconstruct the conversation state\n      latest_run = runs[0]\n      return latest_run.inputs['messages'] + [latest_run.outputs['choices'][0]['message']]\n\n\n  @traceable(name=\"Chat Bot\")\n  def chat_pipeline(messages: list, get_chat_history: bool = False):\n      # Whether to continue an existing thread or start a new one\n      if get_chat_history:\n          run_tree = ls.get_current_run_tree()\n          # Get existing conversation history and append new messages\n          history_messages = get_thread_history(run_tree.extra[\"metadata\"][\"session_id\"], run_tree.session_name)\n          all_messages = history_messages + messages\n          # Include the complete conversation in the input for tracing\n          input_messages = all_messages\n      else:\n          all_messages = messages\n          input_messages = messages\n\n      # Invoke the model\n      chat_completion = client.chat.completions.create(\n          model=\"gpt-4o-mini\", messages=all_messages\n      )\n\n      # Return the complete conversation including input and response\n      response_message = chat_completion.choices[0].message\n      return {\n          \"messages\": input_messages + [response_message]\n      }\n\n  # Format message\n  messages = [\n      {\n          \"content\": \"Hi, my name is Sally\",\n          \"role\": \"user\"\n      }\n  ]\n  get_chat_history = False\n\n  # Call the chat pipeline\n  result = chat_pipeline(messages, get_chat_history, langsmith_extra=langsmith_extra)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import 'dotenv/config';\n  import OpenAI from 'openai';\n  import { traceable, getCurrentRunTree } from 'langsmith/traceable';\n  import { Client } from 'langsmith';\n  import { wrapOpenAI } from 'langsmith/wrappers';\n\n  // Initialize clients\n  const openai = new OpenAI();\n  const client = wrapOpenAI(openai);\n  const langsmithClient = new Client();\n\n  // Configuration\n  const LANGSMITH_PROJECT = \"project-with-threads\";\n  const THREAD_ID = \"thread-id-1\";\n  const langsmithExtra = {\n    project_name: LANGSMITH_PROJECT,\n    metadata: { session_id: THREAD_ID }\n  };\n\n  // Message type definition\n  interface Message {\n    role: 'user' | 'assistant' | 'system';\n    content: string;\n  }\n\n  interface ChatResponse {\n    messages: Message[];\n  }\n\n  interface ChatInput {\n    get_chat_history: boolean;\n    messages: Message[];\n  }\n\n  // gets a history of all LLM calls in the thread to construct conversation history\n  async function getThreadHistory(threadId: string, projectName: string): Promise<Message[]> {\n    // Filter runs by the specific thread and project\n    const filterString = `and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"${threadId}\"))`;\n\n    // Only grab the LLM runs\n    const runs: any[] = [];\n    for await (const run of langsmithClient.listRuns({\n      projectName: projectName,\n      filter: filterString,\n      runType: \"llm\"\n    })) {\n      if (run.run_type === \"llm\") {\n        runs.push(run);\n      }\n    }\n\n    // Sort by start time to get the most recent interaction\n    runs.sort((a: any, b: any) => new Date(b.start_time).getTime() - new Date(a.start_time).getTime());\n\n    // Check if we have any runs\n    if (runs.length === 0) {\n      return [];\n    }\n\n    // The current state of the conversation\n    const latestRun = runs[0];\n    const inputMessages = latestRun.inputs.messages as Message[];\n    const outputMessage = latestRun.outputs.choices[0].message as Message;\n\n    return [...inputMessages, outputMessage];\n  }\n\n  // Updated chat pipeline that accepts JSON input format\n  const chatPipeline = traceable(async (input: ChatInput): Promise<ChatResponse> => {\n    const { messages, get_chat_history } = input;\n    let allMessages: Message[];\n    let inputMessages: Message[];\n\n    // Whether to continue an existing thread or start a new one\n    if (get_chat_history) {\n      const runTree = getCurrentRunTree();\n      // Get existing conversation history and append new messages\n      const sessionId = runTree.extra?.metadata?.session_id || THREAD_ID;\n      const historyMessages = await getThreadHistory(\n        sessionId,\n        runTree.project_name\n      );\n      allMessages = historyMessages.concat(messages);\n      // Include the complete conversation in the input for tracing\n      inputMessages = allMessages;\n    } else {\n      allMessages = messages;\n      inputMessages = messages;\n    }\n\n    // Invoke the model\n    const chatCompletion = await client.chat.completions.create({\n      model: \"gpt-4o-mini\",\n      messages: allMessages\n    });\n\n    // Return the complete conversation including input and response\n    const responseMessage = chatCompletion.choices[0].message as Message;\n    return {\n      messages: [...inputMessages, responseMessage]\n    };\n  }, { name: \"Chat Bot\" });\n\n  // Example input in the requested JSON format\n  const input: ChatInput = {\n    get_chat_history: false,\n    messages: [\n      {\n        content: \"Hi, my name is Sally\",\n        role: \"user\"\n      }\n    ]\n  };\n\n  // Call the chat pipeline\n  const result = await chatPipeline(input);\n  ```\n</CodeGroup>\n\nAfter waiting a few seconds, you can make the following calls to continue the conversation. By passing `get_chat_history=True,`/`getChatHistory: true`,\nyou can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,\ninstead of just responding to the latest message.\n\n<CodeGroup>\n  ```python Python theme={null}\n  # Continue the conversation.\n  messages = [\n      {\n          \"content\": \"What is my name?\",\n          \"role\": \"user\"\n      }\n  ]\n  get_chat_history = True\n\n  chat_pipeline(messages, get_chat_history, langsmith_extra=langsmith_extra)\n  ```\n\n  ```typescript TypeScript theme={null}\n  // Continue the conversation.\n  const input: ChatInput = {\n    get_chat_history: true,\n    messages: [\n      {\n        content: \"What is my name?\",\n        role: \"user\"\n      }\n    ]\n  };\n\n  await chatPipeline(input);\n  ```\n</CodeGroup>\n\nKeep the conversation going. Since past messages are included, the LLM will remember the conversation.\n\n<CodeGroup>\n  ```python Python theme={null}\n  # Continue the conversation.\n  messages = [\n      {\n          \"content\": \"What was the first message I sent you?\",\n          \"role\": \"user\"\n      }\n  ]\n  get_chat_history = True\n\n  chat_pipeline(messages, get_chat_history, langsmith_extra=langsmith_extra)\n  ```\n\n  ```typescript TypeScript theme={null}\n  // Continue the conversation.\n  const input: ChatInput = {\n    get_chat_history: true,\n    messages: [\n      {\n        content: \"What was the first message I sent you?\",\n        role: \"user\"\n      }\n    ]\n  };\n\n  await chatPipeline(input);\n  ```\n</CodeGroup>\n\n## View threads\n\nYou can view threads by clicking on the **Threads** tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.\n\n<div style={{ textAlign: 'center' }}>\n  <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=45e1c11dce5eaaaf0cf8ae01057647b7\" alt=\"LangSmith UI showing the threads table.\" data-og-width=\"1277\" width=\"1277\" data-og-height=\"762\" height=\"762\" data-path=\"langsmith/images/threads-tab-light.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cb5d147a58a3a9ecbb1c550a3308e871 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=a7cc6f9c8def1e15cc9c93382b82473d 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=23013f31edf91e66da89e102cb6ea302 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1bfbd6daad34b699553a30d8f9663540 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d3fc20ddbe02630ac98a0c336a39caa7 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfa17b6f006faef0a6f8537eedab6532 2500w\" />\n\n  <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=b0ec4964ee49a3ead3a1e8042e406abc\" alt=\"LangSmith UI showing the threads table.\" data-og-width=\"1275\" width=\"1275\" data-og-height=\"761\" height=\"761\" data-path=\"langsmith/images/threads-tab-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=98750a8129e2e8283871c096a642f8b8 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=4be143312bdd90ab8318da304c0c1e91 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=c7ace4e000b6a1c925d32b78983fadca 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9ece1b058745be1c3b2dcfffd9a9af58 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=0ab63a1b906fe2a559cfb5772c95dc47 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=6093b81ee1f92978880e74a9630f451d 2500w\" />\n</div>\n\n### View a thread\n\nYou can then click into a particular thread. This will open the history for a particular thread.\n\n<div style={{ textAlign: 'center' }}>\n  <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f7af4c3904073d5f58f28c656603ca19\" alt=\"LangSmith UI showing the threads table.\" data-og-width=\"1273\" width=\"1273\" data-og-height=\"757\" height=\"757\" data-path=\"langsmith/images/thread-overview-light.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cd769088ab3ab2dae09982915f23772d 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=70ae6b5a6b8edb83ba3604d4c6e0262e 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=61d89d8077072221373490edac65363c 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=ad8159fe12f056dbc561c612e3797b97 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=3611f7bcc95c45bcb91c093ca36ef348 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1c0426d7e83562e1d76e079959bda186 2500w\" />\n\n  <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f738de4cac932ed2b8657e8f3b706b77\" alt=\"LangSmith UI showing the threads table.\" data-og-width=\"1273\" width=\"1273\" data-og-height=\"753\" height=\"753\" data-path=\"langsmith/images/thread-overview-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9bc9dd49c63661dceb981899c5f0332b 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=01713d47cf762f99be1a1143b01582e8 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfc77e449d0ce27cdfa51b2f7c6ed655 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d84f671a8f2c1207dbb98c72a37d1832 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d592bd4c8671b3d7f19a49471888a901 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=2405eaef2af227dd5e5efac85fc9e623 2500w\" />\n</div>\n\nThreads can be viewed in two different ways:\n\n* [Thread overview](/langsmith/threads#thread-overview)\n* [Trace view](/langsmith/threads#trace-view)\n\nYou can use the buttons at the top of the page to switch between the two views or use the keyboard shortcut `T` to toggle between the two views.\n\n#### Thread overview\n\nThe thread overview page shows you a chatbot-like UI where you can see the inputs and outputs for each turn of the conversation. You can configure which fields in the inputs and outputs are displayed in the overview, or show multiple fields by clicking the **Configure** button.\n\nThe JSON path for the inputs and outputs supports negative indexing, so you can use `-1` to access the last element of an array. For example, `inputs.messages[-1].content` will access the last message in the `messages` array.\n\n#### Trace view\n\nThe trace view here is similar to the trace view when looking at a single run, except that you have easy access to all the runs for each turn in the thread.\n\n### View feedback\n\nWhen viewing a thread, across the top of the page you will see a section called `Feedback`. This is where you can see the feedback for each of the runs that make up the thread. This feedback is aggregated, so if you evaluate each run of a thread for the same criteria, you will see the average score across all the runs displayed. You can also see [thread level feedback](/langsmith/online-evaluations#configure-multi-turn-online-evaluators) left here.\n\n### Save thread level filter\n\nSimilar to saving filters at the project level, you can also save commonly used filters at the thread level. To save filters on the threads table, set a filter using the filters button and then click the **Save filter** button.\n\nYou can open up the trace or annotate the trace in a side panel by clicking on `Annotate` and `Open trace`, respectively.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/threads.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 17690
}