{
  "title": "Troubleshoot trace nesting",
  "source_url": "https://docs.langchain.com/langsmith/nest-traces",
  "content": "When tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.\n\nIf you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known \"edge cases\".\n\n## Python\n\nThe following outlines common causes for \"split\" traces when building with python.\n\n### Context propagation using asyncio\n\nWhen using async calls (especially with streaming) in Python versions \\< 3.11, you may encounter issues with trace nesting. This is because Python's `asyncio` only [added full support for passing context](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) in version 3.11.\n\n#### Why\n\nLangChain and LangSmith SDK use [contextvars](https://docs.python.org/3/library/contextvars.html) to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), `asyncio` tasks lack proper `contextvar` support, which can lead to disconnected traces.\n\n#### To resolve\n\n1. **Upgrade Python Version (Recommended)** If possible, upgrade to Python 3.11 or later for automatic context propagation.\n\n2. **Manual Context Propagation** If upgrading isn't an option, you'll need to manually propagate the tracing context. The method varies depending on your setup:\n\n   a) **Using LangGraph or LangChain** Pass the parent `config` to the child call:\n\n   ```python  theme={null}\n   import asyncio\n   from langchain_core.runnables import RunnableConfig, RunnableLambda\n\n   @RunnableLambda\n   async def my_child_runnable(\n       inputs: str,\n       # The config arg (present in parent_runnable below) is optional\n   ):\n       yield \"A\"\n       yield \"response\"\n\n   @RunnableLambda\n   async def parent_runnable(inputs: str, config: RunnableConfig):\n       async for chunk in my_child_runnable.astream(inputs, config):\n           yield chunk\n\n   async def main():\n       return [val async for val in parent_runnable.astream(\"call\")]\n\n   asyncio.run(main())\n   ```\n\n   b) **Using LangSmith Directly** Pass the run tree directly:\n\n   ```python  theme={null}\n   import asyncio\n   import langsmith as ls\n\n   @ls.traceable\n   async def my_child_function(inputs: str):\n       yield \"A\"\n       yield \"response\"\n\n   @ls.traceable\n   async def parent_function(\n       inputs: str,\n       # The run tree can be auto-populated by the decorator\n       run_tree: ls.RunTree,\n   ):\n       async for chunk in my_child_function(inputs, langsmith_extra={\"parent\": run_tree}):\n           yield chunk\n\n   async def main():\n       return [val async for val in parent_function(\"call\")]\n\n   asyncio.run(main())\n   ```\n\n   c) **Combining Decorated Code with LangGraph/LangChain** Use a combination of techniques for manual handoff:\n\n   ```python  theme={null}\n   import asyncio\n   import langsmith as ls\n   from langchain_core.runnables import RunnableConfig, RunnableLambda\n\n   @RunnableLambda\n   async def my_child_runnable(inputs: str):\n       yield \"A\"\n       yield \"response\"\n\n   @ls.traceable\n   async def my_child_function(inputs: str, run_tree: ls.RunTree):\n       with ls.tracing_context(parent=run_tree):\n           async for chunk in my_child_runnable.astream(inputs):\n               yield chunk\n\n   @RunnableLambda\n   async def parent_runnable(inputs: str, config: RunnableConfig):\n       # @traceable decorated functions can directly accept a RunnableConfig when passed in via \"config\"\n       async for chunk in my_child_function(inputs, langsmith_extra={\"config\": config}):\n           yield chunk\n\n   @ls.traceable\n   async def parent_function(inputs: str, run_tree: ls.RunTree):\n       # You can set the tracing context manually\n       with ls.tracing_context(parent=run_tree):\n           async for chunk in parent_runnable.astream(inputs):\n               yield chunk\n\n   async def main():\n       return [val async for val in parent_function(\"call\")]\n\n   asyncio.run(main())\n   ```\n\n### Context propagation using threading\n\nIt's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib `ThreadPoolExecutor` by default breaks tracing.\n\n#### Why\n\nPython's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:\n\n#### To resolve\n\n1. **Using LangSmith's ContextThreadPoolExecutor**\n\n   LangSmith provides a `ContextThreadPoolExecutor` that automatically handles context propagation:\n\n   ```python  theme={null}\n   from langsmith.utils import ContextThreadPoolExecutor\n   from langsmith import traceable\n\n   @traceable\n   def outer_func():\n       with ContextThreadPoolExecutor() as executor:\n           inputs = [1, 2]\n           r = list(executor.map(inner_func, inputs))\n\n   @traceable\n   def inner_func(x):\n       print(x)\n\n   outer_func()\n   ```\n\n2. **Manually providing the parent run tree**\n\n   Alternatively, you can manually pass the parent run tree to the inner function:\n\n   ```python  theme={null}\n   from langsmith import traceable, get_current_run_tree\n   from concurrent.futures import ThreadPoolExecutor\n\n   @traceable\n   def outer_func():\n       rt = get_current_run_tree()\n       with ThreadPoolExecutor() as executor:\n           r = list(\n               executor.map(\n                   lambda x: inner_func(x, langsmith_extra={\"parent\": rt}), [1, 2]\n               )\n           )\n\n   @traceable\n   def inner_func(x):\n       print(x)\n\n   outer_func()\n   ```\n\nIn this approach, we use `get_current_run_tree()` to obtain the current run tree and pass it to the inner function using the `langsmith_extra` parameter.\n\nBoth methods ensure that the inner function calls are correctly aggregated under the initial trace stack, even when executed in separate threads.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/nest-traces.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 6189
}