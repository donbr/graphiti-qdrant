{
  "title": "LangChain v1 migration guide",
  "source_url": "https://docs.langchain.com/oss/python/migrate/langchain-v1",
  "content": "This guide outlines the major changes between [LangChain v1](/oss/python/releases/langchain-v1) and previous versions.\n\n## Simplified package\n\nThe `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.\n\n### Namespace\n\n| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                             |\n| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- |\n| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality |\n| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from `langchain-core` |\n| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from `langchain-core` |\n| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization      |\n| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                  |\n\n### `langchain-classic`\n\nIf you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports:\n\n* Legacy chains (`LLMChain`, `ConversationChain`, etc.)\n* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)\n* The indexing API\n* The hub module (for managing prompts programmatically)\n* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings)\n* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports\n* Other deprecated functionality\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  # Chains\n  from langchain_classic.chains import LLMChain\n\n  # Retrievers\n  from langchain_classic.retrievers import ...\n\n  # Indexing\n  from langchain_classic.indexes import ...\n\n  # Hub\n  from langchain_classic import hub\n  ```\n\n  ```python v0 (old) theme={null}\n  # Chains\n  from langchain.chains import LLMChain\n\n  # Retrievers\n  from langchain.retrievers import ...\n\n  # Indexing\n  from langchain.indexes import ...\n\n  # Hub\n  from langchain import hub\n  ```\n</CodeGroup>\n\nInstall with:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain-classic\n  ```\n\n  ```bash uv theme={null}\n  uv add langchain-classic\n  ```\n</CodeGroup>\n\n***\n\n## Migrate to `create_agent`\n\nPrior to v1.0, we recommended using [`langgraph.prebuilt.create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to build agents. Now, we recommend you use [`langchain.agents.create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) to build agents.\n\nThe table below outlines what functionality has changed from [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent):\n\n| Section                                            | TL;DR - What's changed                                                                                                                                                                     |\n| -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [Import path](#import-path)                        | Package moved from `langgraph.prebuilt` to `langchain.agents`                                                                                                                              |\n| [Prompts](#prompts)                                | Parameter renamed to [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)), dynamic prompts use middleware            |\n| [Pre-model hook](#pre-model-hook)                  | Replaced by middleware with `before_model` method                                                                                                                                          |\n| [Post-model hook](#post-model-hook)                | Replaced by middleware with `after_model` method                                                                                                                                           |\n| [Custom state](#custom-state)                      | `TypedDict` only, can be defined via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) or middleware |\n| [Model](#model)                                    | Dynamic selection via middleware, pre-bound models not supported                                                                                                                           |\n| [Tools](#tools)                                    | Tool error handling moved to middleware with `wrap_tool_call`                                                                                                                              |\n| [Structured output](#structured-output)            | prompted output removed, use `ToolStrategy`/`ProviderStrategy`                                                                                                                             |\n| [Streaming node name](#streaming-node-name-rename) | Node name changed from `\"agent\"` to `\"model\"`                                                                                                                                              |\n| [Runtime context](#runtime-context)                | Dependency injection via `context` argument instead of `config[\"configurable\"]`                                                                                                            |\n| [Namespace](#simplified-namespace)                 | Streamlined to focus on agent building blocks, legacy code moved to `langchain-classic`                                                                                                    |\n\n### Import path\n\nThe import path for the agent prebuilt has changed from `langgraph.prebuilt` to `langchain.agents`.\nThe name of the function has changed from [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent):\n\n```python  theme={null}\nfrom langgraph.prebuilt import create_react_agent # [!code --]\nfrom langchain.agents import create_agent # [!code ++]\n```\n\nFor more information, see [Agents](/oss/python/langchain/agents).\n\n### Prompts\n\n#### Static prompt rename\n\nThe `prompt` parameter has been renamed to [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)):\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather],\n      system_prompt=\"You are a helpful assistant\"  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather],\n      prompt=\"You are a helpful assistant\"  # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n#### `SystemMessage` to string\n\nIf using [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) objects in the system prompt, extract the string content:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather],\n      system_prompt=\"You are a helpful assistant\"  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langchain.messages import SystemMessage\n  from langgraph.prebuilt import create_react_agent\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather],\n      prompt=SystemMessage(content=\"You are a helpful assistant\")  # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n#### Dynamic prompts\n\nDynamic prompts are a core context engineering patternâ€” they adapt what you tell the model based on the current conversation state. To do this, use the [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) decorator:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from dataclasses import dataclass\n\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import dynamic_prompt, ModelRequest\n  from langgraph.runtime import Runtime\n\n\n  @dataclass\n  class Context:  # [!code highlight]\n      user_role: str = \"user\"\n\n  @dynamic_prompt  # [!code highlight]\n  def dynamic_prompt(request: ModelRequest) -> str:  # [!code highlight]\n      user_role = request.runtime.context.user_role\n      base_prompt = \"You are a helpful assistant.\"\n\n      if user_role == \"expert\":\n          prompt = (\n              f\"{base_prompt} Provide detailed technical responses.\"\n          )\n      elif user_role == \"beginner\":\n          prompt = (\n              f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n          )\n      else:\n          prompt = base_prompt\n\n      return prompt  # [!code highlight]\n\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=tools,\n      middleware=[dynamic_prompt],  # [!code highlight]\n      context_schema=Context\n  )\n\n  # Use with context\n  agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"Explain async programming\"}]},\n      context=Context(user_role=\"expert\")\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from dataclasses import dataclass\n\n  from langgraph.prebuilt import create_react_agent, AgentState\n  from langgraph.runtime import get_runtime\n\n  @dataclass\n  class Context:\n      user_role: str\n\n  def dynamic_prompt(state: AgentState) -> str:\n      runtime = get_runtime(Context)  # [!code highlight]\n      user_role = runtime.context.user_role\n      base_prompt = \"You are a helpful assistant.\"\n\n      if user_role == \"expert\":\n          return f\"{base_prompt} Provide detailed technical responses.\"\n      elif user_role == \"beginner\":\n          return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n      return base_prompt\n\n  agent = create_react_agent(\n      model=\"gpt-4o\",\n      tools=tools,\n      prompt=dynamic_prompt,\n      context_schema=Context\n  )\n\n  # Use with context\n  agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"Explain async programming\"}]},\n      context=Context(user_role=\"expert\")\n  )\n  ```\n</CodeGroup>\n\n### Pre-model hook\n\nPre-model hooks are now implemented as middleware with the `before_model` method.\nThis new pattern is more extensible--you can define multiple middlewares to run before the model is called,\nreusing common patterns across different agents.\n\nCommon use cases include:\n\n* Summarizing conversation history\n* Trimming messages\n* Input guardrails, like PII redaction\n\nv1 now has summarization middleware as a built in option:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import SummarizationMiddleware\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=tools,\n      middleware=[\n          SummarizationMiddleware(  # [!code highlight]\n              model=\"claude-sonnet-4-5-20250929\",  # [!code highlight]\n              trigger={\"tokens\": 1000}  # [!code highlight]\n          )  # [!code highlight]\n      ]  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent, AgentState\n\n  def custom_summarization_function(state: AgentState):\n      \"\"\"Custom logic for message summarization.\"\"\"\n      ...\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=tools,\n      pre_model_hook=custom_summarization_function\n  )\n  ```\n</CodeGroup>\n\n### Post-model hook\n\nPost-model hooks are now implemented as middleware with the `after_model` method.\nThis new pattern is more extensible--you can define multiple middlewares to run after the model is called,\nreusing common patterns across different agents.\n\nCommon use cases include:\n\n* [Human in the loop](/oss/python/langchain/human-in-the-loop)\n* Output guardrails\n\nv1 has a built in middleware for human in the loop approval for tool calls:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import HumanInTheLoopMiddleware\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[read_email, send_email],\n      middleware=[\n          HumanInTheLoopMiddleware(\n              interrupt_on={\n                  \"send_email\": {\n                      \"description\": \"Please review this email before sending\",\n                      \"allowed_decisions\": [\"approve\", \"reject\"]\n                  }\n              }\n          )\n      ]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent\n  from langgraph.prebuilt import AgentState\n\n  def custom_human_in_the_loop_hook(state: AgentState):\n      \"\"\"Custom logic for human in the loop approval.\"\"\"\n      ...\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[read_email, send_email],\n      post_model_hook=custom_human_in_the_loop_hook\n  )\n  ```\n</CodeGroup>\n\n### Custom state\n\nCustom state extends the default agent state with additional fields. You can define custom state in two ways:\n\n1. **Via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)** - Best for state used in tools\n2. **Via middleware** - Best for state managed by specific middleware hooks and tools attached to said middleware\n\n<Note>\n  Defining custom state via middleware is preferred over defining it via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\n\n  `state_schema` is still supported for backwards compatibility on `create_agent`.\n</Note>\n\n#### Defining state via `state_schema`\n\nUse the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) parameter when your custom state needs to be accessed by tools:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.tools import tool, ToolRuntime\n  from langchain.agents import create_agent, AgentState  # [!code highlight]\n\n\n  # Define custom state extending AgentState\n  class CustomState(AgentState):\n      user_name: str\n\n  @tool  # [!code highlight]\n  def greet(\n      runtime: ToolRuntime[None, CustomState]\n  ) -> str:\n      \"\"\"Use this to greet the user by name.\"\"\"\n      user_name = runtime.state.get(\"user_name\", \"Unknown\")  # [!code highlight]\n      return f\"Hello {user_name}!\"\n\n  agent = create_agent(  # [!code highlight]\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[greet],\n      state_schema=CustomState  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from typing import Annotated\n  from langgraph.prebuilt import InjectedState, create_react_agent\n  from langgraph.prebuilt.chat_agent_executor import AgentState\n\n  class CustomState(AgentState):\n      user_name: str\n\n  def greet(\n      state: Annotated[CustomState, InjectedState]\n  ) -> str:\n      \"\"\"Use this to greet the user by name.\"\"\"\n      user_name = state[\"user_name\"]\n      return f\"Hello {user_name}!\"\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[greet],\n      state_schema=CustomState\n  )\n  ```\n</CodeGroup>\n\n#### Defining state via middleware\n\nMiddleware can also define custom state by setting the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) attribute.\nThis helps to keep state extensions conceptually scoped to the relevant middleware and tools.\n\n```python  theme={null}\nfrom langchain.agents.middleware import AgentState, AgentMiddleware\nfrom typing_extensions import NotRequired\nfrom typing import Any\n\nclass CustomState(AgentState):\n    model_call_count: NotRequired[int]\n\nclass CallCounterMiddleware(AgentMiddleware[CustomState]):\n    state_schema = CustomState  # [!code highlight]\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        count = state.get(\"model_call_count\", 0)\n        if count > 10:\n            return {\"jump_to\": \"end\"}\n        return None\n\n    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[...],\n    middleware=[CallCounterMiddleware()]  # [!code highlight]\n)\n```\n\nSee the [middleware documentation](/oss/python/langchain/middleware#custom-state-schema) for more details on defining custom state via middleware.\n\n#### State type restrictions\n\n[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) only supports `TypedDict` for state schemas. Pydantic models and dataclasses are no longer supported.\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import AgentState, create_agent\n\n  # AgentState is a TypedDict\n  class CustomAgentState(AgentState):  # [!code highlight]\n      user_id: str\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=tools,\n      state_schema=CustomAgentState  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from typing_extensions import Annotated\n\n  from pydantic import BaseModel\n  from langgraph.graph import StateGraph\n  from langgraph.graph.messages import add_messages\n  from langchain.messages import AnyMessage\n\n\n  class AgentState(BaseModel):  # [!code highlight]\n      messages: Annotated[list[AnyMessage], add_messages]\n      user_id: str\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=tools,\n      state_schema=AgentState\n  )\n  ```\n</CodeGroup>\n\nSimply inherit from `langchain.agents.AgentState` instead of `BaseModel` or decorating with `dataclass`.\nIf you need to perform validation, handle it in middleware hooks instead.\n\n### Model\n\nDynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences). [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) released in v0.6 of [`langgraph-prebuilt`](https://pypi.org/project/langgraph-prebuilt) supported dynamic model and tool selection via a callable passed to the `model` parameter.\n\nThis functionality has been ported to the middleware interface in v1.\n\n#### Dynamic model selection\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import (\n      AgentMiddleware, ModelRequest\n  )\n  from langchain.agents.middleware.types import ModelResponse\n  from langchain_openai import ChatOpenAI\n  from typing import Callable\n\n  basic_model = ChatOpenAI(model=\"gpt-5-nano\")\n  advanced_model = ChatOpenAI(model=\"gpt-5\")\n\n  class DynamicModelMiddleware(AgentMiddleware):\n\n      def wrap_model_call(self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n          if len(request.state.messages) > self.messages_threshold:\n              model = advanced_model\n          else:\n              model = basic_model\n          return handler(request.override(model=model))\n\n      def __init__(self, messages_threshold: int) -> None:\n          self.messages_threshold = messages_threshold\n\n  agent = create_agent(\n      model=basic_model,\n      tools=tools,\n      middleware=[DynamicModelMiddleware(messages_threshold=10)]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent, AgentState\n  from langchain_openai import ChatOpenAI\n\n  basic_model = ChatOpenAI(model=\"gpt-5-nano\")\n  advanced_model = ChatOpenAI(model=\"gpt-5\")\n\n  def select_model(state: AgentState) -> BaseChatModel:\n      # use a more advanced model for longer conversations\n      if len(state.messages) > 10:\n          return advanced_model\n      return basic_model\n\n  agent = create_react_agent(\n      model=select_model,\n      tools=tools,\n  )\n  ```\n</CodeGroup>\n\n#### Pre-bound models\n\nTo better support structured output, [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) no longer accepts pre-bound models with tools or configuration:\n\n```python  theme={null}\n# No longer supported\nmodel_with_tools = ChatOpenAI().bind_tools([some_tool])\nagent = create_agent(model_with_tools, tools=[])\n\n# Use instead\nagent = create_agent(\"gpt-4o-mini\", tools=[some_tool])\n```\n\n<Note>\n  Dynamic model functions can return pre-bound models if structured output is *not* used.\n</Note>\n\n### Tools\n\nThe [`tools`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(tools\\)) argument to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) accepts a list of:\n\n* LangChain [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) instances (functions decorated with [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool))\n* Callable objects (functions) with proper type hints and a docstring\n* `dict` that represents a built-in provider tools\n\nThe argument will no longer accept [`ToolNode`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.ToolNode) instances.\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n\n  agent = create_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=[check_weather, search_web]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent, ToolNode\n\n\n  agent = create_react_agent(\n      model=\"claude-sonnet-4-5-20250929\",\n      tools=ToolNode([check_weather, search_web]) # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n#### Handling tool errors\n\nYou can now configure the handling of tool errors with middleware implementing the `wrap_tool_call` method.\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  # Example coming soon\n  ```\n\n  ```python v0 (old) theme={null}\n  # Example coming soon\n  ```\n</CodeGroup>\n\n### Structured output\n\n#### Node changes\n\nStructured output used to be generated in a separate node from the main agent. This is no longer the case.\nWe generate structured output in the main loop, reducing cost and latency.\n\n#### Tool and provider strategies\n\nIn v1, there are two new structured output strategies:\n\n* `ToolStrategy` uses artificial tool calling to generate structured output\n* `ProviderStrategy` uses provider-native structured output generation\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.structured_output import ToolStrategy, ProviderStrategy\n  from pydantic import BaseModel\n\n\n  class OutputSchema(BaseModel):\n      summary: str\n      sentiment: str\n\n  # Using ToolStrategy\n  agent = create_agent(\n      model=\"gpt-4o-mini\",\n      tools=tools,\n      # explicitly using tool strategy\n      response_format=ToolStrategy(OutputSchema)  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent\n  from pydantic import BaseModel\n\n  class OutputSchema(BaseModel):\n      summary: str\n      sentiment: str\n\n  agent = create_react_agent(\n      model=\"gpt-4o-mini\",\n      tools=tools,\n      # using tool strategy by default with no option for provider strategy\n      response_format=OutputSchema  # [!code highlight]\n  )\n\n  # OR\n\n  agent = create_react_agent(\n      model=\"gpt-4o-mini\",\n      tools=tools,\n      # using a custom prompt to instruct the model to generate the output schema\n      response_format=(\"please generate ...\", OutputSchema)  # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n#### Prompted output removed\n\n**Prompted output** is no longer supported via the `response_format` argument. Compared to strategies\nlike artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable.\n\n### Streaming node name rename\n\nWhen streaming events from agents, the node name has changed from `\"agent\"` to `\"model\"` to better reflect the node's purpose.\n\n### Runtime context\n\nWhen you invoke an agent, it's often the case that you want to pass two types of data:\n\n* Dynamic state that changes throughout the conversation (e.g., message history)\n* Static context that doesn't change during the conversation (e.g., user metadata)\n\nIn v1, static context is supported by setting the `context` parameter to `invoke` and `stream`.\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from dataclasses import dataclass\n\n  from langchain.agents import create_agent\n\n\n  @dataclass\n  class Context:\n      user_id: str\n      session_id: str\n\n  agent = create_agent(\n      model=model,\n      tools=tools,\n      context_schema=Context  # [!code highlight]\n  )\n\n  result = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n      context=Context(user_id=\"123\", session_id=\"abc\")  # [!code highlight]\n  )\n  ```\n\n  ```python v0 (old) theme={null}\n  from langgraph.prebuilt import create_react_agent\n\n\n  agent = create_react_agent(model, tools)\n\n  # Pass context via configurable\n  result = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n      config={  # [!code highlight]\n          \"configurable\": {  # [!code highlight]\n              \"user_id\": \"123\",  # [!code highlight]\n              \"session_id\": \"abc\"  # [!code highlight]\n          }  # [!code highlight]\n      }  # [!code highlight]\n  )\n  ```\n</CodeGroup>\n\n<Note>\n  The old `config[\"configurable\"]` pattern still works for backward compatibility, but using the new `context` parameter is recommended for new applications or applications migrating to v1.\n</Note>\n\n***\n\n## Standard content\n\nIn v1, messages gain provider-agnostic standard content blocks. Access them via [`message.content_blocks`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) for a consistent, typed view across providers. The existing [`message.content`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) field remains unchanged for strings or provider-native structures.\n\n### What changed\n\n* New [`content_blocks`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) property on messages for normalized content\n* Standardized block shapes, documented in [Messages](/oss/python/langchain/messages#standard-content-blocks)\n* Optional serialization of standard blocks into `content` via `LC_OUTPUT_VERSION=v1` or `output_version=\"v1\"`\n\n### Read standardized content\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.chat_models import init_chat_model\n\n  model = init_chat_model(\"gpt-5-nano\")\n  response = model.invoke(\"Explain AI\")\n\n  for block in response.content_blocks:\n      if block[\"type\"] == \"reasoning\":\n          print(block.get(\"reasoning\"))\n      elif block[\"type\"] == \"text\":\n          print(block.get(\"text\"))\n  ```\n\n  ```python v0 (old) theme={null}\n  # Provider-native formats vary; you needed per-provider handling\n  response = model.invoke(\"Explain AI\")\n  for item in response.content:\n      if item.get(\"type\") == \"reasoning\":\n          ...  # OpenAI-style reasoning\n      elif item.get(\"type\") == \"thinking\":\n          ...  # Anthropic-style thinking\n      elif item.get(\"type\") == \"text\":\n          ...  # Text\n  ```\n</CodeGroup>\n\n### Create multimodal messages\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  from langchain.messages import HumanMessage\n\n  message = HumanMessage(content_blocks=[\n      {\"type\": \"text\", \"text\": \"Describe this image.\"},\n      {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n  ])\n  res = model.invoke([message])\n  ```\n\n  ```python v0 (old) theme={null}\n  from langchain.messages import HumanMessage\n\n  message = HumanMessage(content=[\n      # Provider-native structure\n      {\"type\": \"text\", \"text\": \"Describe this image.\"},\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}},\n  ])\n  res = model.invoke([message])\n  ```\n</CodeGroup>\n\n### Example block shapes\n\n```python  theme={null}\n# Text block\ntext_block = {\n    \"type\": \"text\",\n    \"text\": \"Hello world\",\n}\n\n# Image block\nimage_block = {\n    \"type\": \"image\",\n    \"url\": \"https://example.com/image.png\",\n    \"mime_type\": \"image/png\",\n}\n```\n\nSee the content blocks [reference](/oss/python/langchain/messages#content-block-reference) for more details.\n\n### Serialize standard content\n\nStandard content blocks are **not serialized** into the `content` attribute by default. If you need to access standard content blocks in the `content` attribute (e.g., when sending messages to a client), you can opt-in to serializing them into `content`.\n\n<CodeGroup>\n  ```bash Environment variable theme={null}\n  export LC_OUTPUT_VERSION=v1\n  ```\n\n  ```python Initialization parameter theme={null}\n  from langchain.chat_models import init_chat_model\n\n  model = init_chat_model(\n      \"gpt-5-nano\",\n      output_version=\"v1\",\n  )\n  ```\n</CodeGroup>\n\n<Note>\n  Learn more: [Messages](/oss/python/langchain/messages#message-content), [Standard content blocks](/oss/python/langchain/messages#standard-content-blocks), and [Multimodal](/oss/python/langchain/messages#multimodal).\n</Note>\n\n***\n\n## Simplified package\n\nThe `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.\n\n### Namespace\n\n| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                             |\n| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- |\n| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality |\n| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from `langchain-core` |\n| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from `langchain-core` |\n| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization      |\n| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                  |\n\n### `langchain-classic`\n\nIf you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports:\n\n* Legacy chains (`LLMChain`, `ConversationChain`, etc.)\n* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)\n* The indexing API\n* The hub module (for managing prompts programmatically)\n* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings)\n* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports\n* Other deprecated functionality\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  # Chains\n  from langchain_classic.chains import LLMChain\n\n  # Retrievers\n  from langchain_classic.retrievers import ...\n\n  # Indexing\n  from langchain_classic.indexes import ...\n\n  # Hub\n  from langchain_classic import hub\n  ```\n\n  ```python v0 (old) theme={null}\n  # Chains\n  from langchain.chains import LLMChain\n\n  # Retrievers\n  from langchain.retrievers import ...\n\n  # Indexing\n  from langchain.indexes import ...\n\n  # Hub\n  from langchain import hub\n  ```\n</CodeGroup>\n\n**Installation**:\n\n```bash  theme={null}\nuv pip install langchain-classic\n```\n\n***\n\n## Breaking changes\n\n### Dropped Python 3.9 support\n\nAll LangChain packages now require **Python 3.10 or higher**. Python 3.9 reaches [end of life](https://devguide.python.org/versions/) in October 2025.\n\n### Updated return type for chat models\n\nThe return type signature for chat model invocation has been fixed from [`BaseMessage`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage) to [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage). Custom chat models implementing [`bind_tools`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.bind_tools) should update their return signature:\n\n<CodeGroup>\n  ```python v1 (new) theme={null}\n  def bind_tools(\n          ...\n      ) -> Runnable[LanguageModelInput, AIMessage]:\n  ```\n\n  ```python v0 (old) theme={null}\n  def bind_tools(\n          ...\n      ) -> Runnable[LanguageModelInput, BaseMessage]:\n  ```\n</CodeGroup>\n\n### Default message format for OpenAI Responses API\n\nWhen interacting with the Responses API, `langchain-openai` now defaults to storing response items in message `content`. To restore previous behavior, set the `LC_OUTPUT_VERSION` environment variable to `v0`, or specify `output_version=\"v0\"` when instantiating [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI).\n\n```python  theme={null}\n# Enforce previous behavior with output_version flag\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", output_version=\"v0\")\n```\n\n### Default `max_tokens` in `langchain-anthropic`\n\nThe `max_tokens` parameter in `langchain-anthropic` now defaults to higher values based on the model chosen, rather than the previous default of `1024`. If you relied on the old default, explicitly set `max_tokens=1024`.\n\n### Legacy code moved to `langchain-classic`\n\nExisting functionality outside the focus of standard interfaces and agents has been moved to the [`langchain-classic`](https://pypi.org/project/langchain-classic) package. See the [Simplified namespace](#simplified-namespace) section for details on what's available in the core `langchain` package and what moved to `langchain-classic`.\n\n### Removal of deprecated APIs\n\nMethods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the [deprecation notices](https://python.langchain.com/docs/versions/migrating_chains) from previous versions for replacement APIs.\n\n### Text property\n\nUse of the `.text()` method on message objects should drop the parentheses, as it is now a property:\n\n```python  theme={null}\n# Property access\ntext = response.text\n\n# Deprecated method call\ntext = response.text()\n```\n\nExisting usage patterns (i.e., `.text()`) will continue to function but now emit a warning. The method form will be removed in v2.\n\n### `example` parameter removed from `AIMessage`\n\nThe `example` parameter has been removed from [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects. We recommend migrating to use `additional_kwargs` for passing extra metadata as needed.\n\n## Minor changes\n\n* `AIMessageChunk` objects now include a `chunk_position` attribute with position `'last'` to indicate the final chunk in a stream. This allows for clearer handling of streamed messages. If the chunk is not the final one, `chunk_position` will be `None`.\n* `LanguageModelOutputVar` is now typed to [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) instead of [`BaseMessage`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage).\n* The logic for merging message chunks (`AIMessageChunk.add`) has been updated with more sophisticated selection handling for the final id for the merged chunk. It prioritizes provider-assigned IDs over LangChain-generated IDs.\n* We now open files with `utf-8` encoding by default.\n* Standard tests now use multimodal content blocks.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langchain-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 41448
}