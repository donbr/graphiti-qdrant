{
  "title": "Use the graph API",
  "source_url": "https://docs.langchain.com/oss/python/langgraph/use-graph-api",
  "content": "This guide demonstrates the basics of LangGraph's Graph API. It walks through [state](#define-and-update-state), as well as composing common graph structures such as [sequences](#create-a-sequence-of-steps), [branches](#create-branches), and [loops](#create-and-control-loops). It also covers LangGraph's control features, including the [Send API](#map-reduce-and-the-send-api) for map-reduce workflows and the [Command API](#combine-control-flow-and-state-updates-with-command) for combining state updates with \"hops\" across nodes.\n\n## Setup\n\nInstall `langgraph`:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph\n  ```\n</CodeGroup>\n\n<Tip>\n  **Set up LangSmith for better debugging**\n\n  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the [docs](/langsmith/observability).\n</Tip>\n\n## Define and update state\n\nHere we show how to define and update [state](/oss/python/langgraph/graph-api#state) in LangGraph. We will demonstrate:\n\n1. How to use state to define a graph's [schema](/oss/python/langgraph/graph-api#schema)\n2. How to use [reducers](/oss/python/langgraph/graph-api#reducers) to control how state updates are processed.\n\n### Define state\n\n[State](/oss/python/langgraph/graph-api#state) in LangGraph can be a `TypedDict`, `Pydantic` model, or dataclass. Below we will use `TypedDict`. See [this section](#use-pydantic-models-for-graph-state) for detail on using Pydantic.\n\nBy default, graphs will have the same input and output schema, and the state determines that schema. See [this section](#define-input-and-output-schemas) for how to define distinct input and output schemas.\n\nLet's consider a simple example using [messages](/oss/python/langgraph/graph-api#messagesstate). This represents a versatile formulation of state for many LLM applications. See our [concepts page](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) for more detail.\n\n```python  theme={null}\nfrom langchain.messages import AnyMessage\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    messages: list[AnyMessage]\n    extra_field: int\n```\n\nThis state tracks a list of [message](https://python.langchain.com/docs/concepts/messages/) objects, as well as an extra integer field.\n\n### Update state\n\nLet's build an example graph with a single node. Our [node](/oss/python/langgraph/graph-api#nodes) is just a Python function that reads our graph's state and makes updates to it. The first argument to this function will always be the state:\n\n```python  theme={null}\nfrom langchain.messages import AIMessage\n\ndef node(state: State):\n    messages = state[\"messages\"]\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\n```\n\nThis node simply appends a message to our message list, and populates an extra field.\n\n<Warning>\n  Nodes should return updates to the state directly, instead of mutating the state.\n</Warning>\n\nLet's next define a simple graph containing this node. We use [`StateGraph`](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state. We then use [`add_node`](/oss/python/langgraph/graph-api#nodes) populate our graph.\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph(State)\nbuilder.add_node(node)\nbuilder.set_entry_point(\"node\")\ngraph = builder.compile()\n```\n\nLangGraph provides built-in utilities for visualizing your graph. Let's inspect our graph. See [this section](#visualize-your-graph) for detail on visualization.\n\n```python  theme={null}\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=cf3d978b707847e166d5ed15bc7cbbe4\" alt=\"Simple graph with single node\" data-og-width=\"107\" width=\"107\" data-og-height=\"134\" height=\"134\" data-path=\"oss/images/graph_api_image_1.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=498bbdb0192eb26ab115d51b53fcb64c 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=94cbad4b92d5b887dff2bfbb6f8e0c6c 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d90d58640d49e3fd4e558ab56acf4817 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=cad59990b0c551a2aa96b684b102b953 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=318736f22c69f66c48f4189db3e39235 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=6740141ec001a9a4275cecfac67b9c55 2500w\" />\n\nIn this case, our graph just executes a single node. Let's proceed with a simple invocation:\n\n```python  theme={null}\nfrom langchain.messages import HumanMessage\n\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\nresult\n```\n\n```\n{'messages': [HumanMessage(content='Hi'), AIMessage(content='Hello!')], 'extra_field': 10}\n```\n\nNote that:\n\n* We kicked off invocation by updating a single key of the state.\n* We receive the entire state in the invocation result.\n\nFor convenience, we frequently inspect the content of [message objects](https://python.langchain.com/docs/concepts/messages/) via pretty-print:\n\n```python  theme={null}\nfor message in result[\"messages\"]:\n    message.pretty_print()\n```\n\n```\n================================ Human Message ================================\n\nHi\n================================== Ai Message ==================================\n\nHello!\n```\n\n### Process state updates with reducers\n\nEach key in the state can have its own independent [reducer](/oss/python/langgraph/graph-api#reducers) function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\n\nFor `TypedDict` state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\n\nIn the earlier example, our node updated the `\"messages\"` key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\n\n```python  theme={null}\nfrom typing_extensions import Annotated\n\ndef add(left, right):\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\n    return left + right\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add]  # [!code highlight]\n    extra_field: int\n```\n\nNow our node can be simplified:\n\n```python  theme={null}\ndef node(state: State):\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": [new_message], \"extra_field\": 10}  # [!code highlight]\n```\n\n```python  theme={null}\nfrom langgraph.graph import START\n\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\n\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n```\n\n```\n================================ Human Message ================================\n\nHi\n================================== Ai Message ==================================\n\nHello!\n```\n\n#### MessagesState\n\nIn practice, there are additional considerations for updating lists of messages:\n\n* We may wish to update an existing message in the state.\n* We may want to accept short-hands for [message formats](/oss/python/langgraph/graph-api#using-messages-in-your-graph), such as [OpenAI format](https://python.langchain.com/docs/concepts/messages/#openai-format).\n\nLangGraph includes a built-in reducer [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) that handles these considerations:\n\n```python  theme={null}\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]  # [!code highlight]\n    extra_field: int\n\ndef node(state: State):\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": [new_message], \"extra_field\": 10}\n\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\n```\n\n```python  theme={null}\ninput_message = {\"role\": \"user\", \"content\": \"Hi\"}  # [!code highlight]\n\nresult = graph.invoke({\"messages\": [input_message]})\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n```\n\n```\n================================ Human Message ================================\n\nHi\n================================== Ai Message ==================================\n\nHello!\n```\n\nThis is a versatile representation of state for applications involving [chat models](https://python.langchain.com/docs/concepts/chat_models/). LangGraph includes a pre-built `MessagesState` for convenience, so that we can have:\n\n```python  theme={null}\nfrom langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    extra_field: int\n```\n\n### Bypass reducers with `Overwrite`\n\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the [`Overwrite`](https://reference.langchain.com/python/langgraph/types/) type for this purpose. When a node returns a value wrapped with `Overwrite`, the reducer is bypassed and the channel is set directly to that value.\n\nThis is useful when you want to reset or replace accumulated state rather than merge it with existing values.\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Overwrite\nfrom typing_extensions import Annotated, TypedDict\nimport operator\n\nclass State(TypedDict):\n    messages: Annotated[list, operator.add]\n\ndef add_message(state: State):\n    return {\"messages\": [\"first message\"]}\n\ndef replace_messages(state: State):\n    # Bypass the reducer and replace the entire messages list\n    return {\"messages\": Overwrite([\"replacement message\"])}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"add_message\", add_message)\nbuilder.add_node(\"replace_messages\", replace_messages)\nbuilder.add_edge(START, \"add_message\")\nbuilder.add_edge(\"add_message\", \"replace_messages\")\nbuilder.add_edge(\"replace_messages\", END)\n\ngraph = builder.compile()\n\nresult = graph.invoke({\"messages\": [\"initial\"]})\nprint(result[\"messages\"])\n```\n\n```\n['replacement message']\n```\n\nYou can also use JSON format with the special key `\"__overwrite__\"`:\n\n```python  theme={null}\ndef replace_messages(state: State):\n    return {\"messages\": {\"__overwrite__\": [\"replacement message\"]}}\n```\n\n<Warning>\n  When nodes execute in parallel, only one node can use `Overwrite` on the same state key in a given super-step. If multiple nodes attempt to overwrite the same key in the same super-step, an `InvalidUpdateError` will be raised.\n</Warning>\n\n### Define input and output schemas\n\nBy default, `StateGraph` operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph.\n\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\n\nBelow, we'll see how to define distinct input and output schema.\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# Define the schema for the input\nclass InputState(TypedDict):\n    question: str\n\n# Define the schema for the output\nclass OutputState(TypedDict):\n    answer: str\n\n# Define the overall schema, combining both input and output\nclass OverallState(InputState, OutputState):\n    pass\n\n# Define the node that processes the input and generates an answer\ndef answer_node(state: InputState):\n    # Example answer and an extra key\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\n\n# Build the graph with input and output schemas specified\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\nbuilder.add_node(answer_node)  # Add the answer node\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\ngraph = builder.compile()  # Compile the graph\n\n# Invoke the graph with an input and print the result\nprint(graph.invoke({\"question\": \"hi\"}))\n```\n\n```\n{'answer': 'bye'}\n```\n\nNotice that the output of invoke only includes the output schema.\n\n### Pass private state between nodes\n\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn't need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\n\nBelow, we'll create an example sequential graph consisting of three nodes (node\\_1, node\\_2 and node\\_3), where private data is passed between the first two steps (node\\_1 and node\\_2), while the third step (node\\_3) only has access to the public overall state.\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(TypedDict):\n    a: str\n\n# Output from node_1 contains private data that is not part of the overall state\nclass Node1Output(TypedDict):\n    private_data: str\n\n# The private data is only shared between node_1 and node_2\ndef node_1(state: OverallState) -> Node1Output:\n    output = {\"private_data\": \"set by node_1\"}\n    print(f\"Entered node `node_1`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n    return output\n\n# Node 2 input only requests the private data available after node_1\nclass Node2Input(TypedDict):\n    private_data: str\n\ndef node_2(state: Node2Input) -> OverallState:\n    output = {\"a\": \"set by node_2\"}\n    print(f\"Entered node `node_2`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n    return output\n\n# Node 3 only has access to the overall state (no access to private data from node_1)\ndef node_3(state: OverallState) -> OverallState:\n    output = {\"a\": \"set by node_3\"}\n    print(f\"Entered node `node_3`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n    return output\n\n# Connect nodes in a sequence\n# node_2 accepts private data from node_1, whereas\n# node_3 does not see the private data.\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n\n# Invoke the graph with the initial state\nresponse = graph.invoke(\n    {\n        \"a\": \"set at start\",\n    }\n)\n\nprint()\nprint(f\"Output of graph invocation: {response}\")\n```\n\n```\nEntered node `node_1`:\n    Input: {'a': 'set at start'}.\n    Returned: {'private_data': 'set by node_1'}\nEntered node `node_2`:\n    Input: {'private_data': 'set by node_1'}.\n    Returned: {'a': 'set by node_2'}\nEntered node `node_3`:\n    Input: {'a': 'set by node_2'}.\n    Returned: {'a': 'set by node_3'}\n\nOutput of graph invocation: {'a': 'set by node_3'}\n```\n\n### Use Pydantic models for graph state\n\nA [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.StateGraph) accepts a [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) argument on initialization that specifies the \"shape\" of the state that the nodes in the graph can access and update.\n\nIn our examples, we typically use a python-native `TypedDict` or [`dataclass`](https://docs.python.org/3/library/dataclasses.html) for `state_schema`, but [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) can be any [type](https://docs.python.org/3/library/stdtypes.html#type-objects).\n\nHere, we'll see how a [Pydantic BaseModel](https://docs.pydantic.dev/latest/api/base_model/) can be used for [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) to add run-time validation on **inputs**.\n\n<Note>\n  **Known Limitations**\n\n  * Currently, the output of the graph will **NOT** be an instance of a pydantic model.\n  * Run-time validation only occurs on inputs to the first node in the graph, not on subsequent nodes or outputs.\n  * The validation error trace from pydantic does not show which node the error arises in.\n  * Pydantic's recursive validation can be slow. For performance-sensitive applications, you may want to consider using a `dataclass` instead.\n</Note>\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\nfrom pydantic import BaseModel\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(BaseModel):\n    a: str\n\ndef node(state: OverallState):\n    return {\"a\": \"goodbye\"}\n\n# Build the state graph\nbuilder = StateGraph(OverallState)\nbuilder.add_node(node)  # node_1 is the first node\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\ngraph = builder.compile()\n\n# Test the graph with a valid input\ngraph.invoke({\"a\": \"hello\"})\n```\n\nInvoke the graph with an **invalid** input\n\n```python  theme={null}\ntry:\n    graph.invoke({\"a\": 123})  # Should be a string\nexcept Exception as e:\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\n    print(e)\n```\n\n```\nAn exception was raised because `a` is an integer rather than a string.\n1 validation error for OverallState\na\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\n```\n\nSee below for additional features of Pydantic model state:\n\n<Accordion title=\"Serialization Behavior\">\n  When using Pydantic models as state schemas, it's important to understand how serialization works, especially when:\n\n  * Passing Pydantic objects as inputs\n  * Receiving outputs from the graph\n  * Working with nested Pydantic models\n\n  Let's see these behaviors in action.\n\n  ```python  theme={null}\n  from langgraph.graph import StateGraph, START, END\n  from pydantic import BaseModel\n\n  class NestedModel(BaseModel):\n      value: str\n\n  class ComplexState(BaseModel):\n      text: str\n      count: int\n      nested: NestedModel\n\n  def process_node(state: ComplexState):\n      # Node receives a validated Pydantic object\n      print(f\"Input state type: {type(state)}\")\n      print(f\"Nested type: {type(state.nested)}\")\n      # Return a dictionary update\n      return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\n\n  # Build the graph\n  builder = StateGraph(ComplexState)\n  builder.add_node(\"process\", process_node)\n  builder.add_edge(START, \"process\")\n  builder.add_edge(\"process\", END)\n  graph = builder.compile()\n\n  # Create a Pydantic instance for input\n  input_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\n  print(f\"Input object type: {type(input_state)}\")\n\n  # Invoke graph with a Pydantic instance\n  result = graph.invoke(input_state)\n  print(f\"Output type: {type(result)}\")\n  print(f\"Output content: {result}\")\n\n  # Convert back to Pydantic model if needed\n  output_model = ComplexState(**result)\n  print(f\"Converted back to Pydantic: {type(output_model)}\")\n  ```\n</Accordion>\n\n<Accordion title=\"Runtime Type Coercion\">\n  Pydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you're not aware of it.\n\n  ```python  theme={null}\n  from langgraph.graph import StateGraph, START, END\n  from pydantic import BaseModel\n\n  class CoercionExample(BaseModel):\n      # Pydantic will coerce string numbers to integers\n      number: int\n      # Pydantic will parse string booleans to bool\n      flag: bool\n\n  def inspect_node(state: CoercionExample):\n      print(f\"number: {state.number} (type: {type(state.number)})\")\n      print(f\"flag: {state.flag} (type: {type(state.flag)})\")\n      return {}\n\n  builder = StateGraph(CoercionExample)\n  builder.add_node(\"inspect\", inspect_node)\n  builder.add_edge(START, \"inspect\")\n  builder.add_edge(\"inspect\", END)\n  graph = builder.compile()\n\n  # Demonstrate coercion with string inputs that will be converted\n  result = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\n\n  # This would fail with a validation error\n  try:\n      graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\n  except Exception as e:\n      print(f\"\\nExpected validation error: {e}\")\n  ```\n</Accordion>\n\n<Accordion title=\"Working with Message Models\">\n  When working with LangChain message types in your state schema, there are important considerations for serialization. You should use `AnyMessage` (rather than `BaseMessage`) for proper serialization/deserialization when using message objects over the wire.\n\n  ```python  theme={null}\n  from langgraph.graph import StateGraph, START, END\n  from pydantic import BaseModel\n  from langchain.messages import HumanMessage, AIMessage, AnyMessage\n  from typing import List\n\n  class ChatState(BaseModel):\n      messages: List[AnyMessage]\n      context: str\n\n  def add_message(state: ChatState):\n      return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\n\n  builder = StateGraph(ChatState)\n  builder.add_node(\"add_message\", add_message)\n  builder.add_edge(START, \"add_message\")\n  builder.add_edge(\"add_message\", END)\n  graph = builder.compile()\n\n  # Create input with a message\n  initial_state = ChatState(\n      messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\n  )\n\n  result = graph.invoke(initial_state)\n  print(f\"Output: {result}\")\n\n  # Convert back to Pydantic model to see message types\n  output_model = ChatState(**result)\n  for i, msg in enumerate(output_model.messages):\n      print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\n  ```\n</Accordion>\n\n## Add runtime configuration\n\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, *without polluting the graph state with these parameters*.\n\nTo add runtime configuration:\n\n1. Specify a schema for your configuration\n2. Add the configuration to the function signature for nodes or conditional edges\n3. Pass the configuration into the graph.\n\nSee below for a simple example:\n\n```python  theme={null}\nfrom langgraph.graph import END, StateGraph, START\nfrom langgraph.runtime import Runtime\nfrom typing_extensions import TypedDict\n\n# 1. Specify config schema\nclass ContextSchema(TypedDict):\n    my_runtime_value: str\n\n# 2. Define a graph that accesses the config in a node\nclass State(TypedDict):\n    my_state_value: str\n\ndef node(state: State, runtime: Runtime[ContextSchema]):  # [!code highlight]\n    if runtime.context[\"my_runtime_value\"] == \"a\":  # [!code highlight]\n        return {\"my_state_value\": 1}\n    elif runtime.context[\"my_runtime_value\"] == \"b\":  # [!code highlight]\n        return {\"my_state_value\": 2}\n    else:\n        raise ValueError(\"Unknown values.\")\n\nbuilder = StateGraph(State, context_schema=ContextSchema)  # [!code highlight]\nbuilder.add_node(node)\nbuilder.add_edge(START, \"node\")\nbuilder.add_edge(\"node\", END)\n\ngraph = builder.compile()\n\n# 3. Pass in configuration at runtime:\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))  # [!code highlight]\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))  # [!code highlight]\n```\n\n```\n{'my_state_value': 1}\n{'my_state_value': 2}\n```\n\n<Accordion title=\"Extended example: specifying LLM at runtime\">\n  Below we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.\n\n  ```python  theme={null}\n  from dataclasses import dataclass\n\n  from langchain.chat_models import init_chat_model\n  from langgraph.graph import MessagesState, END, StateGraph, START\n  from langgraph.runtime import Runtime\n  from typing_extensions import TypedDict\n\n  @dataclass\n  class ContextSchema:\n      model_provider: str = \"anthropic\"\n\n  MODELS = {\n      \"anthropic\": init_chat_model(\"claude-haiku-4-5-20251001\"),\n      \"openai\": init_chat_model(\"gpt-4.1-mini\"),\n  }\n\n  def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\n      model = MODELS[runtime.context.model_provider]\n      response = model.invoke(state[\"messages\"])\n      return {\"messages\": [response]}\n\n  builder = StateGraph(MessagesState, context_schema=ContextSchema)\n  builder.add_node(\"model\", call_model)\n  builder.add_edge(START, \"model\")\n  builder.add_edge(\"model\", END)\n\n  graph = builder.compile()\n\n  # Usage\n  input_message = {\"role\": \"user\", \"content\": \"hi\"}\n  # With no configuration, uses default (Anthropic)\n  response_1 = graph.invoke({\"messages\": [input_message]}, context=ContextSchema())[\"messages\"][-1]\n  # Or, can set OpenAI\n  response_2 = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\"})[\"messages\"][-1]\n\n  print(response_1.response_metadata[\"model_name\"])\n  print(response_2.response_metadata[\"model_name\"])\n  ```\n\n  ```\n  claude-haiku-4-5-20251001\n  gpt-4.1-mini-2025-04-14\n  ```\n</Accordion>\n\n<Accordion title=\"Extended example: specifying model and system message at runtime\">\n  Below we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.\n\n  ```python  theme={null}\n  from dataclasses import dataclass\n  from langchain.chat_models import init_chat_model\n  from langchain.messages import SystemMessage\n  from langgraph.graph import END, MessagesState, StateGraph, START\n  from langgraph.runtime import Runtime\n  from typing_extensions import TypedDict\n\n  @dataclass\n  class ContextSchema:\n      model_provider: str = \"anthropic\"\n      system_message: str | None = None\n\n  MODELS = {\n      \"anthropic\": init_chat_model(\"claude-haiku-4-5-20251001\"),\n      \"openai\": init_chat_model(\"gpt-4.1-mini\"),\n  }\n\n  def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\n      model = MODELS[runtime.context.model_provider]\n      messages = state[\"messages\"]\n      if (system_message := runtime.context.system_message):\n          messages = [SystemMessage(system_message)] + messages\n      response = model.invoke(messages)\n      return {\"messages\": [response]}\n\n  builder = StateGraph(MessagesState, context_schema=ContextSchema)\n  builder.add_node(\"model\", call_model)\n  builder.add_edge(START, \"model\")\n  builder.add_edge(\"model\", END)\n\n  graph = builder.compile()\n\n  # Usage\n  input_message = {\"role\": \"user\", \"content\": \"hi\"}\n  response = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\n  for message in response[\"messages\"]:\n      message.pretty_print()\n  ```\n\n  ```\n  ================================ Human Message ================================\n\n  hi\n  ================================== Ai Message ==================================\n\n  Ciao! Come posso aiutarti oggi?\n  ```\n</Accordion>\n\n## Add retry policies\n\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\n\nTo configure a retry policy, pass the `retry_policy` parameter to the [`add_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_node). The `retry_policy` parameter takes in a `RetryPolicy` named tuple object. Below we instantiate a `RetryPolicy` object with the default parameters and associate it with a node:\n\n```python  theme={null}\nfrom langgraph.types import RetryPolicy\n\nbuilder.add_node(\n    \"node_name\",\n    node_function,\n    retry_policy=RetryPolicy(),\n)\n```\n\nBy default, the `retry_on` parameter uses the `default_retry_on` function, which retries on any exception except for the following:\n\n* `ValueError`\n* `TypeError`\n* `ArithmeticError`\n* `ImportError`\n* `LookupError`\n* `NameError`\n* `SyntaxError`\n* `RuntimeError`\n* `ReferenceError`\n* `StopIteration`\n* `StopAsyncIteration`\n* `OSError`\n\nIn addition, for exceptions from popular http request libraries such as `requests` and `httpx` it only retries on 5xx status codes.\n\n<Accordion title=\"Extended example: customizing retry policies\">\n  Consider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:\n\n  ```python  theme={null}\n  import sqlite3\n  from typing_extensions import TypedDict\n  from langchain.chat_models import init_chat_model\n  from langgraph.graph import END, MessagesState, StateGraph, START\n  from langgraph.types import RetryPolicy\n  from langchain_community.utilities import SQLDatabase\n  from langchain.messages import AIMessage\n\n  db = SQLDatabase.from_uri(\"sqlite:///:memory:\")\n  model = init_chat_model(\"claude-haiku-4-5-20251001\")\n\n  def query_database(state: MessagesState):\n      query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\n      return {\"messages\": [AIMessage(content=query_result)]}\n\n  def call_model(state: MessagesState):\n      response = model.invoke(state[\"messages\"])\n      return {\"messages\": [response]}\n\n  # Define a new graph\n  builder = StateGraph(MessagesState)\n  builder.add_node(\n      \"query_database\",\n      query_database,\n      retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\n  )\n  builder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\n  builder.add_edge(START, \"model\")\n  builder.add_edge(\"model\", \"query_database\")\n  builder.add_edge(\"query_database\", END)\n  graph = builder.compile()\n  ```\n</Accordion>\n\n## Add node caching\n\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\n\nTo configure a cache policy, pass the `cache_policy` parameter to the [`add_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_node) function. In the following example, a [`CachePolicy`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.CachePolicy) object is instantiated with a time to live of 120 seconds and the default `key_func` generator. Then it is associated with a node:\n\n```python  theme={null}\nfrom langgraph.types import CachePolicy\n\nbuilder.add_node(\n    \"node_name\",\n    node_function,\n    cache_policy=CachePolicy(ttl=120),\n)\n```\n\nThen, to enable node-level caching for a graph, set the `cache` argument when compiling the graph. The example below uses `InMemoryCache` to set up a graph with in-memory cache, but `SqliteCache` is also available.\n\n```python  theme={null}\nfrom langgraph.cache.memory import InMemoryCache\n\ngraph = builder.compile(cache=InMemoryCache())\n```\n\n## Create a sequence of steps\n\n<Info>\n  **Prerequisites**\n  This guide assumes familiarity with the above section on [state](#define-and-update-state).\n</Info>\n\nHere we demonstrate how to construct a simple sequence of steps. We will show:\n\n1. How to build a sequential graph\n2. Built-in short-hand for constructing similar graphs.\n\nTo add a sequence of nodes, we use the [`add_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_node) and [`add_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_edge) methods of our [graph](/oss/python/langgraph/graph-api#stategraph):\n\n```python  theme={null}\nfrom langgraph.graph import START, StateGraph\n\nbuilder = StateGraph(State)\n\n# Add nodes\nbuilder.add_node(step_1)\nbuilder.add_node(step_2)\nbuilder.add_node(step_3)\n\n# Add edges\nbuilder.add_edge(START, \"step_1\")\nbuilder.add_edge(\"step_1\", \"step_2\")\nbuilder.add_edge(\"step_2\", \"step_3\")\n```\n\nWe can also use the built-in shorthand `.add_sequence`:\n\n```python  theme={null}\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\nbuilder.add_edge(START, \"step_1\")\n```\n\n<Accordion title=\"Why split application steps into a sequence with LangGraph?\">\n  LangGraph makes it easy to add an underlying persistence layer to your application.\n  This allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\n\n  * How state updates are [checkpointed](/oss/python/langgraph/persistence)\n  * How interruptions are resumed in [human-in-the-loop](/oss/python/langgraph/interrupts) workflows\n  * How we can \"rewind\" and branch-off executions using LangGraph's [time travel](/oss/python/langgraph/use-time-travel) features\n\n  They also determine how execution steps are [streamed](/oss/python/langgraph/streaming), and how your application is visualized and debugged using [Studio](/langsmith/studio).\n\n  Let's demonstrate an end-to-end example. We will create a sequence of three steps:\n\n  1. Populate a value in a key of the state\n  2. Update the same value\n  3. Populate a different value\n\n  Let's first define our [state](/oss/python/langgraph/graph-api#state). This governs the [schema of the graph](/oss/python/langgraph/graph-api#schema), and can also specify how to apply updates. See [this section](#process-state-updates-with-reducers) for more detail.\n\n  In our case, we will just keep track of two values:\n\n  ```python  theme={null}\n  from typing_extensions import TypedDict\n\n  class State(TypedDict):\n      value_1: str\n      value_2: int\n  ```\n\n  Our [nodes](/oss/python/langgraph/graph-api#nodes) are just Python functions that read our graph's state and make updates to it. The first argument to this function will always be the state:\n\n  ```python  theme={null}\n  def step_1(state: State):\n      return {\"value_1\": \"a\"}\n\n  def step_2(state: State):\n      current_value_1 = state[\"value_1\"]\n      return {\"value_1\": f\"{current_value_1} b\"}\n\n  def step_3(state: State):\n      return {\"value_2\": 10}\n  ```\n\n  <Note>\n    Note that when issuing updates to the state, each node can just specify the value of the key it wishes to update.\n\n    By default, this will **overwrite** the value of the corresponding key. You can also use [reducers](/oss/python/langgraph/graph-api#reducers) to control how updates are processed— for example, you can append successive updates to a key instead. See [this section](#process-state-updates-with-reducers) for more detail.\n  </Note>\n\n  Finally, we define the graph. We use [StateGraph](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state.\n\n  We will then use [`add_node`](/oss/python/langgraph/graph-api#messagesstate) and [`add_edge`](/oss/python/langgraph/graph-api#edges) to populate our graph and define its control flow.\n\n  ```python  theme={null}\n  from langgraph.graph import START, StateGraph\n\n  builder = StateGraph(State)\n\n  # Add nodes\n  builder.add_node(step_1)\n  builder.add_node(step_2)\n  builder.add_node(step_3)\n\n  # Add edges\n  builder.add_edge(START, \"step_1\")\n  builder.add_edge(\"step_1\", \"step_2\")\n  builder.add_edge(\"step_2\", \"step_3\")\n  ```\n\n  <Tip>\n    **Specifying custom names**\n    You can specify custom names for nodes using [`add_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_node):\n\n    ```python  theme={null}\n    builder.add_node(\"my_node\", step_1)\n    ```\n  </Tip>\n\n  Note that:\n\n  * [`add_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_edge) takes the names of nodes, which for functions defaults to `node.__name__`.\n  * We must specify the entry point of the graph. For this we add an edge with the [START node](/oss/python/langgraph/graph-api#start-node).\n  * The graph halts when there are no more nodes to execute.\n\n  We next [compile](/oss/python/langgraph/graph-api#compiling-your-graph) our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a [checkpointer](/oss/python/langgraph/persistence), it would also be passed in here.\n\n  ```python  theme={null}\n  graph = builder.compile()\n  ```\n\n  LangGraph provides built-in utilities for visualizing your graph. Let's inspect our sequence. See [this guide](#visualize-your-graph) for detail on visualization.\n\n  ```python  theme={null}\n  from IPython.display import Image, display\n\n  display(Image(graph.get_graph().draw_mermaid_png()))\n  ```\n\n    <img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_2.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=fa0376786cc89d704a5435abba178804\" alt=\"Sequence of steps graph\" data-og-width=\"107\" width=\"107\" data-og-height=\"333\" height=\"333\" data-path=\"oss/images/graph_api_image_2.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_2.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e2d4ec28fa1b03fab44cbcfccd19aa16 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_2.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=5ab128ae8f12f766384f48e03fa2c35c 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_2.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=db4260bece32ab8f5045ea7b9b151c45 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_2.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=8a93a6970742a83f06fb1a5288668eef 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_2.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=269956fccda17f64def8a69db847d4aa 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_2.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=40f495cb5fbca4aa2c960083a50af52e 2500w\" />\n\n  Let's proceed with a simple invocation:\n\n  ```python  theme={null}\n  graph.invoke({\"value_1\": \"c\"})\n  ```\n\n  ```\n  {'value_1': 'a b', 'value_2': 10}\n  ```\n\n  Note that:\n\n  * We kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\n  * The value we passed in was overwritten by the first node.\n  * The second node updated the value.\n  * The third node populated a different value.\n\n  <Tip>\n    **Built-in shorthand**\n    `langgraph>=0.2.46` includes a built-in short-hand `add_sequence` for adding node sequences. You can compile the same graph as follows:\n\n    ```python  theme={null}\n    builder = StateGraph(State).add_sequence([step_1, step_2, step_3])  # [!code highlight]\n    builder.add_edge(START, \"step_1\")\n\n    graph = builder.compile()\n\n    graph.invoke({\"value_1\": \"c\"})\n    ```\n  </Tip>\n</Accordion>\n\n## Create branches\n\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and [conditional\\_edges](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.MessageGraph.add_conditional_edges). Below are some examples showing how to add create branching dataflows that work for you.\n\n### Run graph nodes in parallel\n\nIn this example, we fan out from `Node A` to `B and C` and then fan in to `D`. With our state, [we specify the reducer add operation](/oss/python/langgraph/graph-api#reducers). This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on [state reducers](#process-state-updates-with-reducers) for more detail on updating state with reducers.\n\n```python  theme={null}\nimport operator\nfrom typing import Annotated, Any\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n```\n\n```python  theme={null}\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_3.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=8359f2e8d9dde03d7cc25f9d755a428d\" alt=\"Parallel execution graph\" data-og-width=\"143\" width=\"143\" data-og-height=\"432\" height=\"432\" data-path=\"oss/images/graph_api_image_3.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_3.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=75695e23f3e5e7eddb985785376108c4 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_3.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=cf45dc47fcfcf30ef39922a44119d815 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_3.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=92b3e0a7d06b07becf4deab660ff3717 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_3.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=8c0e296783bde688d32b36e7e8fb669c 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_3.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a4ff2db4eea2ab57343b329f6e21949c 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_3.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=99b0250accefffa610c67662ca4be2a2 2500w\" />\n\nWith the reducer, you can see that the values added in each node are accumulated.\n\n```python  theme={null}\ngraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\n```\n\n```\nAdding \"A\" to []\nAdding \"B\" to ['A']\nAdding \"C\" to ['A']\nAdding \"D\" to ['A', 'B', 'C']\n```\n\n<Note>\n  In the above example, nodes `\"b\"` and `\"c\"` are executed concurrently in the same [superstep](/oss/python/langgraph/graph-api#graphs). Because they are in the same step, node `\"d\"` executes after both `\"b\"` and `\"c\"` are finished.\n\n  Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\n</Note>\n\n<Accordion title=\"Exception handling?\">\n  LangGraph executes nodes within [supersteps](/oss/python/langgraph/graph-api#graphs), meaning that while parallel branches are executed in parallel, the entire superstep is **transactional**. If any of these branches raises an exception, **none** of the updates are applied to the state (the entire superstep errors).\n\n  Importantly, when using a [checkpointer](/oss/python/langgraph/persistence), results from successful nodes within a superstep are saved, and don't repeat when resumed.\n\n  If you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\n\n  1. You can write regular python code within your node to catch and handle exceptions.\n  2. You can set a **[retry\\_policy](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.RetryPolicy)** to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn't worry about performing redundant work.\n\n  Together, these let you perform parallel execution and fully control exception handling.\n</Accordion>\n\n<Tip>\n  **Set max concurrency**\n  You can control the maximum number of concurrent tasks by setting `max_concurrency` in the [configuration](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) when invoking the graph.\n\n  ```python  theme={null}\n  graph.invoke({\"value_1\": \"c\"}, {\"configurable\": {\"max_concurrency\": 10}})\n  ```\n</Tip>\n\n### Defer node execution\n\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\n\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let's add a node `\"b_2\"` in the `\"b\"` branch:\n\n```python  theme={null}\nimport operator\nfrom typing import Annotated, Any\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef b_2(state: State):\n    print(f'Adding \"B_2\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B_2\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(b_2)\nbuilder.add_node(c)\nbuilder.add_node(d, defer=True)  # [!code highlight]\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"b_2\")\nbuilder.add_edge(\"b_2\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n```\n\n```python  theme={null}\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_4.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=44cd97f020dfefeaffbe2b012514f343\" alt=\"Deferred execution graph\" data-og-width=\"161\" width=\"161\" data-og-height=\"531\" height=\"531\" data-path=\"oss/images/graph_api_image_4.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_4.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=645690182cd1ed41151da17c7d103d47 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_4.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=51cdd5ba95c2285baa2b7dc5236c8b63 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_4.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e99de6c886526afdb2e7a538e3d23705 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_4.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=92aba13b5bbc8428e42f2ad50ba7b607 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_4.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=14fda3686ef277c3f72a3ed8618c5e58 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_4.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=65c543b4b79c53b9224c74631b959e0b 2500w\" />\n\n```python  theme={null}\ngraph.invoke({\"aggregate\": []})\n```\n\n```\nAdding \"A\" to []\nAdding \"B\" to ['A']\nAdding \"C\" to ['A']\nAdding \"B_2\" to ['A', 'B', 'C']\nAdding \"D\" to ['A', 'B', 'C', 'B_2']\n```\n\nIn the above example, nodes `\"b\"` and `\"c\"` are executed concurrently in the same superstep. We set `defer=True` on node `d` so it will not execute until all pending tasks are finished. In this case, this means that `\"d\"` waits to execute until the entire `\"b\"` branch is finished.\n\n### Conditional branching\n\nIf your fan-out should vary at runtime based on the state, you can use [`add_conditional_edges`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) to select one or more paths using the graph state. See example below, where node `a` generates a state update that determines the following node.\n\n```python  theme={null}\nimport operator\nfrom typing import Annotated, Literal, Sequence\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    aggregate: Annotated[list, operator.add]\n    # Add a key to the state. We will set this key to determine\n    # how we branch.\n    which: str\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}  # [!code highlight]\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"b\", END)\nbuilder.add_edge(\"c\", END)\n\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\n    # Fill in arbitrary logic here that uses the state\n    # to determine the next node\n    return state[\"which\"]\n\nbuilder.add_conditional_edges(\"a\", conditional_edge)  # [!code highlight]\n\ngraph = builder.compile()\n```\n\n```python  theme={null}\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_5.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3373a383d5acc3e4d6a4d1575e849146\" alt=\"Conditional branching graph\" data-og-width=\"143\" width=\"143\" data-og-height=\"333\" height=\"333\" data-path=\"oss/images/graph_api_image_5.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_5.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=addc707d8e23e088279d93e61cd4429c 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_5.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9b0779c2c5444a984a67617640449b26 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_5.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=77a82cd36bc56637b4c3bdd0bccc656a 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_5.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=fd83ca7056bb93a4a72187b4aeed3873 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_5.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=5c57aebb9c69aa7bce3f77adcaee11a4 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_5.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=0e256ff324997275e003ee62809e030d 2500w\" />\n\n```python  theme={null}\nresult = graph.invoke({\"aggregate\": []})\nprint(result)\n```\n\n```\nAdding \"A\" to []\nAdding \"C\" to ['A']\n{'aggregate': ['A', 'C'], 'which': 'c'}\n```\n\n<Tip>\n  Your conditional edges can route to multiple destination nodes. For example:\n\n  ```python  theme={null}\n  def route_bc_or_cd(state: State) -> Sequence[str]:\n      if state[\"which\"] == \"cd\":\n          return [\"c\", \"d\"]\n      return [\"b\", \"c\"]\n  ```\n</Tip>\n\n## Map-Reduce and the Send API\n\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Send\nfrom typing_extensions import TypedDict, Annotated\nimport operator\n\nclass OverallState(TypedDict):\n    topic: str\n    subjects: list[str]\n    jokes: Annotated[list[str], operator.add]\n    best_selected_joke: str\n\ndef generate_topics(state: OverallState):\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\n\ndef generate_joke(state: OverallState):\n    joke_map = {\n        \"lions\": \"Why don't lions like fast food? Because they can't catch it!\",\n        \"elephants\": \"Why don't elephants use computers? They're afraid of the mouse!\",\n        \"penguins\": \"Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\n    }\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\n\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n\ndef best_joke(state: OverallState):\n    return {\"best_selected_joke\": \"penguins\"}\n\nbuilder = StateGraph(OverallState)\nbuilder.add_node(\"generate_topics\", generate_topics)\nbuilder.add_node(\"generate_joke\", generate_joke)\nbuilder.add_node(\"best_joke\", best_joke)\nbuilder.add_edge(START, \"generate_topics\")\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\nbuilder.add_edge(\"best_joke\", END)\ngraph = builder.compile()\n```\n\n```python  theme={null}\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_6.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=48249d2085e8bfc63a142ccfba5082f5\" alt=\"Map-reduce graph with fanout\" data-og-width=\"160\" width=\"160\" data-og-height=\"432\" height=\"432\" data-path=\"oss/images/graph_api_image_6.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_6.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f37fee0612923f1363e110025a9b9727 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_6.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=83f39ecd3959718bbe11e2a3eaa6d8ef 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_6.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9edacf5d4a433e39922b4bc003906b9d 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_6.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3627608cc06068c975bff51e98247889 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_6.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=70d18d5cb2ed9e706aea7792723d6891 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_6.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=03f4b27152e455d84d589c0c46c2324d 2500w\" />\n\n```python  theme={null}\n# Call the graph: here we call it to generate a list of jokes\nfor step in graph.stream({\"topic\": \"animals\"}):\n    print(step)\n```\n\n```\n{'generate_topics': {'subjects': ['lions', 'elephants', 'penguins']}}\n{'generate_joke': {'jokes': [\"Why don't lions like fast food? Because they can't catch it!\"]}}\n{'generate_joke': {'jokes': [\"Why don't elephants use computers? They're afraid of the mouse!\"]}}\n{'generate_joke': {'jokes': ['Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.']}}\n{'best_joke': {'best_selected_joke': 'penguins'}}\n```\n\n## Create and control loops\n\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) that routes to the [END](/oss/python/langgraph/graph-api#end-node) node once we reach some termination condition.\n\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of [supersteps](/oss/python/langgraph/graph-api#graphs) that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits [here](/oss/python/langgraph/graph-api#recursion-limit).\n\nLet's consider a simple graph with a loop to better understand how these mechanisms work.\n\n<Tip>\n  To return the last value of your state instead of receiving a recursion limit error, see the [next section](#impose-a-recursion-limit).\n</Tip>\n\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\n\n```python  theme={null}\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\ndef route(state: State) -> Literal[\"b\", END]:\n    if termination_condition(state):\n        return END\n    else:\n        return \"b\"\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n```\n\nTo control the recursion limit, specify `\"recursionLimit\"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle:\n\n```python  theme={null}\nfrom langgraph.errors import GraphRecursionError\n\ntry:\n    graph.invoke(inputs, {\"recursion_limit\": 3})\nexcept GraphRecursionError:\n    print(\"Recursion Error\")\n```\n\nLet's define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\n\n```python  theme={null}\nimport operator\nfrom typing import Annotated, Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Node A sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Node B sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\n# Define nodes\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\n# Define edges\ndef route(state: State) -> Literal[\"b\", END]:\n    if len(state[\"aggregate\"]) < 7:\n        return \"b\"\n    else:\n        return END\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n```\n\n```python  theme={null}\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e1b99e7efe45b1fdc5836d590d5fbbc3\" alt=\"Simple loop graph\" data-og-width=\"188\" width=\"188\" data-og-height=\"249\" height=\"249\" data-path=\"oss/images/graph_api_image_7.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a443c1ddc2f6a4e7c73f4482c7d63912 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=f65d82d8aaeb024beb5da1aa2948bcdb 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=b95f4df2fb69f28779a1d8dd113409d0 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=bdb4011d05756c10a1c7b5dea683fdb7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=dde791caa4279a6248b59b70df99dd2c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e4d568719f1761ff3a3d2ea9175241d8 2500w\" />\n\nThis architecture is similar to a [ReAct agent](/oss/python/langgraph/workflows-agents) in which node `\"a\"` is a tool-calling model, and node `\"b\"` represents the tools.\n\nIn our `route` conditional edge, we specify that we should end after the `\"aggregate\"` list in the state passes a threshold length.\n\nInvoking the graph, we see that we alternate between nodes `\"a\"` and `\"b\"` before terminating once we reach the termination condition.\n\n```python  theme={null}\ngraph.invoke({\"aggregate\": []})\n```\n\n```\nNode A sees []\nNode B sees ['A']\nNode A sees ['A', 'B']\nNode B sees ['A', 'B', 'A']\nNode A sees ['A', 'B', 'A', 'B']\nNode B sees ['A', 'B', 'A', 'B', 'A']\nNode A sees ['A', 'B', 'A', 'B', 'A', 'B']\n```\n\n### Impose a recursion limit\n\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph's [recursion limit](/oss/python/langgraph/graph-api#recursion-limit). This will raise a `GraphRecursionError` after a given number of [supersteps](/oss/python/langgraph/graph-api#graphs). We can then catch and handle this exception:\n\n```python  theme={null}\nfrom langgraph.errors import GraphRecursionError\n\ntry:\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\nexcept GraphRecursionError:\n    print(\"Recursion Error\")\n```\n\n```\nNode A sees []\nNode B sees ['A']\nNode C sees ['A', 'B']\nNode D sees ['A', 'B']\nNode A sees ['A', 'B', 'C', 'D']\nRecursion Error\n```\n\n<Accordion title=\"Extended example: return state on hitting recursion limit\">\n  Instead of raising `GraphRecursionError`, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.\n\n  LangGraph implements a special `RemainingSteps` annotation. Under the hood, it creates a `ManagedValue` channel -- a state channel that will exist for the duration of our graph run and no longer.\n\n  ```python  theme={null}\n  import operator\n  from typing import Annotated, Literal\n  from typing_extensions import TypedDict\n  from langgraph.graph import StateGraph, START, END\n  from langgraph.managed.is_last_step import RemainingSteps\n\n  class State(TypedDict):\n      aggregate: Annotated[list, operator.add]\n      remaining_steps: RemainingSteps\n\n  def a(state: State):\n      print(f'Node A sees {state[\"aggregate\"]}')\n      return {\"aggregate\": [\"A\"]}\n\n  def b(state: State):\n      print(f'Node B sees {state[\"aggregate\"]}')\n      return {\"aggregate\": [\"B\"]}\n\n  # Define nodes\n  builder = StateGraph(State)\n  builder.add_node(a)\n  builder.add_node(b)\n\n  # Define edges\n  def route(state: State) -> Literal[\"b\", END]:\n      if state[\"remaining_steps\"] <= 2:\n          return END\n      else:\n          return \"b\"\n\n  builder.add_edge(START, \"a\")\n  builder.add_conditional_edges(\"a\", route)\n  builder.add_edge(\"b\", \"a\")\n  graph = builder.compile()\n\n  # Test it out\n  result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\n  print(result)\n  ```\n\n  ```\n  Node A sees []\n  Node B sees ['A']\n  Node A sees ['A', 'B']\n  {'aggregate': ['A', 'B', 'A']}\n  ```\n</Accordion>\n\n<Accordion title=\"Extended example: loops with branches\">\n  To better understand how the recursion limit works, let's consider a more complex example. Below we implement a loop, but one step fans out into two nodes:\n\n  ```python  theme={null}\n  import operator\n  from typing import Annotated, Literal\n  from typing_extensions import TypedDict\n  from langgraph.graph import StateGraph, START, END\n\n  class State(TypedDict):\n      aggregate: Annotated[list, operator.add]\n\n  def a(state: State):\n      print(f'Node A sees {state[\"aggregate\"]}')\n      return {\"aggregate\": [\"A\"]}\n\n  def b(state: State):\n      print(f'Node B sees {state[\"aggregate\"]}')\n      return {\"aggregate\": [\"B\"]}\n\n  def c(state: State):\n      print(f'Node C sees {state[\"aggregate\"]}')\n      return {\"aggregate\": [\"C\"]}\n\n  def d(state: State):\n      print(f'Node D sees {state[\"aggregate\"]}')\n      return {\"aggregate\": [\"D\"]}\n\n  # Define nodes\n  builder = StateGraph(State)\n  builder.add_node(a)\n  builder.add_node(b)\n  builder.add_node(c)\n  builder.add_node(d)\n\n  # Define edges\n  def route(state: State) -> Literal[\"b\", END]:\n      if len(state[\"aggregate\"]) < 7:\n          return \"b\"\n      else:\n          return END\n\n  builder.add_edge(START, \"a\")\n  builder.add_conditional_edges(\"a\", route)\n  builder.add_edge(\"b\", \"c\")\n  builder.add_edge(\"b\", \"d\")\n  builder.add_edge([\"c\", \"d\"], \"a\")\n  graph = builder.compile()\n  ```\n\n  ```python  theme={null}\n  from IPython.display import Image, display\n\n  display(Image(graph.get_graph().draw_mermaid_png()))\n  ```\n\n    <img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_8.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=20e2a9e8c15760eb9ecb07fc411aa70e\" alt=\"Complex loop graph with branches\" data-og-width=\"297\" width=\"297\" data-og-height=\"348\" height=\"348\" data-path=\"oss/images/graph_api_image_8.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_8.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=65ee62a3adb7bedaf7571d9ecdacb908 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_8.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e7c4c3341baeed9c747082f69d2b3ded 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_8.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=b64849cfc877d1b32422f6666d5f93a0 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_8.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=3d384eba95e1082504c7ef1d5309dfae 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_8.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=2fef71e345a90e5c2321c0dfda15d91b 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_8.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=09cf8e8ac3215e359e6e4304c09b3a9f 2500w\" />\n\n  This graph looks complex, but can be conceptualized as loop of [supersteps](/oss/python/langgraph/graph-api#graphs):\n\n  1. Node A\n  2. Node B\n  3. Nodes C and D\n  4. Node A\n  5. ...\n\n  We have a loop of four supersteps, where nodes C and D are executed concurrently.\n\n  Invoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:\n\n  ```python  theme={null}\n  result = graph.invoke({\"aggregate\": []})\n  ```\n\n  ```\n  Node A sees []\n  Node B sees ['A']\n  Node D sees ['A', 'B']\n  Node C sees ['A', 'B']\n  Node A sees ['A', 'B', 'C', 'D']\n  Node B sees ['A', 'B', 'C', 'D', 'A']\n  Node D sees ['A', 'B', 'C', 'D', 'A', 'B']\n  Node C sees ['A', 'B', 'C', 'D', 'A', 'B']\n  Node A sees ['A', 'B', 'C', 'D', 'A', 'B', 'C', 'D']\n  ```\n\n  However, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:\n\n  ```python  theme={null}\n  from langgraph.errors import GraphRecursionError\n\n  try:\n      result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\n  except GraphRecursionError:\n      print(\"Recursion Error\")\n  ```\n\n  ```\n  Node A sees []\n  Node B sees ['A']\n  Node C sees ['A', 'B']\n  Node D sees ['A', 'B']\n  Node A sees ['A', 'B', 'C', 'D']\n  Recursion Error\n  ```\n</Accordion>\n\n## Async\n\nUsing the async programming paradigm can produce significant performance improvements when running [IO-bound](https://en.wikipedia.org/wiki/I/O_bound) code concurrently (e.g., making concurrent API requests to a chat model provider).\n\nTo convert a `sync` implementation of the graph to an `async` implementation, you will need to:\n\n1. Update `nodes` use `async def` instead of `def`.\n2. Update the code inside to use `await` appropriately.\n3. Invoke the graph with `.ainvoke` or `.astream` as desired.\n\nBecause many LangChain objects implement the [Runnable Protocol](https://python.langchain.com/docs/expression_language/interface/) which has `async` variants of all the `sync` methods it's typically fairly quick to upgrade a `sync` graph to an `async` graph.\n\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"AWS Bedrock\">\n    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    </CodeGroup>\n\n    <Tab title=\"HuggingFace\">\n      👉 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)\n\n      ```shell  theme={null}\n      pip install -U \"langchain[huggingface]\"\n      ```\n\n      <CodeGroup>\n        ```python init_chat_model theme={null}\n        import os\n        from langchain.chat_models import init_chat_model\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        model = init_chat_model(\n            \"microsoft/Phi-3-mini-4k-instruct\",\n            model_provider=\"huggingface\",\n            temperature=0.7,\n            max_tokens=1024,\n        )\n        ```\n\n        ```python Model Class theme={null}\n        import os\n        from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        llm = HuggingFaceEndpoint(\n            repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n            temperature=0.7,\n            max_length=1024,\n        )\n        model = ChatHuggingFace(llm=llm)\n        ```\n      </CodeGroup>\n    </Tab>\n  </Tab>\n</Tabs>\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import MessagesState, StateGraph\n\nasync def node(state: MessagesState):  # [!code highlight]\n    new_message = await llm.ainvoke(state[\"messages\"])  # [!code highlight]\n    return {\"messages\": [new_message]}\n\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\ngraph = builder.compile()\n\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\nresult = await graph.ainvoke({\"messages\": [input_message]})  # [!code highlight]\n```\n\n<Tip>\n  **Async streaming**\n  See the [streaming guide](/oss/python/langgraph/streaming) for examples of streaming with async.\n</Tip>\n\n## Combine control flow and state updates with `Command`\n\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object from node functions:\n\n```python  theme={null}\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n```\n\nWe show an end-to-end example below. Let's create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\n\n```python  theme={null}\nimport random\nfrom typing_extensions import TypedDict, Literal\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.types import Command\n\n# Define graph state\nclass State(TypedDict):\n    foo: str\n\n# Define the nodes\n\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\n    print(\"Called A\")\n    value = random.choice([\"b\", \"c\"])\n    # this is a replacement for a conditional edge function\n    if value == \"b\":\n        goto = \"node_b\"\n    else:\n        goto = \"node_c\"\n\n    # note how Command allows you to BOTH update the graph state AND route to the next node\n    return Command(\n        # this is the state update\n        update={\"foo\": value},\n        # this is a replacement for an edge\n        goto=goto,\n    )\n\ndef node_b(state: State):\n    print(\"Called B\")\n    return {\"foo\": state[\"foo\"] + \"b\"}\n\ndef node_c(state: State):\n    print(\"Called C\")\n    return {\"foo\": state[\"foo\"] + \"c\"}\n```\n\nWe can now create the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) with the above nodes. Notice that the graph doesn't have [conditional edges](/oss/python/langgraph/graph-api#conditional-edges) for routing! This is because control flow is defined with [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) inside `node_a`.\n\n```python  theme={null}\nbuilder = StateGraph(State)\nbuilder.add_edge(START, \"node_a\")\nbuilder.add_node(node_a)\nbuilder.add_node(node_b)\nbuilder.add_node(node_c)\n# NOTE: there are no edges between nodes A, B and C!\n\ngraph = builder.compile()\n```\n\n<Warning>\n  You might have noticed that we used [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) as a return type annotation, e.g. `Command[Literal[\"node_b\", \"node_c\"]]`. This is necessary for the graph rendering and tells LangGraph that `node_a` can navigate to `node_b` and `node_c`.\n</Warning>\n\n```python  theme={null}\nfrom IPython.display import display, Image\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f11e5cddedbf2760d40533f294c44aea\" alt=\"Command-based graph navigation\" data-og-width=\"232\" width=\"232\" data-og-height=\"333\" height=\"333\" data-path=\"oss/images/graph_api_image_11.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c1b27d92b257a6c4ac57f34f007d0ee1 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=695d0062e5fb8ebea5525379edbba476 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7bd3f779df628beba60a397674f85b59 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=85a9194e8b4d9df2d01d10784dcf75d0 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=efd9118d4bcd6d1eb92760c573645fbd 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=1eb2a132386a64d18582af6978e4ac24 2500w\" />\n\nIf we run the graph multiple times, we'd see it take different paths (A -> B or A -> C) based on the random choice in node A.\n\n```python  theme={null}\ngraph.invoke({\"foo\": \"\"})\n```\n\n```\nCalled A\nCalled C\n```\n\n### Navigate to a node in a parent graph\n\nIf you are using [subgraphs](/oss/python/langgraph/use-subgraphs), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:\n\n```python  theme={null}\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n```\n\nLet's demonstrate this using the above example. We'll do so by changing `nodeA` in the above example into a single-node graph that we'll add as a subgraph to our parent graph.\n\n<Warning>\n  **State updates with `Command.PARENT`**\n  When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](/oss/python/langgraph/graph-api#schema), you **must** define a [reducer](/oss/python/langgraph/graph-api#reducers) for the key you're updating in the parent graph state. See the example below.\n</Warning>\n\n```python  theme={null}\nimport operator\nfrom typing_extensions import Annotated\n\nclass State(TypedDict):\n    # NOTE: we define a reducer here\n    foo: Annotated[str, operator.add]  # [!code highlight]\n\ndef node_a(state: State):\n    print(\"Called A\")\n    value = random.choice([\"a\", \"b\"])\n    # this is a replacement for a conditional edge function\n    if value == \"a\":\n        goto = \"node_b\"\n    else:\n        goto = \"node_c\"\n\n    # note how Command allows you to BOTH update the graph state AND route to the next node\n    return Command(\n        update={\"foo\": value},\n        goto=goto,\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\n        graph=Command.PARENT,  # [!code highlight]\n    )\n\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\n\ndef node_b(state: State):\n    print(\"Called B\")\n    # NOTE: since we've defined a reducer, we don't need to manually append\n    # new characters to existing 'foo' value. instead, reducer will append these\n    # automatically (via operator.add)\n    return {\"foo\": \"b\"}  # [!code highlight]\n\ndef node_c(state: State):\n    print(\"Called C\")\n    return {\"foo\": \"c\"}  # [!code highlight]\n\nbuilder = StateGraph(State)\nbuilder.add_edge(START, \"subgraph\")\nbuilder.add_node(\"subgraph\", subgraph)\nbuilder.add_node(node_b)\nbuilder.add_node(node_c)\n\ngraph = builder.compile()\n```\n\n```python  theme={null}\ngraph.invoke({\"foo\": \"\"})\n```\n\n```\nCalled A\nCalled C\n```\n\n### Use inside tools\n\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return `Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]})` from the tool:\n\n```python  theme={null}\n@tool\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\n    return Command(\n        update={\n            # update the state keys\n            \"user_info\": user_info,\n            # update the message history\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\n        }\n    )\n```\n\n<Warning>\n  You MUST include `messages` (or any state key used for the message history) in `Command.update` when returning [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) from a tool and the list of messages in `messages` MUST contain a `ToolMessage`. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\n</Warning>\n\nIf you are using tools that update state via [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command), we recommend using prebuilt [`ToolNode`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.ToolNode) which automatically handles tools returning [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) objects and propagates them to the graph state. If you're writing a custom node that calls tools, you would need to manually propagate [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) objects returned by the tools as the update from the node.\n\n## Visualize your graph\n\nHere we demonstrate how to visualize the graphs you create.\n\nYou can visualize any arbitrary [Graph](https://langchain-ai.github.io/langgraph/reference/graphs/), including [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph).\n\nLet's have some fun by drawing fractals :).\n\n```python  theme={null}\nimport random\nfrom typing import Annotated, Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\nclass MyNode:\n    def __init__(self, name: str):\n        self.name = name\n    def __call__(self, state: State):\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\n\ndef route(state) -> Literal[\"entry_node\", END]:\n    if len(state[\"messages\"]) > 10:\n        return END\n    return \"entry_node\"\n\ndef add_fractal_nodes(builder, current_node, level, max_level):\n    if level > max_level:\n        return\n    # Number of nodes to create at this level\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\n    for i in range(num_nodes):\n        nm = [\"A\", \"B\", \"C\"][i]\n        node_name = f\"node_{current_node}_{nm}\"\n        builder.add_node(node_name, MyNode(node_name))\n        builder.add_edge(current_node, node_name)\n        # Recursively add more nodes\n        r = random.random()\n        if r > 0.2 and level + 1 < max_level:\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\n        elif r > 0.05:\n            builder.add_conditional_edges(node_name, route, node_name)\n        else:\n            # End\n            builder.add_edge(node_name, END)\n\ndef build_fractal_graph(max_level: int):\n    builder = StateGraph(State)\n    entry_point = \"entry_node\"\n    builder.add_node(entry_point, MyNode(entry_point))\n    builder.add_edge(START, entry_point)\n    add_fractal_nodes(builder, entry_point, 1, max_level)\n    # Optional: set a finish point if required\n    builder.add_edge(entry_point, END)  # or any specific node\n    return builder.compile()\n\napp = build_fractal_graph(3)\n```\n\n### Mermaid\n\nWe can also convert a graph class into Mermaid syntax.\n\n```python  theme={null}\nprint(app.get_graph().draw_mermaid())\n```\n\n```\n%%{init: {'flowchart': {'curve': 'linear'}}}%%\ngraph TD;\n    tart__([<p>__start__</p>]):::first\n    ry_node(entry_node)\n    e_entry_node_A(node_entry_node_A)\n    e_entry_node_B(node_entry_node_B)\n    e_node_entry_node_B_A(node_node_entry_node_B_A)\n    e_node_entry_node_B_B(node_node_entry_node_B_B)\n    e_node_entry_node_B_C(node_node_entry_node_B_C)\n    nd__([<p>__end__</p>]):::last\n    tart__ --> entry_node;\n    ry_node --> __end__;\n    ry_node --> node_entry_node_A;\n    ry_node --> node_entry_node_B;\n    e_entry_node_B --> node_node_entry_node_B_A;\n    e_entry_node_B --> node_node_entry_node_B_B;\n    e_entry_node_B --> node_node_entry_node_B_C;\n    e_entry_node_A -.-> entry_node;\n    e_entry_node_A -.-> __end__;\n    e_node_entry_node_B_A -.-> entry_node;\n    e_node_entry_node_B_A -.-> __end__;\n    e_node_entry_node_B_B -.-> entry_node;\n    e_node_entry_node_B_B -.-> __end__;\n    e_node_entry_node_B_C -.-> entry_node;\n    e_node_entry_node_B_C -.-> __end__;\n    ssDef default fill:#f2f0ff,line-height:1.2\n    ssDef first fill-opacity:0\n    ssDef last fill:#bfb6fc\n```\n\n### PNG\n\nIf preferred, we could render the Graph into a `.png`. Here we could use three options:\n\n* Using Mermaid.ink API (does not require additional packages)\n* Using Mermaid + Pyppeteer (requires `pip install pyppeteer`)\n* Using graphviz (which requires `pip install graphviz`)\n\n**Using Mermaid.Ink**\n\nBy default, `draw_mermaid_png()` uses Mermaid.Ink's API to generate the diagram.\n\n```python  theme={null}\nfrom IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\ndisplay(Image(app.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_10.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=6cb916b7c627e81c2816cc74ebf3f913\" alt=\"Fractal graph visualization\" data-og-width=\"2382\" width=\"2382\" data-og-height=\"1131\" height=\"1131\" data-path=\"oss/images/graph_api_image_10.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_10.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=01b02e6994b97c652851bf1a5be524b5 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_10.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9ac63a57750ff509e5bcf0662a141092 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_10.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=5458c09f31e42d0fd8f58ba85626d89c 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_10.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=feb0a463b249cd838ad31105ef695214 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_10.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=1a83b92a2d3b428d9b788720a7e54184 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_10.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=8bf42c6ee15584253dc036ff9b60191a 2500w\" />\n\n**Using Mermaid + Pyppeteer**\n\n```python  theme={null}\nimport nest_asyncio\n\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\n\ndisplay(\n    Image(\n        app.get_graph().draw_mermaid_png(\n            curve_style=CurveStyle.LINEAR,\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n            wrap_label_n_words=9,\n            output_file_path=None,\n            draw_method=MermaidDrawMethod.PYPPETEER,\n            background_color=\"white\",\n            padding=10,\n        )\n    )\n)\n```\n\n**Using Graphviz**\n\n```python  theme={null}\ntry:\n    display(Image(app.get_graph().draw_png()))\nexcept ImportError:\n    print(\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\n    )\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-graph-api.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 91005
}