{
  "title": "Runtime",
  "source_url": "https://docs.langchain.com/oss/python/langchain/runtime",
  "content": "## Overview\n\nLangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) runs on LangGraph's runtime under the hood.\n\nLangGraph exposes a [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object with the following information:\n\n1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation\n2. **Store**: a [BaseStore](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) instance used for [long-term memory](/oss/python/langchain/long-term-memory)\n3. **Stream writer**: an object used for streaming information via the `\"custom\"` stream mode\n\n<Tip>\n  Runtime context provides **dependency injection** for your tools and middleware. Instead of hardcoding values or using global state, you can inject runtime dependencies (like database connections, user IDs, or configuration) when invoking your agent. This makes your tools more testable, reusable, and flexible.\n</Tip>\n\nYou can access the runtime information within [tools](#inside-tools) and [middleware](#inside-middleware).\n\n## Access\n\nWhen creating an agent with [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), you can specify a `context_schema` to define the structure of the `context` stored in the agent [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime).\n\nWhen invoking the agent, pass the `context` argument with the relevant configuration for the run:\n\n```python  theme={null}\nfrom dataclasses import dataclass\n\nfrom langchain.agents import create_agent\n\n\n@dataclass\nclass Context:\n    user_name: str\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    context_schema=Context  # [!code highlight]\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")  # [!code highlight]\n)\n```\n\n### Inside tools\n\nYou can access the runtime information inside tools to:\n\n* Access the context\n* Read or write long-term memory\n* Write to the [custom stream](/oss/python/langchain/streaming#custom-updates) (ex, tool progress / updates)\n\nUse the `ToolRuntime` parameter to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside a tool.\n\n```python  theme={null}\nfrom dataclasses import dataclass\nfrom langchain.tools import tool, ToolRuntime  # [!code highlight]\n\n@dataclass\nclass Context:\n    user_id: str\n\n@tool\ndef fetch_user_email_preferences(runtime: ToolRuntime[Context]) -> str:  # [!code highlight]\n    \"\"\"Fetch the user's email preferences from the store.\"\"\"\n    user_id = runtime.context.user_id  # [!code highlight]\n\n    preferences: str = \"The user prefers you to write a brief and polite email.\"\n    if runtime.store:  # [!code highlight]\n        if memory := runtime.store.get((\"users\",), user_id):  # [!code highlight]\n            preferences = memory.value[\"preferences\"]\n\n    return preferences\n```\n\n### Inside middleware\n\nYou can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.\n\nUse `request.runtime` to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside middleware decorators. The runtime object is available in the [`ModelRequest`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRequest) parameter passed to middleware functions.\n\n```python  theme={null}\nfrom dataclasses import dataclass\n\nfrom langchain.messages import AnyMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest, before_model, after_model\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass Context:\n    user_name: str\n\n# Dynamic prompts\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context.user_name  # [!code highlight]\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n# Before model hook\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Processing request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\n# After model hook\n@after_model\ndef log_after_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Completed request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    middleware=[dynamic_system_prompt, log_before_model, log_after_model],  # [!code highlight]\n    context_schema=Context\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")\n)\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/runtime.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 5419
}