{
  "title": "Manage prompts programmatically",
  "source_url": "https://docs.langchain.com/langsmith/manage-prompts-programmatically",
  "content": "You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.\n\n<Note>\n  Previously this functionality lived in the `langchainhub` package which is now deprecated. All functionality going forward will live in the `langsmith` package.\n</Note>\n\n## Install packages\n\nIn Python, you can directly use the LangSmith SDK (*recommended, full functionality*) or you can use through the LangChain package (limited to pushing and pulling prompts).\n\nIn TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langsmith # version >= 0.1.99\n  ```\n\n  ```bash uv theme={null}\n  uv add langsmith  # version >= 0.1.99\n  ```\n\n  ```bash TypeScript theme={null}\n  yarn add langsmith langchain // langsmith version >= 0.1.99 and langchain version >= 0.2.14\n  ```\n</CodeGroup>\n\n## Configure environment variables\n\nIf you already have `LANGSMITH_API_KEY` set to your current workspace's api key from LangSmith, you can skip this step.\n\nOtherwise, get an API key for your workspace by navigating to `Settings > API Keys > Create API Key` in LangSmith.\n\nSet your environment variable.\n\n```bash  theme={null}\nexport LANGSMITH_API_KEY=\"lsv2_...\"\n```\n\n<Note>\n  What we refer to as \"prompts\" used to be called \"repos\", so any references to \"repo\" in the code are referring to a prompt.\n</Note>\n\n## Push a prompt\n\nTo create a new prompt or update an existing prompt, you can use the `push prompt` method.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n  from langchain_core.prompts import ChatPromptTemplate\n\n  client = Client()\n  prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n  url = client.push_prompt(\"joke-generator\", object=prompt)\n  # url is a link to the prompt in the UI\n  print(url)\n  ```\n\n  ```python LangChain (Python) theme={null}\n  from langchain_classic import hub as prompts\n  from langchain_core.prompts import ChatPromptTemplate\n\n  prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n  url = prompts.push(\"joke-generator\", prompt)\n  # url is a link to the prompt in the UI\n  print(url)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import * as hub from \"langchain/hub\";\n  import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\n  const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");\n  const url = hub.push(\"joke-generator\", {\n    object: prompt,\n  });\n  // url is a link to the prompt in the UI\n  console.log(url);\n  ```\n</CodeGroup>\n\nYou can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground. (see settings here: [Supported Providers](https://langsmith.com/playground))\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n  from langchain_core.prompts import ChatPromptTemplate\n  from langchain_openai import ChatOpenAI\n\n  client = Client()\n  model = ChatOpenAI(model=\"gpt-4o-mini\")\n  prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n  chain = prompt | model\n  client.push_prompt(\"joke-generator-with-model\", object=chain)\n  ```\n\n  ```python LangChain (Python) theme={null}\n  from langchain_classic import hub as prompts\n  from langchain_core.prompts import ChatPromptTemplate\n  from langchain_openai import ChatOpenAI\n\n  model = ChatOpenAI(model=\"gpt-4o-mini\")\n  prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n  chain = prompt | model\n  url = prompts.push(\"joke-generator-with-model\", chain)\n  # url is a link to the prompt in the UI\n  print(url)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import * as hub from \"langchain/hub\";\n  import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n  import { ChatOpenAI } from \"@langchain/openai\";\n\n  const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n  const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");\n  const chain = prompt.pipe(model);\n  await hub.push(\"joke-generator-with-model\", {\n    object: chain,\n  });\n  ```\n</CodeGroup>\n\n## Pull a prompt\n\nTo pull a prompt, you can use the `pull prompt` method, which returns a the prompt as a langchain `PromptTemplate`.\n\nTo pull a **private prompt** you do not need to specify the owner handle (though you can, if you have one set).\n\nTo pull a **public prompt** from the LangChain Hub, you need to specify the handle of the prompt's author.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n  from langchain_openai import ChatOpenAI\n\n  client = Client()\n  prompt = client.pull_prompt(\"joke-generator\")\n  model = ChatOpenAI(model=\"gpt-4o-mini\")\n  chain = prompt | model\n  chain.invoke({\"topic\": \"cats\"})\n  ```\n\n  ```python LangChain (Python) theme={null}\n  from langchain_classic import hub as prompts\n  from langchain_openai import ChatOpenAI\n\n  prompt = prompts.pull(\"joke-generator\")\n  model = ChatOpenAI(model=\"gpt-4o-mini\")\n  chain = prompt | model\n  chain.invoke({\"topic\": \"cats\"})\n  ```\n\n  ```typescript TypeScript theme={null}\n  import * as hub from \"langchain/hub\";\n  import { ChatOpenAI } from \"@langchain/openai\";\n\n  const prompt = await hub.pull(\"joke-generator\");\n  const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n  const chain = prompt.pipe(model);\n  await chain.invoke({\"topic\": \"cats\"});\n  ```\n</CodeGroup>\n\nSimilar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model. Just specify include\\_model when pulling the prompt. If the stored prompt includes a model, it will be returned as a RunnableSequence. Make sure you have the proper environment variables set for the model you are using.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n\n  client = Client()\n  chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)\n  chain.invoke({\"topic\": \"cats\"})\n  ```\n\n  ```python LangChain (Python) theme={null}\n  from langchain_classic import hub as prompts\n\n  chain = prompts.pull(\"joke-generator-with-model\", include_model=True)\n  chain.invoke({\"topic\": \"cats\"})\n  ```\n\n  ```typescript TypeScript theme={null}\n  import * as hub from \"langchain/hub\";\n  import { Runnable } from \"@langchain/core/runnables\";\n\n  const chain = await hub.pull<Runnable>(\"joke-generator-with-model\", { includeModel: true });\n  await chain.invoke({\"topic\": \"cats\"});\n  ```\n</CodeGroup>\n\nWhen pulling a prompt, you can also specify a specific commit hash or [commit tag](/langsmith/manage-prompts#commit-tags) to pull a specific version of the prompt.\n\n<CodeGroup>\n  ```python Python theme={null}\n  prompt = client.pull_prompt(\"joke-generator:12344e88\")\n  ```\n\n  ```python LangChain (Python) theme={null}\n  prompt = prompts.pull(\"joke-generator:12344e88\")\n  ```\n\n  ```typescript TypeScript theme={null}\n  const prompt = await hub.pull(\"joke-generator:12344e88\")\n  ```\n</CodeGroup>\n\nTo pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author.\n\n<CodeGroup>\n  ```python Python theme={null}\n  prompt = client.pull_prompt(\"efriis/my-first-prompt\")\n  ```\n\n  ```python LangChain (Python) theme={null}\n  prompt = prompts.pull(\"efriis/my-first-prompt\")\n  ```\n\n  ```typescript TypeScript theme={null}\n  const prompt = await hub.pull(\"efriis/my-first-prompt\")\n  ```\n</CodeGroup>\n\n<Note>\n  For pulling prompts, if you are using Node.js or an environment that supports dynamic imports, we recommend using the `langchain/hub/node` entrypoint, as it handles deserialization of models associated with your prompt configuration automatically.\n\n  If you are in a non-Node environment, \"includeModel\" is not supported for non-OpenAI models and you should use the base `langchain/hub` entrypoint.\n</Note>\n\n## Use a prompt without LangChain\n\nIf you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API.\n\nThese conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:\n\n### OpenAI\n\n<CodeGroup>\n  ```bash Python theme={null}\n  pip install -U langchain_openai\n  ```\n\n  ```bash TypeScript theme={null}\n  yarn add @langchain/openai @langchain/core // @langchain/openai version >= 0.3.2\n  ```\n</CodeGroup>\n\n<CodeGroup>\n  ```python Python theme={null}\n  from openai import OpenAI\n  from langsmith.client import Client, convert_prompt_to_openai_format\n\n  # langsmith client\n  client = Client()\n  # openai client\n  oai_client = OpenAI()\n\n  # pull prompt and invoke to populate the variables\n  prompt = client.pull_prompt(\"joke-generator\")\n  prompt_value = prompt.invoke({\"topic\": \"cats\"})\n  openai_payload = convert_prompt_to_openai_format(prompt_value)\n  openai_response = oai_client.chat.completions.create(**openai_payload)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import * as hub from \"langchain/hub\";\n  import { convertPromptToOpenAI } from \"@langchain/openai\";\n  import OpenAI from \"openai\";\n\n  const prompt = await hub.pull(\"jacob/joke-generator\");\n  const formattedPrompt = await prompt.invoke({\n    topic: \"cats\",\n  });\n  const { messages } = convertPromptToOpenAI(formattedPrompt);\n\n  const openAIClient = new OpenAI();\n  const openAIResponse = await openAIClient.chat.completions.create({\n    model: \"gpt-4o-mini\",\n    messages,\n  });\n  ```\n</CodeGroup>\n\n### Anthropic\n\n<CodeGroup>\n  ```bash Python theme={null}\n  pip install -U langchain_anthropic\n  ```\n\n  ```bash TypeScript theme={null}\n  yarn add @langchain/anthropic @langchain/core // @langchain/anthropic version >= 0.3.3\n  ```\n</CodeGroup>\n\n<CodeGroup>\n  ```python Python theme={null}\n  from anthropic import Anthropic\n  from langsmith.client import Client, convert_prompt_to_anthropic_format\n\n  # langsmith client\n  client = Client()\n  # anthropic client\n  anthropic_client = Anthropic()\n\n  # pull prompt and invoke to populate the variables\n  prompt = client.pull_prompt(\"joke-generator\")\n  prompt_value = prompt.invoke({\"topic\": \"cats\"})\n  anthropic_payload = convert_prompt_to_anthropic_format(prompt_value)\n  anthropic_response = anthropic_client.messages.create(**anthropic_payload)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import * as hub from \"langchain/hub\";\n  import { convertPromptToAnthropic } from \"@langchain/anthropic\";\n  import Anthropic from \"@anthropic-ai/sdk\";\n\n  const prompt = await hub.pull(\"jacob/joke-generator\");\n  const formattedPrompt = await prompt.invoke({\n    topic: \"cats\",\n  });\n  const { messages, system } = convertPromptToAnthropic(formattedPrompt);\n\n  const anthropicClient = new Anthropic();\n  const anthropicResponse = await anthropicClient.messages.create({\n    model: \"claude-haiku-4-5-20251001\",\n    system,\n    messages,\n    max_tokens: 1024,\n    stream: false,\n  });\n  ```\n</CodeGroup>\n\n## List, delete, and like prompts\n\nYou can also list, delete, and like/unlike prompts using the `list prompts`, `delete prompt`, `like prompt` and `unlike prompt` methods. See the [LangSmith SDK client](https://github.com/langchain-ai/langsmith-sdk) for extensive documentation on these methods.\n\n<CodeGroup>\n  ```python Python theme={null}\n  # List all prompts in my workspace\n  prompts = client.list_prompts()\n\n  # List my private prompts that include \"joke\"\n  prompts = client.list_prompts(query=\"joke\", is_public=False)\n\n  # Delete a prompt\n  client.delete_prompt(\"joke-generator\")\n\n  # Like a prompt\n  client.like_prompt(\"efriis/my-first-prompt\")\n\n  # Unlike a prompt\n  client.unlike_prompt(\"efriis/my-first-prompt\")\n  ```\n\n  ```typescript TypeScript theme={null}\n  // List all prompts in my workspace\n  import Client from \"langsmith\";\n\n  const client = new Client({ apiKey: \"lsv2_...\" });\n  const prompts = client.listPrompts();\n\n  for await (const prompt of prompts) {\n    console.log(prompt);\n  }\n\n  // List my private prompts that include \"joke\"\n  const private_joke_prompts = client.listPrompts({ query: \"joke\", isPublic: false});\n\n  // Delete a prompt\n  client.deletePrompt(\"joke-generator\");\n\n  // Like a prompt\n  client.likePrompt(\"efriis/my-first-prompt\");\n\n  // Unlike a prompt\n  client.unlikePrompt(\"efriis/my-first-prompt\");\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-prompts-programmatically.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 12833
}