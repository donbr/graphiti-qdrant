{
  "title": "Quickstart",
  "source_url": "https://docs.langchain.com/oss/python/langchain/quickstart",
  "content": "This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n\n## Build a basic agent\n\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\n\n<Info>\n  For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.\n</Info>\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n```\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\n## Build a real-world agent\n\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\n\n1. **Detailed system prompts** for better agent behavior\n2. **Create tools** that integrate with external data\n3. **Model configuration** for consistent responses\n4. **Structured output** for predictable results\n5. **Conversational memory** for chat-like interactions\n6. **Create and run the agent** create a fully functional agent\n\nLet's walk through each step:\n\n<Steps>\n  <Step title=\"Define the system prompt\">\n    The system prompt defines your agentâ€™s role and behavior. Keep it specific and actionable:\n\n    ```python wrap theme={null}\n    SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\n    You have access to two tools:\n\n    - get_weather_for_location: use this to get the weather for a specific location\n    - get_user_location: use this to get the user's location\n\n    If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n    ```\n  </Step>\n\n  <Step title=\"Create tools\">\n    [Tools](/oss/python/langchain/tools) let a model interact with external systems by calling functions you define.\n    Tools can depend on [runtime context](/oss/python/langchain/runtime) and also interact with [agent memory](/oss/python/langchain/short-term-memory).\n\n    Notice below how the `get_user_location` tool uses runtime context:\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n\n    @tool\n    def get_weather_for_location(city: str) -> str:\n        \"\"\"Get weather for a given city.\"\"\"\n        return f\"It's always sunny in {city}!\"\n\n    @dataclass\n    class Context:\n        \"\"\"Custom runtime context schema.\"\"\"\n        user_id: str\n\n    @tool\n    def get_user_location(runtime: ToolRuntime[Context]) -> str:\n        \"\"\"Retrieve user information based on user ID.\"\"\"\n        user_id = runtime.context.user_id\n        return \"Florida\" if user_id == \"1\" else \"SF\"\n    ```\n\n    <Tip>\n      Tools should be well-documented: their name, description, and argument names become part of the model's prompt.\n      LangChain's [`@tool` decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) adds metadata and enables runtime injection via the `ToolRuntime` parameter.\n    </Tip>\n  </Step>\n\n  <Step title=\"Configure your model\">\n    Set up your [language model](/oss/python/langchain/models) with the right [parameters](/oss/python/langchain/models#parameters) for your use case:\n\n    ```python  theme={null}\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0.5,\n        timeout=10,\n        max_tokens=1000\n    )\n    ```\n  </Step>\n\n  <Step title=\"Define response format\">\n    Optionally, define a structured response format if you need the agent responses to match\n    a specific schema.\n\n    ```python  theme={null}\n    from dataclasses import dataclass\n\n    # We use a dataclass here, but Pydantic models are also supported.\n    @dataclass\n    class ResponseFormat:\n        \"\"\"Response schema for the agent.\"\"\"\n        # A punny response (always required)\n        punny_response: str\n        # Any interesting information about the weather if available\n        weather_conditions: str | None = None\n    ```\n  </Step>\n\n  <Step title=\"Add memory\">\n    Add [memory](/oss/python/langchain/short-term-memory) to your agent to maintain state across interactions. This allows\n    the agent to remember previous conversations and context.\n\n    ```python  theme={null}\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    checkpointer = InMemorySaver()\n    ```\n\n    <Info>\n      In production, use a persistent checkpointer that saves to a database.\n      See [Add and manage memory](/oss/python/langgraph/add-memory#manage-short-term-memory) for more details.\n    </Info>\n  </Step>\n\n  <Step title=\"Create and run the agent\">\n    Now assemble your agent with all the components and run it!\n\n    ```python  theme={null}\n    agent = create_agent(\n        model=model,\n        system_prompt=SYSTEM_PROMPT,\n        tools=[get_user_location, get_weather_for_location],\n        context_schema=Context,\n        response_format=ToolStrategy(ResponseFormat),\n        checkpointer=checkpointer\n    )\n\n    # `thread_id` is a unique identifier for a given conversation.\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\n    print(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n    #     weather_conditions=\"It's always sunny in Florida!\"\n    # )\n\n\n    # Note that we can continue the conversation using the same `thread_id`.\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\n    print(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n    #     weather_conditions=None\n    # )\n    ```\n  </Step>\n</Steps>\n\n<Expandable title=\"Full example code\">\n  ```python  theme={null}\n  from dataclasses import dataclass\n\n  from langchain.agents import create_agent\n  from langchain.chat_models import init_chat_model\n  from langchain.tools import tool, ToolRuntime\n  from langgraph.checkpoint.memory import InMemorySaver\n  from langchain.agents.structured_output import ToolStrategy\n\n\n  # Define system prompt\n  SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\n  You have access to two tools:\n\n  - get_weather_for_location: use this to get the weather for a specific location\n  - get_user_location: use this to get the user's location\n\n  If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n\n  # Define context schema\n  @dataclass\n  class Context:\n      \"\"\"Custom runtime context schema.\"\"\"\n      user_id: str\n\n  # Define tools\n  @tool\n  def get_weather_for_location(city: str) -> str:\n      \"\"\"Get weather for a given city.\"\"\"\n      return f\"It's always sunny in {city}!\"\n\n  @tool\n  def get_user_location(runtime: ToolRuntime[Context]) -> str:\n      \"\"\"Retrieve user information based on user ID.\"\"\"\n      user_id = runtime.context.user_id\n      return \"Florida\" if user_id == \"1\" else \"SF\"\n\n  # Configure model\n  model = init_chat_model(\n      \"claude-sonnet-4-5-20250929\",\n      temperature=0\n  )\n\n  # Define response format\n  @dataclass\n  class ResponseFormat:\n      \"\"\"Response schema for the agent.\"\"\"\n      # A punny response (always required)\n      punny_response: str\n      # Any interesting information about the weather if available\n      weather_conditions: str | None = None\n\n  # Set up memory\n  checkpointer = InMemorySaver()\n\n  # Create agent\n  agent = create_agent(\n      model=model,\n      system_prompt=SYSTEM_PROMPT,\n      tools=[get_user_location, get_weather_for_location],\n      context_schema=Context,\n      response_format=ToolStrategy(ResponseFormat),\n      checkpointer=checkpointer\n  )\n\n  # Run agent\n  # `thread_id` is a unique identifier for a given conversation.\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n  response = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\n  print(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n  #     weather_conditions=\"It's always sunny in Florida!\"\n  # )\n\n\n  # Note that we can continue the conversation using the same `thread_id`.\n  response = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\n  print(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n  #     weather_conditions=None\n  # )\n  ```\n</Expandable>\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\nCongratulations! You now have an AI agent that can:\n\n* **Understand context** and remember conversations\n* **Use multiple tools** intelligently\n* **Provide structured responses** in a consistent format\n* **Handle user-specific information** through context\n* **Maintain conversation state** across interactions\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/quickstart.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 11185
}