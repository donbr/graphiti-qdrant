{
  "title": "LangChain v1 migration guide",
  "source_url": "https://docs.langchain.com/oss/javascript/migrate/langchain-v1",
  "content": "This migration guide outlines the major changes in LangChain v1. To learn more about the new features of v1, see the [introductory post](/oss/javascript/releases/langchain-v1).\n\nTo upgrade,\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install langchain@latest @langchain/core@latest\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm install langchain@latest @langchain/core@latest\n  ```\n\n  ```bash yarn theme={null}\n  yarn add langchain@latest @langchain/core@latest\n  ```\n\n  ```bash bun theme={null}\n  bun add langchain@latest @langchain/core@latest\n  ```\n</CodeGroup>\n\n## `createAgent`\n\nIn v1, the react agent prebuilt is now in the langchain package. The table below outlines what functionality has changed:\n\n| Section                                            | What changed                                                                             |\n| -------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n| [Import path](#import-path)                        | Package moved from `@langchain/langgraph/prebuilts` to `langchain`                       |\n| [Prompts](#prompts)                                | Parameter renamed to `systemPrompt`, dynamic prompts use middleware                      |\n| [Pre-model hook](#pre-model-hook)                  | Replaced by middleware with `beforeModel` method                                         |\n| [Post-model hook](#post-model-hook)                | Replaced by middleware with `afterModel` method                                          |\n| [Custom state](#custom-state)                      | Defined in middleware, zod objects only                                                  |\n| [Model](#model)                                    | Dynamic selection via middleware, pre-bound models not supported                         |\n| [Tools](#tools)                                    | Tool error handling moved to middleware with `wrapToolCall`                              |\n| [Structured output](#structured-output)            | prompted output removed, use `toolStrategy`/`providerStrategy`                           |\n| [Streaming node name](#streaming-node-name-rename) | Node name changed from `\"agent\"` to `\"model\"`                                            |\n| [Runtime context](#runtime-context)                | `context` property instead of `config.configurable`                                      |\n| [Namespace](#simplified-namespace)                 | Streamlined to focus on agent building blocks, legacy code moved to `@langchain/classic` |\n\n### Import path\n\nThe import path for the react agent prebuilt has changed from `@langchain/langgraph/prebuilts` to `langchain`. The name of the function has changed from `createReactAgent` to `createAgent`:\n\n```typescript  theme={null}\nimport { createReactAgent } from \"@langchain/langgraph/prebuilts\"; // [!code --]\nimport { createAgent } from \"langchain\"; // [!code ++]\n```\n\n### Prompts\n\n#### Static prompt rename\n\nThe `prompt` parameter has been renamed to `systemPrompt`:\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent } from \"langchain\";\n\n  agent = createAgent({\n    model,\n    tools,\n    systemPrompt: \"You are a helpful assistant.\", // [!code highlight]\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n\n  const agent = createReactAgent({\n    model,\n    tools,\n    prompt: \"You are a helpful assistant.\", // [!code highlight]\n  });\n  ```\n</CodeGroup>\n\n#### `SystemMessage`\n\nIf using `SystemMessage` objects in the system prompt, the string content is now used directly:\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { SystemMessage, createAgent } from \"langchain\";\n\n  const agent = createAgent({\n    model,\n    tools,\n    systemPrompt: \"You are a helpful assistant.\", // [!code highlight]\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n\n  const agent = createReactAgent({\n    model,\n    tools,\n    prompt: new SystemMessage(content: \"You are a helpful assistant.\"), // [!code highlight]\n  });\n  ```\n</CodeGroup>\n\n#### Dynamic prompts\n\nDynamic prompts are a core context engineering patternâ€” they adapt what you tell the model based on the current conversation state. To do this, use `dynamicSystemPromptMiddleware`:\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n  import * as z from \"zod\";\n\n  const contextSchema = z.object({\n    userRole: z.enum([\"expert\", \"beginner\"]).default(\"beginner\"),\n  });\n\n  const userRolePrompt = dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>( // [!code highlight]\n      (_state, runtime) => {\n          const userRole = runtime.context.userRole;\n          const basePrompt = \"You are a helpful assistant.\";\n\n          if (userRole === \"expert\") {\n              return `${basePrompt} Provide detailed technical responses.`;\n          } else if (userRole === \"beginner\") {\n              return `${basePrompt} Explain concepts simply and avoid jargon.`;\n          }\n          return basePrompt; // [!code highlight]\n      }\n  );\n\n  const agent = createAgent({\n    model,\n    tools,\n    middleware: [userRolePrompt],\n    contextSchema,\n  });\n\n  await agent.invoke(\n      {\n          messages: [new HumanMessage(\"Explain async programming\")]\n      },\n      {\n          configurable: {\n              userRole: \"expert\",\n          }\n      }\n  );\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n\n  const contextSchema = z.object({\n    userRole: z.enum([\"expert\", \"beginner\"]).default(\"user\"),\n  });\n\n  const agent = createReactAgent({\n    model,\n    tools,\n    prompt: (state) => {\n      const userRole = state.context.userRole;\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    },\n    contextSchema,\n  });\n\n  // Use with context\n  await agent.invoke({\n    messages: [new HumanMessage(\"Explain async programming\")],\n    context: { userRole: \"expert\" },\n  });\n  ```\n</CodeGroup>\n\n### Pre-model hook\n\nPre-model hooks are now implemented as middleware with the `beforeModel` method. This pattern is more extensible--you can define multiple middlewares to run before the model is called and reuse them across agents.\n\nCommon use cases include:\n\n* Summarizing conversation history\n* Trimming messages\n* Input guardrails, like PII redaction\n\nv1 includes built-in summarization middleware:\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent, summarizationMiddleware } from \"langchain\";\n\n  const agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools,\n    middleware: [\n      summarizationMiddleware({\n        model: \"claude-sonnet-4-5-20250929\",\n        trigger: { tokens: 1000 },\n      }),\n    ],\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n\n  function customSummarization(state) {\n    // Custom logic for message summarization\n  }\n\n  const agent = createReactAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools,\n    preModelHook: customSummarization,\n  });\n  ```\n</CodeGroup>\n\n### Post-model hook\n\nPost-model hooks are now implemented as middleware with the `afterModel` method. This lets you compose multiple handlers after the model responds.\n\nCommon use cases include:\n\n* Human-in-the-loop approval\n* Output guardrails\n\nv1 includes a built-in human-in-the-loop middleware:\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent, humanInTheLoopMiddleware } from \"langchain\";\n\n  const agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools: [readEmail, sendEmail],\n    middleware: [\n      humanInTheLoopMiddleware({\n        interruptOn: {\n          sendEmail: { allowedDecisions: [\"approve\", \"edit\", \"reject\"] },\n        },\n      }),\n    ],\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n\n  function customHumanInTheLoopHook(state) {\n    // Custom approval logic\n  }\n\n  const agent = createReactAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools: [readEmail, sendEmail],\n    postModelHook: customHumanInTheLoopHook,\n  });\n  ```\n</CodeGroup>\n\n### Custom state\n\nCustom state is now defined in middleware using the `stateSchema` property. Use Zod to declare additional state fields that are carried through the agent run.\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import * as z from \"zod\";\n  import { createAgent, createMiddleware, tool } from \"langchain\";\n\n  const UserState = z.object({\n    userName: z.string(),\n  });\n\n  const userState = createMiddleware({\n    name: \"UserState\",\n    stateSchema: UserState,\n    beforeModel: (state) => {\n      // Access custom state properties\n      const name = state.userName;\n      // Optionally modify messages/system prompt based on state\n      return;\n    },\n  });\n\n  const greet = tool(\n    async () => {\n      return \"Hello!\";\n    },\n    {\n      name: \"greet\",\n      description: \"Greet the user\",\n      schema: z.object({}),\n    }\n  );\n\n  const agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools: [greet],\n    middleware: [userState],\n  });\n\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"Hi\" }],\n    userName: \"Ada\",\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { getCurrentTaskInput } from \"@langchain/langgraph\";\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n  import * as z from \"zod\";\n\n  const UserState = z.object({\n    userName: z.string(),\n  });\n\n  const greet = tool(\n    async () => {\n      const state = await getCurrentTaskInput();\n      const userName = state.userName;\n      return `Hello ${userName}!`;\n    },\n  );\n\n  // Custom state was provided via agent-level state schema or accessed ad hoc in hooks\n  const agent = createReactAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools: [greet],\n    stateSchema: UserState,\n  });\n  ```\n</CodeGroup>\n\n### Model\n\nDynamic model selection now happens via middleware. Use `wrapModelCall` to swap models (and tools) based on state or runtime context. In `createReactAgent`, this was done via a function passed to the `model` parameter.\n\nThis functionality has been ported to the middleware interface in v1.\n\n#### Dynamic model selection\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent, createMiddleware } from \"langchain\";\n\n  const dynamicModel = createMiddleware({\n    name: \"DynamicModel\",\n    wrapModelCall: (request, handler) => {\n      const messageCount = request.state.messages.length;\n      const model = messageCount > 10 ? \"openai:gpt-5\" : \"openai:gpt-5-nano\";\n      return handler({ ...request, model });\n    },\n  });\n\n  const agent = createAgent({\n    model: \"gpt-5-nano\",\n    tools,\n    middleware: [dynamicModel],\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n\n  function selectModel(state) {\n    return state.messages.length > 10 ? \"openai:gpt-5\" : \"openai:gpt-5-nano\";\n  }\n\n  const agent = createReactAgent({\n    model: selectModel,\n    tools,\n  });\n  ```\n</CodeGroup>\n\n#### Pre-bound models\n\nTo better support structured output, `createAgent` should receive a plain model (string or instance) and a separate `tools` list. Avoid passing models pre-bound with tools when using structured output.\n\n```typescript  theme={null}\n// No longer supported\n// const modelWithTools = new ChatOpenAI({ model: \"gpt-4o-mini\" }).bindTools([someTool]);\n// const agent = createAgent({ model: modelWithTools, tools: [] });\n\n// Use instead\nconst agent = createAgent({ model: \"gpt-4o-mini\", tools: [someTool] });\n```\n\n### Tools\n\nThe `tools` argument to `createAgent` accepts:\n\n* Functions created with `tool`\n* LangChain tool instances\n* Objects that represent built-in provider tools\n\nUse middleware `wrapToolCall` to centralize error handling and logging for tools.\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent, createMiddleware } from \"langchain\";\n\n  const errorHandling = createMiddleware({\n    name: \"ToolErrors\",\n    wrapToolCall: async (request, handler) => {\n      try {\n        return await handler(request);\n      } catch (err) {\n        return `Error executing ${request.toolName}: ${String(err)}`;\n      }\n    },\n  });\n\n  const agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools: [checkWeather, searchWeb],\n    middleware: [errorHandling],\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n\n  const agent = createReactAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools: [checkWeather, searchWeb],\n    // Error handling commonly implemented inside tool code or post hooks\n  });\n  ```\n</CodeGroup>\n\n### Structured output\n\n#### Node changes\n\nStructured output used to be generated in a separate node from the main agent. This is no longer the case. Structured output is generated in the main loop (no extra LLM call), reducing cost and latency.\n\n#### Tool and provider strategies\n\nIn v1, there are two strategies:\n\n* `toolStrategy` uses artificial tool calling to generate structured output\n* `providerStrategy` uses provider-native structured output generation\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent, toolStrategy } from \"langchain\";\n  import * as z from \"zod\";\n\n  const OutputSchema = z.object({\n    summary: z.string(),\n    sentiment: z.string(),\n  });\n\n  const agent = createAgent({\n    model: \"gpt-4o-mini\",\n    tools,\n    // explicitly using tool strategy\n    responseFormat: toolStrategy(OutputSchema), // [!code highlight]\n  });\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent } from \"@langchain/langgraph/prebuilts\";\n  import * as z from \"zod\";\n\n  const OutputSchema = z.object({\n    summary: z.string(),\n    sentiment: z.string(),\n  });\n\n  const agent = createReactAgent({\n    model: \"gpt-4o-mini\",\n    tools,\n    // Structured output was driven primarily via tool-calling with fewer options\n    responseFormat: OutputSchema,\n  });\n  ```\n</CodeGroup>\n\n#### Prompted output removed\n\nPrompted output via custom instructions in `responseFormat` is removed in favor of the above strategies.\n\n### Streaming node name rename\n\nWhen streaming events from agents, the node name was changed from `\"agent\"` to `\"model\"` to better reflect the node's purpose.\n\n### Runtime context\n\nWhen invoking an agent, pass static, read-only configuration via the `context` config argument. This replaces patterns that used `config.configurable`.\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { createAgent, HumanMessage } from \"langchain\";\n  import * as z from \"zod\";\n\n  const agent = createAgent({\n    model: \"gpt-4o\",\n    tools,\n    contextSchema: z.object({ userId: z.string(), sessionId: z.string() }),\n  });\n\n  const result = await agent.invoke(\n    { messages: [new HumanMessage(\"Hello\")] },\n    { context: { userId: \"123\", sessionId: \"abc\" } }, // [!code highlight]\n  );\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { createReactAgent, HumanMessage } from \"@langchain/langgraph/prebuilts\";\n\n  const agent = createReactAgent({ model, tools });\n\n  // Pass context via config.configurable\n  const result = await agent.invoke(\n    { messages: [new HumanMessage(\"Hello\")] },\n    {\n      config: { // [!code highlight]\n        configurable: { userId: \"123\", sessionId: \"abc\" }, // [!code highlight]\n      }, // [!code highlight]\n    }\n  );\n  ```\n</CodeGroup>\n\n<Note>\n  The old `config.configurable` pattern still works for backward compatibility, but using the new `context` parameter is recommended for new applications or applications migrating to v1.\n</Note>\n\n***\n\n## Standard content\n\nIn v1, messages gain provider-agnostic standard content blocks. Access them via `message.contentBlocks` for a consistent, typed view across providers. The existing `message.content` field remains unchanged for strings or provider-native structures.\n\n### What changed\n\n* New `contentBlocks` property on messages for normalized content.\n* New TypeScript types under `ContentBlock` for strong typing.\n* Optional serialization of standard blocks into `content` via `LC_OUTPUT_VERSION=v1` or `outputVersion: \"v1\"`.\n\n### Read standardized content\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { initChatModel } from \"langchain\";\n\n  const model = await initChatModel(\"gpt-5-nano\");\n  const response = await model.invoke(\"Explain AI\");\n\n  for (const block of response.contentBlocks) {\n    if (block.type === \"reasoning\") {\n      console.log(block.reasoning);\n    } else if (block.type === \"text\") {\n      console.log(block.text);\n    }\n  }\n  ```\n\n  ```typescript v0 (old) theme={null}\n  // Provider-native formats vary; you needed per-provider handling.\n  const response = await model.invoke(\"Explain AI\");\n  for (const item of response.content as any[]) {\n    if (item.type === \"reasoning\") {\n      // OpenAI-style reasoning\n    } else if (item.type === \"thinking\") {\n      // Anthropic-style thinking\n    } else if (item.type === \"text\") {\n      // Text\n    }\n  }\n  ```\n</CodeGroup>\n\n### Create multimodal messages\n\n<CodeGroup>\n  ```typescript v1 (new) theme={null}\n  import { HumanMessage } from \"langchain\";\n\n  const message = new HumanMessage({\n    contentBlocks: [\n      { type: \"text\", text: \"Describe this image.\" },\n      { type: \"image\", url: \"https://example.com/image.jpg\" },\n    ],\n  });\n  const res = await model.invoke([message]);\n  ```\n\n  ```typescript v0 (old) theme={null}\n  import { HumanMessage } from \"langchain\";\n\n  const message = new HumanMessage({\n    // Provider-native structure\n    content: [\n      { type: \"text\", text: \"Describe this image.\" },\n      { type: \"image_url\", image_url: { url: \"https://example.com/image.jpg\" } },\n    ],\n  });\n  const res = await model.invoke([message]);\n  ```\n</CodeGroup>\n\n### Example block types\n\n```typescript  theme={null}\nimport { ContentBlock } from \"langchain\";\n\nconst textBlock: ContentBlock.Text = {\n  type: \"text\",\n  text: \"Hello world\",\n};\n\nconst imageBlock: ContentBlock.Multimodal.Image = {\n  type: \"image\",\n  url: \"https://example.com/image.png\",\n  mimeType: \"image/png\",\n};\n```\n\nSee the content blocks [reference](/oss/javascript/langchain/messages#content-block-reference) for more details.\n\n### Serialize standard content\n\nStandard content blocks are **not serialized** into the `content` attribute by default. If you need to access standard content blocks in the `content` attribute (e.g., when sending messages to a client), you can opt-in to serializing them into `content`.\n\n<CodeGroup>\n  ```bash  theme={null}\n  export LC_OUTPUT_VERSION=v1\n  ```\n\n  ```typescript  theme={null}\n  import { initChatModel } from \"langchain\";\n\n  const model = await initChatModel(\"gpt-5-nano\", {\n    outputVersion: \"v1\",\n  });\n  ```\n</CodeGroup>\n\n<Note>\n  Learn more: [Messages](/oss/javascript/langchain/messages#message-content) and [Standard content blocks](/oss/javascript/langchain/messages#standard-content-blocks). See [Multimodal](/oss/javascript/langchain/messages#multimodal) for input examples.\n</Note>\n\n***\n\n## Simplified package\n\nThe `langchain` package namespace is streamlined to focus on agent building blocks. Legacy functionality has moved to `@langchain/classic`. The new package exposes only the most useful and relevant functionality.\n\n### Exports\n\nThe v1 package includes:\n\n| Module      | What's available                              | Notes                              |\n| ----------- | --------------------------------------------- | ---------------------------------- |\n| Agents      | `createAgent`, `AgentState`                   | Core agent creation functionality  |\n| Messages    | Message types, content blocks, `trimMessages` | Re-exported from `@langchain/core` |\n| Tools       | `tool`, tool classes                          | Re-exported from `@langchain/core` |\n| Chat models | `initChatModel`, `BaseChatModel`              | Unified model initialization       |\n\n### `@langchain/classic`\n\nIf you use legacy chains, the indexing API, or functionality previously re-exported from `@langchain/community`, install `@langchain/classic` and update imports:\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm install @langchain/classic\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm install @langchain/classic\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/classic\n  ```\n\n  ```bash bun theme={null}\n  bun add @langchain/classic\n  ```\n</CodeGroup>\n\n```typescript  theme={null}\n// v1 (new)\nimport { ... } from \"@langchain/classic\";\nimport { ... } from \"@langchain/classic/chains\";\n\n// v0 (old)\nimport { ... } from \"langchain\";\nimport { ... } from \"langchain/chains\";\n```\n\n***\n\n## Breaking changes\n\n### Dropped Node 18 support\n\nAll LangChain packages now require **Node.js 20 or higher**. Node.js 18 reached [end of life](https://nodejs.org/en/about/releases/) in March 2025.\n\n### New build outputs\n\nBuilds for all langchain packages now use a bundler based approach instead of using raw typescript outputs. If you were importing files from the `dist/` directory (which is not recommended), you will need to update your imports to use the new module system.\n\n### Legacy code moved to `@langchain/classic`\n\nLegacy functionality outside the focus of standard interfaces and agents has been moved to the [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic) package. See the [Simplified package](#simplified-package) section for details on what's available in the core `langchain` package and what moved to `@langchain/classic`.\n\n### Removal of deprecated APIs\n\nMethods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted.\n\n<Accordion title=\"View removed deprecated APIs\">\n  The following deprecated APIs have been removed in v1:\n\n  #### Core functionality\n\n  * `TraceGroup` - Use LangSmith tracing instead\n  * `BaseDocumentLoader.loadAndSplit` - Use `.load()` followed by a text splitter\n  * `RemoteRunnable` - No longer supported\n\n  #### Prompts\n\n  * `BasePromptTemplate.serialize` and `.deserialize` - Use JSON serialization directly\n  * `ChatPromptTemplate.fromPromptMessages` - Use `ChatPromptTemplate.fromMessages`\n\n  #### Retrievers\n\n  * `BaseRetrieverInterface.getRelevantDocuments` - Use `.invoke()` instead\n\n  #### Runnables\n\n  * `Runnable.bind` - Use `.bindTools()` or other specific binding methods\n  * `Runnable.map` - Use `.batch()` instead\n  * `RunnableBatchOptions.maxConcurrency` - Use `maxConcurrency` in the config object\n\n  #### Chat models\n\n  * `BaseChatModel.predictMessages` - Use `.invoke()` instead\n  * `BaseChatModel.predict` - Use `.invoke()` instead\n  * `BaseChatModel.serialize` - Use JSON serialization directly\n  * `BaseChatModel.callPrompt` - Use `.invoke()` instead\n  * `BaseChatModel.call` - Use `.invoke()` instead\n\n  #### LLMs\n\n  * `BaseLLMParams.concurrency` - Use `maxConcurrency` in the config object\n  * `BaseLLM.call` - Use `.invoke()` instead\n  * `BaseLLM.predict` - Use `.invoke()` instead\n  * `BaseLLM.predictMessages` - Use `.invoke()` instead\n  * `BaseLLM.serialize` - Use JSON serialization directly\n\n  #### Streaming\n\n  * `createChatMessageChunkEncoderStream` - Use `.stream()` method directly\n\n  #### Tracing\n\n  * `BaseTracer.runMap` - Use LangSmith tracing APIs\n  * `getTracingCallbackHandler` - Use LangSmith tracing\n  * `getTracingV2CallbackHandler` - Use LangSmith tracing\n  * `LangChainTracerV1` - Use LangSmith tracing\n\n  #### Memory and storage\n\n  * `BaseListChatMessageHistory.addAIChatMessage` - Use `.addMessage()` with `AIMessage`\n  * `BaseStoreInterface` - Use specific store implementations\n\n  #### Utilities\n\n  * `getRuntimeEnvironmentSync` - Use async `getRuntimeEnvironment()`\n</Accordion>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/migrate/langchain-v1.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 24560
}