{
  "title": "A2A endpoint in Agent Server",
  "source_url": "https://docs.langchain.com/langsmith/server-a2a",
  "content": "[Agent2Agent (A2A)](https://a2a-protocol.org/latest/) is Google's protocol for enabling communication between conversational AI agents. [LangSmith implements A2A support](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/a2a/post/a2a/\\{assistant_id}), allowing your agents to communicate with other A2A-compatible agents through a standardized protocol.\n\nThe A2A endpoint is available in [Agent Server](/langsmith/agent-server) at `/a2a/{assistant_id}`.\n\n## Supported methods\n\nAgent Server supports the following A2A RPC methods:\n\n* **message/send**: Send a message to an assistant and receive a complete response\n* **message/stream**: Send a message and stream responses in real-time using Server-Sent Events (SSE)\n* **tasks/get**: Retrieve the status and results of a previously created task\n\n## Agent Card Discovery\n\nEach assistant automatically exposes an A2A Agent Card that describes its capabilities and provides the information needed for other agents to connect. You can retrieve the agent card for any assistant using:\n\n```\nGET /.well-known/agent-card.json?assistant_id={assistant_id}\n```\n\nThe agent card includes the assistant's name, description, available skills, supported input/output modes, and the A2A endpoint URL for communication.\n\n## Requirements\n\nTo use A2A, ensure you have the following dependencies installed:\n\n* `langgraph-api >= 0.4.21`\n\nInstall with:\n\n```bash  theme={null}\npip install \"langgraph-api>=0.4.21\"\n```\n\n## Usage overview\n\nTo enable A2A:\n\n* Upgrade to use langgraph-api>=0.4.21.\n* Deploy your agent with message-based state structure.\n* Connect with other A2A-compatible agents using the endpoint.\n\n## Creating an A2A-compatible agent\n\nThis example creates an A2A-compatible agent that processes incoming messages using OpenAI's API and maintains conversational state. The agent defines a message-based state structure and handles the A2A protocol's message format.\n\nTo be compatible with the [A2A \"text\" parts](https://a2a-protocol.org/dev/specification/#651-textpart-object), the agent must have a `messages` key in state. Here's an example:\n\n```python  theme={null}\n\"\"\"LangGraph A2A conversational agent.\n\nSupports the A2A protocol with messages input for conversational interactions.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, TypedDict\n\nfrom langgraph.graph import StateGraph\nfrom langgraph.runtime import Runtime\nfrom openai import AsyncOpenAI\n\n\nclass Context(TypedDict):\n    \"\"\"Context parameters for the agent.\"\"\"\n    my_configurable_param: str\n\n\n@dataclass\nclass State:\n    \"\"\"Input state for the agent.\n\n    Defines the initial structure for A2A conversational messages.\n    \"\"\"\n    messages: List[Dict[str, Any]]\n\n\nasync def call_model(state: State, runtime: Runtime[Context]) -> Dict[str, Any]:\n    \"\"\"Process conversational messages and returns output using OpenAI.\"\"\"\n    # Initialize OpenAI client\n    client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n    # Process the incoming messages\n    latest_message = state.messages[-1] if state.messages else {}\n    user_content = latest_message.get(\"content\", \"No message content\")\n\n    # Create messages for OpenAI API\n    openai_messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful conversational agent. Keep responses brief and engaging.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": user_content\n        }\n    ]\n\n    try:\n        # Make OpenAI API call\n        response = await client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=openai_messages,\n            max_tokens=100,\n            temperature=0.7\n        )\n\n        ai_response = response.choices[0].message.content\n\n    except Exception as e:\n        ai_response = f\"I received your message but had trouble processing it. Error: {str(e)[:50]}...\"\n\n    # Create a response message\n    response_message = {\n        \"role\": \"assistant\",\n        \"content\": ai_response\n    }\n\n    return {\n        \"messages\": state.messages + [response_message]\n    }\n\n\n# Define the graph\ngraph = (\n    StateGraph(State, context_schema=Context)\n    .add_node(call_model)\n    .add_edge(\"__start__\", \"call_model\")\n    .compile()\n)\n```\n\n## Agent-to-agent communication\n\nOnce your agents are running locally via `langgraph dev` or [deployed to production](/langsmith/deployments), you can facilitate communication between them using the A2A protocol.\n\nThis example demonstrates how two agents can communicate by sending JSON-RPC messages to each other's A2A endpoints. The script simulates a multi-turn conversation where each agent processes the other's response and continues the dialogue.\n\n```python  theme={null}\n#!/usr/bin/env python3\n\"\"\"Agent-to-Agent conversation simulation using LangGraph A2A protocol.\"\"\"\n\nimport asyncio\nimport aiohttp\nimport os\n\nasync def send_message(session, port, assistant_id, text):\n    \"\"\"Send a message to an agent and return the response text.\"\"\"\n    url = f\"http://127.0.0.1:{port}/a2a/{assistant_id}\"\n    payload = {\n        \"jsonrpc\": \"2.0\",\n        \"id\": \"\",\n        \"method\": \"message/send\",\n        \"params\": {\n            \"message\": {\n                \"role\": \"user\",\n                \"parts\": [{\"kind\": \"text\", \"text\": text}]\n            },\n            \"messageId\": \"\",\n            \"thread\": {\"threadId\": \"\"}\n        }\n    }\n\n    headers = {\"Accept\": \"application/json\"}\n    async with session.post(url, json=payload, headers=headers) as response:\n        try:\n            result = await response.json()\n            return result[\"result\"][\"artifacts\"][0][\"parts\"][0][\"text\"]\n        except Exception as e:\n            text = await response.text()\n            print(f\"Response error from port {port}: {response.status} - {text}\")\n            return f\"Error from port {port}: {response.status}\"\n\nasync def simulate_conversation():\n    \"\"\"Simulate a conversation between two agents.\"\"\"\n    agent_a_id = os.getenv(\"AGENT_A_ID\")\n    agent_b_id = os.getenv(\"AGENT_B_ID\")\n\n    if not agent_a_id or not agent_b_id:\n        print(\"Set AGENT_A_ID and AGENT_B_ID environment variables\")\n        return\n\n    message = \"Hello! Let's have a conversation.\"\n\n    async with aiohttp.ClientSession() as session:\n        for i in range(3):\n            print(f\"--- Round {i + 1} ---\")\n\n            # Agent A responds\n            message = await send_message(session, 2024, agent_a_id, message)\n            print(f\"ðŸ”µ Agent A: {message}\")\n\n            # Agent B responds\n            message = await send_message(session, 2025, agent_b_id, message)\n            print(f\"ðŸ”´ Agent B: {message}\")\n            print()\n\nif __name__ == \"__main__\":\n    asyncio.run(simulate_conversation())\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/server-a2a.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 7110
}