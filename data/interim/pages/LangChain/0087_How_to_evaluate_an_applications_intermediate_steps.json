{
  "title": "How to evaluate an application's intermediate steps",
  "source_url": "https://docs.langchain.com/langsmith/evaluate-on-intermediate-steps",
  "content": "While, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline.\n\nFor example, for retrieval-augmented generation (RAG), you might want to\n\n1. Evaluate the retrieval step to ensure that the correct documents are retrieved w\\.r.t the input query.\n2. Evaluate the generation step to ensure that the correct answer is generated w\\.r.t the retrieved documents.\n\nIn this guide, we will use a simple, fully-custom evaluator for evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both scenarios.\n\nIn order to evaluate the intermediate steps of your pipeline, your evaluator function should traverse and process the `run`/`rootRun` argument, which is a `Run` object that contains the intermediate steps of your pipeline.\n\n## 1. Define your LLM pipeline\n\nThe below RAG pipeline consists of 1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from Wikipedia, and 3) generating an answer given the retrieved documents.\n\n<CodeGroup>\n  ```bash Python theme={null}\n  pip install -U langsmith langchain[openai] wikipedia\n  ```\n\n  ```bash TypeScript theme={null}\n  yarn add langsmith langchain @langchain/openai wikipedia\n  ```\n</CodeGroup>\n\nRequires `langsmith>=0.3.13`\n\n<CodeGroup>\n  ```python Python theme={null}\n  import wikipedia as wp\n  from openai import OpenAI\n  from langsmith import traceable, wrappers\n\n  oai_client = wrappers.wrap_openai(OpenAI())\n\n  @traceable\n  def generate_wiki_search(question: str) -> str:\n      \"\"\"Generate the query to search in wikipedia.\"\"\"\n      instructions = (\n          \"Generate a search query to pass into wikipedia to answer the user's question. \"\n          \"Return only the search query and nothing more. \"\n          \"This will passed in directly to the wikipedia search engine.\"\n      )\n      messages = [\n          {\"role\": \"system\", \"content\": instructions},\n          {\"role\": \"user\", \"content\": question}\n      ]\n      result = oai_client.chat.completions.create(\n          messages=messages,\n          model=\"gpt-4o-mini\",\n          temperature=0,\n      )\n      return result.choices[0].message.content\n\n  @traceable(run_type=\"retriever\")\n  def retrieve(query: str) -> list:\n      \"\"\"Get up to two search wikipedia results.\"\"\"\n      results = []\n      for term in wp.search(query, results = 10):\n          try:\n              page = wp.page(term, auto_suggest=False)\n              results.append({\n                  \"page_content\": page.summary,\n                  \"type\": \"Document\",\n                  \"metadata\": {\"url\": page.url}\n              })\n          except wp.DisambiguationError:\n              pass\n          if len(results) >= 2:\n              return results\n\n  @traceable\n  def generate_answer(question: str, context: str) -> str:\n      \"\"\"Answer the question based on the retrieved information.\"\"\"\n      instructions = f\"Answer the user's question based ONLY on the content below:\\n\\n{context}\"\n      messages = [\n          {\"role\": \"system\", \"content\": instructions},\n          {\"role\": \"user\", \"content\": question}\n      ]\n      result = oai_client.chat.completions.create(\n          messages=messages,\n          model=\"gpt-4o-mini\",\n          temperature=0\n      )\n      return result.choices[0].message.content\n\n  @traceable\n  def qa_pipeline(question: str) -> str:\n      \"\"\"The full pipeline.\"\"\"\n      query = generate_wiki_search(question)\n      context = \"\\n\\n\".join([doc[\"page_content\"] for doc in retrieve(query)])\n      return generate_answer(question, context)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import OpenAI from \"openai\";\n  import wiki from \"wikipedia\";\n  import { Client } from \"langsmith\";\n  import { traceable } from \"langsmith/traceable\";\n  import { wrapOpenAI } from \"langsmith/wrappers\";\n\n  const openai = wrapOpenAI(new OpenAI());\n\n  const generateWikiSearch = traceable(\n    async (input: { question: string }) => {\n      const messages = [\n        {\n          role: \"system\" as const,\n          content:\n            \"Generate a search query to pass into Wikipedia to answer the user's question. Return only the search query and nothing more. This will be passed in directly to the Wikipedia search engine.\",\n        },\n        { role: \"user\" as const, content: input.question },\n      ];\n      const chatCompletion = await openai.chat.completions.create({\n        model: \"gpt-4o-mini\",\n        messages: messages,\n        temperature: 0,\n      });\n      return chatCompletion.choices[0].message.content ?? \"\";\n    },\n    { name: \"generateWikiSearch\" }\n  );\n\n  const retrieve = traceable(\n    async (input: { query: string; numDocuments: number }) => {\n      const { results } = await wiki.search(input.query, { limit: 10 });\n      const finalResults: Array<{\n        page_content: string;\n        type: \"Document\";\n        metadata: { url: string };\n      }> = [];\n      for (const result of results) {\n        if (finalResults.length >= input.numDocuments) {\n          // Just return the top 2 pages for now\n          break;\n        }\n        const page = await wiki.page(result.title, { autoSuggest: false });\n        const summary = await page.summary();\n        finalResults.push({\n          page_content: summary.extract,\n          type: \"Document\",\n          metadata: { url: page.fullurl },\n        });\n      }\n      return finalResults;\n    },\n    { name: \"retrieve\", run_type: \"retriever\" }\n  );\n\n  const generateAnswer = traceable(\n    async (input: { question: string; context: string }) => {\n      const messages = [\n        {\n          role: \"system\" as const,\n          content: `Answer the user's question based only on the content below:\\n\\n${input.context}`,\n        },\n        { role: \"user\" as const, content: input.question },\n      ];\n      const chatCompletion = await openai.chat.completions.create({\n        model: \"gpt-4o-mini\",\n        messages: messages,\n        temperature: 0,\n      });\n      return chatCompletion.choices[0].message.content ?? \"\";\n    },\n    { name: \"generateAnswer\" }\n  );\n\n  const ragPipeline = traceable(\n    async ({ question }: { question: string }, numDocuments: number = 2) => {\n      const query = await generateWikiSearch({ question });\n      const retrieverResults = await retrieve({ query, numDocuments });\n      const context = retrieverResults\n        .map((result) => result.page_content)\n        .join(\"\\n\\n\");\n      const answer = await generateAnswer({ question, context });\n      return answer;\n    },\n    { name: \"ragPipeline\" }\n  );\n  ```\n</CodeGroup>\n\nThis pipeline will produce a trace that looks something like: <img src=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3b691ca56f9d60035dcba2c248692fa1\" alt=\"evaluation_intermediate_trace.png\" data-og-width=\"2586\" width=\"2586\" data-og-height=\"1676\" height=\"1676\" data-path=\"langsmith/images/evaluation-intermediate-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=23a9b9abdb3e43e0f6326f0d4293ab7d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4b771691bd1afffe9f371a105f7eaebe 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=beb01776de9a5fa663c82d4380bc78cd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec84bd3345df3d2cef38878b902c355b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=043a7f22da6b158e070c853d67bacd69 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=42a9ea799157fee30a6b243c02615a02 2500w\" />\n\n## 2. Create a dataset and examples to evaluate the pipeline\n\nWe are building a very simple dataset with a couple of examples to evaluate the pipeline.\n\nRequires `langsmith>=0.3.13`\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n\n  ls_client = Client()\n  dataset_name = \"Wikipedia RAG\"\n\n  if not ls_client.has_dataset(dataset_name=dataset_name):\n      dataset = ls_client.create_dataset(dataset_name=dataset_name)\n      examples = [\n        {\"inputs\": {\"question\": \"What is LangChain?\"}},\n        {\"inputs\": {\"question\": \"What is LangSmith?\"}},\n      ]\n      ls_client.create_examples(\n        dataset_id=dataset.id,\n        examples=examples,\n      )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n\n  const client = new Client();\n  const examples = [\n    [\n      \"What is LangChain?\",\n      \"LangChain is an open-source framework for building applications using large language models.\",\n    ],\n    [\n      \"What is LangSmith?\",\n      \"LangSmith is an observability and evaluation tool for LLM products, built by LangChain Inc.\",\n    ],\n  ];\n  const datasetName = \"Wikipedia RAG\";\n  const inputs = examples.map(([input, _]) => ({ input }));\n  const outputs = examples.map(([_, expected]) => ({ expected }));\n  const dataset = await client.createDataset(datasetName);\n  await client.createExamples({ datasetId: dataset.id, inputs, outputs });\n  ```\n</CodeGroup>\n\n## 3. Define your custom evaluators\n\nAs mentioned above, we will define two evaluators: one that evaluates the relevance of the retrieved documents w\\.r.t the input query and another that evaluates the hallucination of the generated answer w\\.r.t the retrieved documents. We will be using LangChain LLM wrappers, along with [`with_structured_output`](https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/) to define the evaluator for hallucination.\n\nThe key here is that the evaluator function should traverse the `run` / `rootRun` argument to access the intermediate steps of the pipeline. The evaluator can then process the inputs and outputs of the intermediate steps to evaluate according to the desired criteria.\n\nExample uses `langchain` for convenience, this is not required.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain.chat_models import init_chat_model\n  from langsmith.schemas import Run\n  from pydantic import BaseModel, Field\n\n  def document_relevance(run: Run) -> bool:\n      \"\"\"Checks if retriever input exists in the retrieved docs.\"\"\"\n      qa_pipeline_run = next(\n          r for run in run.child_runs if r.name == \"qa_pipeline\"\n      )\n      retrieve_run = next(\n          r for run in qa_pipeline_run.child_runs if r.name == \"retrieve\"\n      )\n      page_contents = \"\\n\\n\".join(\n          doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"]\n      )\n      return retrieve_run.inputs[\"query\"] in page_contents\n\n  # Data model\n  class GradeHallucinations(BaseModel):\n      \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n      is_grounded: bool = Field(..., description=\"True if the answer is grounded in the facts, False otherwise.\")\n\n  # LLM with structured output for grading hallucinations\n  # For more see: https://python.langchain.com/docs/how_to/structured_output/\n  grader_llm= init_chat_model(\"gpt-4o-mini\", temperature=0).with_structured_output(\n      GradeHallucinations,\n      method=\"json_schema\",\n      strict=True,\n  )\n\n  def no_hallucination(run: Run) -> bool:\n      \"\"\"Check if the answer is grounded in the documents.\n      Return True if there is no hallucination, False otherwise.\n      \"\"\"\n      # Get documents and answer\n      qa_pipeline_run = next(\n          r for r in run.child_runs if r.name == \"qa_pipeline\"\n      )\n      retrieve_run = next(\n          r for r in qa_pipeline_run.child_runs if r.name == \"retrieve\"\n      )\n      retrieved_content = \"\\n\\n\".join(\n          doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"]\n      )\n\n      # Construct prompt\n      instructions = (\n          \"You are a grader assessing whether an LLM generation is grounded in / \"\n          \"supported by a set of retrieved facts. Give a binary score 1 or 0, \"\n          \"where 1 means that the answer is grounded in / supported by the set of facts.\"\n      )\n      messages = [\n          {\"role\": \"system\", \"content\": instructions},\n          {\"role\": \"user\", \"content\": f\"Set of facts:\\n{retrieved_content}\\n\\nLLM generation: {run.outputs['answer']}\"},\n      ]\n      grade = grader_llm.invoke(messages)\n      return grade.is_grounded\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { EvaluationResult } from \"langsmith/evaluation\";\n  import { Run, Example } from \"langsmith/schemas\";\n  import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { z } from \"zod\";\n\n  function findNestedRun(run: Run, search: (run: Run) => boolean): Run | null {\n    const queue: Run[] = [run];\n    while (queue.length > 0) {\n      const currentRun = queue.shift()!;\n      if (search(currentRun)) return currentRun;\n      queue.push(...currentRun.child_runs);\n    }\n    return null;\n  }\n\n  // A very simple evaluator that checks to see if the input of the retrieval step exists\n  // in the retrieved docs.\n  function documentRelevance(rootRun: Run, example: Example): EvaluationResult {\n    const retrieveRun = findNestedRun(rootRun, (run) => run.name === \"retrieve\");\n    const docs: Array<{ page_content: string }> | undefined =\n      retrieveRun.outputs?.outputs;\n    const pageContents = docs?.map((doc) => doc.page_content).join(\"\\n\\n\");\n    const score = pageContents.includes(retrieveRun.inputs?.query);\n    return { key: \"simple_document_relevance\", score };\n  }\n\n  async function hallucination(\n    rootRun: Run,\n    example: Example\n  ): Promise<EvaluationResult> {\n    const rag = findNestedRun(rootRun, (run) => run.name === \"ragPipeline\");\n    const retrieve = findNestedRun(rootRun, (run) => run.name === \"retrieve\");\n    const docs: Array<{ page_content: string }> | undefined =\n      retrieve.outputs?.outputs;\n    const documents = docs?.map((doc) => doc.page_content).join(\"\\n\\n\");\n\n    const prompt = ChatPromptTemplate.fromMessages<{\n      documents: string;\n      generation: string;\n    }>([\n      [\n        \"system\",\n        [\n          `You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n`,\n          `Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.`,\n        ].join(\"\\n\"),\n      ],\n      [\n        \"human\",\n        \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\",\n      ],\n    ]);\n\n    const llm = new ChatOpenAI({\n      model: \"gpt-4o-mini\",\n      temperature: 0,\n    }).withStructuredOutput(\n      z\n        .object({\n          binary_score: z\n            .number()\n            .describe(\"Answer is grounded in the facts, 1 or 0\"),\n        })\n        .describe(\"Binary score for hallucination present in generation answer.\")\n    );\n\n    const grader = prompt.pipe(llm);\n    const score = await grader.invoke({\n      documents,\n      generation: rag.outputs?.outputs,\n    });\n    return { key: \"answer_hallucination\", score: score.binary_score };\n  }\n  ```\n</CodeGroup>\n\n## 4. Evaluate the pipeline\n\nFinally, we'll run `evaluate` with the custom evaluators defined above.\n\n<CodeGroup>\n  ```python Python theme={null}\n  def qa_wrapper(inputs: dict) -> dict:\n    \"\"\"Wrap the qa_pipeline so it can accept the Example.inputs dict as input.\"\"\"\n    return {\"answer\": qa_pipeline(inputs[\"question\"])}\n\n  experiment_results = ls_client.evaluate(\n      qa_wrapper,\n      data=dataset_name,\n      evaluators=[document_relevance, no_hallucination],\n      experiment_prefix=\"rag-wiki-oai\"\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { evaluate } from \"langsmith/evaluation\";\n\n  await evaluate((inputs) => ragPipeline({ question: inputs.input }), {\n    data: datasetName,\n    evaluators: [hallucination, documentRelevance],\n    experimentPrefix: \"rag-wiki-oai\",\n  });\n  ```\n</CodeGroup>\n\nThe experiment will contain the results of the evaluation, including the scores and comments from the evaluators: <img src=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e926744573c6b9757ba22ff245a3da2c\" alt=\"evaluation_intermediate_experiment.png\" data-og-width=\"2446\" width=\"2446\" data-og-height=\"1244\" height=\"1244\" data-path=\"langsmith/images/evaluation-intermediate-experiment.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7b6e321b15a06b2adc7f1cacb8e07a35 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c677007bcc1e2af4b3767d6b44fcb327 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a39153399b6721b7c51693f5a59cf2b0 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1132228eba6761a724ae98d85fcf536c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5d74785384737df0cf67145b397b1934 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bef00e4bdc12289d9f1e4b77ed8489cf 2500w\" />\n\n## Related\n\n* [Evaluate a `langgraph` graph](/langsmith/evaluate-on-intermediate-steps)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-on-intermediate-steps.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 18631
}