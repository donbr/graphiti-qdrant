{
  "title": "Agents",
  "source_url": "https://docs.langchain.com/oss/python/langchain/agents",
  "content": "Agents combine language models with [tools](/oss/python/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid  theme={null}\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n  [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) builds a **graph**-based agent runtime using [LangGraph](/oss/python/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n\n  Learn more about the [Graph API](/oss/python/langgraph/graph-api).\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/python/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n```python wrap theme={null}\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=tools\n)\n```\n\n<Tip>\n  Model identifier strings support automatic inference (e.g., `\"gpt-5\"` will be inferred as `\"openai:gpt-5\"`). Refer to the [reference](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\\(model\\)) to see a full list of model identifier string mappings.\n</Tip>\n\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI). See [Chat models](/oss/python/integrations/chat) for other available chat model classes.\n\n```python wrap theme={null}\nfrom langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-5\",\n    temperature=0.1,\n    max_tokens=1000,\n    timeout=30\n    # ... (other params)\n)\nagent = create_agent(model, tools=tools)\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific [parameters](/oss/python/langchain/models#parameters) like `temperature`, `max_tokens`, `timeouts`, `base_url`, and other provider-specific settings. Refer to the [reference](/oss/python/integrations/providers/all_providers) to see available params and methods on your model.\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\nTo use a dynamic model, create middleware using the [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) decorator that modifies the model in the request:\n\n```python  theme={null}\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n\n\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\n\n@wrap_model_call\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n    \"\"\"Choose model based on conversation complexity.\"\"\"\n    message_count = len(request.state[\"messages\"])\n\n    if message_count > 10:\n        # Use an advanced model for longer conversations\n        model = advanced_model\n    else:\n        model = basic_model\n\n    return handler(request.override(model=model))\n\nagent = create_agent(\n    model=basic_model,  # Default model\n    tools=tools,\n    middleware=[dynamic_model_selection]\n)\n```\n\n<Warning>\n  Pre-bound models (models with [`bind_tools`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.bind_tools) already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\n</Warning>\n\n<Tip>\n  For model configuration details, see [Models](/oss/python/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/python/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/python/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n<Tip>\n  Tools can be specified as plain Python functions or <Tooltip tip=\"A method that can suspend execution and resume at a later time\">coroutines</Tooltip>.\n\n  The [tool decorator](/oss/python/langchain/tools#create-tools) can be used to customize tool names, descriptions, argument schemas, and other properties.\n</Tip>\n\n```python wrap theme={null}\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather information for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72Â°F\"\n\nagent = create_agent(model, tools=[search, get_weather])\n```\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\nTo customize how tool errors are handled, use the [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call) decorator to create middleware:\n\n```python wrap theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain.messages import ToolMessage\n\n\n@wrap_tool_call\ndef handle_tool_errors(request, handler):\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n    try:\n        return handler(request)\n    except Exception as e:\n        # Return a custom error message to the model\n        return ToolMessage(\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n            tool_call_id=request.tool_call[\"id\"]\n        )\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search, get_weather],\n    middleware=[handle_tool_errors]\n)\n```\n\nThe agent will return a [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) with the custom error message when a tool fails:\n\n```python  theme={null}\n[\n    ...\n    ToolMessage(\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\n        tool_call_id=\"...\"\n    ),\n    ...\n]\n```\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n  **Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n  ```\n  ================================ Human Message =================================\n\n  Find the most popular wireless headphones right now and check if they're in stock\n  ```\n\n  * **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n  * **Acting**: Call `search_products(\"wireless headphones\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    search_products (call_abc123)\n   Call ID: call_abc123\n    Args:\n      query: wireless headphones\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n  ```\n\n  * **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n  * **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n  ```\n  ================================== Ai Message ==================================\n  Tool Calls:\n    check_inventory (call_def456)\n   Call ID: call_def456\n    Args:\n      product_id: WH-1000XM5\n  ```\n\n  ```\n  ================================= Tool Message =================================\n\n  Product WH-1000XM5: 10 units in stock\n  ```\n\n  * **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n  * **Acting**: Produce final answer\n\n  ```\n  ================================== Ai Message ==================================\n\n  I found wireless headphones (model WH-1000XM5) with 10 units in stock...\n  ```\n</Accordion>\n\n<Tip>\n  To learn more about tools, see [Tools](/oss/python/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)) parameter can be provided as a string:\n\n```python wrap theme={null}\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\n)\n```\n\nWhen no [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)) is provided, the agent will infer its task from the messages directly.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/python/langchain/middleware).\n\nThe [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) decorator creates middleware that generates system prompts based on the model request:\n\n```python wrap theme={null}\nfrom typing import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass Context(TypedDict):\n    user_role: str\n\n@dynamic_prompt\ndef user_role_prompt(request: ModelRequest) -> str:\n    \"\"\"Generate system prompt based on user role.\"\"\"\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\n    base_prompt = \"You are a helpful assistant.\"\n\n    if user_role == \"expert\":\n        return f\"{base_prompt} Provide detailed technical responses.\"\n    elif user_role == \"beginner\":\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n\n    return base_prompt\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[web_search],\n    middleware=[user_role_prompt],\n    context_schema=Context\n)\n\n# The system prompt will be set dynamically based on context\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n    context={\"user_role\": \"expert\"}\n)\n```\n\n<Tip>\n  For more details on message types and formatting, see [Messages](/oss/python/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/python/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/python/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/python/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n```python  theme={null}\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]}\n)\n```\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/python/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/python/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the `response_format` parameter.\n\n#### ToolStrategy\n\n`ToolStrategy` uses artificial tool calling to generate structured output. This works with any model that supports tool calling:\n\n```python wrap theme={null}\nfrom pydantic import BaseModel\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str\n\nagent = create_agent(\n    model=\"gpt-4o-mini\",\n    tools=[search_tool],\n    response_format=ToolStrategy(ContactInfo)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n```\n\n#### ProviderStrategy\n\n`ProviderStrategy` uses the model provider's native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):\n\n```python wrap theme={null}\nfrom langchain.agents.structured_output import ProviderStrategy\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    response_format=ProviderStrategy(ContactInfo)\n)\n```\n\n<Note>\n  As of `langchain 1.0`, simply passing a schema (e.g., `response_format=ContactInfo`) is no longer supported. You must explicitly use `ToolStrategy` or `ProviderStrategy`.\n</Note>\n\n<Tip>\n  To learn about structured output, see [Structured output](/oss/python/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/python/langchain/short-term-memory) of the agent:\n\nCustom state schemas must extend [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) as a `TypedDict`.\n\nThere are two ways to define custom state:\n\n1. Via [middleware](/oss/python/langchain/middleware) (preferred)\n2. Via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)\n\n#### Defining state via middleware\n\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\n\n```python  theme={null}\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\nfrom typing import Any\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nclass CustomMiddleware(AgentMiddleware):\n    state_schema = CustomState\n    tools = [tool1, tool2]\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        ...\n\nagent = create_agent(\n    model,\n    tools=tools,\n    middleware=[CustomMiddleware()]\n)\n\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n```\n\n#### Defining state via `state_schema`\n\nUse the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) parameter as a shortcut to define custom state that is only used in tools.\n\n```python  theme={null}\nfrom langchain.agents import AgentState\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nagent = create_agent(\n    model,\n    tools=[tool1, tool2],\n    state_schema=CustomState\n)\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n```\n\n<Note>\n  As of `langchain 1.0`, custom state schemas **must** be `TypedDict` types. Pydantic models and dataclasses are no longer supported. See the [v1 migration guide](/oss/python/migrate/langchain-v1#state-type-restrictions) for more details.\n</Note>\n\n<Note>\n  Defining custom state via middleware is preferred over defining it via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\n\n  [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) is still supported for backwards compatibility on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent).\n</Note>\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/python/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/python/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python  theme={null}\nfor chunk in agent.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n}, stream_mode=\"values\"):\n    # Each chunk contains the full state at that point\n    latest_message = chunk[\"messages\"][-1]\n    if latest_message.content:\n        print(f\"Agent: {latest_message.content}\")\n    elif latest_message.tool_calls:\n        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/python/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/python/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n<Tip>\n  For comprehensive middleware documentation including decorators like [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model), [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model), and [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call), see [Middleware](/oss/python/langchain/middleware).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 21513
}