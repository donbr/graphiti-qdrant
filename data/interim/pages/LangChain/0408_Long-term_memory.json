{
  "title": "Long-term memory",
  "source_url": "https://docs.langchain.com/oss/python/deepagents/long-term-memory",
  "content": "Learn how to extend deep agents with persistent memory across threads\n\nDeep agents come with a local filesystem to offload memory. By default, this filesystem is stored in agent state and is **transient to a single thread**â€”files are lost when the conversation ends.\n\nYou can extend deep agents with **long-term memory** by using a **CompositeBackend** that routes specific paths to persistent storage. This enables hybrid storage where some files persist across threads while others remain ephemeral.\n\n```mermaid  theme={null}\ngraph LR\n    Agent[Deep Agent] --> Router{Path Router}\n\n    Router --> |/memories/*| Store[Store Backend]\n    Router --> |other| State[State Backend]\n\n    Store --> Persist[(Persistent<br/>across threads)]\n    State --> Ephemeral[(Ephemeral<br/>single thread)]\n```\n\n## Setup\n\nConfigure long-term memory by using a `CompositeBackend` that routes the `/memories/` path to a `StoreBackend`:\n\n```python  theme={null}\nfrom deepagents import create_deep_agent\nfrom deepagents.backends import CompositeBackend, StateBackend, StoreBackend\nfrom langgraph.store.memory import InMemoryStore\n\ndef make_backend(runtime):\n    return CompositeBackend(\n        default=StateBackend(runtime),  # Ephemeral storage\n        routes={\n            \"/memories/\": StoreBackend(runtime)  # Persistent storage\n        }\n    )\n\nagent = create_deep_agent(\n    store=InMemoryStore(),  # Required for StoreBackend\n    backend=make_backend\n)\n```\n\n## How it works\n\nWhen using `CompositeBackend`, deep agents maintain **two separate filesystems**:\n\n### 1. Short-term (transient) filesystem\n\n* Stored in the agent's state (via `StateBackend`)\n* Persists only within a single thread\n* Files are lost when the thread ends\n* Accessed through standard paths: `/notes.txt`, `/workspace/draft.md`\n\n### 2. Long-term (persistent) filesystem\n\n* Stored in a LangGraph Store (via `StoreBackend`)\n* Persists across all threads and conversations\n* Survives agent restarts\n* Accessed through paths prefixed with `/memories/`: `/memories/preferences.txt`\n\n### Path routing\n\nThe `CompositeBackend` routes file operations based on path prefixes:\n\n* Files with paths starting with `/memories/` are stored in the Store (persistent)\n* Files without this prefix remain in transient state\n* All filesystem tools (`ls`, `read_file`, `write_file`, `edit_file`) work with both\n\n```python  theme={null}\n# Transient file (lost after thread ends)\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write draft to /draft.txt\"}]\n})\n\n# Persistent file (survives across threads)\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save final report to /memories/report.txt\"}]\n})\n```\n\n## Cross-thread persistence\n\nFiles in `/memories/` can be accessed from any thread:\n\n```python  theme={null}\nimport uuid\n\n# Thread 1: Write to long-term memory\nconfig1 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save my preferences to /memories/preferences.txt\"}]\n}, config=config1)\n\n# Thread 2: Read from long-term memory (different conversation!)\nconfig2 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"What are my preferences?\"}]\n}, config=config2)\n# Agent can read /memories/preferences.txt from the first thread\n```\n\n## Use cases\n\n### User preferences\n\nStore user preferences that persist across sessions:\n\n```python  theme={null}\nagent = create_deep_agent(\n    store=InMemoryStore(),\n    backend=lambda rt: CompositeBackend(\n        default=StateBackend(rt),\n        routes={\"/memories/\": StoreBackend(rt)}\n    ),\n    system_prompt=\"\"\"When users tell you their preferences, save them to\n    /memories/user_preferences.txt so you remember them in future conversations.\"\"\"\n)\n```\n\n### Self-improving instructions\n\nAn agent can update its own instructions based on feedback:\n\n```python  theme={null}\nagent = create_deep_agent(\n    store=InMemoryStore(),\n    backend=lambda rt: CompositeBackend(\n        default=StateBackend(rt),\n        routes={\"/memories/\": StoreBackend(rt)}\n    ),\n    system_prompt=\"\"\"You have a file at /memories/instructions.txt with additional\n    instructions and preferences.\n\n    Read this file at the start of conversations to understand user preferences.\n\n    When users provide feedback like \"please always do X\" or \"I prefer Y\",\n    update /memories/instructions.txt using the edit_file tool.\"\"\"\n)\n```\n\nOver time, the instructions file accumulates user preferences, helping the agent improve.\n\n### Knowledge base\n\nBuild up knowledge over multiple conversations:\n\n```python  theme={null}\n# Conversation 1: Learn about a project\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"We're building a web app with React. Save project notes.\"}]\n})\n\n# Conversation 2: Use that knowledge\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"What framework are we using?\"}]\n})\n# Agent reads /memories/project_notes.txt from previous conversation\n```\n\n### Research projects\n\nMaintain research state across sessions:\n\n```python  theme={null}\nresearch_agent = create_deep_agent(\n    store=InMemoryStore(),\n    backend=lambda rt: CompositeBackend(\n        default=StateBackend(rt),\n        routes={\"/memories/\": StoreBackend(rt)}\n    ),\n    system_prompt=\"\"\"You are a research assistant.\n\n    Save your research progress to /memories/research/:\n    - /memories/research/sources.txt - List of sources found\n    - /memories/research/notes.txt - Key findings and notes\n    - /memories/research/report.md - Final report draft\n\n    This allows research to continue across multiple sessions.\"\"\"\n)\n```\n\n## Store implementations\n\nAny LangGraph `BaseStore` implementation works:\n\n### InMemoryStore (development)\n\nGood for testing and development, but data is lost on restart:\n\n```python  theme={null}\nfrom langgraph.store.memory import InMemoryStore\n\nstore = InMemoryStore()\nagent = create_deep_agent(\n    store=store,\n    backend=lambda rt: CompositeBackend(\n        default=StateBackend(rt),\n        routes={\"/memories/\": StoreBackend(rt)}\n    )\n)\n```\n\n### PostgresStore (production)\n\nFor production, use a persistent store:\n\n```python  theme={null}\nfrom langgraph.store.postgres import PostgresStore\nimport os\n\nstore = PostgresStore(connection_string=os.environ[\"DATABASE_URL\"])\nagent = create_deep_agent(\n    store=store,\n    backend=lambda rt: CompositeBackend(\n        default=StateBackend(rt),\n        routes={\"/memories/\": StoreBackend(rt)}\n    )\n)\n```\n\n## Best practices\n\n### Use descriptive paths\n\nOrganize persistent files with clear paths:\n\n```\n/memories/user_preferences.txt\n/memories/research/topic_a/sources.txt\n/memories/research/topic_a/notes.txt\n/memories/project/requirements.md\n```\n\n### Document the memory structure\n\nTell the agent what's stored where in your system prompt:\n\n```\nYour persistent memory structure:\n- /memories/preferences.txt: User preferences and settings\n- /memories/context/: Long-term context about the user\n- /memories/knowledge/: Facts and information learned over time\n```\n\n### Prune old data\n\nImplement periodic cleanup of outdated persistent files to keep storage manageable.\n\n### Choose the right storage\n\n* **Development**: Use `InMemoryStore` for quick iteration\n* **Production**: Use `PostgresStore` or other persistent stores\n* **Multi-tenant**: Consider using assistant\\_id-based namespacing in your store\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/long-term-memory.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 7749
}