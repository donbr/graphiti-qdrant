{
  "title": "How to add semantic search to your agent deployment",
  "source_url": "https://docs.langchain.com/langsmith/semantic-search",
  "content": "This guide explains how to add semantic search to your deployment's cross-thread [store](/oss/python/langgraph/persistence#memory-store), so that your agent can search for memories and other documents by semantic similarity.\n\n## Prerequisites\n\n* A deployment (refer to [how to set up an application for deployment](/langsmith/setup-app-requirements-txt)) and details on [hosting options](/langsmith/platform-setup).\n* API keys for your embedding provider (in this case, OpenAI).\n* `langchain >= 0.3.8` (if you specify using the string format below).\n\n## Steps\n\n1. Update your `langgraph.json` configuration file to include the store configuration:\n\n```json  theme={null}\n{\n    ...\n    \"store\": {\n        \"index\": {\n            \"embed\": \"openai:text-embedding-3-small\",\n            \"dims\": 1536,\n            \"fields\": [\"$\"]\n        }\n    }\n}\n```\n\nThis configuration:\n\n* Uses OpenAI's text-embedding-3-small model for generating embeddings\n* Sets the embedding dimension to 1536 (matching the model's output)\n* Indexes all fields in your stored data (`[\"$\"]` means index everything, or specify specific fields like `[\"text\", \"metadata.title\"]`)\n\n1. To use the string embedding format above, make sure your dependencies include `langchain >= 0.3.8`:\n\n```toml  theme={null}\n# In pyproject.toml\n[project]\ndependencies = [\n    \"langchain>=0.3.8\"\n]\n```\n\nOr if using [requirements.txt](/langsmith/setup-app-requirements-txt):\n\n```\nlangchain>=0.3.8\n```\n\n## Usage\n\nOnce configured, you can use semantic search in your [nodes](/oss/python/langgraph/graph-api#nodes). The store requires a namespace tuple to organize memories:\n\n```python  theme={null}\ndef search_memory(state: State, *, store: BaseStore):\n    # Search the store using semantic similarity\n    # The namespace tuple helps organize different types of memories\n    # e.g., (\"user_facts\", \"preferences\") or (\"conversation\", \"summaries\")\n    results = store.search(\n        namespace=(\"memory\", \"facts\"),  # Organize memories by type\n        query=\"your search query\",\n        limit=3  # number of results to return\n    )\n    return results\n```\n\n## Custom Embeddings\n\nIf you want to use custom embeddings, you can pass a path to a custom embedding function:\n\n```json  theme={null}\n{\n    ...\n    \"store\": {\n        \"index\": {\n            \"embed\": \"path/to/embedding_function.py:embed\",\n            \"dims\": 1536,\n            \"fields\": [\"$\"]\n        }\n    }\n}\n```\n\nThe deployment will look for the function in the specified path. The function must be async and accept a list of strings:\n\n```python  theme={null}\n# path/to/embedding_function.py\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nasync def aembed_texts(texts: list[str]) -> list[list[float]]:\n    \"\"\"Custom embedding function that must:\n    1. Be async\n    2. Accept a list of strings\n    3. Return a list of float arrays (embeddings)\n    \"\"\"\n    response = await client.embeddings.create(\n        model=\"text-embedding-3-small\",\n        input=texts\n    )\n    return [e.embedding for e in response.data]\n```\n\n## Querying via the API\n\nYou can also query the store using the LangGraph SDK. Since the SDK uses async operations:\n\n```python  theme={null}\nfrom langgraph_sdk import get_client\n\nasync def search_store():\n    client = get_client()\n    results = await client.store.search_items(\n        (\"memory\", \"facts\"),\n        query=\"your search query\",\n        limit=3  # number of results to return\n    )\n    return results\n\n# Use in an async context\nresults = await search_store()\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/semantic-search.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 3858
}