{
  "title": "Streaming",
  "source_url": "https://docs.langchain.com/oss/python/langchain/streaming",
  "content": "LangChain implements a streaming system to surface real-time updates.\n\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\n## Overview\n\nLangChain's streaming system lets you surface live feedback from agent runs to your application.\n\nWhat's possible with LangChain streaming:\n\n* <Icon icon=\"brain\" size={16} /> [**Stream agent progress**](#agent-progress) — get state updates after each agent step.\n* <Icon icon=\"square-binary\" size={16} /> [**Stream LLM tokens**](#llm-tokens) — stream language model tokens as they're generated.\n* <Icon icon=\"table\" size={16} /> [**Stream custom updates**](#custom-updates) — emit user-defined signals (e.g., `\"Fetched 10/100 records\"`).\n* <Icon icon=\"layer-plus\" size={16} /> [**Stream multiple modes**](#stream-multiple-modes) — choose from `updates` (agent progress), `messages` (LLM tokens + metadata), or `custom` (arbitrary user data).\n\n## Agent progress\n\nTo stream agent progress, use the [`stream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods with `stream_mode=\"updates\"`. This emits an event after every agent step.\n\nFor example, if you have an agent that calls a tool once, you should see the following updates:\n\n* **LLM node**: [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) with tool call requests\n* **Tool node**: [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) with execution result\n* **LLM node**: Final AI response\n\n```python title=\"Streaming agent progress\" theme={null}\nfrom langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor chunk in agent.stream(  # [!code highlight]\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"updates\",\n):\n    for step, data in chunk.items():\n        print(f\"step: {step}\")\n        print(f\"content: {data['messages'][-1].content_blocks}\")\n```\n\n```shell title=\"Output\" theme={null}\nstep: model\ncontent: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]\n\nstep: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\nstep: model\ncontent: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}]\n```\n\n## LLM tokens\n\nTo stream tokens as they are produced by the LLM, use `stream_mode=\"messages\"`. Below you can see the output of the agent streaming tool calls and the final response.\n\n```python title=\"Streaming LLM tokens\" theme={null}\nfrom langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor token, metadata in agent.stream(  # [!code highlight]\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"messages\",\n):\n    print(f\"node: {metadata['langgraph_node']}\")\n    print(f\"content: {token.content_blocks}\")\n    print(\"\\n\")\n```\n\n```shell title=\"Output\" expandable theme={null}\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\":\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\"}', 'index': 0}]\n\n\nnode: model\ncontent: []\n\n\nnode: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\n\nnode: model\ncontent: []\n\n\nnode: model\ncontent: [{'type': 'text', 'text': 'Here'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ''s'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' what'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' I'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' got'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ':'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' \"'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': \"It's\"}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' always'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' sunny'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' in'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' San'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' Francisco'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': '!\"\\n\\n'}]\n```\n\n## Custom updates\n\nTo stream updates from tools as they are executed, you can use [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer).\n\n```python title=\"Streaming custom updates\" theme={null}\nfrom langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer  # [!code highlight]\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()  # [!code highlight]\n    # stream any arbitrary data\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n)\n\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"custom\"  # [!code highlight]\n):\n    print(chunk)\n```\n\n```shell title=\"Output\" theme={null}\nLooking up data for city: San Francisco\nAcquired data for city: San Francisco\n```\n\n<Note>\n  If you add [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.\n</Note>\n\n## Stream multiple modes\n\nYou can specify multiple streaming modes by passing stream mode as a list: `stream_mode=[\"updates\", \"custom\"]`:\n\n```python title=\"Streaming multiple modes\" theme={null}\nfrom langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\n\nfor stream_mode, chunk in agent.stream(  # [!code highlight]\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=[\"updates\", \"custom\"]\n):\n    print(f\"stream_mode: {stream_mode}\")\n    print(f\"content: {chunk}\")\n    print(\"\\n\")\n```\n\n```shell title=\"Output\" theme={null}\nstream_mode: updates\ncontent: {'model': {'messages': [AIMessage(content='', response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 132, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_KTNQIftMrl9vgNwEfAJMVu7r', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 280, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}}\n\n\nstream_mode: custom\ncontent: Looking up data for city: San Francisco\n\n\nstream_mode: custom\ncontent: Acquired data for city: San Francisco\n\n\nstream_mode: updates\ncontent: {'tools': {'messages': [ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', tool_call_id='call_KTNQIftMrl9vgNwEfAJMVu7r')]}}\n\n\nstream_mode: updates\ncontent: {'model': {'messages': [AIMessage(content='San Francisco weather: It's always sunny in San Francisco!\\n\\n', response_metadata={'token_usage': {'completion_tokens': 764, 'prompt_tokens': 168, 'total_tokens': 932, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage_metadata={'input_tokens': 168, 'output_tokens': 764, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}}\n```\n\n## Disable streaming\n\nIn some applications you might need to disable streaming of individual tokens for a given model.\n\nThis is useful in [multi-agent](/oss/python/langchain/multi-agent) systems to control which agents stream their output.\n\nSee the [Models](/oss/python/langchain/models#disable-streaming) guide to learn how to disable streaming.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/streaming.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 10530
}