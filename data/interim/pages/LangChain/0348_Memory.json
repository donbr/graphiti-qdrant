{
  "title": "Memory",
  "source_url": "https://docs.langchain.com/oss/javascript/langgraph/add-memory",
  "content": "AI applications need [memory](/oss/javascript/concepts/memory) to share context across multiple interactions. In LangGraph, you can add two types of memory:\n\n* [Add short-term memory](#add-short-term-memory) as a part of your agent's [state](/oss/javascript/langgraph/graph-api#state) to enable multi-turn conversations.\n* [Add long-term memory](#add-long-term-memory) to store user-specific or application-level data across sessions.\n\n## Add short-term memory\n\n**Short-term** memory (thread-level [persistence](/oss/javascript/langgraph/persistence)) enables agents to track multi-turn conversations. To add short-term memory:\n\n```typescript  theme={null}\nimport { MemorySaver, StateGraph } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ checkpointer });\n\nawait graph.invoke(\n  { messages: [{ role: \"user\", content: \"hi! i am Bob\" }] },\n  { configurable: { thread_id: \"1\" } }\n);\n```\n\n### Use in production\n\nIn production, use a checkpointer backed by a database:\n\n```typescript  theme={null}\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ checkpointer });\n```\n\n<Accordion title=\"Example: using Postgres checkpointer\">\n  ```\n  npm install @langchain/langgraph-checkpoint-postgres\n  ```\n\n  <Tip>\n    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer\n  </Tip>\n\n  ```typescript  theme={null}\n  import { ChatAnthropic } from \"@langchain/anthropic\";\n  import { StateGraph, MessagesZodMeta, START } from \"@langchain/langgraph\";\n  import { BaseMessage } from \"@langchain/core/messages\";\n  import { registry } from \"@langchain/langgraph/zod\";\n  import * as z from \"zod\";\n  import { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n\n  const MessagesZodState = z.object({\n    messages: z\n      .array(z.custom<BaseMessage>())\n      .register(registry, MessagesZodMeta),\n  });\n\n  const model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\n\n  const DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\n  const checkpointer = PostgresSaver.fromConnString(DB_URI);\n  // await checkpointer.setup();\n\n  const builder = new StateGraph(MessagesZodState)\n    .addNode(\"call_model\", async (state) => {\n      const response = await model.invoke(state.messages);\n      return { messages: [response] };\n    })\n    .addEdge(START, \"call_model\");\n\n  const graph = builder.compile({ checkpointer });\n\n  const config = {\n    configurable: {\n      thread_id: \"1\"\n    }\n  };\n\n  for await (const chunk of await graph.stream(\n    { messages: [{ role: \"user\", content: \"hi! I'm bob\" }] },\n    { ...config, streamMode: \"values\" }\n  )) {\n    console.log(chunk.messages.at(-1)?.content);\n  }\n\n  for await (const chunk of await graph.stream(\n    { messages: [{ role: \"user\", content: \"what's my name?\" }] },\n    { ...config, streamMode: \"values\" }\n  )) {\n    console.log(chunk.messages.at(-1)?.content);\n  }\n  ```\n</Accordion>\n\n### Use in subgraphs\n\nIf your graph contains [subgraphs](/oss/javascript/langgraph/use-subgraphs), you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.\n\n```typescript  theme={null}\nimport { StateGraph, START, MemorySaver } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst State = z.object({ foo: z.string() });\n\nconst subgraphBuilder = new StateGraph(State)\n  .addNode(\"subgraph_node_1\", (state) => {\n    return { foo: state.foo + \"bar\" };\n  })\n  .addEdge(START, \"subgraph_node_1\");\nconst subgraph = subgraphBuilder.compile();\n\nconst builder = new StateGraph(State)\n  .addNode(\"node_1\", subgraph)\n  .addEdge(START, \"node_1\");\n\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n```\n\nIf you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/javascript/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories.\n\n```typescript  theme={null}\nconst subgraphBuilder = new StateGraph(...);\nconst subgraph = subgraphBuilder.compile({ checkpointer: true });  // [!code highlight]\n```\n\n## Add long-term memory\n\nUse long-term memory to store user-specific or application-specific data across conversations.\n\n```typescript  theme={null}\nimport { InMemoryStore, StateGraph } from \"@langchain/langgraph\";\n\nconst store = new InMemoryStore();\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ store });\n```\n\n### Use in production\n\nIn production, use a store backed by a database:\n\n```typescript  theme={null}\nimport { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst store = PostgresStore.fromConnString(DB_URI);\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ store });\n```\n\n<Accordion title=\"Example: using Postgres store\">\n  ```\n  npm install @langchain/langgraph-checkpoint-postgres\n  ```\n\n  <Tip>\n    You need to call `store.setup()` the first time you're using Postgres store\n  </Tip>\n\n  ```typescript  theme={null}\n  import { ChatAnthropic } from \"@langchain/anthropic\";\n  import { StateGraph, MessagesZodMeta, START, LangGraphRunnableConfig } from \"@langchain/langgraph\";\n  import { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n  import { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";\n  import { BaseMessage } from \"@langchain/core/messages\";\n  import { registry } from \"@langchain/langgraph/zod\";\n  import * as z from \"zod\";\n  import { v4 as uuidv4 } from \"uuid\";\n\n  const MessagesZodState = z.object({\n    messages: z\n      .array(z.custom<BaseMessage>())\n      .register(registry, MessagesZodMeta),\n  });\n\n  const model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\n\n  const DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\n\n  const store = PostgresStore.fromConnString(DB_URI);\n  const checkpointer = PostgresSaver.fromConnString(DB_URI);\n  // await store.setup();\n  // await checkpointer.setup();\n\n  const callModel = async (\n    state: z.infer<typeof MessagesZodState>,\n    config: LangGraphRunnableConfig,\n  ) => {\n    const userId = config.configurable?.userId;\n    const namespace = [\"memories\", userId];\n    const memories = await config.store?.search(namespace, { query: state.messages.at(-1)?.content });\n    const info = memories?.map(d => d.value.data).join(\"\\n\") || \"\";\n    const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;\n\n    // Store new memories if the user asks the model to remember\n    const lastMessage = state.messages.at(-1);\n    if (lastMessage?.content?.toLowerCase().includes(\"remember\")) {\n      const memory = \"User name is Bob\";\n      await config.store?.put(namespace, uuidv4(), { data: memory });\n    }\n\n    const response = await model.invoke([\n      { role: \"system\", content: systemMsg },\n      ...state.messages\n    ]);\n    return { messages: [response] };\n  };\n\n  const builder = new StateGraph(MessagesZodState)\n    .addNode(\"call_model\", callModel)\n    .addEdge(START, \"call_model\");\n\n  const graph = builder.compile({\n    checkpointer,\n    store,\n  });\n\n  const config = {\n    configurable: {\n      thread_id: \"1\",\n      userId: \"1\",\n    }\n  };\n\n  for await (const chunk of await graph.stream(\n    { messages: [{ role: \"user\", content: \"Hi! Remember: my name is Bob\" }] },\n    { ...config, streamMode: \"values\" }\n  )) {\n    console.log(chunk.messages.at(-1)?.content);\n  }\n\n  const config2 = {\n    configurable: {\n      thread_id: \"2\",\n      userId: \"1\",\n    }\n  };\n\n  for await (const chunk of await graph.stream(\n    { messages: [{ role: \"user\", content: \"what is my name?\" }] },\n    { ...config2, streamMode: \"values\" }\n  )) {\n    console.log(chunk.messages.at(-1)?.content);\n  }\n  ```\n</Accordion>\n\n### Use semantic search\n\nEnable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity.\n\n```typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\n\n// Create store with semantic search enabled\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\nconst store = new InMemoryStore({\n  index: {\n    embeddings,\n    dims: 1536,\n  },\n});\n\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\n\nconst items = await store.search([\"user_123\", \"memories\"], {\n  query: \"I'm hungry\",\n  limit: 1,\n});\n```\n\n<Accordion title=\"Long-term memory with semantic search\">\n  ```typescript  theme={null}\n  import { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\n  import { StateGraph, START, MessagesZodMeta, InMemoryStore } from \"@langchain/langgraph\";\n  import { BaseMessage } from \"@langchain/core/messages\";\n  import { registry } from \"@langchain/langgraph/zod\";\n  import * as z from \"zod\";\n\n  const MessagesZodState = z.object({\n      messages: z\n      .array(z.custom<BaseMessage>())\n      .register(registry, MessagesZodMeta),\n  });\n\n  const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n\n  // Create store with semantic search enabled\n  const embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\n  const store = new InMemoryStore({\n      index: {\n      embeddings,\n      dims: 1536,\n      }\n  });\n\n  await store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\n  await store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\n\n  const chat = async (state: z.infer<typeof MessagesZodState>, config) => {\n      // Search based on user's last message\n      const items = await config.store.search(\n      [\"user_123\", \"memories\"],\n      { query: state.messages.at(-1)?.content, limit: 2 }\n      );\n      const memories = items.map(item => item.value.text).join(\"\\n\");\n      const memoriesText = memories ? `## Memories of user\\n${memories}` : \"\";\n\n      const response = await model.invoke([\n      { role: \"system\", content: `You are a helpful assistant.\\n${memoriesText}` },\n      ...state.messages,\n      ]);\n\n      return { messages: [response] };\n  };\n\n  const builder = new StateGraph(MessagesZodState)\n      .addNode(\"chat\", chat)\n      .addEdge(START, \"chat\");\n  const graph = builder.compile({ store });\n\n  for await (const [message, metadata] of await graph.stream(\n      { messages: [{ role: \"user\", content: \"I'm hungry\" }] },\n      { streamMode: \"messages\" }\n  )) {\n      if (message.content) {\n      console.log(message.content);\n      }\n  }\n  ```\n</Accordion>\n\n## Manage short-term memory\n\nWith [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:\n\n* [Trim messages](#trim-messages): Remove first or last N messages (before calling LLM)\n* [Delete messages](#delete-messages) from LangGraph state permanently\n* [Summarize messages](#summarize-messages): Summarize earlier messages in the history and replace them with a summary\n* [Manage checkpoints](#manage-checkpoints) to store and retrieve message history\n* Custom strategies (e.g., message filtering, etc.)\n\nThis allows the agent to keep track of the conversation without exceeding the LLM's context window.\n\n### Trim messages\n\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `maxTokens`) to use for handling the boundary.\n\nTo trim message history, use the [`trimMessages`](https://js.langchain.com/docs/how_to/trim_messages/) function:\n\n```typescript  theme={null}\nimport { trimMessages } from \"@langchain/core/messages\";\n\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\n  const messages = trimMessages(state.messages, {\n    strategy: \"last\",\n    maxTokens: 128,\n    startOn: \"human\",\n    endOn: [\"human\", \"tool\"],\n  });\n  const response = await model.invoke(messages);\n  return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", callModel);\n// ...\n```\n\n<Accordion title=\"Full example: trim messages\">\n  ```typescript  theme={null}\n  import { trimMessages, BaseMessage } from \"@langchain/core/messages\";\n  import { ChatAnthropic } from \"@langchain/anthropic\";\n  import { StateGraph, START, MessagesZodMeta, MemorySaver } from \"@langchain/langgraph\";\n  import { registry } from \"@langchain/langgraph/zod\";\n  import * as z from \"zod\";\n\n  const MessagesZodState = z.object({\n    messages: z\n      .array(z.custom<BaseMessage>())\n      .register(registry, MessagesZodMeta),\n  });\n\n  const model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\n\n  const callModel = async (state: z.infer<typeof MessagesZodState>) => {\n    const messages = trimMessages(state.messages, {\n      strategy: \"last\",\n      maxTokens: 128,\n      startOn: \"human\",\n      endOn: [\"human\", \"tool\"],\n      tokenCounter: model,\n    });\n    const response = await model.invoke(messages);\n    return { messages: [response] };\n  };\n\n  const checkpointer = new MemorySaver();\n  const builder = new StateGraph(MessagesZodState)\n    .addNode(\"call_model\", callModel)\n    .addEdge(START, \"call_model\");\n  const graph = builder.compile({ checkpointer });\n\n  const config = { configurable: { thread_id: \"1\" } };\n  await graph.invoke({ messages: [{ role: \"user\", content: \"hi, my name is bob\" }] }, config);\n  await graph.invoke({ messages: [{ role: \"user\", content: \"write a short poem about cats\" }] }, config);\n  await graph.invoke({ messages: [{ role: \"user\", content: \"now do the same but for dogs\" }] }, config);\n  const finalResponse = await graph.invoke({ messages: [{ role: \"user\", content: \"what's my name?\" }] }, config);\n\n  console.log(finalResponse.messages.at(-1)?.content);\n  ```\n\n  ```\n  Your name is Bob, as you mentioned when you first introduced yourself.\n  ```\n</Accordion>\n\n### Delete messages\n\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\n\nTo delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with [`messagesStateReducer`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.messagesStateReducer.html) [reducer](/oss/javascript/langgraph/graph-api#reducers), like `MessagesZodState`.\n\nTo remove specific messages:\n\n```typescript  theme={null}\nimport { RemoveMessage } from \"@langchain/core/messages\";\n\nconst deleteMessages = (state) => {\n  const messages = state.messages;\n  if (messages.length > 2) {\n    // remove the earliest two messages\n    return {\n      messages: messages\n        .slice(0, 2)\n        .map((m) => new RemoveMessage({ id: m.id })),\n    };\n  }\n};\n```\n\n<Warning>\n  When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:\n\n  * Some providers expect message history to start with a `user` message\n  * Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.\n</Warning>\n\n<Accordion title=\"Full example: delete messages\">\n  ```typescript  theme={null}\n  import { RemoveMessage, BaseMessage } from \"@langchain/core/messages\";\n  import { ChatAnthropic } from \"@langchain/anthropic\";\n  import { StateGraph, START, MemorySaver, MessagesZodMeta } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n  import { registry } from \"@langchain/langgraph/zod\";\n\n  const MessagesZodState = z.object({\n    messages: z\n      .array(z.custom<BaseMessage>())\n      .register(registry, MessagesZodMeta),\n  });\n\n  const model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\n\n  const deleteMessages = (state: z.infer<typeof MessagesZodState>) => {\n    const messages = state.messages;\n    if (messages.length > 2) {\n      // remove the earliest two messages\n      return { messages: messages.slice(0, 2).map(m => new RemoveMessage({ id: m.id })) };\n    }\n    return {};\n  };\n\n  const callModel = async (state: z.infer<typeof MessagesZodState>) => {\n    const response = await model.invoke(state.messages);\n    return { messages: [response] };\n  };\n\n  const builder = new StateGraph(MessagesZodState)\n    .addNode(\"call_model\", callModel)\n    .addNode(\"delete_messages\", deleteMessages)\n    .addEdge(START, \"call_model\")\n    .addEdge(\"call_model\", \"delete_messages\");\n\n  const checkpointer = new MemorySaver();\n  const app = builder.compile({ checkpointer });\n\n  const config = { configurable: { thread_id: \"1\" } };\n\n  for await (const event of await app.stream(\n    { messages: [{ role: \"user\", content: \"hi! I'm bob\" }] },\n    { ...config, streamMode: \"values\" }\n  )) {\n    console.log(event.messages.map(message => [message.getType(), message.content]));\n  }\n\n  for await (const event of await app.stream(\n    { messages: [{ role: \"user\", content: \"what's my name?\" }] },\n    { ...config, streamMode: \"values\" }\n  )) {\n    console.log(event.messages.map(message => [message.getType(), message.content]));\n  }\n  ```\n\n  ```\n  [['human', \"hi! I'm bob\"]]\n  [['human', \"hi! I'm bob\"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?']]\n  [['human', \"hi! I'm bob\"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'], ['human', \"what's my name?\"]]\n  [['human', \"hi! I'm bob\"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'], ['human', \"what's my name?\"], ['ai', 'Your name is Bob.']]\n  [['human', \"what's my name?\"], ['ai', 'Your name is Bob.']]\n  ```\n</Accordion>\n\n### Summarize messages\n\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c8ed3facdccd4ef5c7e52902c72ba938\" alt=\"\" data-og-width=\"609\" width=\"609\" data-og-height=\"242\" height=\"242\" data-path=\"oss/images/summary.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4208b9b0cc9f459f3dc4e5219918471b 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=7acb77c081545f57042368f4e9d0c8cb 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2fcfdb0c481d2e1d361e76db763a41e5 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4abdac693a562788aa0db8681bef8ea7 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=40acfefa91dcb11b247a6e4a7705f22b 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d765aaf7551e8b0fc2720de7d2ac2a8 2500w\" />\n\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can include a `summary` key in the state alongside the `messages` key:\n\n```typescript  theme={null}\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { MessagesZodMeta } from \"@langchain/langgraph\";\nimport { registry } from \"@langchain/langgraph/zod\";\nimport * as z from \"zod\";\n\nconst State = z.object({\n  messages: z\n    .array(z.custom<BaseMessage>())\n    .register(registry, MessagesZodMeta),\n  summary: z.string().optional(),\n});\n```\n\nThen, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarizeConversation` node can be called after some number of messages have accumulated in the `messages` state key.\n\n```typescript  theme={null}\nimport { RemoveMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst summarizeConversation = async (state: z.infer<typeof State>) => {\n  // First, we get any existing summary\n  const summary = state.summary || \"\";\n\n  // Create our summarization prompt\n  let summaryMessage: string;\n  if (summary) {\n    // A summary already exists\n    summaryMessage =\n      `This is a summary of the conversation to date: ${summary}\\n\\n` +\n      \"Extend the summary by taking into account the new messages above:\";\n  } else {\n    summaryMessage = \"Create a summary of the conversation above:\";\n  }\n\n  // Add prompt to our history\n  const messages = [\n    ...state.messages,\n    new HumanMessage({ content: summaryMessage })\n  ];\n  const response = await model.invoke(messages);\n\n  // Delete all but the 2 most recent messages\n  const deleteMessages = state.messages\n    .slice(0, -2)\n    .map(m => new RemoveMessage({ id: m.id }));\n\n  return {\n    summary: response.content,\n    messages: deleteMessages\n  };\n};\n```\n\n<Accordion title=\"Full example: summarize messages\">\n  ```typescript  theme={null}\n  import { ChatAnthropic } from \"@langchain/anthropic\";\n  import {\n    SystemMessage,\n    HumanMessage,\n    RemoveMessage,\n    type BaseMessage\n  } from \"@langchain/core/messages\";\n  import {\n    MessagesZodMeta,\n    StateGraph,\n    START,\n    END,\n    MemorySaver,\n  } from \"@langchain/langgraph\";\n  import { BaseMessage } from \"@langchain/core/messages\";\n  import { registry } from \"@langchain/langgraph/zod\";\n  import * as z from \"zod\";\n  import { v4 as uuidv4 } from \"uuid\";\n\n  const memory = new MemorySaver();\n\n  // We will add a `summary` attribute (in addition to `messages` key,\n  // which MessagesZodState already has)\n  const GraphState = z.object({\n    messages: z\n      .array(z.custom<BaseMessage>())\n      .register(registry, MessagesZodMeta),\n    summary: z.string().default(\"\"),\n  });\n\n  // We will use this model for both the conversation and the summarization\n  const model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\n\n  // Define the logic to call the model\n  const callModel = async (state: z.infer<typeof GraphState>) => {\n    // If a summary exists, we add this in as a system message\n    const { summary } = state;\n    let { messages } = state;\n    if (summary) {\n      const systemMessage = new SystemMessage({\n        id: uuidv4(),\n        content: `Summary of conversation earlier: ${summary}`,\n      });\n      messages = [systemMessage, ...messages];\n    }\n    const response = await model.invoke(messages);\n    // We return an object, because this will get added to the existing state\n    return { messages: [response] };\n  };\n\n  // We now define the logic for determining whether to end or summarize the conversation\n  const shouldContinue = (state: z.infer<typeof GraphState>) => {\n    const messages = state.messages;\n    // If there are more than six messages, then we summarize the conversation\n    if (messages.length > 6) {\n      return \"summarize_conversation\";\n    }\n    // Otherwise we can just end\n    return END;\n  };\n\n  const summarizeConversation = async (state: z.infer<typeof GraphState>) => {\n    // First, we summarize the conversation\n    const { summary, messages } = state;\n    let summaryMessage: string;\n    if (summary) {\n      // If a summary already exists, we use a different system prompt\n      // to summarize it than if one didn't\n      summaryMessage =\n        `This is summary of the conversation to date: ${summary}\\n\\n` +\n        \"Extend the summary by taking into account the new messages above:\";\n    } else {\n      summaryMessage = \"Create a summary of the conversation above:\";\n    }\n\n    const allMessages = [\n      ...messages,\n      new HumanMessage({ id: uuidv4(), content: summaryMessage }),\n    ];\n\n    const response = await model.invoke(allMessages);\n\n    // We now need to delete messages that we no longer want to show up\n    // I will delete all but the last two messages, but you can change this\n    const deleteMessages = messages\n      .slice(0, -2)\n      .map((m) => new RemoveMessage({ id: m.id! }));\n\n    if (typeof response.content !== \"string\") {\n      throw new Error(\"Expected a string response from the model\");\n    }\n\n    return { summary: response.content, messages: deleteMessages };\n  };\n\n  // Define a new graph\n  const workflow = new StateGraph(GraphState)\n    // Define the conversation node and the summarize node\n    .addNode(\"conversation\", callModel)\n    .addNode(\"summarize_conversation\", summarizeConversation)\n    // Set the entrypoint as conversation\n    .addEdge(START, \"conversation\")\n    // We now add a conditional edge\n    .addConditionalEdges(\n      // First, we define the start node. We use `conversation`.\n      // This means these are the edges taken after the `conversation` node is called.\n      \"conversation\",\n      // Next, we pass in the function that will determine which node is called next.\n      shouldContinue,\n    )\n    // We now add a normal edge from `summarize_conversation` to END.\n    // This means that after `summarize_conversation` is called, we end.\n    .addEdge(\"summarize_conversation\", END);\n\n  // Finally, we compile it!\n  const app = workflow.compile({ checkpointer: memory });\n  ```\n</Accordion>\n\n### Manage checkpoints\n\nYou can view and delete the information stored by the checkpointer.\n\n<a id=\"checkpoint\" />\n\n#### View thread state\n\n```typescript  theme={null}\nconst config = {\n  configurable: {\n    thread_id: \"1\",\n    // optionally provide an ID for a specific checkpoint,\n    // otherwise the latest checkpoint is shown\n    // checkpoint_id: \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\n  },\n};\nawait graph.getState(config);\n```\n\n```\n{\n  values: { messages: [HumanMessage(...), AIMessage(...), HumanMessage(...), AIMessage(...)] },\n  next: [],\n  config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },\n  metadata: {\n    source: 'loop',\n    writes: { call_model: { messages: AIMessage(...) } },\n    step: 4,\n    parents: {},\n    thread_id: '1'\n  },\n  createdAt: '2025-05-05T16:01:24.680462+00:00',\n  parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },\n  tasks: [],\n  interrupts: []\n}\n```\n\n<a id=\"checkpoints\" />\n\n#### View the history of the thread\n\n```typescript  theme={null}\nconst config = {\n  configurable: {\n    thread_id: \"1\",\n  },\n};\n\nconst history = [];\nfor await (const state of graph.getStateHistory(config)) {\n  history.push(state);\n}\n```\n\n#### Delete all checkpoints for a thread\n\n```typescript  theme={null}\nconst threadId = \"1\";\nawait checkpointer.deleteThread(threadId);\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 27864
}