{
  "title": "Memory",
  "source_url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
  "content": "AI applications need [memory](/oss/python/concepts/memory) to share context across multiple interactions. In LangGraph, you can add two types of memory:\n\n* [Add short-term memory](#add-short-term-memory) as a part of your agent's [state](/oss/python/langgraph/graph-api#state) to enable multi-turn conversations.\n* [Add long-term memory](#add-long-term-memory) to store user-specific or application-level data across sessions.\n\n## Add short-term memory\n\n**Short-term** memory (thread-level [persistence](/oss/python/langgraph/persistence)) enables agents to track multi-turn conversations. To add short-term memory:\n\n```python  theme={null}\nfrom langgraph.checkpoint.memory import InMemorySaver  # [!code highlight]\nfrom langgraph.graph import StateGraph\n\ncheckpointer = InMemorySaver()  # [!code highlight]\n\nbuilder = StateGraph(...)\ngraph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\ngraph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]},\n    {\"configurable\": {\"thread_id\": \"1\"}},  # [!code highlight]\n)\n```\n\n### Use in production\n\nIn production, use a checkpointer backed by a database:\n\n```python  theme={null}\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n    builder = StateGraph(...)\n    graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n```\n\n<Accordion title=\"Example: using Postgres checkpointer\">\n  ```\n  pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\n  ```\n\n  <Tip>\n    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer\n  </Tip>\n\n  <Tabs>\n    <Tab title=\"Sync\">\n      ```python  theme={null}\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.postgres import PostgresSaver  # [!code highlight]\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n      with PostgresSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n          # checkpointer.setup()\n\n          def call_model(state: MessagesState):\n              response = model.invoke(state[\"messages\"])\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\"  # [!code highlight]\n              }\n          }\n\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n\n    <Tab title=\"Async\">\n      ```python  theme={null}\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver  # [!code highlight]\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n      async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n          # await checkpointer.setup()\n\n          async def call_model(state: MessagesState):\n              response = await model.ainvoke(state[\"messages\"])\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\"  # [!code highlight]\n              }\n          }\n\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n  </Tabs>\n</Accordion>\n\n<Accordion title=\"Example: using [MongoDB](https://pypi.org/project/langgraph-checkpoint-mongodb/) checkpointer\">\n  ```\n  pip install -U pymongo langgraph langgraph-checkpoint-mongodb\n  ```\n\n  <Note>\n    **Setup**\n    To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow [this guide](https://www.mongodb.com/docs/guides/atlas/cluster/) to create a cluster if you don't already have one.\n  </Note>\n\n  <Tabs>\n    <Tab title=\"Sync\">\n      ```python  theme={null}\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.mongodb import MongoDBSaver  # [!code highlight]\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"localhost:27017\"\n      with MongoDBSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n\n          def call_model(state: MessagesState):\n              response = model.invoke(state[\"messages\"])\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\"  # [!code highlight]\n              }\n          }\n\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n\n    <Tab title=\"Async\">\n      ```python  theme={null}\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.mongodb.aio import AsyncMongoDBSaver  # [!code highlight]\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"localhost:27017\"\n      async with AsyncMongoDBSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n\n          async def call_model(state: MessagesState):\n              response = await model.ainvoke(state[\"messages\"])\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\"  # [!code highlight]\n              }\n          }\n\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n  </Tabs>\n</Accordion>\n\n<Accordion title=\"Example: using Redis checkpointer\">\n  ```\n  pip install -U langgraph langgraph-checkpoint-redis\n  ```\n\n  <Tip>\n    You need to call `checkpointer.setup()` the first time you're using Redis checkpointer.\n  </Tip>\n\n  <Tabs>\n    <Tab title=\"Sync\">\n      ```python  theme={null}\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.redis import RedisSaver  # [!code highlight]\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"redis://localhost:6379\"\n      with RedisSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n          # checkpointer.setup()\n\n          def call_model(state: MessagesState):\n              response = model.invoke(state[\"messages\"])\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\"  # [!code highlight]\n              }\n          }\n\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n\n    <Tab title=\"Async\">\n      ```python  theme={null}\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.redis.aio import AsyncRedisSaver  # [!code highlight]\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"redis://localhost:6379\"\n      async with AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n          # await checkpointer.asetup()\n\n          async def call_model(state: MessagesState):\n              response = await model.ainvoke(state[\"messages\"])\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\"  # [!code highlight]\n              }\n          }\n\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\"\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n  </Tabs>\n</Accordion>\n\n### Use in subgraphs\n\nIf your graph contains [subgraphs](/oss/python/langgraph/use-subgraphs), you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.\n\n```python  theme={null}\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()  # [!code highlight]\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)  # [!code highlight]\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n```\n\nIf you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/python/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories.\n\n```python  theme={null}\nsubgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)  # [!code highlight]\n```\n\n## Add long-term memory\n\nUse long-term memory to store user-specific or application-specific data across conversations.\n\n```python  theme={null}\nfrom langgraph.store.memory import InMemoryStore  # [!code highlight]\nfrom langgraph.graph import StateGraph\n\nstore = InMemoryStore()  # [!code highlight]\n\nbuilder = StateGraph(...)\ngraph = builder.compile(store=store)  # [!code highlight]\n```\n\n### Use in production\n\nIn production, use a store backed by a database:\n\n```python  theme={null}\nfrom langgraph.store.postgres import PostgresStore\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresStore.from_conn_string(DB_URI) as store:  # [!code highlight]\n    builder = StateGraph(...)\n    graph = builder.compile(store=store)  # [!code highlight]\n```\n\n<Accordion title=\"Example: using Postgres store\">\n  ```\n  pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\n  ```\n\n  <Tip>\n    You need to call `store.setup()` the first time you're using Postgres store\n  </Tip>\n\n  <Tabs>\n    <Tab title=\"Sync\">\n      ```python  theme={null}\n      from langchain_core.runnables import RunnableConfig\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.postgres import PostgresSaver\n      from langgraph.store.postgres import PostgresStore  # [!code highlight]\n      from langgraph.store.base import BaseStore\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n\n      with (\n          PostgresStore.from_conn_string(DB_URI) as store,  # [!code highlight]\n          PostgresSaver.from_conn_string(DB_URI) as checkpointer,\n      ):\n          # store.setup()\n          # checkpointer.setup()\n\n          def call_model(\n              state: MessagesState,\n              config: RunnableConfig,\n              *,\n              store: BaseStore,  # [!code highlight]\n          ):\n              user_id = config[\"configurable\"][\"user_id\"]\n              namespace = (\"memories\", user_id)\n              memories = store.search(namespace, query=str(state[\"messages\"][-1].content))  # [!code highlight]\n              info = \"\\n\".join([d.value[\"data\"] for d in memories])\n              system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n              # Store new memories if the user asks the model to remember\n              last_message = state[\"messages\"][-1]\n              if \"remember\" in last_message.content.lower():\n                  memory = \"User name is Bob\"\n                  store.put(namespace, str(uuid.uuid4()), {\"data\": memory})  # [!code highlight]\n\n              response = model.invoke(\n                  [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n              )\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(\n              checkpointer=checkpointer,\n              store=store,  # [!code highlight]\n          )\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\",  # [!code highlight]\n                  \"user_id\": \"1\",  # [!code highlight]\n              }\n          }\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\",\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"2\",  # [!code highlight]\n                  \"user_id\": \"1\",\n              }\n          }\n\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\",\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n\n    <Tab title=\"Async\">\n      ```python  theme={null}\n      from langchain_core.runnables import RunnableConfig\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n      from langgraph.store.postgres.aio import AsyncPostgresStore  # [!code highlight]\n      from langgraph.store.base import BaseStore\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n\n      async with (\n          AsyncPostgresStore.from_conn_string(DB_URI) as store,  # [!code highlight]\n          AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer,\n      ):\n          # await store.setup()\n          # await checkpointer.setup()\n\n          async def call_model(\n              state: MessagesState,\n              config: RunnableConfig,\n              *,\n              store: BaseStore,  # [!code highlight]\n          ):\n              user_id = config[\"configurable\"][\"user_id\"]\n              namespace = (\"memories\", user_id)\n              memories = await store.asearch(namespace, query=str(state[\"messages\"][-1].content))  # [!code highlight]\n              info = \"\\n\".join([d.value[\"data\"] for d in memories])\n              system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n              # Store new memories if the user asks the model to remember\n              last_message = state[\"messages\"][-1]\n              if \"remember\" in last_message.content.lower():\n                  memory = \"User name is Bob\"\n                  await store.aput(namespace, str(uuid.uuid4()), {\"data\": memory})  # [!code highlight]\n\n              response = await model.ainvoke(\n                  [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n              )\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(\n              checkpointer=checkpointer,\n              store=store,  # [!code highlight]\n          )\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\",  # [!code highlight]\n                  \"user_id\": \"1\",  # [!code highlight]\n              }\n          }\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\",\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"2\",  # [!code highlight]\n                  \"user_id\": \"1\",\n              }\n          }\n\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\",\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n  </Tabs>\n</Accordion>\n\n<Accordion title=\"Example: using Redis store\">\n  ```\n  pip install -U langgraph langgraph-checkpoint-redis\n  ```\n\n  <Tip>\n    You need to call `store.setup()` the first time you're using [Redis store](https://pypi.org/project/langgraph-checkpoint-redis/).\n  </Tip>\n\n  <Tabs>\n    <Tab title=\"Sync\">\n      ```python  theme={null}\n      from langchain_core.runnables import RunnableConfig\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.redis import RedisSaver\n      from langgraph.store.redis import RedisStore  # [!code highlight]\n      from langgraph.store.base import BaseStore\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"redis://localhost:6379\"\n\n      with (\n          RedisStore.from_conn_string(DB_URI) as store,  # [!code highlight]\n          RedisSaver.from_conn_string(DB_URI) as checkpointer,\n      ):\n          store.setup()\n          checkpointer.setup()\n\n          def call_model(\n              state: MessagesState,\n              config: RunnableConfig,\n              *,\n              store: BaseStore,  # [!code highlight]\n          ):\n              user_id = config[\"configurable\"][\"user_id\"]\n              namespace = (\"memories\", user_id)\n              memories = store.search(namespace, query=str(state[\"messages\"][-1].content))  # [!code highlight]\n              info = \"\\n\".join([d.value[\"data\"] for d in memories])\n              system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n              # Store new memories if the user asks the model to remember\n              last_message = state[\"messages\"][-1]\n              if \"remember\" in last_message.content.lower():\n                  memory = \"User name is Bob\"\n                  store.put(namespace, str(uuid.uuid4()), {\"data\": memory})  # [!code highlight]\n\n              response = model.invoke(\n                  [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n              )\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(\n              checkpointer=checkpointer,\n              store=store,  # [!code highlight]\n          )\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\",  # [!code highlight]\n                  \"user_id\": \"1\",  # [!code highlight]\n              }\n          }\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\",\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"2\",  # [!code highlight]\n                  \"user_id\": \"1\",\n              }\n          }\n\n          for chunk in graph.stream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\",\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n\n    <Tab title=\"Async\">\n      ```python  theme={null}\n      from langchain_core.runnables import RunnableConfig\n      from langchain.chat_models import init_chat_model\n      from langgraph.graph import StateGraph, MessagesState, START\n      from langgraph.checkpoint.redis.aio import AsyncRedisSaver\n      from langgraph.store.redis.aio import AsyncRedisStore  # [!code highlight]\n      from langgraph.store.base import BaseStore\n\n      model = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n      DB_URI = \"redis://localhost:6379\"\n\n      async with (\n          AsyncRedisStore.from_conn_string(DB_URI) as store,  # [!code highlight]\n          AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer,\n      ):\n          # await store.setup()\n          # await checkpointer.asetup()\n\n          async def call_model(\n              state: MessagesState,\n              config: RunnableConfig,\n              *,\n              store: BaseStore,  # [!code highlight]\n          ):\n              user_id = config[\"configurable\"][\"user_id\"]\n              namespace = (\"memories\", user_id)\n              memories = await store.asearch(namespace, query=str(state[\"messages\"][-1].content))  # [!code highlight]\n              info = \"\\n\".join([d.value[\"data\"] for d in memories])\n              system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n              # Store new memories if the user asks the model to remember\n              last_message = state[\"messages\"][-1]\n              if \"remember\" in last_message.content.lower():\n                  memory = \"User name is Bob\"\n                  await store.aput(namespace, str(uuid.uuid4()), {\"data\": memory})  # [!code highlight]\n\n              response = await model.ainvoke(\n                  [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n              )\n              return {\"messages\": response}\n\n          builder = StateGraph(MessagesState)\n          builder.add_node(call_model)\n          builder.add_edge(START, \"call_model\")\n\n          graph = builder.compile(\n              checkpointer=checkpointer,\n              store=store,  # [!code highlight]\n          )\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"1\",  # [!code highlight]\n                  \"user_id\": \"1\",  # [!code highlight]\n              }\n          }\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\",\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n\n          config = {\n              \"configurable\": {\n                  \"thread_id\": \"2\",  # [!code highlight]\n                  \"user_id\": \"1\",\n              }\n          }\n\n          async for chunk in graph.astream(\n              {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n              config,  # [!code highlight]\n              stream_mode=\"values\",\n          ):\n              chunk[\"messages\"][-1].pretty_print()\n      ```\n    </Tab>\n  </Tabs>\n</Accordion>\n\n### Use semantic search\n\nEnable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity.\n\n```python  theme={null}\nfrom langchain.embeddings import init_embeddings\nfrom langgraph.store.memory import InMemoryStore\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\nitems = store.search(\n    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n)\n```\n\n<Accordion title=\"Long-term memory with semantic search\">\n  ```python  theme={null}\n\n  from langchain.embeddings import init_embeddings\n  from langchain.chat_models import init_chat_model\n  from langgraph.store.base import BaseStore\n  from langgraph.store.memory import InMemoryStore\n  from langgraph.graph import START, MessagesState, StateGraph\n\n  model = init_chat_model(\"gpt-4o-mini\")\n\n  # Create store with semantic search enabled\n  embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n  store = InMemoryStore(\n      index={\n          \"embed\": embeddings,\n          \"dims\": 1536,\n      }\n  )\n\n  store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n  store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\n  def chat(state, *, store: BaseStore):\n      # Search based on user's last message\n      items = store.search(\n          (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n      )\n      memories = \"\\n\".join(item.value[\"text\"] for item in items)\n      memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n      response = model.invoke(\n          [\n              {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n              *state[\"messages\"],\n          ]\n      )\n      return {\"messages\": [response]}\n\n\n  builder = StateGraph(MessagesState)\n  builder.add_node(chat)\n  builder.add_edge(START, \"chat\")\n  graph = builder.compile(store=store)\n\n  for message, metadata in graph.stream(\n      input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n      stream_mode=\"messages\",\n  ):\n      print(message.content, end=\"\")\n  ```\n</Accordion>\n\n## Manage short-term memory\n\nWith [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:\n\n* [Trim messages](#trim-messages): Remove first or last N messages (before calling LLM)\n* [Delete messages](#delete-messages) from LangGraph state permanently\n* [Summarize messages](#summarize-messages): Summarize earlier messages in the history and replace them with a summary\n* [Manage checkpoints](#manage-checkpoints) to store and retrieve message history\n* Custom strategies (e.g., message filtering, etc.)\n\nThis allows the agent to keep track of the conversation without exceeding the LLM's context window.\n\n### Trim messages\n\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.\n\nTo trim message history, use the [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function:\n\n```python  theme={null}\nfrom langchain_core.messages.utils import (  # [!code highlight]\n    trim_messages,  # [!code highlight]\n    count_tokens_approximately  # [!code highlight]\n)  # [!code highlight]\n\ndef call_model(state: MessagesState):\n    messages = trim_messages(  # [!code highlight]\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\n...\n```\n\n<Accordion title=\"Full example: trim messages\">\n  ```python  theme={null}\n  from langchain_core.messages.utils import (\n      trim_messages,  # [!code highlight]\n      count_tokens_approximately  # [!code highlight]\n  )\n  from langchain.chat_models import init_chat_model\n  from langgraph.graph import StateGraph, START, MessagesState\n\n  model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n  summarization_model = model.bind(max_tokens=128)\n\n  def call_model(state: MessagesState):\n      messages = trim_messages(  # [!code highlight]\n          state[\"messages\"],\n          strategy=\"last\",\n          token_counter=count_tokens_approximately,\n          max_tokens=128,\n          start_on=\"human\",\n          end_on=(\"human\", \"tool\"),\n      )\n      response = model.invoke(messages)\n      return {\"messages\": [response]}\n\n  checkpointer = InMemorySaver()\n  builder = StateGraph(MessagesState)\n  builder.add_node(call_model)\n  builder.add_edge(START, \"call_model\")\n  graph = builder.compile(checkpointer=checkpointer)\n\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n  graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n  graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n  graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n  final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\n  final_response[\"messages\"][-1].pretty_print()\n  ```\n\n  ```\n  ================================== Ai Message ==================================\n\n  Your name is Bob, as you mentioned when you first introduced yourself.\n  ```\n</Accordion>\n\n### Delete messages\n\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\n\nTo delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) [reducer](/oss/python/langgraph/graph-api#reducers), like [`MessagesState`](/oss/python/langgraph/graph-api#messagesstate).\n\nTo remove specific messages:\n\n```python  theme={null}\nfrom langchain.messages import RemoveMessage  # [!code highlight]\n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]\n```\n\nTo remove **all** messages:\n\n```python  theme={null}\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES  # [!code highlight]\n\ndef delete_messages(state):\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]\n```\n\n<Warning>\n  When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:\n\n  * Some providers expect message history to start with a `user` message\n  * Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.\n</Warning>\n\n<Accordion title=\"Full example: delete messages\">\n  ```python  theme={null}\n  from langchain.messages import RemoveMessage  # [!code highlight]\n\n  def delete_messages(state):\n      messages = state[\"messages\"]\n      if len(messages) > 2:\n          # remove the earliest two messages\n          return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]\n\n  def call_model(state: MessagesState):\n      response = model.invoke(state[\"messages\"])\n      return {\"messages\": response}\n\n  builder = StateGraph(MessagesState)\n  builder.add_sequence([call_model, delete_messages])\n  builder.add_edge(START, \"call_model\")\n\n  checkpointer = InMemorySaver()\n  app = builder.compile(checkpointer=checkpointer)\n\n  for event in app.stream(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n      config,\n      stream_mode=\"values\"\n  ):\n      print([(message.type, message.content) for message in event[\"messages\"]])\n\n  for event in app.stream(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n      config,\n      stream_mode=\"values\"\n  ):\n      print([(message.type, message.content) for message in event[\"messages\"]])\n  ```\n\n  ```\n  [('human', \"hi! I'm bob\")]\n  [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n  [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n  [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n  [('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n  ```\n</Accordion>\n\n### Summarize messages\n\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c8ed3facdccd4ef5c7e52902c72ba938\" alt=\"\" data-og-width=\"609\" width=\"609\" data-og-height=\"242\" height=\"242\" data-path=\"oss/images/summary.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4208b9b0cc9f459f3dc4e5219918471b 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=7acb77c081545f57042368f4e9d0c8cb 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2fcfdb0c481d2e1d361e76db763a41e5 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4abdac693a562788aa0db8681bef8ea7 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=40acfefa91dcb11b247a6e4a7705f22b 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d765aaf7551e8b0fc2720de7d2ac2a8 2500w\" />\n\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the [`MessagesState`](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) to include a `summary` key:\n\n```python  theme={null}\nfrom langgraph.graph import MessagesState\nclass State(MessagesState):\n    summary: str\n```\n\nThen, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarize_conversation` node can be called after some number of messages have accumulated in the `messages` state key.\n\n```python  theme={null}\ndef summarize_conversation(state: State):\n\n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt\n    if summary:\n\n        # A summary already exists\n        summary_message = (\n            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n\n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n\n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n```\n\n<Accordion title=\"Full example: summarize messages\">\n  ```python  theme={null}\n  from typing import Any, TypedDict\n\n  from langchain.chat_models import init_chat_model\n  from langchain.messages import AnyMessage\n  from langchain_core.messages.utils import count_tokens_approximately\n  from langgraph.graph import StateGraph, START, MessagesState\n  from langgraph.checkpoint.memory import InMemorySaver\n  from langmem.short_term import SummarizationNode, RunningSummary  # [!code highlight]\n\n  model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n  summarization_model = model.bind(max_tokens=128)\n\n  class State(MessagesState):\n      context: dict[str, RunningSummary]  # [!code highlight]\n\n  class LLMInputState(TypedDict):  # [!code highlight]\n      summarized_messages: list[AnyMessage]\n      context: dict[str, RunningSummary]\n\n  summarization_node = SummarizationNode(  # [!code highlight]\n      token_counter=count_tokens_approximately,\n      model=summarization_model,\n      max_tokens=256,\n      max_tokens_before_summary=256,\n      max_summary_tokens=128,\n  )\n\n  def call_model(state: LLMInputState):  # [!code highlight]\n      response = model.invoke(state[\"summarized_messages\"])\n      return {\"messages\": [response]}\n\n  checkpointer = InMemorySaver()\n  builder = StateGraph(State)\n  builder.add_node(call_model)\n  builder.add_node(\"summarize\", summarization_node)  # [!code highlight]\n  builder.add_edge(START, \"summarize\")\n  builder.add_edge(\"summarize\", \"call_model\")\n  graph = builder.compile(checkpointer=checkpointer)\n\n  # Invoke the graph\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n  graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n  graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n  graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n  final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\n  final_response[\"messages\"][-1].pretty_print()\n  print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\n  ```\n\n  1. We will keep track of our running summary in the `context` field\n\n  (expected by the `SummarizationNode`).\n\n  1. Define private state that will be used only for filtering\n\n  the inputs to `call_model` node.\n\n  1. We're passing a private input state here to isolate the messages returned by the summarization node\n\n  ```\n  ================================== Ai Message ==================================\n\n  From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n\n  Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n  ```\n</Accordion>\n\n### Manage checkpoints\n\nYou can view and delete the information stored by the checkpointer.\n\n<a id=\"checkpoint\" />\n\n#### View thread state\n\n<Tabs>\n  <Tab title=\"Graph/Functional API\">\n    ```python  theme={null}\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\",  # [!code highlight]\n            # optionally provide an ID for a specific checkpoint,\n            # otherwise the latest checkpoint is shown\n            # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  # [!code highlight]\n\n        }\n    }\n    graph.get_state(config)  # [!code highlight]\n    ```\n\n    ```\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        metadata={\n            'source': 'loop',\n            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n            'step': 4,\n            'parents': {},\n            'thread_id': '1'\n        },\n        created_at='2025-05-05T16:01:24.680462+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        tasks=(),\n        interrupts=()\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Checkpointer API\">\n    ```python  theme={null}\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\",  # [!code highlight]\n            # optionally provide an ID for a specific checkpoint,\n            # otherwise the latest checkpoint is shown\n            # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  # [!code highlight]\n\n        }\n    }\n    checkpointer.get_tuple(config)  # [!code highlight]\n    ```\n\n    ```\n    CheckpointTuple(\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        checkpoint={\n            'v': 3,\n            'ts': '2025-05-05T16:01:24.680462+00:00',\n            'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',\n            'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n            'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n        },\n        metadata={\n            'source': 'loop',\n            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n            'step': 4,\n            'parents': {},\n            'thread_id': '1'\n        },\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        pending_writes=[]\n    )\n    ```\n  </Tab>\n</Tabs>\n\n<a id=\"checkpoints\" />\n\n#### View the history of the thread\n\n<Tabs>\n  <Tab title=\"Graph/Functional API\">\n    ```python  theme={null}\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"  # [!code highlight]\n        }\n    }\n    list(graph.get_state_history(config))  # [!code highlight]\n    ```\n\n    ```\n    [\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n            next=(),\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:24.680462+00:00',\n            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n            tasks=(),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]},\n            next=('call_model',),\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:23.863421+00:00',\n            parent_config={...}\n            tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n            next=('__start__',),\n            config={...},\n            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:23.863173+00:00',\n            parent_config={...}\n            tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n            next=(),\n            config={...},\n            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:23.862295+00:00',\n            parent_config={...}\n            tasks=(),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\")]},\n            next=('call_model',),\n            config={...},\n            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:22.278960+00:00',\n            parent_config={...}\n            tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': []},\n            next=('__start__',),\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:22.277497+00:00',\n            parent_config=None,\n            tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),),\n            interrupts=()\n        )\n    ]\n    ```\n  </Tab>\n\n  <Tab title=\"Checkpointer API\">\n    ```python  theme={null}\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"  # [!code highlight]\n        }\n    }\n    list(checkpointer.list(config))  # [!code highlight]\n    ```\n\n    ```\n    [\n        CheckpointTuple(\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:24.680462+00:00',\n                'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',\n                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n                'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n            },\n            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\n            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n            pending_writes=[]\n        ),\n        CheckpointTuple(\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:23.863421+00:00',\n                'id': '1f029ca3-1790-6b0a-8003-baf965b6a38f',\n                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n                'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")], 'branch:to:call_model': None}\n            },\n            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n            parent_config={...},\n            pending_writes=[('8ab4155e-6b15-b885-9ce5-bed69a2c305c', 'messages', AIMessage(content='Your name is Bob.'))]\n        ),\n        CheckpointTuple(\n            config={...},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:23.863173+00:00',\n                'id': '1f029ca3-1790-616e-8002-9e021694a0cd',\n                'channel_versions': {'__start__': '00000000000000000000000000000004.0.5736472536395331', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\n                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}, 'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\n            },\n            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n            parent_config={...},\n            pending_writes=[('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'messages', [{'role': 'user', 'content': \"what's my name?\"}]), ('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'branch:to:call_model', None)]\n        ),\n        CheckpointTuple(\n            config={...},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:23.862295+00:00',\n                'id': '1f029ca3-178d-6f54-8001-d7b180db0c89',\n                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\n                'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\n            },\n            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\n            parent_config={...},\n            pending_writes=[]\n        ),\n        CheckpointTuple(\n            config={...},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:22.278960+00:00',\n                'id': '1f029ca3-0874-6612-8000-339f2abc83b1',\n                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000002.0.30296526818059655', 'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}},\n                'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\")], 'branch:to:call_model': None}\n            },\n            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n            parent_config={...},\n            pending_writes=[('8cbd75e0-3720-b056-04f7-71ac805140a0', 'messages', AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'))]\n        ),\n        CheckpointTuple(\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:22.277497+00:00',\n                'id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565',\n                'channel_versions': {'__start__': '00000000000000000000000000000001.0.7040775356287469'},\n                'versions_seen': {'__input__': {}},\n                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}\n            },\n            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n            parent_config=None,\n            pending_writes=[('d458367b-8265-812c-18e2-33001d199ce6', 'messages', [{'role': 'user', 'content': \"hi! I'm bob\"}]), ('d458367b-8265-812c-18e2-33001d199ce6', 'branch:to:call_model', None)]\n        )\n    ]\n    ```\n  </Tab>\n</Tabs>\n\n#### Delete all checkpoints for a thread\n\n```python  theme={null}\nthread_id = \"1\"\ncheckpointer.delete_thread(thread_id)\n```\n\n## Prebuilt memory tools\n\n**LangMem** is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the [LangMem documentation](https://langchain-ai.github.io/langmem/) for usage examples.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 58857
}