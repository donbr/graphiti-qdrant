{
  "title": "Trace with API",
  "source_url": "https://docs.langchain.com/langsmith/trace-with-api",
  "content": "Learn how to trace your LLM applications using the LangSmith API directly.\n\nIt is **highly** recommended to use our Python or TypeScript SDKs to send traces to LangSmith. We have designed these SDKs with optimizations like batching and backgrounding to ensure that your application's performance is not impacted by sending traces to LangSmith. However, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application. This guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation  for a full list of endpoints and request/response schemas.\n\n## Basic tracing\n\nThe simplest way to log runs is via the POST and PATCH `/runs` endpoint. These routes expect minimal contextual information about the tree structure to\n\n<Note>\n  When using the LangSmith REST API, you will need to provide your API key in the request headers as `\"x-api-key\"`.\n\n  If your API key is linked to multiple workspaces, you will need to specify the workspace being used in the header with `\"x-tenant-id\"`.\n\n  In the simple example, you do not need to set the `dotted_order` opr `trace_id` fields in the request body. These fields will be automatically generated by the system. Though this is simpler, it is slower and has a lower rate limit in LangSmith.\n</Note>\n\nThe following example shows how you might leverage our API directly in Python. The same principles apply to other languages.\n\n```python  theme={null}\nimport openai\nimport os\nimport requests\nfrom datetime import datetime, timezone\nfrom uuid import uuid4\n\n# Send your API Key in the request headers\nheaders = {\n    \"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"],\n    \"x-tenant-id\": os.environ[\"LANGSMITH_WORKSPACE_ID\"]\n}\n\ndef post_run(run_id, name, run_type, inputs, parent_id=None):\n    \"\"\"Function to post a new run to the API.\"\"\"\n    data = {\n        \"id\": run_id.hex,\n        \"name\": name,\n        \"run_type\": run_type,\n        \"inputs\": inputs,\n        \"start_time\": datetime.utcnow().isoformat(),\n        # \"session_name\": \"project-name\",  # the name of the project to trace to\n        # \"session_id\": \"project-id\",  # the ID of the project to trace to. specify one of session_name or session_id\n    }\n    if parent_id:\n        data[\"parent_run_id\"] = parent_id.hex\n\n    requests.post(\n        \"https://api.smith.langchain.com/runs\",  # Update appropriately for self-hosted installations or the EU region\n        json=data,\n        headers=headers\n    )\n\ndef patch_run(run_id, outputs):\n    \"\"\"Function to patch a run with outputs.\"\"\"\n    requests.patch(\n        f\"https://api.smith.langchain.com/runs/{run_id}\",\n        json={\n            \"outputs\": outputs,\n            \"end_time\": datetime.now(timezone.utc).isoformat(),\n        },\n        headers=headers,\n    )\n\n# This can be a user input to your app\nquestion = \"Can you summarize this morning's meetings?\"\n\n# This can be retrieved in a retrieval step\ncontext = \"During this morning's meeting, we solved all world conflict.\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"},\n    {\"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n]\n\n# Create parent run\nparent_run_id = uuid4()\npost_run(parent_run_id, \"Chat Pipeline\", \"chain\", {\"question\": question})\n\n# Create child run\nchild_run_id = uuid4()\npost_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)\n\n# Generate a completion\nclient = openai.Client()\nchat_completion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages\n)\n\n# End runs\npatch_run(child_run_id, chat_completion.dict())\npatch_run(parent_run_id, {\"answer\": chat_completion.choices[0].message.content})\n```\n\nSee the doc on the [Run (span) data format](/langsmith/run-data-format) for more information.\n\n## Batch Ingestion\n\nFor faster ingestion of runs and higher rate limits, you can use the POST `/runs/multipart` [link](https://api.smith.langchain.com/redoc#tag/run/operation/multipart_ingest_runs_api_v1_runs_multipart_post) endpoint. Below is an example. It requires `orjson` (for fast json ) and `requests_toolbelt` to run\n\n```python  theme={null}\nimport json\nimport os\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Dict, List\nimport requests\nfrom requests_toolbelt import MultipartEncoder\n\ndef create_dotted_order(\n    start_time: datetime | None = None,\n    run_id: uuid.UUID | None = None\n) -> str:\n    \"\"\"Create a dotted order string for run ordering and hierarchy.\n\n    The dotted order is used to establish the sequence and relationships between runs.\n    It combines a timestamp with a unique identifier to ensure proper ordering and tracing.\n    \"\"\"\n    st = start_time or datetime.now(timezone.utc)\n    id_ = run_id or uuid.uuid4()\n    return f\"{st.strftime('%Y%m%dT%H%M%S%fZ')}{id_}\"\n\ndef create_run_base(\n    name: str,\n    run_type: str,\n    inputs: dict,\n    start_time: datetime\n) -> dict:\n    \"\"\"Create the base structure for a run.\"\"\"\n    run_id = uuid.uuid4()\n    return {\n        \"id\": str(run_id),\n        \"trace_id\": str(run_id),\n        \"name\": name,\n        \"start_time\": start_time.isoformat(),\n        \"inputs\": inputs,\n        \"run_type\": run_type,\n    }\n\ndef construct_run(\n    name: str,\n    run_type: str,\n    inputs: dict,\n    parent_dotted_order: str | None = None,\n) -> dict:\n    \"\"\"Construct a run dictionary with the given parameters.\n\n    This function creates a run with a unique ID and dotted order, establishing its place\n    in the trace hierarchy if it's a child run.\n    \"\"\"\n    start_time = datetime.now(timezone.utc)\n    run = create_run_base(name, run_type, inputs, start_time)\n    current_dotted_order = create_dotted_order(start_time, uuid.UUID(run[\"id\"]))\n\n    if parent_dotted_order:\n        current_dotted_order = f\"{parent_dotted_order}.{current_dotted_order}\"\n        run[\"trace_id\"] = parent_dotted_order.split(\".\")[0].split(\"Z\")[1]\n        run[\"parent_run_id\"] = parent_dotted_order.split(\".\")[-1].split(\"Z\")[1]\n\n    run[\"dotted_order\"] = current_dotted_order\n    return run\n\ndef serialize_run(operation: str, run_data: dict) -> List[tuple]:\n    \"\"\"Serialize a run for the multipart request.\n\n    This function separates the run data into parts for efficient transmission and storage.\n    The main run data and optional fields (inputs, outputs, events) are serialized separately.\n    \"\"\"\n    run_id = run_data.get(\"id\", str(uuid.uuid4()))\n\n    # Separate optional fields\n    inputs = run_data.pop(\"inputs\", None)\n    outputs = run_data.pop(\"outputs\", None)\n    events = run_data.pop(\"events\", None)\n\n    parts = []\n\n    # Serialize main run data\n    run_data_json = json.dumps(run_data).encode(\"utf-8\")\n    parts.append(\n        (\n            f\"{operation}.{run_id}\",\n            (\n                None,\n                run_data_json,\n                \"application/json\",\n                {\"Content-Length\": str(len(run_data_json))},\n            ),\n        )\n    )\n\n    # Serialize optional fields\n    for key, value in [(\"inputs\", inputs), (\"outputs\", outputs), (\"events\", events)]:\n        if value:\n            serialized_value = json.dumps(value).encode(\"utf-8\")\n            parts.append(\n                (\n                    f\"{operation}.{run_id}.{key}\",\n                    (\n                        None,\n                        serialized_value,\n                        \"application/json\",\n                        {\"Content-Length\": str(len(serialized_value))},\n                    ),\n                )\n            )\n\n    return parts\n\ndef batch_ingest_runs(\n    api_url: str,\n    api_key: str,\n    posts: list[dict] | None = None,\n    patches: list[dict] | None = None,\n) -> None:\n    \"\"\"Ingest multiple runs in a single batch request.\n\n    This function handles both creating new runs (posts) and updating existing runs (patches).\n    It's more efficient for ingesting multiple runs compared to individual API calls.\n    \"\"\"\n    boundary = uuid.uuid4().hex\n    all_parts = []\n\n    for operation, runs in zip((\"post\", \"patch\"), (posts, patches)):\n        if runs:\n            all_parts.extend(\n                [part for run in runs for part in serialize_run(operation, run)]\n            )\n\n    encoder = MultipartEncoder(fields=all_parts, boundary=boundary)\n    headers = {\"Content-Type\": encoder.content_type, \"x-api-key\": api_key}\n\n    try:\n        response = requests.post(\n            f\"{api_url}/runs/multipart\",\n            data=encoder,\n            headers=headers\n        )\n        response.raise_for_status()\n        print(\"Successfully ingested runs.\")\n    except requests.RequestException as e:\n        print(f\"Error ingesting runs: {e}\")\n        # In a production environment, you might want to log this error or handle it more robustly\n\n# Configure API URL and key\n# For production use, consider using a configuration file or environment variables\napi_url = \"https://api.smith.langchain.com\"\napi_key = os.environ.get(\"LANGSMITH_API_KEY\")\n\nif not api_key:\n    raise ValueError(\"LANGSMITH_API_KEY environment variable is not set\")\n\n# Create a parent run\nparent_run = construct_run(\n    name=\"Parent Run\",\n    run_type=\"chain\",\n    inputs={\"main_question\": \"Tell me about France\"},\n)\n\n# Create a child run, linked to the parent\nchild_run = construct_run(\n    name=\"Child Run\",\n    run_type=\"llm\",\n    inputs={\"question\": \"What is the capital of France?\"},\n    parent_dotted_order=parent_run[\"dotted_order\"],\n)\n\n# First, post the runs to create them\nposts = [parent_run, child_run]\nbatch_ingest_runs(api_url, api_key, posts=posts)\n\n# Then, update the runs with their end times and any outputs\nchild_run_update = {\n    **child_run,\n    \"end_time\": datetime.now(timezone.utc).isoformat(),\n    \"outputs\": {\"answer\": \"Paris is the capital of France.\"},\n}\n\nparent_run_update = {\n    **parent_run,\n    \"end_time\": datetime.now(timezone.utc).isoformat(),\n    \"outputs\": {\"summary\": \"Discussion about France, including its capital.\"},\n}\n\npatches = [parent_run_update, child_run_update]\nbatch_ingest_runs(api_url, api_key, patches=patches)\n\n# Note: This example requires the `requests` and `requests_toolbelt` libraries.\n# You can install them using pip:\n# pip install requests requests_toolbelt\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-api.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 10677
}