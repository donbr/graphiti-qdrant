{
  "title": "How to use the REST API",
  "source_url": "https://docs.langchain.com/langsmith/run-evals-api-only",
  "content": "The [Python](https://reference.langchain.com/python/langsmith/) and [TypeScript](https://reference.langchain.com/javascript/modules/langsmith.html) SDKs are the recommended way to run [evaluations](/langsmith/evaluation-concepts) in LangSmith. They include optimizations and features that enhance performance and reliability.\n\nIf you cannot use the SDKs—for example, if you are working in a different language or a restricted environment—you can use the REST API directly. This guide demonstrates how to run evaluations using the [REST API](https://api.smith.langchain.com/redoc) with Python's [`requests`](https://requests.readthedocs.io/) library, but the same principles apply to any language.\n\nBefore diving into this content, it might be helpful to read the following:\n\n* [Evaluate LLM applications](/langsmith/evaluate-llm-application).\n* [LangSmith API Reference](https://api.smith.langchain.com/redoc): Complete API documentation for all endpoints used in this guide.\n\n## Create a dataset\n\nFor this example, we use the Python SDK to create a [dataset](/langsmith/evaluation-concepts#datasets) quickly. To create datasets via the API or UI instead, refer to [Managing datasets](/langsmith/manage-datasets-in-application).\n\n```python  theme={null}\nimport os\nimport requests\n\nfrom datetime import datetime\nfrom langsmith import Client\nfrom openai import OpenAI\nfrom uuid import uuid4\n\nclient = Client()\noa_client = OpenAI()\n\n#  Create a dataset\nexamples = [\n    {\n        \"inputs\": {\"text\": \"Shut up, idiot\"},\n        \"outputs\": {\"label\": \"Toxic\"},\n    },\n    {\n        \"inputs\": {\"text\": \"You're a wonderful person\"},\n        \"outputs\": {\"label\": \"Not toxic\"},\n    },\n    {\n        \"inputs\": {\"text\": \"This is the worst thing ever\"},\n        \"outputs\": {\"label\": \"Toxic\"},\n    },\n    {\n        \"inputs\": {\"text\": \"I had a great day today\"},\n        \"outputs\": {\"label\": \"Not toxic\"},\n    },\n    {\n        \"inputs\": {\"text\": \"Nobody likes you\"},\n        \"outputs\": {\"label\": \"Toxic\"},\n    },\n    {\n        \"inputs\": {\"text\": \"This is unacceptable. I want to speak to the manager.\"},\n        \"outputs\": {\"label\": \"Not toxic\"},\n    },\n]\n\ndataset_name = \"Toxic Queries - API Example\"\ndataset = client.create_dataset(dataset_name=dataset_name)\nclient.create_examples(dataset_id=dataset.id, examples=examples)\n```\n\n## Run a single experiment\n\nTo run an experiment via the API, you'll need to:\n\n1. Fetch the examples from your dataset.\n2. Create an experiment (also called a \"session\" in the API).\n3. For each example, create runs that reference both the example and the experiment.\n4. Close the experiment by setting its `end_time`.\n\nFirst, pull all of the examples you'd want to use in your experiment using the `/examples` endpoint:\n\n```python  theme={null}\n#  Pick a dataset id. In this case, we are using the dataset we created above.\n#  API Reference: https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get\ndataset_id = dataset.id\nparams = { \"dataset\": dataset_id }\n\nresp = requests.get(\n    \"https://api.smith.langchain.com/api/v1/examples\",\n    params=params,\n    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\n\nexamples = resp.json()\n```\n\nNext, define a function that will run your model on a single example and log the results to LangSmith. When using the API directly, you're responsible for:\n\n* Creating run objects via POST to `/runs` with `reference_example_id` and `session_id` set.\n* Tracking parent-child relationships between runs (e.g., a parent \"chain\" run containing a child \"llm\" run).\n* Updating runs with outputs via PATCH to `/runs/{run_id}`.\n\n```python  theme={null}\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\ndef run_completion_on_example(example, model_name, experiment_id):\n    \"\"\"Run completions on a list of examples.\"\"\"\n    # We are using the OpenAI API here, but you can use any model you like\n\n    def _post_run(run_id, name, run_type, inputs, parent_id=None):\n        \"\"\"Function to post a new run to the API.\n        API Reference: https://api.smith.langchain.com/redoc#tag/run/operation/create_run_api_v1_runs_post\n        \"\"\"\n        data = {\n            \"id\": run_id.hex,\n            \"name\": name,\n            \"run_type\": run_type,\n            \"inputs\": inputs,\n            \"start_time\": datetime.utcnow().isoformat(),\n            \"reference_example_id\": example[\"id\"],\n            \"session_id\": experiment_id,\n        }\n        if parent_id:\n            data[\"parent_run_id\"] = parent_id.hex\n        resp = requests.post(\n            \"https://api.smith.langchain.com/api/v1/runs\", # Update appropriately for self-hosted installations or the EU region\n            json=data,\n            headers=headers\n        )\n        resp.raise_for_status()\n\n    def _patch_run(run_id, outputs):\n        \"\"\"Function to patch a run with outputs.\n        API Reference: https://api.smith.langchain.com/redoc#tag/run/operation/update_run_api_v1_runs__run_id__patch\n        \"\"\"\n        resp = requests.patch(\n            f\"https://api.smith.langchain.com/api/v1/runs/{run_id}\",\n            json={\n                \"outputs\": outputs,\n                \"end_time\": datetime.utcnow().isoformat(),\n            },\n            headers=headers,\n        )\n        resp.raise_for_status()\n\n    # Send your API Key in the request headers\n    headers = {\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n\n    text = example[\"inputs\"][\"text\"]\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",\n        },\n        {\"role\": \"user\", \"content\": text},\n    ]\n\n\n    # Create parent run\n    parent_run_id = uuid4()\n    _post_run(parent_run_id, \"LLM Pipeline\", \"chain\", {\"text\": text})\n\n    # Create child run\n    child_run_id = uuid4()\n    _post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)\n\n    # Generate completion\n    chat_completion = oa_client.chat.completions.create(model=model_name, messages=messages)\n    output_text = chat_completion.choices[0].message.content\n\n    # End run\n    _patch_run(child_run_id, {\n    \"messages\": messages,\n        \"output\": output_text,\n        \"model\": model_name\n    })\n\n    _patch_run(parent_run_id, {\"label\": output_text})\n```\n\nNow create the experiments and run completions on all examples. In the API, an \"experiment\" is represented as a session (or \"tracer session\") that references a dataset via `reference_dataset_id`. The key difference from regular tracing is that runs in an experiment must have a `reference_example_id` that links each run to a specific example in the dataset.\n\n```python  theme={null}\n#  Create a new experiment using the /sessions endpoint\n#  An experiment is a collection of runs with a reference to the dataset used\n#  API Reference: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_post\n\nmodel_names = (\"gpt-3.5-turbo\", \"gpt-4o-mini\")\nexperiment_ids = []\nfor model_name in model_names:\n    resp = requests.post(\n        \"https://api.smith.langchain.com/api/v1/sessions\",\n        json={\n            \"start_time\": datetime.utcnow().isoformat(),\n            \"reference_dataset_id\": str(dataset_id),\n            \"description\": \"An optional description for the experiment\",\n            \"name\": f\"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}\",  # A name for the experiment\n            \"extra\": {\n                \"metadata\": {\"foo\": \"bar\"},  # Optional metadata\n            },\n        },\n        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n    )\n\n    experiment = resp.json()\n    experiment_ids.append(experiment[\"id\"])\n\n    # Run completions on all examples\n    for example in examples:\n        run_completion_on_example(example, model_name, experiment[\"id\"])\n\n    # Issue a patch request to \"end\" the experiment by updating the end_time\n    requests.patch(\n        f\"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}\",\n        json={\"end_time\": datetime.utcnow().isoformat()},\n        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n    )\n```\n\n### Add evaluation feedback\n\nAfter running your [experiments](/langsmith/evaluation-concepts#experiment), you'll typically want to evaluate the results by adding feedback scores. This allows you to track metrics like correctness, accuracy, or any custom evaluation criteria.\n\nIn this example, the evaluation checks if each model's output matches the expected label in the dataset. The code posts a \"correctness\" score (1.0 for correct, 0.0 for incorrect) to track how accurately each model classifies toxic vs. non-toxic text.\n\nThe following code adds feedback to the runs from the [single experiment example](#run-a-single-experiment):\n\n```python  theme={null}\n# Fetch the runs from one of the experiments\n# API Reference: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post\nexperiment_id = experiment_ids[0]  # Evaluate the first experiment\n\nruns_resp = requests.post(\n    \"https://api.smith.langchain.com/api/v1/runs/query\",\n    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]},\n    json={\n        \"session\": [experiment_id],\n        \"is_root\": True,  # Only fetch root runs\n        \"select\": [\"id\", \"reference_example_id\", \"outputs\"],\n    }\n)\n\nruns = runs_resp.json()[\"runs\"]\n\n# Evaluate each run by comparing outputs to expected values\nfor run in runs:\n    # Get the expected output from the original example\n    example_id = run[\"reference_example_id\"]\n    expected_output = next(\n        ex[\"outputs\"][\"label\"]\n        for ex in examples\n        if ex[\"id\"] == example_id\n    )\n\n    # Compare the model output to the expected output\n    actual_output = run[\"outputs\"].get(\"label\", \"\")\n    is_correct = expected_output.lower() == actual_output.lower()\n\n    # Post feedback score\n    # API Reference: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post\n    feedback = {\n        \"run_id\": str(run[\"id\"]),\n        \"key\": \"correctness\",  # The name of your evaluation metric\n        \"score\": 1.0 if is_correct else 0.0,\n        \"comment\": f\"Expected: {expected_output}, Got: {actual_output}\",  # Optional\n    }\n\n    resp = requests.post(\n        \"https://api.smith.langchain.com/api/v1/feedback\",\n        json=feedback,\n        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n    )\n    resp.raise_for_status()\n```\n\nYou can add multiple feedback scores with different keys to track various metrics. For example, you might add both a \"correctness\" score and a \"toxicity\\_detected\" score.\n\n## Run a pairwise experiment\n\nNext, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.\n\nFor more information, check out [this guide](/langsmith/evaluate-pairwise).\n\n```python  theme={null}\n#  A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments\n#  API Reference: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_post\nresp = requests.post(\n    \"https://api.smith.langchain.com/api/v1/datasets/comparative\",\n    json={\n        \"experiment_ids\": experiment_ids,\n        \"name\": \"Toxicity detection - API Example - Comparative - \" + str(uuid4())[0:8],\n        \"description\": \"An optional description for the comparative experiment\",\n        \"extra\": {\n            \"metadata\": {\"foo\": \"bar\"},  # Optional metadata\n        },\n        \"reference_dataset_id\": str(dataset_id),\n    },\n    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\n\ncomparative_experiment = resp.json()\ncomparative_experiment_id = comparative_experiment[\"id\"]\n\n#  You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs\n\n#  Fetch the comparative experiment\nresp = requests.get(\n    f\"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative\",\n    params={\"id\": comparative_experiment_id},\n    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\n\ncomparative_experiment = resp.json()[0]\nexperiment_ids = [info[\"id\"] for info in comparative_experiment[\"experiments_info\"]]\n\nfrom collections import defaultdict\nexample_id_to_runs_map = defaultdict(list)\n\n#  API Reference: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post\nruns = requests.post(\n    f\"https://api.smith.langchain.com/api/v1/runs/query\",\n    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]},\n    json={\n        \"session\": experiment_ids,\n        \"is_root\": True, # Only fetch root runs (spans) which contain the end outputs\n        \"select\": [\"id\", \"reference_example_id\", \"outputs\"],\n    }\n).json()\nruns = runs[\"runs\"]\nfor run in runs:\n    example_id = run[\"reference_example_id\"]\n    example_id_to_runs_map[example_id].append(run)\n\nfor example_id, runs in example_id_to_runs_map.items():\n    print(f\"Example ID: {example_id}\")\n    # Preferentially rank the outputs, in this case we will always prefer the first output\n    # In reality, you can use an LLM to rank the outputs\n    feedback_group_id = uuid4()\n\n    # Post a feedback score for each run, with the first run being the preferred one\n    # API Reference: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post\n    # We'll use the feedback group ID to associate the feedback scores with the same group\n    for i, run in enumerate(runs):\n        print(f\"Run ID: {run['id']}\")\n        feedback = {\n            \"score\": 1 if i == 0 else 0,\n            \"run_id\": str(run[\"id\"]),\n            \"key\": \"ranked_preference\",\n            \"feedback_group_id\": str(feedback_group_id),\n            \"comparative_experiment_id\": comparative_experiment_id,\n        }\n        resp = requests.post(\n            \"https://api.smith.langchain.com/api/v1/feedback\",\n            json=feedback,\n            headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n        )\n        resp.raise_for_status()\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-evals-api-only.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 14559
}