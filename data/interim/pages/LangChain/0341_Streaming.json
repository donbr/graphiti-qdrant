{
  "title": "Streaming",
  "source_url": "https://docs.langchain.com/oss/javascript/langchain/streaming",
  "content": "LangChain implements a streaming system to surface real-time updates.\n\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\n## Overview\n\nLangChain's streaming system lets you surface live feedback from agent runs to your application.\n\nWhat's possible with LangChain streaming:\n\n* <Icon icon=\"brain\" size={16} /> [**Stream agent progress**](#agent-progress) — get state updates after each agent step.\n* <Icon icon=\"square-binary\" size={16} /> [**Stream LLM tokens**](#llm-tokens) — stream language model tokens as they're generated.\n* <Icon icon=\"table\" size={16} /> [**Stream custom updates**](#custom-updates) — emit user-defined signals (e.g., `\"Fetched 10/100 records\"`).\n* <Icon icon=\"layer-plus\" size={16} /> [**Stream multiple modes**](#stream-multiple-modes) — choose from `updates` (agent progress), `messages` (LLM tokens + metadata), or `custom` (arbitrary user data).\n\n## Agent progress\n\nTo stream agent progress, use the [`stream`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.CompiledStateGraph.html#stream) method with `streamMode: \"updates\"`. This emits an event after every agent step.\n\nFor example, if you have an agent that calls a tool once, you should see the following updates:\n\n* **LLM node**: [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html) with tool call requests\n* **Tool node**: [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) with execution result\n* **LLM node**: Final AI response\n\n```typescript  theme={null}\nimport z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n    async ({ city }) => {\n        return `The weather in ${city} is always sunny!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n        city: z.string(),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-5-nano\",\n    tools: [getWeather],\n});\n\nfor await (const chunk of await agent.stream(\n    { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n    { streamMode: \"updates\" }\n)) {\n    const [step, content] = Object.entries(chunk)[0];\n    console.log(`step: ${step}`);\n    console.log(`content: ${JSON.stringify(content, null, 2)}`);\n}\n/**\n * step: model\n * content: {\n *   \"messages\": [\n *     {\n *       \"kwargs\": {\n *         // ...\n *         \"tool_calls\": [\n *           {\n *             \"name\": \"get_weather\",\n *             \"args\": {\n *               \"city\": \"San Francisco\"\n *             },\n *             \"type\": \"tool_call\",\n *             \"id\": \"call_0qLS2Jp3MCmaKJ5MAYtr4jJd\"\n *           }\n *         ],\n *         // ...\n *       }\n *     }\n *   ]\n * }\n * step: tools\n * content: {\n *   \"messages\": [\n *     {\n *       \"kwargs\": {\n *         \"content\": \"The weather in San Francisco is always sunny!\",\n *         \"name\": \"get_weather\",\n *         // ...\n *       }\n *     }\n *   ]\n * }\n * step: model\n * content: {\n *   \"messages\": [\n *     {\n *       \"kwargs\": {\n *         \"content\": \"The latest update says: The weather in San Francisco is always sunny!\\n\\nIf you'd like real-time details (current temperature, humidity, wind, and today's forecast), I can pull the latest data for you. Want me to fetch that?\",\n *         // ...\n *       }\n *     }\n *   ]\n * }\n */\n```\n\n## LLM tokens\n\nTo stream tokens as they are produced by the LLM, use `streamMode: \"messages\"`:\n\n```typescript  theme={null}\nimport z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n    async ({ city }) => {\n        return `The weather in ${city} is always sunny!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n        city: z.string(),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-4o-mini\",\n    tools: [getWeather],\n});\n\nfor await (const [token, metadata] of await agent.stream(\n    { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n    { streamMode: \"messages\" }\n)) {\n    console.log(`node: ${metadata.langgraph_node}`);\n    console.log(`content: ${JSON.stringify(token.contentBlocks, null, 2)}`);\n}\n```\n\n## Custom updates\n\nTo stream updates from tools as they are executed, you can use the `writer` parameter from the configuration.\n\n```typescript  theme={null}\nimport z from \"zod\";\nimport { tool, createAgent } from \"langchain\";\nimport { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst getWeather = tool(\n    async (input, config: LangGraphRunnableConfig) => {\n        // Stream any arbitrary data\n        config.writer?.(`Looking up data for city: ${input.city}`);\n        // ... fetch city data\n        config.writer?.(`Acquired data for city: ${input.city}`);\n        return `It's always sunny in ${input.city}!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n        city: z.string().describe(\"The city to get weather for.\"),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-4o-mini\",\n    tools: [getWeather],\n});\n\nfor await (const chunk of await agent.stream(\n    { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n    { streamMode: \"custom\" }\n)) {\n    console.log(chunk);\n}\n```\n\n```shell title=\"Output\" theme={null}\nLooking up data for city: San Francisco\nAcquired data for city: San Francisco\n```\n\n<Note>\n  If you add the `writer` parameter to your tool, you won't be able to invoke the tool outside of a LangGraph execution context without providing a writer function.\n</Note>\n\n## Stream multiple modes\n\nYou can specify multiple streaming modes by passing streamMode as an array: `streamMode: [\"updates\", \"messages\", \"custom\"]`:\n\n```typescript  theme={null}\nimport z from \"zod\";\nimport { tool, createAgent } from \"langchain\";\nimport { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst getWeather = tool(\n    async (input, config: LangGraphRunnableConfig) => {\n        // Stream any arbitrary data\n        config.writer?.(`Looking up data for city: ${input.city}`);\n        // ... fetch city data\n        config.writer?.(`Acquired data for city: ${input.city}`);\n        return `It's always sunny in ${input.city}!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n        city: z.string().describe(\"The city to get weather for.\"),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-4o-mini\",\n    tools: [getWeather],\n});\n\nfor await (const [streamMode, chunk] of await agent.stream(\n    { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n    { streamMode: [\"updates\", \"messages\", \"custom\"] }\n)) {\n    console.log(`${streamMode}: ${JSON.stringify(chunk, null, 2)}`);\n}\n```\n\n## Disable streaming\n\nIn some applications you might need to disable streaming of individual tokens for a given model.\n\nThis is useful in [multi-agent](/oss/javascript/langchain/multi-agent) systems to control which agents stream their output.\n\nSee the [Models](/oss/javascript/langchain/models#disable-streaming) guide to learn how to disable streaming.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/streaming.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 7808
}