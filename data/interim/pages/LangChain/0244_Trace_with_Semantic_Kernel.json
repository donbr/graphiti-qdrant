{
  "title": "Trace with Semantic Kernel",
  "source_url": "https://docs.langchain.com/langsmith/trace-with-semantic-kernel",
  "content": "LangSmith can capture traces generated by [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) using OpenInference's OpenAI instrumentation. This guide shows you how to automatically capture traces from your Semantic Kernel applications and send them to LangSmith for monitoring and analysis.\n\n## Installation\n\nInstall the required packages using your preferred package manager:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langsmith semantic-kernel openinference-instrumentation-openai\n  ```\n\n  ```bash uv theme={null}\n  uv add langsmith semantic-kernel openinference-instrumentation-openai\n  ```\n</CodeGroup>\n\n<Info>\n  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.\n</Info>\n\n## Setup\n\n### 1. Configure environment variables\n\nSet your API keys and project name:\n\n<CodeGroup>\n  ```bash Shell theme={null}\n  export LANGSMITH_API_KEY=<your_langsmith_api_key>\n  export LANGSMITH_PROJECT=<your_project_name>\n  export OPENAI_API_KEY=<your_openai_api_key>\n  ```\n</CodeGroup>\n\n### 2. Configure OpenTelemetry integration\n\nIn your Semantic Kernel application, import and configure the LangSmith OpenTelemetry integration along with the OpenAI instrumentor:\n\n```python  theme={null}\nfrom langsmith.integrations.otel import configure\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\n# Configure LangSmith tracing\nconfigure(project_name=\"semantic-kernel-demo\")\n\n# Instrument OpenAI calls\nOpenAIInstrumentor().instrument()\n```\n\n<Note>\n  You do not need to set any OpenTelemetry environment variables or configure exporters manuallyâ€”`configure()` handles everything automatically.\n</Note>\n\n### 3. Create and run your Semantic Kernel application\n\nOnce configured, your Semantic Kernel application will automatically send traces to LangSmith:\n\nThis example includes a minimal app that configures the kernel, defines prompt-based functions, and invokes them to generate traced activity.\n\n```python  theme={null}\nimport os\nimport asyncio\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\nfrom semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom langsmith.integrations.otel import configure\nimport dotenv\n\n# Load environment variables\ndotenv.load_dotenv(\".env.local\")\n\n# Configure LangSmith tracing\nconfigure(project_name=\"semantic-kernel-assistant\")\n\n# Instrument OpenAI calls\nOpenAIInstrumentor().instrument()\n\n# Configure Semantic Kernel\nkernel = Kernel()\nkernel.add_service(OpenAIChatCompletion())\n\n# Create a code analysis prompt template\ncode_analysis_prompt = \"\"\"\nAnalyze the following code and provide insights:\n\nCode: {{$code}}\n\nPlease provide:\n1. A brief summary of what the code does\n2. Any potential improvements\n3. Code quality assessment\n\"\"\"\n\nprompt_template_config = PromptTemplateConfig(\n    template=code_analysis_prompt,\n    name=\"code_analyzer\",\n    template_format=\"semantic-kernel\",\n    input_variables=[\n        InputVariable(name=\"code\", description=\"The code to analyze\", is_required=True),\n    ],\n)\n\n# Add the function to the kernel\ncode_analyzer = kernel.add_function(\n    function_name=\"analyzeCode\",\n    plugin_name=\"codeAnalysisPlugin\",\n    prompt_template_config=prompt_template_config,\n)\n\n# Create a documentation generator\ndoc_prompt = \"\"\"\nGenerate comprehensive documentation for the following function:\n\n{{$function_code}}\n\nInclude:\n- Purpose and functionality\n- Parameters and return values\n- Usage examples\n- Any important notes\n\"\"\"\n\ndoc_template_config = PromptTemplateConfig(\n    template=doc_prompt,\n    name=\"doc_generator\",\n    template_format=\"semantic-kernel\",\n    input_variables=[\n        InputVariable(name=\"function_code\", description=\"The function code to document\", is_required=True),\n    ],\n)\n\ndoc_generator = kernel.add_function(\n    function_name=\"generateDocs\",\n    plugin_name=\"documentationPlugin\",\n    prompt_template_config=doc_template_config,\n)\n\nasync def main():\n    # Example code to analyze\n    sample_code = \"\"\"\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n    \"\"\"\n\n    # Analyze the code\n    analysis_result = await kernel.invoke(code_analyzer, code=sample_code)\n    print(\"Code Analysis:\")\n    print(analysis_result)\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n    # Generate documentation\n    doc_result = await kernel.invoke(doc_generator, function_code=sample_code)\n    print(\"Generated Documentation:\")\n    print(doc_result)\n\n    return {\"analysis\": str(analysis_result), \"documentation\": str(doc_result)}\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Advanced usage\n\n### Custom metadata and tags\n\nYou can add custom metadata to your traces by setting span attributes:\n\n```python  theme={null}\nfrom opentelemetry import trace\n\n# Get the current tracer\ntracer = trace.get_tracer(__name__)\n\nasync def main():\n    with tracer.start_as_current_span(\"semantic_kernel_workflow\") as span:\n        # Add custom metadata\n        span.set_attribute(\"langsmith.metadata.workflow_type\", \"code_analysis\")\n        span.set_attribute(\"langsmith.metadata.user_id\", \"developer_123\")\n        span.set_attribute(\"langsmith.span.tags\", \"semantic-kernel,code-analysis\")\n\n        # Your Semantic Kernel code here\n        result = await kernel.invoke(code_analyzer, code=sample_code)\n        return result\n```\n\n### Combining with other instrumentors\n\nYou can combine Semantic Kernel instrumentation with other instrumentors (e.g., DSPy, AutoGen) by adding them and initializing them as instrumentors:\n\n```python  theme={null}\nfrom langsmith.integrations.otel import configure\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom openinference.instrumentation.dspy import DSPyInstrumentor\n\n# Configure LangSmith tracing\nconfigure(project_name=\"multi-framework-app\")\n\n# Initialize multiple instrumentors\nOpenAIInstrumentor().instrument()\nDSPyInstrumentor().instrument()\n\n# Your application code using multiple frameworks\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-semantic-kernel.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 6442
}