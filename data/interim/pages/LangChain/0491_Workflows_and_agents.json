{
  "title": "Workflows and agents",
  "source_url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
  "content": "This guide reviews common workflow and agent patterns.\n\n* Workflows have predetermined code paths and are designed to operate in a certain order.\n* Agents are dynamic and define their own processes and tool usage.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c217c9ef517ee556cae3fc928a21dc55\" alt=\"Agent Workflow\" data-og-width=\"4572\" width=\"4572\" data-og-height=\"2047\" height=\"2047\" data-path=\"oss/images/agent_workflow.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=290e50cff2f72d524a107421ec8e3ff0 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a2bfc87080aee7dd4844f7f24035825e 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ae1fa9087b33b9ff8bc3446ccaa23e3d 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=06003ee1fe07d7a1ea8cf9200e7d0a10 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bc98b459a9b1fb226c2887de1696bde0 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=1933bcdfd5c5b69b98ce96aafa456848 2500w\" />\n\nLangGraph offers several benefits when building agents and workflows, including [persistence](/oss/python/langgraph/persistence), [streaming](/oss/python/langgraph/streaming), and support for debugging as well as [deployment](/oss/python/langgraph/deploy).\n\n## Setup\n\nTo build a workflow or agent, you can use [any chat model](/oss/python/integrations/chat) that supports structured outputs and tool calling. The following example uses Anthropic:\n\n1. Install dependencies:\n\n```bash  theme={null}\npip install langchain_core langchain-anthropic langgraph\n```\n\n2. Initialize the LLM:\n\n```python  theme={null}\nimport os\nimport getpass\n\nfrom langchain_anthropic import ChatAnthropic\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n```\n\n## LLMs and augmentations\n\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. [Tool calling](/oss/python/langchain/tools), [structured outputs](/oss/python/langchain/structured-output), and [short term memory](/oss/python/langchain/short-term-memory) are a few options for tailoring LLMs to your needs.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7ea9656f46649b3ebac19e8309ae9006\" alt=\"LLM augmentations\" data-og-width=\"1152\" width=\"1152\" data-og-height=\"778\" height=\"778\" data-path=\"oss/images/augmented_llm.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=53613048c1b8bd3241bd27900a872ead 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7ba1f4427fd847bd410541ae38d66d40 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=503822cf29a28500deb56f463b4244e4 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=279e0440278d3a26b73c72695636272e 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d936838b98bc9dce25168e2b2cfd23d0 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=fa2115f972bc1152b5e03ae590600fa3 2500w\" />\n\n```python  theme={null}\n# Schema for structured output\nfrom pydantic import BaseModel, Field\n\n\nclass SearchQuery(BaseModel):\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n    justification: str = Field(\n        None, description=\"Why this query is relevant to the user's request.\"\n    )\n\n\n# Augment the LLM with schema for structured output\nstructured_llm = llm.with_structured_output(SearchQuery)\n\n# Invoke the augmented LLM\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n\n# Define a tool\ndef multiply(a: int, b: int) -> int:\n    return a * b\n\n# Augment the LLM with tools\nllm_with_tools = llm.bind_tools([multiply])\n\n# Invoke the LLM with input that triggers the tool call\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\n\n# Get the tool call\nmsg.tool_calls\n```\n\n## Prompt chaining\n\nPrompt chaining is when each LLM call processes the output of the previous call. It's often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\n\n* Translating documents into different languages\n* Verifying generated content for consistency\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=762dec147c31b8dc6ebb0857e236fc1f\" alt=\"Prompt chaining\" data-og-width=\"1412\" width=\"1412\" data-og-height=\"444\" height=\"444\" data-path=\"oss/images/prompt_chain.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=fda27cf4f997e350d4ce48be16049c47 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1374b6de11900d394fc73722a3a6040e 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=25246c7111a87b5df5a2af24a0181efe 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=0c57da86a49cf966cc090497ade347f1 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a1b5c8fc644d7a80c0792b71769c97da 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8a3f66f0e365e503a85b30be48bc1a76 2500w\" />\n\n<CodeGroup>\n  ```python Graph API theme={null}\n  from typing_extensions import TypedDict\n  from langgraph.graph import StateGraph, START, END\n  from IPython.display import Image, display\n\n\n  # Graph state\n  class State(TypedDict):\n      topic: str\n      joke: str\n      improved_joke: str\n      final_joke: str\n\n\n  # Nodes\n  def generate_joke(state: State):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n\n      msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n      return {\"joke\": msg.content}\n\n\n  def check_punchline(state: State):\n      \"\"\"Gate function to check if the joke has a punchline\"\"\"\n\n      # Simple check - does the joke contain \"?\" or \"!\"\n      if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n          return \"Pass\"\n      return \"Fail\"\n\n\n  def improve_joke(state: State):\n      \"\"\"Second LLM call to improve the joke\"\"\"\n\n      msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n      return {\"improved_joke\": msg.content}\n\n\n  def polish_joke(state: State):\n      \"\"\"Third LLM call for final polish\"\"\"\n      msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n      return {\"final_joke\": msg.content}\n\n\n  # Build workflow\n  workflow = StateGraph(State)\n\n  # Add nodes\n  workflow.add_node(\"generate_joke\", generate_joke)\n  workflow.add_node(\"improve_joke\", improve_joke)\n  workflow.add_node(\"polish_joke\", polish_joke)\n\n  # Add edges to connect nodes\n  workflow.add_edge(START, \"generate_joke\")\n  workflow.add_conditional_edges(\n      \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\n  )\n  workflow.add_edge(\"improve_joke\", \"polish_joke\")\n  workflow.add_edge(\"polish_joke\", END)\n\n  # Compile\n  chain = workflow.compile()\n\n  # Show workflow\n  display(Image(chain.get_graph().draw_mermaid_png()))\n\n  # Invoke\n  state = chain.invoke({\"topic\": \"cats\"})\n  print(\"Initial joke:\")\n  print(state[\"joke\"])\n  print(\"\\n--- --- ---\\n\")\n  if \"improved_joke\" in state:\n      print(\"Improved joke:\")\n      print(state[\"improved_joke\"])\n      print(\"\\n--- --- ---\\n\")\n\n      print(\"Final joke:\")\n      print(state[\"final_joke\"])\n  else:\n      print(\"Final joke:\")\n      print(state[\"joke\"])\n  ```\n\n  ```python Functional API theme={null}\n  from langgraph.func import entrypoint, task\n\n\n  # Tasks\n  @task\n  def generate_joke(topic: str):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n      msg = llm.invoke(f\"Write a short joke about {topic}\")\n      return msg.content\n\n\n  def check_punchline(joke: str):\n      \"\"\"Gate function to check if the joke has a punchline\"\"\"\n      # Simple check - does the joke contain \"?\" or \"!\"\n      if \"?\" in joke or \"!\" in joke:\n          return \"Fail\"\n\n      return \"Pass\"\n\n\n  @task\n  def improve_joke(joke: str):\n      \"\"\"Second LLM call to improve the joke\"\"\"\n      msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\n      return msg.content\n\n\n  @task\n  def polish_joke(joke: str):\n      \"\"\"Third LLM call for final polish\"\"\"\n      msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\n      return msg.content\n\n\n  @entrypoint()\n  def prompt_chaining_workflow(topic: str):\n      original_joke = generate_joke(topic).result()\n      if check_punchline(original_joke) == \"Pass\":\n          return original_joke\n\n      improved_joke = improve_joke(original_joke).result()\n      return polish_joke(improved_joke).result()\n\n  # Invoke\n  for step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\n      print(step)\n      print(\"\\n\")\n  ```\n</CodeGroup>\n\n## Parallelization\n\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\n\n* Split up subtasks and run them in parallel, which increases speed\n* Run tasks multiple times to check for different outputs, which increases confidence\n\nSome examples include:\n\n* Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors\n* Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8afe3c427d8cede6fed1e4b2a5107b71\" alt=\"parallelization.png\" data-og-width=\"1020\" width=\"1020\" data-og-height=\"684\" height=\"684\" data-path=\"oss/images/parallelization.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=88e51062b14d9186a6f0ea246bc48635 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=934941ca52019b7cbce7fbdd31d00f0f 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=30b5c86c545d0e34878ff0a2c367dd0a 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=6227d2c39f332eaeda23f7db66871dd7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=283f3ee2924a385ab88f2cbfd9c9c48c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=69f6a97716b38998b7b399c3d8ac7d9c 2500w\" />\n\n<CodeGroup>\n  ```python Graph API theme={null}\n  # Graph state\n  class State(TypedDict):\n      topic: str\n      joke: str\n      story: str\n      poem: str\n      combined_output: str\n\n\n  # Nodes\n  def call_llm_1(state: State):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n\n      msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n      return {\"joke\": msg.content}\n\n\n  def call_llm_2(state: State):\n      \"\"\"Second LLM call to generate story\"\"\"\n\n      msg = llm.invoke(f\"Write a story about {state['topic']}\")\n      return {\"story\": msg.content}\n\n\n  def call_llm_3(state: State):\n      \"\"\"Third LLM call to generate poem\"\"\"\n\n      msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n      return {\"poem\": msg.content}\n\n\n  def aggregator(state: State):\n      \"\"\"Combine the joke and story into a single output\"\"\"\n\n      combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n      combined += f\"STORY:\\n{state['story']}\\n\\n\"\n      combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n      combined += f\"POEM:\\n{state['poem']}\"\n      return {\"combined_output\": combined}\n\n\n  # Build workflow\n  parallel_builder = StateGraph(State)\n\n  # Add nodes\n  parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n  parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n  parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n  parallel_builder.add_node(\"aggregator\", aggregator)\n\n  # Add edges to connect nodes\n  parallel_builder.add_edge(START, \"call_llm_1\")\n  parallel_builder.add_edge(START, \"call_llm_2\")\n  parallel_builder.add_edge(START, \"call_llm_3\")\n  parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n  parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n  parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n  parallel_builder.add_edge(\"aggregator\", END)\n  parallel_workflow = parallel_builder.compile()\n\n  # Show workflow\n  display(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n\n  # Invoke\n  state = parallel_workflow.invoke({\"topic\": \"cats\"})\n  print(state[\"combined_output\"])\n  ```\n\n  ```python Functional API theme={null}\n  @task\n  def call_llm_1(topic: str):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n      msg = llm.invoke(f\"Write a joke about {topic}\")\n      return msg.content\n\n\n  @task\n  def call_llm_2(topic: str):\n      \"\"\"Second LLM call to generate story\"\"\"\n      msg = llm.invoke(f\"Write a story about {topic}\")\n      return msg.content\n\n\n  @task\n  def call_llm_3(topic):\n      \"\"\"Third LLM call to generate poem\"\"\"\n      msg = llm.invoke(f\"Write a poem about {topic}\")\n      return msg.content\n\n\n  @task\n  def aggregator(topic, joke, story, poem):\n      \"\"\"Combine the joke and story into a single output\"\"\"\n\n      combined = f\"Here's a story, joke, and poem about {topic}!\\n\\n\"\n      combined += f\"STORY:\\n{story}\\n\\n\"\n      combined += f\"JOKE:\\n{joke}\\n\\n\"\n      combined += f\"POEM:\\n{poem}\"\n      return combined\n\n\n  # Build workflow\n  @entrypoint()\n  def parallel_workflow(topic: str):\n      joke_fut = call_llm_1(topic)\n      story_fut = call_llm_2(topic)\n      poem_fut = call_llm_3(topic)\n      return aggregator(\n          topic, joke_fut.result(), story_fut.result(), poem_fut.result()\n      ).result()\n\n  # Invoke\n  for step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n      print(step)\n      print(\"\\n\")\n  ```\n</CodeGroup>\n\n## Routing\n\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=272e0e9b681b89cd7d35d5c812c50ee6\" alt=\"routing.png\" data-og-width=\"1214\" width=\"1214\" data-og-height=\"678\" height=\"678\" data-path=\"oss/images/routing.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ab85efe91d20c816f9a4e491e92a61f7 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=769e29f9be058a47ee85e0c9228e6e44 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=3711ee40746670731a0ce3e96b7cfeb1 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=9aaa28410da7643f4a2587f7bfae0f21 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=6706326c7fef0511805c684d1e4f7082 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=f6d603145ca33791b18c8c8afec0bb4d 2500w\" />\n\n<CodeGroup>\n  ```python Graph API theme={null}\n  from typing_extensions import Literal\n  from langchain.messages import HumanMessage, SystemMessage\n\n\n  # Schema for structured output to use as routing logic\n  class Route(BaseModel):\n      step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n          None, description=\"The next step in the routing process\"\n      )\n\n\n  # Augment the LLM with schema for structured output\n  router = llm.with_structured_output(Route)\n\n\n  # State\n  class State(TypedDict):\n      input: str\n      decision: str\n      output: str\n\n\n  # Nodes\n  def llm_call_1(state: State):\n      \"\"\"Write a story\"\"\"\n\n      result = llm.invoke(state[\"input\"])\n      return {\"output\": result.content}\n\n\n  def llm_call_2(state: State):\n      \"\"\"Write a joke\"\"\"\n\n      result = llm.invoke(state[\"input\"])\n      return {\"output\": result.content}\n\n\n  def llm_call_3(state: State):\n      \"\"\"Write a poem\"\"\"\n\n      result = llm.invoke(state[\"input\"])\n      return {\"output\": result.content}\n\n\n  def llm_call_router(state: State):\n      \"\"\"Route the input to the appropriate node\"\"\"\n\n      # Run the augmented LLM with structured output to serve as routing logic\n      decision = router.invoke(\n          [\n              SystemMessage(\n                  content=\"Route the input to story, joke, or poem based on the user's request.\"\n              ),\n              HumanMessage(content=state[\"input\"]),\n          ]\n      )\n\n      return {\"decision\": decision.step}\n\n\n  # Conditional edge function to route to the appropriate node\n  def route_decision(state: State):\n      # Return the node name you want to visit next\n      if state[\"decision\"] == \"story\":\n          return \"llm_call_1\"\n      elif state[\"decision\"] == \"joke\":\n          return \"llm_call_2\"\n      elif state[\"decision\"] == \"poem\":\n          return \"llm_call_3\"\n\n\n  # Build workflow\n  router_builder = StateGraph(State)\n\n  # Add nodes\n  router_builder.add_node(\"llm_call_1\", llm_call_1)\n  router_builder.add_node(\"llm_call_2\", llm_call_2)\n  router_builder.add_node(\"llm_call_3\", llm_call_3)\n  router_builder.add_node(\"llm_call_router\", llm_call_router)\n\n  # Add edges to connect nodes\n  router_builder.add_edge(START, \"llm_call_router\")\n  router_builder.add_conditional_edges(\n      \"llm_call_router\",\n      route_decision,\n      {  # Name returned by route_decision : Name of next node to visit\n          \"llm_call_1\": \"llm_call_1\",\n          \"llm_call_2\": \"llm_call_2\",\n          \"llm_call_3\": \"llm_call_3\",\n      },\n  )\n  router_builder.add_edge(\"llm_call_1\", END)\n  router_builder.add_edge(\"llm_call_2\", END)\n  router_builder.add_edge(\"llm_call_3\", END)\n\n  # Compile workflow\n  router_workflow = router_builder.compile()\n\n  # Show the workflow\n  display(Image(router_workflow.get_graph().draw_mermaid_png()))\n\n  # Invoke\n  state = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\n  print(state[\"output\"])\n  ```\n\n  ```python Functional API theme={null}\n  from typing_extensions import Literal\n  from pydantic import BaseModel\n  from langchain.messages import HumanMessage, SystemMessage\n\n\n  # Schema for structured output to use as routing logic\n  class Route(BaseModel):\n      step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n          None, description=\"The next step in the routing process\"\n      )\n\n\n  # Augment the LLM with schema for structured output\n  router = llm.with_structured_output(Route)\n\n\n  @task\n  def llm_call_1(input_: str):\n      \"\"\"Write a story\"\"\"\n      result = llm.invoke(input_)\n      return result.content\n\n\n  @task\n  def llm_call_2(input_: str):\n      \"\"\"Write a joke\"\"\"\n      result = llm.invoke(input_)\n      return result.content\n\n\n  @task\n  def llm_call_3(input_: str):\n      \"\"\"Write a poem\"\"\"\n      result = llm.invoke(input_)\n      return result.content\n\n\n  def llm_call_router(input_: str):\n      \"\"\"Route the input to the appropriate node\"\"\"\n      # Run the augmented LLM with structured output to serve as routing logic\n      decision = router.invoke(\n          [\n              SystemMessage(\n                  content=\"Route the input to story, joke, or poem based on the user's request.\"\n              ),\n              HumanMessage(content=input_),\n          ]\n      )\n      return decision.step\n\n\n  # Create workflow\n  @entrypoint()\n  def router_workflow(input_: str):\n      next_step = llm_call_router(input_)\n      if next_step == \"story\":\n          llm_call = llm_call_1\n      elif next_step == \"joke\":\n          llm_call = llm_call_2\n      elif next_step == \"poem\":\n          llm_call = llm_call_3\n\n      return llm_call(input_).result()\n\n  # Invoke\n  for step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\n      print(step)\n      print(\"\\n\")\n  ```\n</CodeGroup>\n\n## Orchestrator-worker\n\nIn an orchestrator-worker configuration, the orchestrator:\n\n* Breaks down tasks into subtasks\n* Delegates subtasks to workers\n* Synthesizes worker outputs into a final result\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2e423c67cd4f12e049cea9c169ff0676\" alt=\"worker.png\" data-og-width=\"1486\" width=\"1486\" data-og-height=\"548\" height=\"548\" data-path=\"oss/images/worker.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=037222991ea08f889306be035c4730b6 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=081f3ff05cc1fe50770c864d74084b5b 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0ef6c1b9ceb5159030aa34d0f05f1ada 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=92ec7353a89ae96e221a5a8f65c88adf 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=71b201dd99fa234ebfb918915aac3295 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4f7b6e2064db575027932394a3658fbd 2500w\" />\n\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with [parallelization](#parallelization). This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\n\n<CodeGroup>\n  ```python Graph API theme={null}\n  from typing import Annotated, List\n  import operator\n\n\n  # Schema for structured output to use in planning\n  class Section(BaseModel):\n      name: str = Field(\n          description=\"Name for this section of the report.\",\n      )\n      description: str = Field(\n          description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n      )\n\n\n  class Sections(BaseModel):\n      sections: List[Section] = Field(\n          description=\"Sections of the report.\",\n      )\n\n\n  # Augment the LLM with schema for structured output\n  planner = llm.with_structured_output(Sections)\n  ```\n\n  ```python Functional API theme={null}\n  from typing import List\n\n\n  # Schema for structured output to use in planning\n  class Section(BaseModel):\n      name: str = Field(\n          description=\"Name for this section of the report.\",\n      )\n      description: str = Field(\n          description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n      )\n\n\n  class Sections(BaseModel):\n      sections: List[Section] = Field(\n          description=\"Sections of the report.\",\n      )\n\n\n  # Augment the LLM with schema for structured output\n  planner = llm.with_structured_output(Sections)\n\n\n  @task\n  def orchestrator(topic: str):\n      \"\"\"Orchestrator that generates a plan for the report\"\"\"\n      # Generate queries\n      report_sections = planner.invoke(\n          [\n              SystemMessage(content=\"Generate a plan for the report.\"),\n              HumanMessage(content=f\"Here is the report topic: {topic}\"),\n          ]\n      )\n\n      return report_sections.sections\n\n\n  @task\n  def llm_call(section: Section):\n      \"\"\"Worker writes a section of the report\"\"\"\n\n      # Generate section\n      result = llm.invoke(\n          [\n              SystemMessage(content=\"Write a report section.\"),\n              HumanMessage(\n                  content=f\"Here is the section name: {section.name} and description: {section.description}\"\n              ),\n          ]\n      )\n\n      # Write the updated section to completed sections\n      return result.content\n\n\n  @task\n  def synthesizer(completed_sections: list[str]):\n      \"\"\"Synthesize full report from sections\"\"\"\n      final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n      return final_report\n\n\n  @entrypoint()\n  def orchestrator_worker(topic: str):\n      sections = orchestrator(topic).result()\n      section_futures = [llm_call(section) for section in sections]\n      final_report = synthesizer(\n          [section_fut.result() for section_fut in section_futures]\n      ).result()\n      return final_report\n\n  # Invoke\n  report = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\n  from IPython.display import Markdown\n  Markdown(report)\n  ```\n</CodeGroup>\n\n### Creating workers in LangGraph\n\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The `Send` API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the `Send` API to send a section to each worker.\n\n```python  theme={null}\nfrom langgraph.types import Send\n\n\n# Graph state\nclass State(TypedDict):\n    topic: str  # Report topic\n    sections: list[Section]  # List of report sections\n    completed_sections: Annotated[\n        list, operator.add\n    ]  # All workers write to this key in parallel\n    final_report: str  # Final report\n\n\n# Worker state\nclass WorkerState(TypedDict):\n    section: Section\n    completed_sections: Annotated[list, operator.add]\n\n\n# Nodes\ndef orchestrator(state: State):\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n\n    # Generate queries\n    report_sections = planner.invoke(\n        [\n            SystemMessage(content=\"Generate a plan for the report.\"),\n            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n        ]\n    )\n\n    return {\"sections\": report_sections.sections}\n\n\ndef llm_call(state: WorkerState):\n    \"\"\"Worker writes a section of the report\"\"\"\n\n    # Generate section\n    section = llm.invoke(\n        [\n            SystemMessage(\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n            ),\n            HumanMessage(\n                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n            ),\n        ]\n    )\n\n    # Write the updated section to completed sections\n    return {\"completed_sections\": [section.content]}\n\n\ndef synthesizer(state: State):\n    \"\"\"Synthesize full report from sections\"\"\"\n\n    # List of completed sections\n    completed_sections = state[\"completed_sections\"]\n\n    # Format completed section to str to use as context for final sections\n    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n\n    return {\"final_report\": completed_report_sections}\n\n\n# Conditional edge function to create llm_call workers that each write a section of the report\ndef assign_workers(state: State):\n    \"\"\"Assign a worker to each section in the plan\"\"\"\n\n    # Kick off section writing in parallel via Send() API\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n\n\n# Build workflow\norchestrator_worker_builder = StateGraph(State)\n\n# Add the nodes\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n\n# Add edges to connect nodes\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\norchestrator_worker_builder.add_conditional_edges(\n    \"orchestrator\", assign_workers, [\"llm_call\"]\n)\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\n\n# Compile the workflow\norchestrator_worker = orchestrator_worker_builder.compile()\n\n# Show the workflow\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\n\nfrom IPython.display import Markdown\nMarkdown(state[\"final_report\"])\n```\n\n## Evaluator-optimizer\n\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a [human-in-the-loop](/oss/python/langgraph/interrupts) determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\n\nEvaluator-optimizer workflows are commonly used when there's particular success criteria for a task, but iteration is required to meet that criteria. For example, there's not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9bd0474f42b6040b14ed6968a9ab4e3c\" alt=\"evaluator_optimizer.png\" data-og-width=\"1004\" width=\"1004\" data-og-height=\"340\" height=\"340\" data-path=\"oss/images/evaluator_optimizer.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ab36856e5f9a518b22e71278aa8b1711 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ec597c92270278c2bac203d36b611c2 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ad3bfb734a0e509d9b87fdb4e808bfd 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e82bd25a463d3cdf76036649c03358a9 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d31717ae3e76243dd975a53f46e8c1f6 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a9bb4fb1583f6ad06c0b13602cd14811 2500w\" />\n\n<CodeGroup>\n  ```python Graph API theme={null}\n  # Graph state\n  class State(TypedDict):\n      joke: str\n      topic: str\n      feedback: str\n      funny_or_not: str\n\n\n  # Schema for structured output to use in evaluation\n  class Feedback(BaseModel):\n      grade: Literal[\"funny\", \"not funny\"] = Field(\n          description=\"Decide if the joke is funny or not.\",\n      )\n      feedback: str = Field(\n          description=\"If the joke is not funny, provide feedback on how to improve it.\",\n      )\n\n\n  # Augment the LLM with schema for structured output\n  evaluator = llm.with_structured_output(Feedback)\n\n\n  # Nodes\n  def llm_call_generator(state: State):\n      \"\"\"LLM generates a joke\"\"\"\n\n      if state.get(\"feedback\"):\n          msg = llm.invoke(\n              f\"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}\"\n          )\n      else:\n          msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n      return {\"joke\": msg.content}\n\n\n  def llm_call_evaluator(state: State):\n      \"\"\"LLM evaluates the joke\"\"\"\n\n      grade = evaluator.invoke(f\"Grade the joke {state['joke']}\")\n      return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\n\n\n  # Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\n  def route_joke(state: State):\n      \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\n\n      if state[\"funny_or_not\"] == \"funny\":\n          return \"Accepted\"\n      elif state[\"funny_or_not\"] == \"not funny\":\n          return \"Rejected + Feedback\"\n\n\n  # Build workflow\n  optimizer_builder = StateGraph(State)\n\n  # Add the nodes\n  optimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\n  optimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\n\n  # Add edges to connect nodes\n  optimizer_builder.add_edge(START, \"llm_call_generator\")\n  optimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\n  optimizer_builder.add_conditional_edges(\n      \"llm_call_evaluator\",\n      route_joke,\n      {  # Name returned by route_joke : Name of next node to visit\n          \"Accepted\": END,\n          \"Rejected + Feedback\": \"llm_call_generator\",\n      },\n  )\n\n  # Compile the workflow\n  optimizer_workflow = optimizer_builder.compile()\n\n  # Show the workflow\n  display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\n\n  # Invoke\n  state = optimizer_workflow.invoke({\"topic\": \"Cats\"})\n  print(state[\"joke\"])\n  ```\n\n  ```python Functional API theme={null}\n  # Schema for structured output to use in evaluation\n  class Feedback(BaseModel):\n      grade: Literal[\"funny\", \"not funny\"] = Field(\n          description=\"Decide if the joke is funny or not.\",\n      )\n      feedback: str = Field(\n          description=\"If the joke is not funny, provide feedback on how to improve it.\",\n      )\n\n\n  # Augment the LLM with schema for structured output\n  evaluator = llm.with_structured_output(Feedback)\n\n\n  # Nodes\n  @task\n  def llm_call_generator(topic: str, feedback: Feedback):\n      \"\"\"LLM generates a joke\"\"\"\n      if feedback:\n          msg = llm.invoke(\n              f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\n          )\n      else:\n          msg = llm.invoke(f\"Write a joke about {topic}\")\n      return msg.content\n\n\n  @task\n  def llm_call_evaluator(joke: str):\n      \"\"\"LLM evaluates the joke\"\"\"\n      feedback = evaluator.invoke(f\"Grade the joke {joke}\")\n      return feedback\n\n\n  @entrypoint()\n  def optimizer_workflow(topic: str):\n      feedback = None\n      while True:\n          joke = llm_call_generator(topic, feedback).result()\n          feedback = llm_call_evaluator(joke).result()\n          if feedback.grade == \"funny\":\n              break\n\n      return joke\n\n  # Invoke\n  for step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\n      print(step)\n      print(\"\\n\")\n  ```\n</CodeGroup>\n\n## Agents\n\nAgents are typically implemented as an LLM performing actions using [tools](/oss/python/langchain/tools). They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd8da41dbf8b5e6fc9ea6bb10cb63e38\" alt=\"agent.png\" data-og-width=\"1732\" width=\"1732\" data-og-height=\"712\" height=\"712\" data-path=\"oss/images/agent.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f7a590604edc49cfa273b5856f3a3ee3 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=dff9b17d345fe0fea25616b3b0dc6ebf 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d53318b0c9c898a6146991691cbac058 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ea66fb96bc07c595d321b8b71e651ddb 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=b02599a3c9ba2a5c830b9a346f9d26c9 2500w\" />\n\n<Note>\n  To get started with agents, see the [quickstart](/oss/python/langchain/quickstart) or read more about [how they work](/oss/python/langchain/agents) in LangChain.\n</Note>\n\n```python Using tools theme={null}\nfrom langchain.tools import tool\n\n\n# Define tools\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a * b\n\n\n@tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a + b\n\n\n@tool\ndef divide(a: int, b: int) -> float:\n    \"\"\"Divide `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a / b\n\n\n# Augment the LLM with tools\ntools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nllm_with_tools = llm.bind_tools(tools)\n```\n\n<CodeGroup>\n  ```python Graph API theme={null}\n  from langgraph.graph import MessagesState\n  from langchain.messages import SystemMessage, HumanMessage, ToolMessage\n\n\n  # Nodes\n  def llm_call(state: MessagesState):\n      \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n      return {\n          \"messages\": [\n              llm_with_tools.invoke(\n                  [\n                      SystemMessage(\n                          content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                      )\n                  ]\n                  + state[\"messages\"]\n              )\n          ]\n      }\n\n\n  def tool_node(state: dict):\n      \"\"\"Performs the tool call\"\"\"\n\n      result = []\n      for tool_call in state[\"messages\"][-1].tool_calls:\n          tool = tools_by_name[tool_call[\"name\"]]\n          observation = tool.invoke(tool_call[\"args\"])\n          result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n      return {\"messages\": result}\n\n\n  # Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n  def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n      \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n      messages = state[\"messages\"]\n      last_message = messages[-1]\n\n      # If the LLM makes a tool call, then perform an action\n      if last_message.tool_calls:\n          return \"tool_node\"\n\n      # Otherwise, we stop (reply to the user)\n      return END\n\n\n  # Build workflow\n  agent_builder = StateGraph(MessagesState)\n\n  # Add nodes\n  agent_builder.add_node(\"llm_call\", llm_call)\n  agent_builder.add_node(\"tool_node\", tool_node)\n\n  # Add edges to connect nodes\n  agent_builder.add_edge(START, \"llm_call\")\n  agent_builder.add_conditional_edges(\n      \"llm_call\",\n      should_continue,\n      [\"tool_node\", END]\n  )\n  agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n  # Compile the agent\n  agent = agent_builder.compile()\n\n  # Show the agent\n  display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n  # Invoke\n  messages = [HumanMessage(content=\"Add 3 and 4.\")]\n  messages = agent.invoke({\"messages\": messages})\n  for m in messages[\"messages\"]:\n      m.pretty_print()\n  ```\n\n  ```python Functional API theme={null}\n  from langgraph.graph import add_messages\n  from langchain.messages import (\n      SystemMessage,\n      HumanMessage,\n      ToolCall,\n  )\n  from langchain_core.messages import BaseMessage\n\n\n  @task\n  def call_llm(messages: list[BaseMessage]):\n      \"\"\"LLM decides whether to call a tool or not\"\"\"\n      return llm_with_tools.invoke(\n          [\n              SystemMessage(\n                  content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n              )\n          ]\n          + messages\n      )\n\n\n  @task\n  def call_tool(tool_call: ToolCall):\n      \"\"\"Performs the tool call\"\"\"\n      tool = tools_by_name[tool_call[\"name\"]]\n      return tool.invoke(tool_call)\n\n\n  @entrypoint()\n  def agent(messages: list[BaseMessage]):\n      llm_response = call_llm(messages).result()\n\n      while True:\n          if not llm_response.tool_calls:\n              break\n\n          # Execute tools\n          tool_result_futures = [\n              call_tool(tool_call) for tool_call in llm_response.tool_calls\n          ]\n          tool_results = [fut.result() for fut in tool_result_futures]\n          messages = add_messages(messages, [llm_response, *tool_results])\n          llm_response = call_llm(messages).result()\n\n      messages = add_messages(messages, llm_response)\n      return messages\n\n  # Invoke\n  messages = [HumanMessage(content=\"Add 3 and 4.\")]\n  for chunk in agent.stream(messages, stream_mode=\"updates\"):\n      print(chunk)\n      print(\"\\n\")\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/workflows-agents.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 43736
}