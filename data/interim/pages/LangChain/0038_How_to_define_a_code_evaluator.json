{
  "title": "How to define a code evaluator",
  "source_url": "https://docs.langchain.com/langsmith/code-evaluator",
  "content": "<Info>\n  * [Evaluators](/langsmith/evaluation-concepts#evaluators)\n</Info>\n\nCode evaluators are just functions that take a dataset example and the resulting application output, and return one or more metrics. These functions can be passed directly into [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate).\n\n## Basic example\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import evaluate\n\n  def correct(outputs: dict, reference_outputs: dict) -> bool:\n      \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n      return outputs[\"answer\"] == reference_outputs[\"answer\"]\n\n  def dummy_app(inputs: dict) -> dict:\n      return {\"answer\": \"hmm i'm not sure\", \"reasoning\": \"i didn't understand the question\"}\n\n  results = evaluate(\n      dummy_app,\n      data=\"dataset_name\",\n      evaluators=[correct]\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import type { EvaluationResult } from \"langsmith/evaluation\";\n\n  const correct = async ({ outputs, referenceOutputs }: {\n    outputs: Record<string, any>;\n    referenceOutputs?: Record<string, any>;\n  }): Promise<EvaluationResult> => {\n    const score = outputs?.answer === referenceOutputs?.answer;\n    return { key: \"correct\", score };\n  }\n  ```\n</CodeGroup>\n\n## Evaluator args\n\ncode evaluator functions must have specific argument names. They can take any subset of the following arguments:\n\n* `run: Run`: The full [Run](/langsmith/run-data-format) object generated by the application on the given example.\n* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metdata (if available).\n* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.\n* `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.\n* `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.\n\nFor most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.\n\nWhen using JS/TS these should all be passed in as part of a single object argument.\n\n## Evaluator output\n\nCode evaluators are expected to return one of the following types:\n\nPython and JS/TS\n\n* `dict`: dicts of the form `{\"score\" | \"value\": ..., \"key\": ...}` allow you to customize the metric type (\"score\" for numerical and \"value\" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.\n\nPython only\n\n* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.\n* `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.\n* `list[dict]`: return multiple metrics using a single function.\n\n## Additional examples\n\nRequires `langsmith>=0.2.0`\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import evaluate, wrappers\n  from langsmith.schemas import Run, Example\n  from openai import AsyncOpenAI\n  # Assumes you've installed pydantic.\n  from pydantic import BaseModel\n\n  # We can still pass in Run and Example objects if we'd like\n  def correct_old_signature(run: Run, example: Example) -> dict:\n      \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n      return {\"key\": \"correct\", \"score\": run.outputs[\"answer\"] == example.outputs[\"answer\"]}\n\n  # Just evaluate actual outputs\n  def concision(outputs: dict) -> int:\n      \"\"\"Score how concise the answer is. 1 is the most concise, 5 is the least concise.\"\"\"\n      return min(len(outputs[\"answer\"]) // 1000, 4) + 1\n\n  # Use an LLM-as-a-judge\n  oai_client = wrappers.wrap_openai(AsyncOpenAI())\n\n  async def valid_reasoning(inputs: dict, outputs: dict) -> bool:\n      \"\"\"Use an LLM to judge if the reasoning and the answer are consistent.\"\"\"\n      instructions = \"\"\"\n  Given the following question, answer, and reasoning, determine if the reasoning for the\n  answer is logically valid and consistent with question and the answer.\"\"\"\n\n      class Response(BaseModel):\n          reasoning_is_valid: bool\n\n      msg = f\"Question: {inputs['question']}\\nAnswer: {outputs['answer']}\\nReasoning: {outputs['reasoning']}\"\n      response = await oai_client.beta.chat.completions.parse(\n          model=\"gpt-4o-mini\",\n          messages=[{\"role\": \"system\", \"content\": instructions,}, {\"role\": \"user\", \"content\": msg}],\n          response_format=Response\n      )\n      return response.choices[0].message.parsed.reasoning_is_valid\n\n  def dummy_app(inputs: dict) -> dict:\n      return {\"answer\": \"hmm i'm not sure\", \"reasoning\": \"i didn't understand the question\"}\n\n  results = evaluate(\n      dummy_app,\n      data=\"dataset_name\",\n      evaluators=[correct_old_signature, concision, valid_reasoning]\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n  import { evaluate } from \"langsmith/evaluation\";\n  import { Run, Example } from \"langsmith/schemas\";\n  import OpenAI from \"openai\";\n\n  // Type definitions\n  interface AppInputs {\n      question: string;\n  }\n\n  interface AppOutputs {\n      answer: string;\n      reasoning: string;\n  }\n\n  interface Response {\n      reasoning_is_valid: boolean;\n  }\n\n  // Old signature evaluator\n  function correctOldSignature(run: Run, example: Example) {\n      return {\n          key: \"correct\",\n          score: run.outputs?.[\"answer\"] === example.outputs?.[\"answer\"],\n      };\n  }\n\n  // Output-only evaluator\n  function concision({ outputs }: { outputs: AppOutputs }) {\n      return {\n          key: \"concision\",\n          score: Math.min(Math.floor(outputs.answer.length / 1000), 4) + 1,\n      };\n  }\n\n  // LLM-as-judge evaluator\n  const openai = new OpenAI();\n\n  async function validReasoning({\n      inputs,\n      outputs\n  }: {\n      inputs: AppInputs;\n      outputs: AppOutputs;\n  }) {\n      const instructions = `\\\n    Given the following question, answer, and reasoning, determine if the reasoning for the \\\n    answer is logically valid and consistent with question and the answer.`;\n\n      const msg = `Question: ${inputs.question}\n  Answer: ${outputs.answer}\n  Reasoning: ${outputs.reasoning}`;\n\n      const response = await openai.chat.completions.create({\n          model: \"gpt-4\",\n          messages: [\n              { role: \"system\", content: instructions },\n              { role: \"user\", content: msg }\n          ],\n          response_format: { type: \"json_object\" },\n          functions: [{\n              name: \"parse_response\",\n              parameters: {\n                  type: \"object\",\n                  properties: {\n                      reasoning_is_valid: {\n                          type: \"boolean\",\n                          description: \"Whether the reasoning is valid\"\n                      }\n                  },\n                  required: [\"reasoning_is_valid\"]\n              }\n          }]\n      });\n\n      const parsed = JSON.parse(response.choices[0].message.content ?? \"{}\") as Response;\n      return {\n          key: \"valid_reasoning\",\n          score: parsed.reasoning_is_valid ? 1 : 0\n      };\n  }\n\n  // Example application\n  function dummyApp(inputs: AppInputs): AppOutputs {\n      return {\n          answer: \"hmm i'm not sure\",\n          reasoning: \"i didn't understand the question\"\n      };\n  }\n\n  const results = await evaluate(dummyApp, {\n      data: \"dataset_name\",\n      evaluators: [correctOldSignature, concision, validReasoning],\n      client: new Client()\n  });\n  ```\n</CodeGroup>\n\n## Related\n\n* [Evaluate aggregate experiment results](/langsmith/summary): Define summary evaluators, which compute metrics for an entire experiment.\n* [Run an evaluation comparing two experiments](/langsmith/evaluate-pairwise): Define pairwise evaluators, which compute metrics by comparing two (or more) experiments against each other.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/code-evaluator.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 8547
}