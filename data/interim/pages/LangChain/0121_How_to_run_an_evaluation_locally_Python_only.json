{
  "title": "How to run an evaluation locally (Python only)",
  "source_url": "https://docs.langchain.com/langsmith/local",
  "content": "Sometimes it is helpful to run an evaluation locally without uploading any results to LangSmith. For example, if you're quickly iterating on a prompt and want to smoke test it on a few examples, or if you're validating that your target and evaluator functions are defined correctly, you may not want to record these evaluations.\n\nYou can do this by using the LangSmith Python SDK and passing `upload_results=False` to `evaluate()` / `aevaluate()`.\n\nThis will run you application and evaluators exactly as it always does and return the same output, but nothing will be recorded to LangSmith. This includes not just the experiment results but also the application and evaluator traces.\n\n<Note>\n  If you want to upload results to LangSmith but also need to process them in your script (for quality gates, custom aggregations, etc.), refer to [Read experiment results locally](/langsmith/read-local-experiment-results).\n</Note>\n\n## Example\n\nLet's take a look at an example:\n\nRequires `langsmith>=0.2.0`. Example also uses `pandas`.\n\n```python  theme={null}\nfrom langsmith import Client\n\n# 1. Create and/or select your dataset\nls_client = Client()\ndataset = ls_client.clone_public_dataset(\n    \"https://smith.langchain.com/public/a63525f9-bdf2-4512-83e3-077dc9417f96/d\"\n)\n\n# 2. Define an evaluator\ndef is_concise(outputs: dict, reference_outputs: dict) -> bool:\n    return len(outputs[\"answer\"]) < (3 * len(reference_outputs[\"answer\"]))\n\n# 3. Define the interface to your app\ndef chatbot(inputs: dict) -> dict:\n    return {\"answer\": inputs[\"question\"] + \" is a good question. I don't know the answer.\"}\n\n# 4. Run an evaluation\nexperiment = ls_client.evaluate(\n    chatbot,\n    data=dataset,\n    evaluators=[is_concise],\n    experiment_prefix=\"my-first-experiment\",\n    # 'upload_results' is the relevant arg.\n    upload_results=False\n)\n\n# 5. Analyze results locally\nresults = list(experiment)\n\n# Check if 'is_concise' returned False.\nfailed = [r for r in results if not r[\"evaluation_results\"][\"results\"][0].score]\n\n# Explore the failed inputs and outputs.\nfor r in failed:\n    print(r[\"example\"].inputs)\n    print(r[\"run\"].outputs)\n\n# Explore the results as a Pandas DataFrame.\n# Must have 'pandas' installed.\ndf = experiment.to_pandas()\ndf[[\"inputs.question\", \"outputs.answer\", \"reference.answer\", \"feedback.is_concise\"]]\n```\n\n```python  theme={null}\n{'question': 'What is the largest mammal?'}\n{'answer': \"What is the largest mammal? is a good question. I don't know the answer.\"}\n{'question': 'What do mammals and birds have in common?'}\n{'answer': \"What do mammals and birds have in common? is a good question. I don't know the answer.\"}\n```\n\n|   | inputs.question                           | outputs.answer                                                                         | reference.answer           | feedback.is\\_concise |\n| - | ----------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------- | -------------------- |\n| 0 | What is the largest mammal?               | What is the largest mammal? is a good question. I don't know the answer.               | The blue whale             | False                |\n| 1 | What do mammals and birds have in common? | What do mammals and birds have in common? is a good question. I don't know the answer. | They are both warm-blooded | False                |\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/local.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 3754
}