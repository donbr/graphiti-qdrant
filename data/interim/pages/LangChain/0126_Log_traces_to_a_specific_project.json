{
  "title": "Log traces to a specific project",
  "source_url": "https://docs.langchain.com/langsmith/log-traces-to-project",
  "content": "You can change the destination project of your traces both statically through environment variables and dynamically at runtime.\n\n## Set the destination project statically\n\nAs mentioned in the [Tracing Concepts](/langsmith/observability-concepts#projects) section, LangSmith uses the concept of a `Project` to group traces. If left unspecified, the project is set to `default`. You can set the `LANGSMITH_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\n\n```\nexport LANGSMITH_PROJECT=my-custom-project\n```\n\n<Warning>\n  The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.\n</Warning>\n\nIf the project specified does not exist, it will be created automatically when the first trace is ingested.\n\n## Set the destination project dynamically\n\nYou can also set the project name at program runtime in various ways, depending on how you are [annotating your code for tracing](/langsmith/annotate-code). This is useful when you want to log traces to different projects within the same application.\n\n<Note>\n  Setting the project name dynamically using one of the below methods overrides the project name set by the `LANGSMITH_PROJECT` environment variable.\n</Note>\n\n<CodeGroup>\n  ```python Python theme={null}\n  import openai\n  from langsmith import traceable\n  from langsmith.run_trees import RunTree\n\n  client = openai.Client()\n  messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n\n  # Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith\n  # Ensure that the LANGSMITH_TRACING environment variables is set for @traceable to work\n  @traceable(\n    run_type=\"llm\",\n    name=\"OpenAI Call Decorator\",\n    project_name=\"My Project\"\n  )\n  def call_openai(\n    messages: list[dict], model: str = \"gpt-4o-mini\"\n  ) -> str:\n    return client.chat.completions.create(\n        model=model,\n        messages=messages,\n    ).choices[0].message.content\n\n  # Call the decorated function\n  call_openai(messages)\n\n  # You can also specify the Project via the project_name parameter\n  # This will override the project_name specified in the @traceable decorator\n  call_openai(\n    messages,\n    langsmith_extra={\"project_name\": \"My Overridden Project\"},\n  )\n\n  # The wrapped OpenAI client accepts all the same langsmith_extra parameters\n  # as @traceable decorated functions, and logs traces to LangSmith automatically.\n  # Ensure that the LANGSMITH_TRACING environment variables is set for the wrapper to work.\n  from langsmith import wrappers\n  wrapped_client = wrappers.wrap_openai(client)\n  wrapped_client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n    langsmith_extra={\"project_name\": \"My Project\"},\n  )\n\n  # Alternatively, create a RunTree object\n  # You can set the project name using the project_name parameter\n  rt = RunTree(\n    run_type=\"llm\",\n    name=\"OpenAI Call RunTree\",\n    inputs={\"messages\": messages},\n    project_name=\"My Project\"\n  )\n  chat_completion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n  )\n  # End and submit the run\n  rt.end(outputs=chat_completion)\n  rt.post()\n  ```\n\n  ```typescript TypeScript theme={null}\n  import OpenAI from \"openai\";\n  import { traceable } from \"langsmith/traceable\";\n  import { wrapOpenAI } from \"langsmith/wrappers\";\n  import { RunTree} from \"langsmith\";\n\n  const client = new OpenAI();\n  const messages = [\n    {role: \"system\", content: \"You are a helpful assistant.\"},\n    {role: \"user\", content: \"Hello!\"}\n  ];\n\n  const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[], model: string) => {\n    const completion = await client.chat.completions.create({\n        model: model,\n        messages: messages,\n    });\n    return completion.choices[0].message.content;\n  },{\n    run_type: \"llm\",\n    name: \"OpenAI Call Traceable\",\n    project_name: \"My Project\"\n  });\n\n  // Call the traceable function\n  await traceableCallOpenAI(messages, \"gpt-4o-mini\");\n\n  // Create and use a RunTree object\n  const rt = new RunTree({\n    run_type: \"llm\",\n    name: \"OpenAI Call RunTree\",\n    inputs: { messages },\n    project_name: \"My Project\"\n  });\n  await rt.postRun();\n\n  // Execute a chat completion and handle it within RunTree\n  rt.end({outputs: chatCompletion});\n  await rt.patchRun();\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-traces-to-project.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 4893
}