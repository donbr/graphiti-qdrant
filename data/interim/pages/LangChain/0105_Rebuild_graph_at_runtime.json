{
  "title": "Rebuild graph at runtime",
  "source_url": "https://docs.langchain.com/langsmith/graph-rebuild",
  "content": "You might need to rebuild your graph with a different configuration for a new run. For example, you might need to use a different graph state or graph structure depending on the config. This guide shows how you can do this.\n\n<Note>\n  **Note**\n  In most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it\n</Note>\n\n## Prerequisites\n\nMake sure to check out [this how-to guide](/langsmith/setup-app-requirements-txt) on setting up your app for deployment first.\n\n## Define graphs\n\nLet's say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:\n\n```\nmy-app/\n|-- requirements.txt\n|-- .env\n|-- openai_agent.py     # code for your graph\n```\n\nwhere the graph is defined in `openai_agent.py`.\n\n### No rebuild\n\nIn the standard LangGraph API configuration, the server uses the compiled graph instance that's defined at the top level of `openai_agent.py`, which looks like the following:\n\n```python  theme={null}\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, START, MessageGraph\n\nmodel = ChatOpenAI(temperature=0)\n\ngraph_workflow = MessageGraph()\n\ngraph_workflow.add_node(\"agent\", model)\ngraph_workflow.add_edge(\"agent\", END)\ngraph_workflow.add_edge(START, \"agent\")\n\nagent = graph_workflow.compile()\n```\n\nTo make the server aware of your graph, you need to specify a path to the variable that contains the [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) instance in your LangGraph API configuration (`langgraph.json`), e.g.:\n\n```\n{\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"openai_agent\": \"./openai_agent.py:agent\",\n    },\n    \"env\": \"./.env\"\n}\n```\n\n### Rebuild\n\nTo make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a *function* that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:\n\n```python  theme={null}\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, START, MessageGraph\nfrom langgraph.graph.state import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langchain.tools import tool\nfrom langgraph.prebuilt import ToolNode\nfrom langchain_core.messages import BaseMessage\nfrom langchain_core.runnables import RunnableConfig\n\n\nclass State(TypedDict):\n    messages: Annotated[list[BaseMessage], add_messages]\n\n\nmodel = ChatOpenAI(temperature=0)\n\ndef make_default_graph():\n    \"\"\"Make a simple LLM agent\"\"\"\n    graph_workflow = StateGraph(State)\n    def call_model(state):\n        return {\"messages\": [model.invoke(state[\"messages\"])]}\n\n    graph_workflow.add_node(\"agent\", call_model)\n    graph_workflow.add_edge(\"agent\", END)\n    graph_workflow.add_edge(START, \"agent\")\n\n    agent = graph_workflow.compile()\n    return agent\n\n\ndef make_alternative_graph():\n    \"\"\"Make a tool-calling agent\"\"\"\n\n    @tool\n    def add(a: float, b: float):\n        \"\"\"Adds two numbers.\"\"\"\n        return a + b\n\n    tool_node = ToolNode([add])\n    model_with_tools = model.bind_tools([add])\n    def call_model(state):\n        return {\"messages\": [model_with_tools.invoke(state[\"messages\"])]}\n\n    def should_continue(state: State):\n        if state[\"messages\"][-1].tool_calls:\n            return \"tools\"\n        else:\n            return END\n\n    graph_workflow = StateGraph(State)\n\n    graph_workflow.add_node(\"agent\", call_model)\n    graph_workflow.add_node(\"tools\", tool_node)\n    graph_workflow.add_edge(\"tools\", \"agent\")\n    graph_workflow.add_edge(START, \"agent\")\n    graph_workflow.add_conditional_edges(\"agent\", should_continue)\n\n    agent = graph_workflow.compile()\n    return agent\n\n\n# this is the graph making function that will decide which graph to\n# build based on the provided config\ndef make_graph(config: RunnableConfig):\n    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n    # route to different graph state / structure based on the user ID\n    if user_id == \"1\":\n        return make_default_graph()\n    else:\n        return make_alternative_graph()\n```\n\nFinally, you need to specify the path to your graph-making function (`make_graph`) in `langgraph.json`:\n\n```\n{\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"openai_agent\": \"./openai_agent.py:make_graph\",\n    },\n    \"env\": \"./.env\"\n}\n```\n\nSee more info on LangGraph API configuration file [here](/langsmith/cli#configuration-file)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/graph-rebuild.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 5093
}