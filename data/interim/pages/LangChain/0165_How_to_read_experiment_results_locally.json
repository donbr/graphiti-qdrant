{
  "title": "How to read experiment results locally",
  "source_url": "https://docs.langchain.com/langsmith/read-local-experiment-results",
  "content": "When running [evaluations](/langsmith/evaluation-concepts), you may want to process results programmatically in your script rather than viewing them in the [LangSmith UI](https://smith.langchain.com). This is useful for scenarios like:\n\n* **CI/CD pipelines**: Implement quality gates that fail builds if evaluation scores drop below a threshold.\n* **Local debugging**: Inspect and analyze results without API calls.\n* **Custom aggregations**: Calculate metrics and statistics using your own logic.\n* **Integration testing**: Use evaluation results to gate merges or deployments.\n\nThis guide shows you how to iterate over and process [experiment](/langsmith/evaluation-concepts#experiment) results from the [`ExperimentResults`](https://reference.langchain.com/python/langsmith/observability/sdk/evaluation/#langsmith.evaluation._runner.ExperimentResults) object returned by [`Client.evaluate()`](https://reference.langchain.com/python/langsmith/observability/sdk/client/#langsmith.client.Client.evaluate).\n\n<Note>\n  This page focuses on processing results programmatically while still uploading them to LangSmith.\n\n  If you want to run evaluations locally **without** recording anything to LangSmith (for quick testing or validation), refer to [Run an evaluation locally](/langsmith/local) which uses `upload_results=False`.\n</Note>\n\n## Iterate over evaluation results\n\nThe [`evaluate()`](https://reference.langchain.com/python/langsmith/observability/sdk/client/#langsmith.client.Client.evaluate) function returns an [`ExperimentResults`](https://reference.langchain.com/python/langsmith/observability/sdk/evaluation/#langsmith.evaluation._runner.ExperimentResults) object that you can iterate over. The `blocking` parameter controls when results become available:\n\n* `blocking=False`: Returns immediately with an iterator that yields results as they're produced. This allows you to process results in real-time as the evaluation runs.\n* `blocking=True` (default): Blocks until all evaluations complete before returning. When you iterate over the results, all data is already available.\n\nBoth modes return the same `ExperimentResults` type; the difference is whether the function waits for completion before returning. Use `blocking=False` for streaming and real-time debugging, or `blocking=True` for batch processing when you need the complete dataset.\n\nThe following example demonstrates `blocking=False`. It iterates over results as they stream in, collects them in a list, then processes them in a separate loop:\n\n```python  theme={null}\nfrom langsmith import Client\nimport random\n\nclient = Client()\n\ndef target(inputs):\n    \"\"\"Your application or LLM chain\"\"\"\n    return {\"output\": \"MY OUTPUT\"}\n\ndef evaluator(run, example):\n    \"\"\"Your evaluator function\"\"\"\n    return {\"key\": \"randomness\", \"score\": random.randint(0, 1)}\n\n# Run evaluation with blocking=False to get an iterator\nstreamed_results = client.evaluate(\n    target,\n    data=\"MY_DATASET_NAME\",\n    evaluators=[evaluator],\n    blocking=False\n)\n\n# Collect results as they stream in\naggregated_results = []\nfor result in streamed_results:\n    aggregated_results.append(result)\n\n# Separate loop to avoid logging at the same time as logs from evaluate()\nfor result in aggregated_results:\n    print(\"Input:\", result[\"run\"].inputs)\n    print(\"Output:\", result[\"run\"].outputs)\n    print(\"Evaluation Results:\", result[\"evaluation_results\"][\"results\"])\n    print(\"--------------------------------\")\n```\n\nThis produces output like:\n\n```\nInput: {'input': 'MY INPUT'}\nOutput: {'output': 'MY OUTPUT'}\nEvaluation Results: [EvaluationResult(key='randomness', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7ebb4900-91c0-40b0-bb10-f2f6a451fd3c'), target_run_id=None, extra=None)]\n--------------------------------\n```\n\n## Understand the result structure\n\nEach result in the iterator contains:\n\n* `result[\"run\"]`: The execution of your target function.\n  * `result[\"run\"].inputs`: The inputs from your [dataset](/langsmith/evaluation-concepts#datasets) example.\n  * `result[\"run\"].outputs`: The outputs produced by your target function.\n  * `result[\"run\"].id`: The unique ID for this run.\n\n* `result[\"evaluation_results\"][\"results\"]`: A list of `EvaluationResult` objects, one per evaluator.\n  * `key`: The metric name (from your evaluator's return value).\n  * `score`: The numeric score (typically 0-1 or boolean).\n  * `comment`: Optional explanatory text.\n  * `source_run_id`: The ID of the evaluator run.\n\n* `result[\"example\"]`: The dataset example that was evaluated.\n  * `result[\"example\"].inputs`: The input values.\n  * `result[\"example\"].outputs`: The reference outputs (if any).\n\n## Examples\n\n### Implement a quality gate\n\nThis example uses evaluation results to pass or fail a CI/CD build automatically based on quality thresholds. The script iterates through results, calculates an average accuracy score, and exits with a non-zero status code if the accuracy falls below 85%. This ensures that you can deploy code changes that meet quality standards.\n\n```python  theme={null}\nfrom langsmith import Client\nimport sys\n\nclient = Client()\n\ndef my_application(inputs):\n    # Your application logic\n    return {\"response\": \"...\"}\n\ndef accuracy_evaluator(run, example):\n    # Your evaluation logic\n    is_correct = run.outputs[\"response\"] == example.outputs[\"expected\"]\n    return {\"key\": \"accuracy\", \"score\": 1 if is_correct else 0}\n\n# Run evaluation\nresults = client.evaluate(\n    my_application,\n    data=\"my_test_dataset\",\n    evaluators=[accuracy_evaluator],\n    blocking=False\n)\n\n# Calculate aggregate metrics\ntotal_score = 0\ncount = 0\n\nfor result in results:\n    eval_result = result[\"evaluation_results\"][\"results\"][0]\n    total_score += eval_result.score\n    count += 1\n\naverage_accuracy = total_score / count\n\nprint(f\"Average accuracy: {average_accuracy:.2%}\")\n\n# Fail the build if accuracy is too low\nif average_accuracy < 0.85:\n    print(\"❌ Evaluation failed! Accuracy below 85% threshold.\")\n    sys.exit(1)\n\nprint(\"✅ Evaluation passed!\")\n```\n\n### Batch processing with blocking=True\n\nWhen you need to perform operations that require the complete dataset (like calculating percentiles, sorting by score, or generating summary reports), use `blocking=True` to wait for all evaluations to complete before processing:\n\n```python  theme={null}\n# Run evaluation and wait for all results\nresults = client.evaluate(\n    target,\n    data=dataset,\n    evaluators=[evaluator],\n    blocking=True  # Wait for all evaluations to complete\n)\n\n# Process all results after evaluation completes\nfor result in results:\n    print(\"Input:\", result[\"run\"].inputs)\n    print(\"Output:\", result[\"run\"].outputs)\n\n    # Access individual evaluation results\n    for eval_result in result[\"evaluation_results\"][\"results\"]:\n        print(f\"  {eval_result.key}: {eval_result.score}\")\n```\n\nWith `blocking=True`, your processing code runs only after all evaluations are complete, avoiding mixed output with evaluation logs.\n\nFor more information on running evaluations without uploading results, refer to [Run an evaluation locally](/langsmith/local).\n\n## Related\n\n* [Evaluate your LLM application](/langsmith/evaluate-llm-application)\n* [Run an evaluation locally](/langsmith/local)\n* [Fetch performance metrics from an experiment](/langsmith/fetch-perf-metrics-experiment)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/read-local-experiment-results.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 7745
}