{
  "title": "Streaming",
  "source_url": "https://docs.langchain.com/oss/javascript/langgraph/streaming",
  "content": "LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\nWhat's possible with LangGraph streaming:\n\n* <Icon icon=\"share-nodes\" size={16} /> [**Stream graph state**](#stream-graph-state) — get state updates / values with `updates` and `values` modes.\n* <Icon icon=\"square-poll-horizontal\" size={16} /> [**Stream subgraph outputs**](#stream-subgraph-outputs) — include outputs from both the parent graph and any nested subgraphs.\n* <Icon icon=\"square-binary\" size={16} /> [**Stream LLM tokens**](#messages) — capture token streams from anywhere: inside nodes, subgraphs, or tools.\n* <Icon icon=\"table\" size={16} /> [**Stream custom data**](#stream-custom-data) — send custom updates or progress signals directly from tool functions.\n* <Icon icon=\"layer-plus\" size={16} /> [**Use multiple streaming modes**](#stream-multiple-modes) — choose from `values` (full state), `updates` (state deltas), `messages` (LLM tokens + metadata), `custom` (arbitrary user data), or `debug` (detailed traces).\n\n## Supported stream modes\n\nPass one or more of the following stream modes as a list to the [`stream`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.CompiledStateGraph.html#stream) method:\n\n| Mode       | Description                                                                                                                                                                         |\n| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `values`   | Streams the full value of the state after each step of the graph.                                                                                                                   |\n| `updates`  | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |\n| `custom`   | Streams custom data from inside your graph nodes.                                                                                                                                   |\n| `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.                                                                                                |\n| `debug`    | Streams as much information as possible throughout the execution of the graph.                                                                                                      |\n\n## Basic usage example\n\nLangGraph graphs expose the [`stream`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.pregel.Pregel.html#stream) method to yield streamed outputs as iterators.\n\n```typescript  theme={null}\nfor await (const chunk of await graph.stream(inputs, {\n  streamMode: \"updates\",\n})) {\n  console.log(chunk);\n}\n```\n\n<Accordion title=\"Extended example: streaming updates\">\n  ```typescript  theme={null}\n  import { StateGraph, START, END } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  const State = z.object({\n    topic: z.string(),\n    joke: z.string(),\n  });\n\n  const graph = new StateGraph(State)\n    .addNode(\"refineTopic\", (state) => {\n      return { topic: state.topic + \" and cats\" };\n    })\n    .addNode(\"generateJoke\", (state) => {\n      return { joke: `This is a joke about ${state.topic}` };\n    })\n    .addEdge(START, \"refineTopic\")\n    .addEdge(\"refineTopic\", \"generateJoke\")\n    .addEdge(\"generateJoke\", END)\n    .compile();\n\n  for await (const chunk of await graph.stream(\n    { topic: \"ice cream\" },\n    // Set streamMode: \"updates\" to stream only the updates to the graph state after each node\n    // Other stream modes are also available. See supported stream modes for details\n    { streamMode: \"updates\" }\n  )) {\n    console.log(chunk);\n  }\n  ```\n\n  ```output  theme={null}\n  {'refineTopic': {'topic': 'ice cream and cats'}}\n  {'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}\n  ```\n</Accordion>\n\n## Stream multiple modes\n\nYou can pass an array as the `streamMode` parameter to stream multiple modes at once.\n\nThe streamed outputs will be tuples of `[mode, chunk]` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.\n\n```typescript  theme={null}\nfor await (const [mode, chunk] of await graph.stream(inputs, {\n  streamMode: [\"updates\", \"custom\"],\n})) {\n  console.log(chunk);\n}\n```\n\n## Stream graph state\n\nUse the stream modes `updates` and `values` to stream the state of the graph as it executes.\n\n* `updates` streams the **updates** to the state after each step of the graph.\n* `values` streams the **full value** of the state after each step of the graph.\n\n```typescript  theme={null}\nimport { StateGraph, START, END } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst State = z.object({\n  topic: z.string(),\n  joke: z.string(),\n});\n\nconst graph = new StateGraph(State)\n  .addNode(\"refineTopic\", (state) => {\n    return { topic: state.topic + \" and cats\" };\n  })\n  .addNode(\"generateJoke\", (state) => {\n    return { joke: `This is a joke about ${state.topic}` };\n  })\n  .addEdge(START, \"refineTopic\")\n  .addEdge(\"refineTopic\", \"generateJoke\")\n  .addEdge(\"generateJoke\", END)\n  .compile();\n```\n\n<Tabs>\n  <Tab title=\"updates\">\n    Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.\n\n    ```typescript  theme={null}\n    for await (const chunk of await graph.stream(\n      { topic: \"ice cream\" },\n      { streamMode: \"updates\" }\n    )) {\n      console.log(chunk);\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"values\">\n    Use this to stream the **full state** of the graph after each step.\n\n    ```typescript  theme={null}\n    for await (const chunk of await graph.stream(\n      { topic: \"ice cream\" },\n      { streamMode: \"values\" }\n    )) {\n      console.log(chunk);\n    }\n    ```\n  </Tab>\n</Tabs>\n\n## Stream subgraph outputs\n\nTo include outputs from [subgraphs](/oss/javascript/langgraph/use-subgraphs) in the streamed outputs, you can set `subgraphs: true` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\n\nThe outputs will be streamed as tuples `[namespace, data]`, where `namespace` is a tuple with the path to the node where a subgraph is invoked, e.g. `[\"parent_node:<task_id>\", \"child_node:<task_id>\"]`.\n\n```typescript  theme={null}\nfor await (const chunk of await graph.stream(\n  { foo: \"foo\" },\n  {\n    // Set subgraphs: true to stream outputs from subgraphs\n    subgraphs: true,\n    streamMode: \"updates\",\n  }\n)) {\n  console.log(chunk);\n}\n```\n\n<Accordion title=\"Extended example: streaming from subgraphs\">\n  ```typescript  theme={null}\n  import { StateGraph, START } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  // Define subgraph\n  const SubgraphState = z.object({\n    foo: z.string(), // note that this key is shared with the parent graph state\n    bar: z.string(),\n  });\n\n  const subgraphBuilder = new StateGraph(SubgraphState)\n    .addNode(\"subgraphNode1\", (state) => {\n      return { bar: \"bar\" };\n    })\n    .addNode(\"subgraphNode2\", (state) => {\n      return { foo: state.foo + state.bar };\n    })\n    .addEdge(START, \"subgraphNode1\")\n    .addEdge(\"subgraphNode1\", \"subgraphNode2\");\n  const subgraph = subgraphBuilder.compile();\n\n  // Define parent graph\n  const ParentState = z.object({\n    foo: z.string(),\n  });\n\n  const builder = new StateGraph(ParentState)\n    .addNode(\"node1\", (state) => {\n      return { foo: \"hi! \" + state.foo };\n    })\n    .addNode(\"node2\", subgraph)\n    .addEdge(START, \"node1\")\n    .addEdge(\"node1\", \"node2\");\n  const graph = builder.compile();\n\n  for await (const chunk of await graph.stream(\n    { foo: \"foo\" },\n    {\n      streamMode: \"updates\",\n      // Set subgraphs: true to stream outputs from subgraphs\n      subgraphs: true,\n    }\n  )) {\n    console.log(chunk);\n  }\n  ```\n\n  ```\n  [[], {'node1': {'foo': 'hi! foo'}}]\n  [['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode1': {'bar': 'bar'}}]\n  [['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode2': {'foo': 'hi! foobar'}}]\n  [[], {'node2': {'foo': 'hi! foobar'}}]\n  ```\n\n  **Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\n</Accordion>\n\n<a id=\"debug\" />\n\n### Debugging\n\nUse the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\n\n```typescript  theme={null}\nfor await (const chunk of await graph.stream(\n  { topic: \"ice cream\" },\n  { streamMode: \"debug\" }\n)) {\n  console.log(chunk);\n}\n```\n\n<a id=\"messages\" />\n\n## LLM tokens\n\nUse the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.\n\nThe streamed output from [`messages` mode](#supported-stream-modes) is a tuple `[message_chunk, metadata]` where:\n\n* `message_chunk`: the token or message segment from the LLM.\n* `metadata`: a dictionary containing details about the graph node and LLM invocation.\n\n> If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.\n\n```typescript  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { StateGraph, START } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst MyState = z.object({\n  topic: z.string(),\n  joke: z.string().default(\"\"),\n});\n\nconst model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n\nconst callModel = async (state: z.infer<typeof MyState>) => {\n  // Call the LLM to generate a joke about a topic\n  // Note that message events are emitted even when the LLM is run using .invoke rather than .stream\n  const modelResponse = await model.invoke([\n    { role: \"user\", content: `Generate a joke about ${state.topic}` },\n  ]);\n  return { joke: modelResponse.content };\n};\n\nconst graph = new StateGraph(MyState)\n  .addNode(\"callModel\", callModel)\n  .addEdge(START, \"callModel\")\n  .compile();\n\n// The \"messages\" stream mode returns an iterator of tuples [messageChunk, metadata]\n// where messageChunk is the token streamed by the LLM and metadata is a dictionary\n// with information about the graph node where the LLM was called and other information\nfor await (const [messageChunk, metadata] of await graph.stream(\n  { topic: \"ice cream\" },\n  { streamMode: \"messages\" }\n)) {\n  if (messageChunk.content) {\n    console.log(messageChunk.content + \"|\");\n  }\n}\n```\n\n#### Filter by LLM invocation\n\nYou can associate `tags` with LLM invocations to filter the streamed tokens by LLM invocation.\n\n```typescript  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// model1 is tagged with \"joke\"\nconst model1 = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  tags: ['joke']\n});\n// model2 is tagged with \"poem\"\nconst model2 = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  tags: ['poem']\n});\n\nconst graph = // ... define a graph that uses these LLMs\n\n// The streamMode is set to \"messages\" to stream LLM tokens\n// The metadata contains information about the LLM invocation, including the tags\nfor await (const [msg, metadata] of await graph.stream(\n  { topic: \"cats\" },\n  { streamMode: \"messages\" }\n)) {\n  // Filter the streamed tokens by the tags field in the metadata to only include\n  // the tokens from the LLM invocation with the \"joke\" tag\n  if (metadata.tags?.includes(\"joke\")) {\n    console.log(msg.content + \"|\");\n  }\n}\n```\n\n<Accordion title=\"Extended example: filtering by tags\">\n  ```typescript  theme={null}\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { StateGraph, START } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  // The jokeModel is tagged with \"joke\"\n  const jokeModel = new ChatOpenAI({\n    model: \"gpt-4o-mini\",\n    tags: [\"joke\"]\n  });\n  // The poemModel is tagged with \"poem\"\n  const poemModel = new ChatOpenAI({\n    model: \"gpt-4o-mini\",\n    tags: [\"poem\"]\n  });\n\n  const State = z.object({\n    topic: z.string(),\n    joke: z.string(),\n    poem: z.string(),\n  });\n\n  const graph = new StateGraph(State)\n    .addNode(\"callModel\", async (state) => {\n      const topic = state.topic;\n      console.log(\"Writing joke...\");\n\n      const jokeResponse = await jokeModel.invoke([\n        { role: \"user\", content: `Write a joke about ${topic}` }\n      ]);\n\n      console.log(\"\\n\\nWriting poem...\");\n      const poemResponse = await poemModel.invoke([\n        { role: \"user\", content: `Write a short poem about ${topic}` }\n      ]);\n\n      return {\n        joke: jokeResponse.content,\n        poem: poemResponse.content\n      };\n    })\n    .addEdge(START, \"callModel\")\n    .compile();\n\n  // The streamMode is set to \"messages\" to stream LLM tokens\n  // The metadata contains information about the LLM invocation, including the tags\n  for await (const [msg, metadata] of await graph.stream(\n    { topic: \"cats\" },\n    { streamMode: \"messages\" }\n  )) {\n    // Filter the streamed tokens by the tags field in the metadata to only include\n    // the tokens from the LLM invocation with the \"joke\" tag\n    if (metadata.tags?.includes(\"joke\")) {\n      console.log(msg.content + \"|\");\n    }\n  }\n  ```\n</Accordion>\n\n#### Filter by node\n\nTo stream tokens only from specific nodes, use `stream_mode=\"messages\"` and filter the outputs by the `langgraph_node` field in the streamed metadata:\n\n```typescript  theme={null}\n// The \"messages\" stream mode returns a tuple of [messageChunk, metadata]\n// where messageChunk is the token streamed by the LLM and metadata is a dictionary\n// with information about the graph node where the LLM was called and other information\nfor await (const [msg, metadata] of await graph.stream(\n  inputs,\n  { streamMode: \"messages\" }\n)) {\n  // Filter the streamed tokens by the langgraph_node field in the metadata\n  // to only include the tokens from the specified node\n  if (msg.content && metadata.langgraph_node === \"some_node_name\") {\n    // ...\n  }\n}\n```\n\n<Accordion title=\"Extended example: streaming LLM tokens from specific nodes\">\n  ```typescript  theme={null}\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { StateGraph, START } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n\n  const State = z.object({\n    topic: z.string(),\n    joke: z.string(),\n    poem: z.string(),\n  });\n\n  const graph = new StateGraph(State)\n    .addNode(\"writeJoke\", async (state) => {\n      const topic = state.topic;\n      const jokeResponse = await model.invoke([\n        { role: \"user\", content: `Write a joke about ${topic}` }\n      ]);\n      return { joke: jokeResponse.content };\n    })\n    .addNode(\"writePoem\", async (state) => {\n      const topic = state.topic;\n      const poemResponse = await model.invoke([\n        { role: \"user\", content: `Write a short poem about ${topic}` }\n      ]);\n      return { poem: poemResponse.content };\n    })\n    // write both the joke and the poem concurrently\n    .addEdge(START, \"writeJoke\")\n    .addEdge(START, \"writePoem\")\n    .compile();\n\n  // The \"messages\" stream mode returns a tuple of [messageChunk, metadata]\n  // where messageChunk is the token streamed by the LLM and metadata is a dictionary\n  // with information about the graph node where the LLM was called and other information\n  for await (const [msg, metadata] of await graph.stream(\n    { topic: \"cats\" },\n    { streamMode: \"messages\" }\n  )) {\n    // Filter the streamed tokens by the langgraph_node field in the metadata\n    // to only include the tokens from the writePoem node\n    if (msg.content && metadata.langgraph_node === \"writePoem\") {\n      console.log(msg.content + \"|\");\n    }\n  }\n  ```\n</Accordion>\n\n## Stream custom data\n\nTo send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:\n\n1. Use the `writer` parameter from the `LangGraphRunnableConfig` to emit custom data.\n2. Set `streamMode: \"custom\"` when calling `.stream()` to get the custom data in the stream. You can combine multiple modes (e.g., `[\"updates\", \"custom\"]`), but at least one must be `\"custom\"`.\n\n<Tabs>\n  <Tab title=\"node\">\n    ```typescript  theme={null}\n    import { StateGraph, START, LangGraphRunnableConfig } from \"@langchain/langgraph\";\n    import * as z from \"zod\";\n\n    const State = z.object({\n      query: z.string(),\n      answer: z.string(),\n    });\n\n    const graph = new StateGraph(State)\n      .addNode(\"node\", async (state, config) => {\n        // Use the writer to emit a custom key-value pair (e.g., progress update)\n        config.writer({ custom_key: \"Generating custom data inside node\" });\n        return { answer: \"some data\" };\n      })\n      .addEdge(START, \"node\")\n      .compile();\n\n    const inputs = { query: \"example\" };\n\n    // Set streamMode: \"custom\" to receive the custom data in the stream\n    for await (const chunk of await graph.stream(inputs, { streamMode: \"custom\" })) {\n      console.log(chunk);\n    }\n    ```\n  </Tab>\n\n  <Tab title=\"tool\">\n    ```typescript  theme={null}\n    import { tool } from \"@langchain/core/tools\";\n    import { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n    import * as z from \"zod\";\n\n    const queryDatabase = tool(\n      async (input, config: LangGraphRunnableConfig) => {\n        // Use the writer to emit a custom key-value pair (e.g., progress update)\n        config.writer({ data: \"Retrieved 0/100 records\", type: \"progress\" });\n        // perform query\n        // Emit another custom key-value pair\n        config.writer({ data: \"Retrieved 100/100 records\", type: \"progress\" });\n        return \"some-answer\";\n      },\n      {\n        name: \"query_database\",\n        description: \"Query the database.\",\n        schema: z.object({\n          query: z.string().describe(\"The query to execute.\"),\n        }),\n      }\n    );\n\n    const graph = // ... define a graph that uses this tool\n\n    // Set streamMode: \"custom\" to receive the custom data in the stream\n    for await (const chunk of await graph.stream(inputs, { streamMode: \"custom\" })) {\n      console.log(chunk);\n    }\n    ```\n  </Tab>\n</Tabs>\n\n## Use with any LLM\n\nYou can use `streamMode: \"custom\"` to stream data from **any LLM API** — even if that API does **not** implement the LangChain chat model interface.\n\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\n\n```typescript  theme={null}\nimport { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst callArbitraryModel = async (\n  state: any,\n  config: LangGraphRunnableConfig\n) => {\n  // Example node that calls an arbitrary model and streams the output\n  // Assume you have a streaming client that yields chunks\n  // Generate LLM tokens using your custom streaming client\n  for await (const chunk of yourCustomStreamingClient(state.topic)) {\n    // Use the writer to send custom data to the stream\n    config.writer({ custom_llm_chunk: chunk });\n  }\n  return { result: \"completed\" };\n};\n\nconst graph = new StateGraph(State)\n  .addNode(\"callArbitraryModel\", callArbitraryModel)\n  // Add other nodes and edges as needed\n  .compile();\n\n// Set streamMode: \"custom\" to receive the custom data in the stream\nfor await (const chunk of await graph.stream(\n  { topic: \"cats\" },\n  { streamMode: \"custom\" }\n)) {\n  // The chunk will contain the custom data streamed from the llm\n  console.log(chunk);\n}\n```\n\n<Accordion title=\"Extended example: streaming arbitrary chat model\">\n  ```typescript  theme={null}\n  import { StateGraph, START, MessagesZodMeta, LangGraphRunnableConfig } from \"@langchain/langgraph\";\n  import { BaseMessage } from \"@langchain/core/messages\";\n  import { registry } from \"@langchain/langgraph/zod\";\n  import * as z from \"zod\";\n  import OpenAI from \"openai\";\n\n  const openaiClient = new OpenAI();\n  const modelName = \"gpt-4o-mini\";\n\n  async function* streamTokens(modelName: string, messages: any[]) {\n    const response = await openaiClient.chat.completions.create({\n      messages,\n      model: modelName,\n      stream: true,\n    });\n\n    let role: string | null = null;\n    for await (const chunk of response) {\n      const delta = chunk.choices[0]?.delta;\n\n      if (delta?.role) {\n        role = delta.role;\n      }\n\n      if (delta?.content) {\n        yield { role, content: delta.content };\n      }\n    }\n  }\n\n  // this is our tool\n  const getItems = tool(\n    async (input, config: LangGraphRunnableConfig) => {\n      let response = \"\";\n      for await (const msgChunk of streamTokens(\n        modelName,\n        [\n          {\n            role: \"user\",\n            content: `Can you tell me what kind of items i might find in the following place: '${input.place}'. List at least 3 such items separating them by a comma. And include a brief description of each item.`,\n          },\n        ]\n      )) {\n        response += msgChunk.content;\n        config.writer?.(msgChunk);\n      }\n      return response;\n    },\n    {\n      name: \"get_items\",\n      description: \"Use this tool to list items one might find in a place you're asked about.\",\n      schema: z.object({\n        place: z.string().describe(\"The place to look up items for.\"),\n      }),\n    }\n  );\n\n  const State = z.object({\n    messages: z\n      .array(z.custom<BaseMessage>())\n      .register(registry, MessagesZodMeta),\n  });\n\n  const graph = new StateGraph(State)\n    // this is the tool-calling graph node\n    .addNode(\"callTool\", async (state) => {\n      const aiMessage = state.messages.at(-1);\n      const toolCall = aiMessage.tool_calls?.at(-1);\n\n      const functionName = toolCall?.function?.name;\n      if (functionName !== \"get_items\") {\n        throw new Error(`Tool ${functionName} not supported`);\n      }\n\n      const functionArguments = toolCall?.function?.arguments;\n      const args = JSON.parse(functionArguments);\n\n      const functionResponse = await getItems.invoke(args);\n      const toolMessage = {\n        tool_call_id: toolCall.id,\n        role: \"tool\",\n        name: functionName,\n        content: functionResponse,\n      };\n      return { messages: [toolMessage] };\n    })\n    .addEdge(START, \"callTool\")\n    .compile();\n  ```\n\n  Let's invoke the graph with an [`AIMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.AIMessage.html) that includes a tool call:\n\n  ```typescript  theme={null}\n  const inputs = {\n    messages: [\n      {\n        content: null,\n        role: \"assistant\",\n        tool_calls: [\n          {\n            id: \"1\",\n            function: {\n              arguments: '{\"place\":\"bedroom\"}',\n              name: \"get_items\",\n            },\n            type: \"function\",\n          }\n        ],\n      }\n    ]\n  };\n\n  for await (const chunk of await graph.stream(\n    inputs,\n    { streamMode: \"custom\" }\n  )) {\n    console.log(chunk.content + \"|\");\n  }\n  ```\n</Accordion>\n\n## Disable streaming for specific chat models\n\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\nmodels that do not support it.\n\nSet `streaming: false` when initializing the model.\n\n```typescript  theme={null}\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"o1-preview\",\n  // Set streaming: false to disable streaming for the chat model\n  streaming: false,\n});\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 24242
}