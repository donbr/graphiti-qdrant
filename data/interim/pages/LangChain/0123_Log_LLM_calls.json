{
  "title": "Log LLM calls",
  "source_url": "https://docs.langchain.com/langsmith/log-llm-trace",
  "content": "This guide will cover how to log LLM calls to LangSmith when you are using a custom model or a custom input/output format. To make the most of LangSmith's LLM trace processing, you should log your LLM traces in one of the specified formats.\n\nLangSmith offers the following benefits for LLM traces:\n\n* Rich, structured rendering of message lists\n* Token and cost tracking per LLM call, per trace and across traces over time\n\nIf you don't log your LLM traces in the suggested formats, you will still be able to log the data to LangSmith, but it may not be processed or rendered in expected ways.\n\nIf you are using [LangChain OSS](https://python.langchain.com/docs/tutorials/llm_chain/) to call language models or LangSmith wrappers ([OpenAI](/langsmith/trace-openai), [Anthropic](/langsmith/trace-anthropic)), these approaches will automatically log traces in the correct format.\n\n<Note>\n  The examples on this page use the `traceable` decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the [RunTree](/langsmith/annotate-code#use-the-runtree-api) or [API](https://api.smith.langchain.com/redoc) directly.\n</Note>\n\n## Messages Format\n\nWhen tracing a custom model or a custom input/output format, it must either follow the LangChain format, OpenAI completions format or Anthropic messages format. For more details,  refer to the [OpenAI Chat Completions](https://platform.openai.com/docs/api-reference/chat/create) or [Anthropic Messages](https://platform.claude.com/docs/en/api/messages) documentation. The LangChain format is:\n\n<Expandable title=\"LangChain format\">\n  <ParamField path=\"messages\" type=\"array\" required>\n    A list of messages containing the content of the conversation.\n\n    <ParamField path=\"role\" type=\"string\" required>\n      Identifies the message type. One of: <code>system</code> | <code>reasoning</code> | <code>user</code> | <code>assistant</code> | <code>tool</code>\n    </ParamField>\n\n    <ParamField path=\"content\" type=\"array\" required>\n      Content of the message. List of typed dictionaries.\n\n      <Expandable title=\"Content options\">\n        <ParamField path=\"type\" type=\"string\" required>\n          One of: <code>text</code> | <code>image</code> | <code>file</code> | <code>audio</code> | <code>video</code> | <code>tool\\_call</code> | <code>server\\_tool\\_call</code> | <code>server\\_tool\\_result</code>.\n        </ParamField>\n\n        <Expandable title=\"text\">\n          <ParamField path=\"type\" type=\"literal('text')\" required />\n\n          <ParamField path=\"text\" type=\"string\" required>\n            Text content.\n          </ParamField>\n\n          <ParamField path=\"annotations\" type=\"object[]\">\n            List of annotations for the text\n          </ParamField>\n\n          <ParamField path=\"extras\" type=\"object\">\n            Additional provider-specific data.\n          </ParamField>\n        </Expandable>\n\n        <Expandable title=\"reasoning\">\n          <ParamField path=\"type\" type=\"literal('reasoning')\" required />\n\n          <ParamField path=\"text\" type=\"string\" required>\n            Text content.\n          </ParamField>\n\n          <ParamField path=\"extras\" type=\"object\">\n            Additional provider-specific data.\n          </ParamField>\n        </Expandable>\n\n        <Expandable title=\"image\">\n          <ParamField path=\"type\" type=\"literal('image')\" required />\n\n          <ParamField path=\"url\" type=\"string\">\n            URL pointing to the image location.\n          </ParamField>\n\n          <ParamField path=\"base64\" type=\"string\" required>\n            Base64-encoded image data.\n          </ParamField>\n\n          <ParamField path=\"id\" type=\"string\">\n            Reference ID to an externally stored image (e.g., in a provider’s file system or in a bucket).\n          </ParamField>\n\n          <ParamField path=\"mime_type\" type=\"string\">\n            Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`).\n          </ParamField>\n        </Expandable>\n\n        <Expandable title=\"file (e.g., PDFs)\">\n          <ParamField path=\"type\" type=\"literal('file')\" required />\n\n          <ParamField path=\"url\" type=\"string\">\n            URL pointing to the file.\n          </ParamField>\n\n          <ParamField path=\"base64\" type=\"string\" required>\n            Base64-encoded file data.\n          </ParamField>\n\n          <ParamField path=\"id\" type=\"string\">\n            Reference ID to an externally stored file (e.g., in a provider’s file system or in a bucket).\n          </ParamField>\n\n          <ParamField path=\"mime_type\" type=\"string\">\n            File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `application/pdf`).\n          </ParamField>\n        </Expandable>\n\n        <Expandable title=\"audio\">\n          <ParamField path=\"type\" type=\"literal('audio')\" required />\n\n          <ParamField path=\"url\" type=\"string\">\n            URL pointing to the audio file.\n          </ParamField>\n\n          <ParamField path=\"base64\" type=\"string\" required>\n            Base64-encoded audio data.\n          </ParamField>\n\n          <ParamField path=\"id\" type=\"string\">\n            Reference ID to an externally stored audio file (e.g., in a provider’s file system or in a bucket).\n          </ParamField>\n\n          <ParamField path=\"mime_type\" type=\"string\">\n            Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `audio/mpeg`, `audio/wav`).\n          </ParamField>\n        </Expandable>\n\n        <Expandable title=\"video\">\n          <ParamField path=\"type\" type=\"literal('video')\" required />\n\n          <ParamField path=\"url\" type=\"string\">\n            URL pointing to the video file.\n          </ParamField>\n\n          <ParamField path=\"base64\" type=\"string\" required>\n            Base64-encoded video data.\n          </ParamField>\n\n          <ParamField path=\"id\" type=\"string\">\n            Reference ID to an externally stored video file (e.g., in a provider’s file system or in a bucket).\n          </ParamField>\n\n          <ParamField path=\"mime_type\" type=\"string\">\n            Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `video/mp4`, `video/webm`).\n          </ParamField>\n        </Expandable>\n\n        <Expandable title=\"tool_call\">\n          <ParamField path=\"type\" type=\"literal('tool_call')\" required />\n\n          <ParamField path=\"name\" type=\"string\" />\n\n          <ParamField path=\"args\" type=\"object\" required>\n            Arguments to pass to the tool.\n          </ParamField>\n\n          <ParamField path=\"id\" type=\"string\">\n            Unique identifier for this tool call.\n          </ParamField>\n        </Expandable>\n\n        <Expandable title=\"server_tool_call\">\n          <ParamField path=\"type\" type=\"literal('server_tool_call')\" required />\n\n          <ParamField path=\"id\" type=\"string\" required>\n            Unique identifier for this tool call.\n          </ParamField>\n\n          <ParamField path=\"name\" type=\"string\" required>\n            The name of the tool to be called.\n          </ParamField>\n\n          <ParamField path=\"args\" type=\"object\" required>\n            Arguments to pass to the tool.\n          </ParamField>\n        </Expandable>\n\n        <Expandable title=\"server_tool_result\">\n          <ParamField path=\"type\" type=\"literal('server_tool_result')\" required />\n\n          <ParamField path=\"tool_call_id\" type=\"string\" required>\n            Identifier of the corresponding server tool call.\n          </ParamField>\n\n          <ParamField path=\"id\" type=\"string\">\n            Unique identifier for this tool call.\n          </ParamField>\n\n          <ParamField path=\"status\" type=\"string\" required>\n            Execution status of the server-side tool. One of: <code>success</code> | <code>error</code>.\n          </ParamField>\n\n          <ParamField path=\"output\">\n            Output of the executed tool.\n          </ParamField>\n        </Expandable>\n      </Expandable>\n    </ParamField>\n\n    <ParamField path=\"tool_call_id\" type=\"string\">\n      Must match the <code>id</code> of a prior <code>assistant</code> message’s <code>tool\\_calls\\[i]</code> entry. Only valid when <code>role</code> is <code>tool</code>.\n    </ParamField>\n\n    <ParamField path=\"usage_metadata\" type=\"object\">\n      Use this field to send token counts and/or costs with your model's output. See [this guide](/langsmith/log-llm-trace#provide-token-and-cost-information) for more details.\n    </ParamField>\n  </ParamField>\n</Expandable>\n\n### Examples\n\n<CodeGroup>\n  ```python Text and reasoning theme={null}\n   inputs = {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"Hi, can you tell me the capital of France?\"\n          }\n        ]\n      }\n    ]\n  }\n\n  outputs = {\n    \"messages\": [\n      {\n        \"role\": \"assistant\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"The capital of France is Paris.\"\n          },\n          {\n            \"type\": \"reasoning\",\n            \"text\": \"The user is asking about...\"\n          }\n        ]\n      }\n    ]\n  }\n\n  ```\n\n  ```python Tool calls theme={null}\n  input = {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What's the weather in San Francisco?\"\n          }\n        ]\n      }\n    ]\n  }\n\n  outputs = {\n    \"messages\": [\n      {\n        \"role\": \"assistant\",\n        \"content\": [{\"type\": \"tool_call\", \"name\": \"get_weather\", \"args\": {\"city\": \"San Francisco\"}, \"id\": \"call_1\"}],\n      },\n      {\n        \"role\": \"tool\",\n        \"tool_call_id\": \"call_1\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"{\\\"temperature\\\": \\\"18°C\\\", \\\"condition\\\": \\\"Sunny\\\"}\"\n          }\n        ]\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"The weather in San Francisco is 18°C and sunny.\"\n          }\n        ]\n      }\n    ]\n  }\n  ```\n\n  ```python Multimodal theme={null}\n  inputs = {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What breed is this dog?\"\n          },\n          {\n            \"type\": \"image\",\n            \"url\": \"https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U\",\n            # alternative to a url, you can provide a base64 encoded image\n            # \"base64\": \"<base64 encoded image>\",\n            \"mime_type\": \"image/jpeg\",\n          }\n        ]\n      }\n    ]\n  }\n\n  outputs = {\n    \"messages\": [\n      {\n        \"role\": \"assistant\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"This looks like a Black Labrador.\"\n          }\n        ]\n      }\n    ]\n  }\n  ```\n\n  ```python Server-side tool calls theme={null}\n  input = {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What is the price of AAPL?\"\n          }\n        ]\n      }\n    ]\n  }\n\n  output = {\n    \"messages\": [\n      {\n        \"role\": \"assistant\",\n        \"content\": [\n          {\n            \"type\": \"server_tool_call\",\n            \"name\": \"web_search\",\n            \"args\": {\n              \"query\": \"price of AAPL\",\n              \"type\": \"search\"\n            },\n            \"id\": \"call_1\"\n          },\n          {\n            \"type\": \"server_tool_result\",\n            \"tool_call_id\": \"call_1\",\n            \"status\": \"success\"\n          },\n          {\n            \"type\": \"text\",\n            \"text\": \"The price of AAPL is $150.00\"\n          }\n        ]\n      }\n    ]\n  }\n  ```\n</CodeGroup>\n\n## Converting custom I/O formats into LangSmith compatible formats\n\nIf you're using a custom input or output format, you can convert it to a LangSmith compatible format using `process_inputs`/`processInputs` and `process_outputs`/`processOutputs` functions on the [`@traceable` decorator](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) (Python) or [`traceable` function](https://docs.smith.langchain.com/reference/js/functions/traceable.traceable) (TS).\n\n`process_inputs`/`processInputs` and `process_outputs`/`processOutputs` accept functions that allow you to transform the inputs and outputs of a specific trace before they are logged to LangSmith. They have access to the trace's inputs and outputs, and can return a new dictionary with the processed data.\n\nHere's a boilerplate example of how to use `process_inputs` and `process_outputs` to convert a custom I/O format into a LangSmith compatible format:\n\n<Expandable title=\"the code\">\n  <CodeGroup>\n    ```python Python theme={null}\n    class OriginalInputs(BaseModel):\n        \"\"\"Your app's custom request shape\"\"\"\n\n    class OriginalOutputs(BaseModel):\n        \"\"\"Your app's custom response shape.\"\"\"\n\n    class LangSmithInputs(BaseModel):\n        \"\"\"The input format LangSmith expects.\"\"\"\n\n    class LangSmithOutputs(BaseModel):\n        \"\"\"The output format LangSmith expects.\"\"\"\n\n    def process_inputs(inputs: dict) -> dict:\n        \"\"\"Dict -> OriginalInputs -> LangSmithInputs -> dict\"\"\"\n\n    def process_outputs(output: Any) -> dict:\n        \"\"\"OriginalOutputs -> LangSmithOutputs -> dict\"\"\"\n\n\n    @traceable(run_type=\"llm\", process_inputs=process_inputs, process_outputs=process_outputs)\n    def chat_model(inputs: dict) -> dict:\n        \"\"\"\n        Your app's model call. Keeps your custom I/O shape.\n        The decorators call process_* to log LangSmith-compatible format.\n        \"\"\"\n\n    ```\n  </CodeGroup>\n</Expandable>\n\n## Identifying a custom model in traces\n\nWhen using a custom model, it is recommended to also provide the following `metadata` fields to identify the model when viewing traces and when filtering.\n\n* `ls_provider`: The provider of the model, eg \"openai\", \"anthropic\", etc.\n* `ls_model_name`: The name of the model, eg \"gpt-4o-mini\", \"claude-3-opus-20240229\", etc.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import traceable\n\n  inputs = [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},\n  ]\n  output = {\n      \"choices\": [\n          {\n              \"message\": {\n                  \"role\": \"assistant\",\n                  \"content\": \"Sure, what time would you like to book the table for?\"\n              }\n          }\n      ]\n  }\n\n  @traceable(\n      run_type=\"llm\",\n      metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"}\n  )\n  def chat_model(messages: list):\n      return output\n\n  chat_model(inputs)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { traceable } from \"langsmith/traceable\";\n\n  const messages = [\n      { role: \"system\", content: \"You are a helpful assistant.\" },\n      { role: \"user\", content: \"I'd like to book a table for two.\" }\n  ];\n  const output = {\n      choices: [\n          {\n              message: {\n                  role: \"assistant\",\n                  content: \"Sure, what time would you like to book the table for?\",\n              },\n          },\n      ],\n      usage_metadata: {\n          input_tokens: 27,\n          output_tokens: 13,\n          total_tokens: 40,\n      },\n  };\n\n  // Can also use one of:\n  // const output = {\n  //     message: {\n  //         role: \"assistant\",\n  //         content: \"Sure, what time would you like to book the table for?\"\n  //     }\n  // };\n  //\n  // const output = {\n  //     role: \"assistant\",\n  //     content: \"Sure, what time would you like to book the table for?\"\n  // };\n  //\n  // const output = [\"assistant\", \"Sure, what time would you like to book the table for?\"];\n\n  const chatModel = traceable(\n      async ({ messages }: { messages: { role: string; content: string }[] }) => {\n          return output;\n      },\n      {\n          run_type: \"llm\",\n          name: \"chat_model\",\n          metadata: {\n              ls_provider: \"my_provider\",\n              ls_model_name: \"my_model\"\n          }\n      }\n  );\n\n  await chatModel({ messages });\n  ```\n</CodeGroup>\n\nThis code will log the following trace:\n\n<div style={{ textAlign: 'center' }}>\n  <img className=\"block dark:hidden\" src=\"https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=f152f49a6313d98e29d3a7b42b76c11f\" alt=\"LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output.\" data-og-width=\"1169\" width=\"1169\" data-og-height=\"548\" height=\"548\" data-path=\"langsmith/images/chat-model-light.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=280&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=083affd641c8eb41b0fcce26c8485076 280w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=560&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=8ff2ced008bb1be8db587c40cc4a6cd8 560w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=840&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=8891eae04f01247ec86c6e6b3de7a9cb 840w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=1100&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=3287ed0315422c879ff151bf2561e199 1100w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=1650&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=f9995381307324951553d7cfe8d00cdd 1650w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=2500&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=4fd10d74ce1253b1b3da84e49a439e33 2500w\" />\n\n  <img className=\"hidden dark:block\" src=\"https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=1da2f0a1adc972aa6de6df94cbfc1407\" alt=\"LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output.\" data-og-width=\"1168\" width=\"1168\" data-og-height=\"563\" height=\"563\" data-path=\"langsmith/images/chat-model-dark.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=280&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=6662af6b4871f8250ab39659fd594df8 280w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=560&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=c3986c36300cef013831eb0ba951b0fc 560w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=840&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=05d5f21e2509e36e8176fb8ace2c1e79 840w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=1100&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=464d2babbc58e4c42b3e720520af680c 1100w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=1650&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=db6c45b71fa4ff605edc8c7f88404ec2 1650w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=2500&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=cffed55abf3973cb5b908725433e001e 2500w\" />\n</div>\n\nIf you implement a custom streaming chat\\_model, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python.\n\n```python  theme={null}\ndef _reduce_chunks(chunks: list):\n    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\n    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\n\n@traceable(\n    run_type=\"llm\",\n    reduce_fn=_reduce_chunks,\n    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"}\n)\ndef my_streaming_chat_model(messages: list):\n    for chunk in [\"Hello, \" + messages[1][\"content\"]]:\n        yield {\n            \"choices\": [\n                {\n                    \"message\": {\n                        \"content\": chunk,\n                        \"role\": \"assistant\",\n                    }\n                }\n            ]\n        }\n\nlist(\n    my_streaming_chat_model(\n        [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},\n            {\"role\": \"user\", \"content\": \"polly the parrot\"},\n        ],\n    )\n)\n```\n\n<Check>\n  If `ls_model_name` is not present in `extra.metadata`, other fields might be used from the `extra.metadata` for estimating token counts. The following fields are used in the order of precedence:\n\n  1. `metadata.ls_model_name`\n  2. `inputs.model`\n  3. `inputs.model_name`\n</Check>\n\nTo learn more about how to use the `metadata` fields, refer to the [Add metadata and tags](/langsmith/add-metadata-tags) guide.\n\n## Provide token and cost information\n\nLangSmith calculates costs derived from token counts and model prices automatically. Learn about [how to provide tokens and/or costs in a run](/langsmith/cost-tracking#cost-tracking) and [viewing costs in the LangSmith UI](/langsmith/cost-tracking#viewing-costs-in-the-langsmith-ui).\n\n## Time-to-first-token\n\nIf you are using `traceable` or one of our SDK wrappers, LangSmith will automatically populate time-to-first-token for streaming LLM runs.\nHowever, if you are using the `RunTree` API directly, you will need to add a `new_token` event to the run tree in order to properly populate time-to-first-token.\n\nHere's an example:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith.run_trees import RunTree\n  run_tree = RunTree(\n      name=\"CustomChatModel\",\n      run_type=\"llm\",\n      inputs={ ... }\n  )\n  run_tree.post()\n  llm_stream = ...\n  first_token = None\n  for token in llm_stream:\n      if first_token is None:\n        first_token = token\n        run_tree.add_event({\n          \"name\": \"new_token\"\n        })\n  run_tree.end(outputs={ ... })\n  run_tree.patch()\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { RunTree } from \"langsmith\";\n  const runTree = new RunTree({\n      name: \"CustomChatModel\",\n      run_type: \"llm\",\n      inputs: { ... },\n  });\n  await runTree.postRun();\n  const llmStream = ...;\n  let firstToken;\n  for (const token of llmStream) {\n      if (firstToken == null) {\n          firstToken = token;\n          runTree.addEvent({ name: \"new_token\" });\n      }\n  }\n  await runTree.end({\n      outputs: { ... },\n  });\n  await runTree.patchRun();\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-llm-trace.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 23111
}