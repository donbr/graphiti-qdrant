{
  "title": "How to run an evaluation asynchronously",
  "source_url": "https://docs.langchain.com/langsmith/evaluation-async",
  "content": "<Info>\n  [Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets) | [Experiments](/langsmith/evaluation-concepts#experiments)\n</Info>\n\nWe can run evaluations asynchronously via the SDK using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), which accepts all of the same arguments as [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) but expects the application function to be asynchronous. You can learn more about how to use the `evaluate()` function [here](/langsmith/evaluate-llm-application).\n\n<Info>\n  This guide is only relevant when using the Python SDK. In JS/TS the `evaluate()` function is already async. You can see how to use it [here](/langsmith/evaluate-llm-application).\n</Info>\n\n## Use `aevaluate()`\n\n* Python\n\nRequires `langsmith>=0.3.13`\n\n```python  theme={null}\nfrom langsmith import wrappers, Client\nfrom openai import AsyncOpenAI\n\n# Optionally wrap the OpenAI client to trace all model calls.\noai_client = wrappers.wrap_openai(AsyncOpenAI())\n\n# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.\n@traceable\nasync def researcher_app(inputs: dict) -> str:\n    instructions = \"\"\"You are an excellent researcher. Given a high-level research idea, \\\nlist 5 concrete questions that should be investigated to determine if the idea is worth pursuing.\"\"\"\n\n    response = await oai_client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": instructions},\n            {\"role\": \"user\", \"content\": inputs[\"idea\"]},\n        ],\n    )\n    return response.choices[0].message.content\n\n# Evaluator functions can be sync or async\ndef concise(inputs: dict, outputs: dict) -> bool:\n    return len(outputs[\"output\"]) < 3 * len(inputs[\"idea\"])\n\nls_client = Client()\nideas = [\n    \"universal basic income\",\n    \"nuclear fusion\",\n    \"hyperloop\",\n    \"nuclear powered rockets\",\n]\ndataset = ls_client.create_dataset(\"research ideas\")\nls_client.create_examples(\n    dataset_name=dataset.name,\n    examples=[{\"inputs\": {\"idea\": i}} for i in ideas],\n)\n\n# Can equivalently use the 'aevaluate' function directly:\n# from langsmith import aevaluate\n# await aevaluate(...)\nresults = await ls_client.aevaluate(\n    researcher_app,\n    data=dataset,\n    evaluators=[concise],\n    # Optional, add concurrency.\n    max_concurrency=2,  # Optional, add concurrency.\n    experiment_prefix=\"gpt-4o-mini-baseline\"  # Optional, random by default.\n)\n```\n\n## Related\n\n* [Run an evaluation (synchronously)](/langsmith/evaluate-llm-application)\n* [Handle model rate limits](/langsmith/rate-limiting)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-async.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 3183
}