{
  "title": "How to set up a JavaScript application",
  "source_url": "https://docs.langchain.com/langsmith/setup-javascript",
  "content": "An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up a JavaScript application for deployment using `package.json` to specify project dependencies.\n\nThis walkthrough is based on [this repository](https://github.com/langchain-ai/langgraphjs-studio-starter), which you can play around with to learn more about how to set up your application for deployment.\n\nThe final repository structure will look something like this:\n\n```bash  theme={null}\nmy-app/\n├── src # all project code lies within here\n│   ├── utils # optional utilities for your graph\n│   │   ├── tools.ts # tools for your graph\n│   │   ├── nodes.ts # node functions for your graph\n│   │   └── state.ts # state definition of your graph\n│   └── agent.ts # code for constructing your graph\n├── package.json # package dependencies\n├── .env # environment variables\n└── langgraph.json # configuration file for LangGraph\n```\n\n<Tip>\n  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).\n</Tip>\n\nAfter each step, an example file directory is provided to demonstrate how code can be organized.\n\n## Specify dependencies\n\nDependencies can be specified in a `package.json`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-api-config).\n\nExample `package.json` file:\n\n```json  theme={null}\n{\n  \"name\": \"langgraphjs-studio-starter\",\n  \"packageManager\": \"yarn@1.22.22\",\n  \"dependencies\": {\n    \"@langchain/community\": \"^0.2.31\",\n    \"@langchain/core\": \"^0.2.31\",\n    \"@langchain/langgraph\": \"^0.2.0\",\n    \"@langchain/openai\": \"^0.2.8\"\n  }\n}\n```\n\nWhen deploying your app, the dependencies will be installed using the package manager of your choice, provided they adhere to the compatible version ranges listed below:\n\n```\n\"@langchain/core\": \"^0.3.42\",\n\"@langchain/langgraph\": \"^0.2.57\",\n\"@langchain/langgraph-checkpoint\": \"~0.0.16\",\n```\n\nExample file directory:\n\n```bash  theme={null}\nmy-app/\n└── package.json # package dependencies\n```\n\n## Specify environment variables\n\nEnvironment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.\n\nExample `.env` file:\n\n```\nMY_ENV_VAR_1=foo\nMY_ENV_VAR_2=bar\nOPENAI_API_KEY=key\nTAVILY_API_KEY=key_2\n```\n\nExample file directory:\n\n```bash  theme={null}\nmy-app/\n├── package.json\n└── .env # environment variables\n```\n\n## Define graphs\n\nImplement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each compiled graph to be included in the application. The variable names will be used later when creating the [configuration file](/langsmith/cli#configuration-file).\n\nHere is an example `agent.ts`:\n\n```ts  theme={null}\nimport type { AIMessage } from \"@langchain/core/messages\";\nimport { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nimport { MessagesAnnotation, StateGraph } from \"@langchain/langgraph\";\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\n\nconst tools = [new TavilySearchResults({ maxResults: 3 })];\n\n// Define the function that calls the model\nasync function callModel(state: typeof MessagesAnnotation.State) {\n  /**\n   * Call the LLM powering our agent.\n   * Feel free to customize the prompt, model, and other logic!\n   */\n  const model = new ChatOpenAI({\n    model: \"gpt-4o\",\n  }).bindTools(tools);\n\n  const response = await model.invoke([\n    {\n      role: \"system\",\n      content: `You are a helpful assistant. The current date is ${new Date().getTime()}.`,\n    },\n    ...state.messages,\n  ]);\n\n  // MessagesAnnotation supports returning a single message or array of messages\n  return { messages: response };\n}\n\n// Define the function that determines whether to continue or not\nfunction routeModelOutput(state: typeof MessagesAnnotation.State) {\n  const messages = state.messages;\n  const lastMessage: AIMessage = messages[messages.length - 1];\n  // If the LLM is invoking tools, route there.\n  if ((lastMessage?.tool_calls?.length ?? 0) > 0) {\n    return \"tools\";\n  }\n  // Otherwise end the graph.\n  return \"__end__\";\n}\n\n// Define a new graph.\n// See https://langchain-ai.github.io/langgraphjs/how-tos/define-state/#getting-started for\n// more on defining custom graph states.\nconst workflow = new StateGraph(MessagesAnnotation)\n  // Define the two nodes we will cycle between\n  .addNode(\"callModel\", callModel)\n  .addNode(\"tools\", new ToolNode(tools))\n  // Set the entrypoint as `callModel`\n  // This means that this node is the first one called\n  .addEdge(\"__start__\", \"callModel\")\n  .addConditionalEdges(\n    // First, we define the edges' source node. We use `callModel`.\n    // This means these are the edges taken after the `callModel` node is called.\n    \"callModel\",\n    // Next, we pass in the function that will determine the sink node(s), which\n    // will be called after the source node is called.\n    routeModelOutput,\n    // List of the possible destinations the conditional edge can route to.\n    // Required for conditional edges to properly render the graph in Studio\n    [\"tools\", \"__end__\"]\n  )\n  // This means that after `tools` is called, `callModel` node is called next.\n  .addEdge(\"tools\", \"callModel\");\n\n// Finally, we compile it!\n// This compiles it into a graph you can invoke and deploy.\nexport const graph = workflow.compile();\n```\n\nExample file directory:\n\n```bash  theme={null}\nmy-app/\n├── src # all project code lies within here\n│   ├── utils # optional utilities for your graph\n│   │   ├── tools.ts # tools for your graph\n│   │   ├── nodes.ts # node functions for your graph\n│   │   └── state.ts # state definition of your graph\n│   └── agent.ts # code for constructing your graph\n├── package.json # package dependencies\n├── .env # environment variables\n└── langgraph.json # configuration file for LangGraph\n```\n\n## Create the API config\n\nCreate a [configuration file](/langsmith/cli#configuration-file) called `langgraph.json`. See the [configuration file reference](/langsmith/cli#configuration-file) for detailed explanations of each key in the JSON object of the configuration file.\n\nExample `langgraph.json` file:\n\n```json  theme={null}\n{\n  \"node_version\": \"20\",\n  \"dockerfile_lines\": [],\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"agent\": \"./src/agent.ts:graph\"\n  },\n  \"env\": \".env\"\n}\n```\n\nNote that the variable name of the `CompiledGraph` appears at the end of the value of each subkey in the top-level `graphs` key (i.e. `:<variable_name>`).\n\n<Info>\n  **Configuration Location**\n  The configuration file must be placed in a directory that is at the same level or higher than the TypeScript files that contain compiled graphs and associated dependencies.\n</Info>\n\n## Next\n\nAfter you setup your project and place it in a GitHub repository, it's time to [deploy your app](/langsmith/deployment-quickstart).\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/setup-javascript.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 7805
}