{
  "title": "Custom instrumentation",
  "source_url": "https://docs.langchain.com/langsmith/annotate-code",
  "content": "<Note>\n  If you've decided you no longer want to trace your runs, you can remove the `LANGSMITH_TRACING` environment variable. Note that this does not affect the `RunTree` objects or API users, as these are meant to be low-level and not affected by the tracing toggle.\n</Note>\n\nThere are several ways to log traces to LangSmith.\n\n<Check>\n  If you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the [LangChain-specific instructions](/langsmith/trace-with-langchain).\n</Check>\n\n## Use `@traceable` / `traceable`\n\nLangSmith makes it easy to log traces with minimal changes to your existing code with the `@traceable` decorator in Python and `traceable` function in TypeScript.\n\n<Note>\n  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `@traceable` or `traceable`. This allows you to toggle tracing on and off without changing your code.\n\n  Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).\n\n  By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).\n</Note>\n\nThe `@traceable` decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with `@traceable`.\n\nNote that when wrapping a sync function with `traceable`, (e.g. `formatPrompt` in the example below), you should use the `await` keyword when calling it to\nensure the trace is logged correctly.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import traceable\n  from openai import Client\n\n  openai = Client()\n\n  @traceable\n  def format_prompt(subject):\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"What's a good name for a store that sells {subject}?\"\n        }\n    ]\n\n  @traceable(run_type=\"llm\")\n  def invoke_llm(messages):\n    return openai.chat.completions.create(\n        messages=messages, model=\"gpt-4o-mini\", temperature=0\n    )\n\n  @traceable\n  def parse_output(response):\n    return response.choices[0].message.content\n\n  @traceable\n  def run_pipeline():\n    messages = format_prompt(\"colorful socks\")\n    response = invoke_llm(messages)\n    return parse_output(response)\n\n  run_pipeline()\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { traceable } from \"langsmith/traceable\";\n  import OpenAI from \"openai\";\n\n  const openai = new OpenAI();\n\n  const formatPrompt = traceable((subject: string) => {\n    return [\n      {\n        role: \"system\" as const,\n        content: \"You are a helpful assistant.\",\n      },\n      {\n        role: \"user\" as const,\n        content: `What's a good name for a store that sells ${subject}?`,\n      },\n    ];\n  },{ name: \"formatPrompt\" });\n\n  const invokeLLM = traceable(\n    async ({ messages }: { messages: { role: string; content: string }[] }) => {\n        return openai.chat.completions.create({\n            model: \"gpt-4o-mini\",\n            messages: messages,\n            temperature: 0,\n        });\n    },\n    { run_type: \"llm\", name: \"invokeLLM\" }\n  );\n\n  const parseOutput = traceable(\n    (response: any) => {\n        return response.choices[0].message.content;\n    },\n    { name: \"parseOutput\" }\n  );\n\n  const runPipeline = traceable(\n    async () => {\n        const messages = await formatPrompt(\"colorful socks\");\n        const response = await invokeLLM({ messages });\n        return parseOutput(response);\n    },\n    { name: \"runPipeline\" }\n  );\n\n  await runPipeline();\n  ```\n</CodeGroup>\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-code-trace.gif?s=bb81d0cb45382f2d793d43624db6e9ba\" alt=\"\" data-og-width=\"822\" width=\"822\" data-og-height=\"480\" height=\"480\" data-path=\"langsmith/images/annotate-code-trace.gif\" data-optimize=\"true\" data-opv=\"3\" />\n\n## Use the `trace` context manager (Python only)\n\nIn Python, you can use the `trace` context manager to log traces to LangSmith. This is useful in situations where:\n\n1. You want to log traces for a specific block of code.\n2. You want control over the inputs, outputs, and other attributes of the trace.\n3. It is not feasible to use a decorator or wrapper.\n4. Any or all of the above.\n\nThe context manager integrates seamlessly with the `traceable` decorator and `wrap_openai` wrapper, so you can use them together in the same application.\n\n```python  theme={null}\nimport openai\nimport langsmith as ls\nfrom langsmith.wrappers import wrap_openai\n\nclient = wrap_openai(openai.Client())\n\n@ls.traceable(run_type=\"tool\", name=\"Retrieve Context\")\ndef my_tool(question: str) -> str:\n    return \"During this morning's meeting, we solved all world conflict.\"\n\ndef chat_pipeline(question: str):\n    context = my_tool(question)\n    messages = [\n        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n    ]\n    chat_completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\", messages=messages\n    )\n    return chat_completion.choices[0].message.content\n\napp_inputs = {\"input\": \"Can you summarize this morning's meetings?\"}\n\nwith ls.trace(\"Chat Pipeline\", \"chain\", project_name=\"my_test\", inputs=app_inputs) as rt:\n    output = chat_pipeline(\"Can you summarize this morning's meetings?\")\n    rt.end(outputs={\"output\": output})\n```\n\n## Use the `RunTree` API\n\nAnother, more explicit way to log traces to LangSmith is via the `RunTree` API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method.\n\nThis method is not recommended, as it's easier to make mistakes in propagating trace context.\n\n<CodeGroup>\n  ```python Python theme={null}\n  import openai\n  from langsmith.run_trees import RunTree\n\n  # This can be a user input to your app\n  question = \"Can you summarize this morning's meetings?\"\n\n  # Create a top-level run\n  pipeline = RunTree(\n    name=\"Chat Pipeline\",\n    run_type=\"chain\",\n    inputs={\"question\": question}\n  )\n  pipeline.post()\n\n  # This can be retrieved in a retrieval step\n  context = \"During this morning's meeting, we solved all world conflict.\"\n  messages = [\n    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n    { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n  ]\n\n  # Create a child run\n  child_llm_run = pipeline.create_child(\n    name=\"OpenAI Call\",\n    run_type=\"llm\",\n    inputs={\"messages\": messages},\n  )\n  child_llm_run.post()\n\n  # Generate a completion\n  client = openai.Client()\n  chat_completion = client.chat.completions.create(\n    model=\"gpt-4o-mini\", messages=messages\n  )\n\n  # End the runs and log them\n  child_llm_run.end(outputs=chat_completion)\n  child_llm_run.patch()\n  pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})\n  pipeline.patch()\n  ```\n\n  ```typescript TypeScript theme={null}\n  import OpenAI from \"openai\";\n  import { RunTree } from \"langsmith\";\n\n  // This can be a user input to your app\n  const question = \"Can you summarize this morning's meetings?\";\n\n  const pipeline = new RunTree({\n    name: \"Chat Pipeline\",\n    run_type: \"chain\",\n    inputs: { question }\n  });\n  await pipeline.postRun();\n\n  // This can be retrieved in a retrieval step\n  const context = \"During this morning's meeting, we solved all world conflict.\";\n  const messages = [\n    { role: \"system\", content: \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n    { role: \"user\", content: `Question: ${question}Context: ${context}` }\n  ];\n\n  // Create a child run\n  const childRun = await pipeline.createChild({\n    name: \"OpenAI Call\",\n    run_type: \"llm\",\n    inputs: { messages },\n  });\n  await childRun.postRun();\n\n  // Generate a completion\n  const client = new OpenAI();\n  const chatCompletion = await client.chat.completions.create({\n    model: \"gpt-4o-mini\",\n    messages: messages,\n  });\n\n  // End the runs and log them\n  childRun.end(chatCompletion);\n  await childRun.patchRun();\n  pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });\n  await pipeline.patchRun();\n  ```\n</CodeGroup>\n\n## Example usage\n\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\n\nTrace any public method in a class:\n\n```python  theme={null}\nfrom typing import Any, Callable, Type, TypeVar\n\nT = TypeVar(\"T\")\n\ndef traceable_cls(cls: Type[T]) -> Type[T]:\n    \"\"\"Instrument all public methods in a class.\"\"\"\n    def wrap_method(name: str, method: Any) -> Any:\n        if callable(method) and not name.startswith(\"__\"):\n            return traceable(name=f\"{cls.__name__}.{name}\")(method)\n        return method\n\n    # Handle __dict__ case\n    for name in dir(cls):\n        if not name.startswith(\"_\"):\n            try:\n                method = getattr(cls, name)\n                setattr(cls, name, wrap_method(name, method))\n            except AttributeError:\n                # Skip attributes that can't be set (e.g., some descriptors)\n                pass\n\n    # Handle __slots__ case\n    if hasattr(cls, \"__slots__\"):\n        for slot in cls.__slots__:  # type: ignore[attr-defined]\n            if not slot.startswith(\"__\"):\n                try:\n                    method = getattr(cls, slot)\n                    setattr(cls, slot, wrap_method(slot, method))\n                except AttributeError:\n                    # Skip slots that don't have a value yet\n                    pass\n\n    return cls\n\n@traceable_cls\nclass MyClass:\n    def __init__(self, some_val: int):\n        self.some_val = some_val\n\n    def combine(self, other_val: int):\n        return self.some_val + other_val\n\n# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/r\nMyClass(13).combine(29)\n```\n\n## Ensure all traces are submitted before exiting\n\nLangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. Here are some options for ensuring all traces are submitted before exiting your application.\n\n### Using the LangSmith SDK\n\nIf you are using the LangSmith SDK standalone, you can use the `flush` method before exit:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n\n  client = Client()\n\n  @traceable(client=client)\n  async def my_traced_func():\n    # Your code here...\n    pass\n\n  try:\n    await my_traced_func()\n  finally:\n    await client.flush()\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n\n  const langsmithClient = new Client({});\n\n  const myTracedFunc = traceable(async () => {\n    // Your code here...\n  },{ client: langsmithClient });\n\n  try {\n    await myTracedFunc();\n  } finally {\n    await langsmithClient.flush();\n  }\n  ```\n</CodeGroup>\n\n### Using LangChain\n\nIf you are using LangChain, please refer to our [LangChain tracing guide](/langsmith/trace-with-langchain#ensure-all-traces-are-submitted-before-exiting).\n\nIf you prefer a video tutorial, check out the [Tracing Basics video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotate-code.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 12019
}