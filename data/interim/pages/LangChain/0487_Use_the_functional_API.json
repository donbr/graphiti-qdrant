{
  "title": "Use the functional API",
  "source_url": "https://docs.langchain.com/oss/python/langgraph/use-functional-api",
  "content": "The [**Functional API**](/oss/python/langgraph/functional-api) allows you to add LangGraph's key features — [persistence](/oss/python/langgraph/persistence), [memory](/oss/python/langgraph/add-memory), [human-in-the-loop](/oss/python/langgraph/interrupts), and [streaming](/oss/python/langgraph/streaming) — to your applications with minimal changes to your existing code.\n\n<Tip>\n  For conceptual information on the functional API, see [Functional API](/oss/python/langgraph/functional-api).\n</Tip>\n\n## Creating a simple workflow\n\nWhen defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.\n\n```python  theme={null}\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -> int:\n    value = inputs[\"value\"]\n    another_value = inputs[\"another_value\"]\n    ...\n\nmy_workflow.invoke({\"value\": 1, \"another_value\": 2})\n```\n\n<Accordion title=\"Extended example: simple workflow\">\n  ```python  theme={null}\n  import uuid\n  from langgraph.func import entrypoint, task\n  from langgraph.checkpoint.memory import InMemorySaver\n\n  # Task that checks if a number is even\n  @task\n  def is_even(number: int) -> bool:\n      return number % 2 == 0\n\n  # Task that formats a message\n  @task\n  def format_message(is_even: bool) -> str:\n      return \"The number is even.\" if is_even else \"The number is odd.\"\n\n  # Create a checkpointer for persistence\n  checkpointer = InMemorySaver()\n\n  @entrypoint(checkpointer=checkpointer)\n  def workflow(inputs: dict) -> str:\n      \"\"\"Simple workflow to classify a number.\"\"\"\n      even = is_even(inputs[\"number\"]).result()\n      return format_message(even).result()\n\n  # Run the workflow with a unique thread ID\n  config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n  result = workflow.invoke({\"number\": 7}, config=config)\n  print(result)\n  ```\n</Accordion>\n\n<Accordion title=\"Extended example: Compose an essay with an LLM\">\n  This example demonstrates how to use the `@task` and `@entrypoint` decorators\n  syntactically. Given that a checkpointer is provided, the workflow results will\n  be persisted in the checkpointer.\n\n  ```python  theme={null}\n  import uuid\n  from langchain.chat_models import init_chat_model\n  from langgraph.func import entrypoint, task\n  from langgraph.checkpoint.memory import InMemorySaver\n\n  model = init_chat_model('gpt-3.5-turbo')\n\n  # Task: generate essay using an LLM\n  @task\n  def compose_essay(topic: str) -> str:\n      \"\"\"Generate an essay about the given topic.\"\"\"\n      return model.invoke([\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes essays.\"},\n          {\"role\": \"user\", \"content\": f\"Write an essay about {topic}.\"}\n      ]).content\n\n  # Create a checkpointer for persistence\n  checkpointer = InMemorySaver()\n\n  @entrypoint(checkpointer=checkpointer)\n  def workflow(topic: str) -> str:\n      \"\"\"Simple workflow that generates an essay with an LLM.\"\"\"\n      return compose_essay(topic).result()\n\n  # Execute the workflow\n  config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n  result = workflow.invoke(\"the history of flight\", config=config)\n  print(result)\n  ```\n</Accordion>\n\n## Parallel execution\n\nTasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).\n\n```python  theme={null}\n@task\ndef add_one(number: int) -> int:\n    return number + 1\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(numbers: list[int]) -> list[str]:\n    futures = [add_one(i) for i in numbers]\n    return [f.result() for f in futures]\n```\n\n<Accordion title=\"Extended example: parallel LLM calls\">\n  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.\n\n  ```python  theme={null}\n  import uuid\n  from langchain.chat_models import init_chat_model\n  from langgraph.func import entrypoint, task\n  from langgraph.checkpoint.memory import InMemorySaver\n\n  # Initialize the LLM model\n  model = init_chat_model(\"gpt-3.5-turbo\")\n\n  # Task that generates a paragraph about a given topic\n  @task\n  def generate_paragraph(topic: str) -> str:\n      response = model.invoke([\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"},\n          {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"}\n      ])\n      return response.content\n\n  # Create a checkpointer for persistence\n  checkpointer = InMemorySaver()\n\n  @entrypoint(checkpointer=checkpointer)\n  def workflow(topics: list[str]) -> str:\n      \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\"\n      futures = [generate_paragraph(topic) for topic in topics]\n      paragraphs = [f.result() for f in futures]\n      return \"\\n\\n\".join(paragraphs)\n\n  # Run the workflow\n  config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n  result = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config)\n  print(result)\n  ```\n\n  This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.\n</Accordion>\n\n## Calling graphs\n\nThe **Functional API** and the [**Graph API**](/oss/python/langgraph/graph-api) can be used together in the same application as they share the same underlying runtime.\n\n```python  theme={null}\nfrom langgraph.func import entrypoint\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph()\n...\nsome_graph = builder.compile()\n\n@entrypoint()\ndef some_workflow(some_input: dict) -> int:\n    # Call a graph defined using the graph API\n    result_1 = some_graph.invoke(...)\n    # Call another graph defined using the graph API\n    result_2 = another_graph.invoke(...)\n    return {\n        \"result_1\": result_1,\n        \"result_2\": result_2\n    }\n```\n\n<Accordion title=\"Extended example: calling a simple graph from the functional API\">\n  ```python  theme={null}\n  import uuid\n  from typing import TypedDict\n  from langgraph.func import entrypoint\n  from langgraph.checkpoint.memory import InMemorySaver\n  from langgraph.graph import StateGraph\n\n  # Define the shared state type\n  class State(TypedDict):\n      foo: int\n\n  # Define a simple transformation node\n  def double(state: State) -> State:\n      return {\"foo\": state[\"foo\"] * 2}\n\n  # Build the graph using the Graph API\n  builder = StateGraph(State)\n  builder.add_node(\"double\", double)\n  builder.set_entry_point(\"double\")\n  graph = builder.compile()\n\n  # Define the functional API workflow\n  checkpointer = InMemorySaver()\n\n  @entrypoint(checkpointer=checkpointer)\n  def workflow(x: int) -> dict:\n      result = graph.invoke({\"foo\": x})\n      return {\"bar\": result[\"foo\"]}\n\n  # Execute the workflow\n  config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n  print(workflow.invoke(5, config=config))  # Output: {'bar': 10}\n  ```\n</Accordion>\n\n## Call other entrypoints\n\nYou can call other **entrypoints** from within an **entrypoint** or a **task**.\n\n```python  theme={null}\n@entrypoint() # Will automatically use the checkpointer from the parent entrypoint\ndef some_other_workflow(inputs: dict) -> int:\n    return inputs[\"value\"]\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -> int:\n    value = some_other_workflow.invoke({\"value\": 1})\n    return value\n```\n\n<Accordion title=\"Extended example: calling another entrypoint\">\n  ```python  theme={null}\n  import uuid\n  from langgraph.func import entrypoint\n  from langgraph.checkpoint.memory import InMemorySaver\n\n  # Initialize a checkpointer\n  checkpointer = InMemorySaver()\n\n  # A reusable sub-workflow that multiplies a number\n  @entrypoint()\n  def multiply(inputs: dict) -> int:\n      return inputs[\"a\"] * inputs[\"b\"]\n\n  # Main workflow that invokes the sub-workflow\n  @entrypoint(checkpointer=checkpointer)\n  def main(inputs: dict) -> dict:\n      result = multiply.invoke({\"a\": inputs[\"x\"], \"b\": inputs[\"y\"]})\n      return {\"product\": result}\n\n  # Execute the main workflow\n  config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n  print(main.invoke({\"x\": 6, \"y\": 7}, config=config))  # Output: {'product': 42}\n  ```\n</Accordion>\n\n## Streaming\n\nThe **Functional API** uses the same streaming mechanism as the **Graph API**. Please\nread the [**streaming guide**](/oss/python/langgraph/streaming) section for more details.\n\nExample of using the streaming API to stream both updates and custom data.\n\n```python  theme={null}\nfrom langgraph.func import entrypoint\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.config import get_stream_writer   # [!code highlight]\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs: dict) -> int:\n    writer = get_stream_writer()   # [!code highlight]\n    writer(\"Started processing\")   # [!code highlight]\n    result = inputs[\"x\"] * 2\n    writer(f\"Result is {result}\")   # [!code highlight]\n    return result\n\nconfig = {\"configurable\": {\"thread_id\": \"abc\"}}\n\nfor mode, chunk in main.stream(   # [!code highlight]\n    {\"x\": 5},\n    stream_mode=[\"custom\", \"updates\"],   # [!code highlight]\n    config=config\n):\n    print(f\"{mode}: {chunk}\")\n```\n\n1. Import [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) from `langgraph.config`.\n2. Obtain a stream writer instance within the entrypoint.\n3. Emit custom data before computation begins.\n4. Emit another custom message after computing the result.\n5. Use `.stream()` to process streamed output.\n6. Specify which streaming modes to use.\n\n```pycon  theme={null}\n('updates', {'add_one': 2})\n('updates', {'add_two': 3})\n('custom', 'hello')\n('custom', 'world')\n('updates', {'main': 5})\n```\n\n<Warning>\n  **Async with Python \\< 3.11**\n  If using Python \\< 3.11 and writing async code, using [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) will not work. Instead please\n  use the `StreamWriter` class directly. See [Async with Python \\< 3.11](/oss/python/langgraph/streaming#async) for more details.\n\n  ```python  theme={null}\n  from langgraph.types import StreamWriter\n\n  @entrypoint(checkpointer=checkpointer)\n  async def main(inputs: dict, writer: StreamWriter) -> int:  # [!code highlight]\n  ...\n  ```\n</Warning>\n\n## Retry policy\n\n```python  theme={null}\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import RetryPolicy\n\n# This variable is just used for demonstration purposes to simulate a network failure.\n# It's not something you will have in your actual code.\nattempts = 0\n\n# Let's configure the RetryPolicy to retry on ValueError.\n# The default RetryPolicy is optimized for retrying specific network errors.\nretry_policy = RetryPolicy(retry_on=ValueError)\n\n@task(retry_policy=retry_policy)\ndef get_info():\n    global attempts\n    attempts += 1\n\n    if attempts < 2:\n        raise ValueError('Failure')\n    return \"OK\"\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs, writer):\n    return get_info().result()\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\n\nmain.invoke({'any_input': 'foobar'}, config=config)\n```\n\n```pycon  theme={null}\n'OK'\n```\n\n## Caching Tasks\n\n```python  theme={null}\nimport time\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import CachePolicy\n\n\n@task(cache_policy=CachePolicy(ttl=120))    # [!code highlight]\ndef slow_add(x: int) -> int:\n    time.sleep(1)\n    return x * 2\n\n\n@entrypoint(cache=InMemoryCache())\ndef main(inputs: dict) -> dict[str, int]:\n    result1 = slow_add(inputs[\"x\"]).result()\n    result2 = slow_add(inputs[\"x\"]).result()\n    return {\"result1\": result1, \"result2\": result2}\n\n\nfor chunk in main.stream({\"x\": 5}, stream_mode=\"updates\"):\n    print(chunk)\n\n#> {'slow_add': 10}\n#> {'slow_add': 10, '__metadata__': {'cached': True}}\n#> {'main': {'result1': 10, 'result2': 10}}\n```\n\n1. `ttl` is specified in seconds. The cache will be invalidated after this time.\n\n## Resuming after an error\n\n```python  theme={null}\nimport time\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import StreamWriter\n\n# This variable is just used for demonstration purposes to simulate a network failure.\n# It's not something you will have in your actual code.\nattempts = 0\n\n@task()\ndef get_info():\n    \"\"\"\n    Simulates a task that fails once before succeeding.\n    Raises an exception on the first attempt, then returns \"OK\" on subsequent tries.\n    \"\"\"\n    global attempts\n    attempts += 1\n\n    if attempts < 2:\n        raise ValueError(\"Failure\")  # Simulate a failure on the first attempt\n    return \"OK\"\n\n# Initialize an in-memory checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@task\ndef slow_task():\n    \"\"\"\n    Simulates a slow-running task by introducing a 1-second delay.\n    \"\"\"\n    time.sleep(1)\n    return \"Ran slow task.\"\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs, writer: StreamWriter):\n    \"\"\"\n    Main workflow function that runs the slow_task and get_info tasks sequentially.\n\n    Parameters:\n    - inputs: Dictionary containing workflow input values.\n    - writer: StreamWriter for streaming custom data.\n\n    The workflow first executes `slow_task` and then attempts to execute `get_info`,\n    which will fail on the first invocation.\n    \"\"\"\n    slow_task_result = slow_task().result()  # Blocking call to slow_task\n    get_info().result()  # Exception will be raised here on the first attempt\n    return slow_task_result\n\n# Workflow execution configuration with a unique thread identifier\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"  # Unique identifier to track workflow execution\n    }\n}\n\n# This invocation will take ~1 second due to the slow_task execution\ntry:\n    # First invocation will raise an exception due to the `get_info` task failing\n    main.invoke({'any_input': 'foobar'}, config=config)\nexcept ValueError:\n    pass  # Handle the failure gracefully\n```\n\nWhen we resume execution, we won't need to re-run the `slow_task` as its result is already saved in the checkpoint.\n\n```python  theme={null}\nmain.invoke(None, config=config)\n```\n\n```pycon  theme={null}\n'Ran slow task.'\n```\n\n## Human-in-the-loop\n\nThe functional API supports [human-in-the-loop](/oss/python/langgraph/interrupts) workflows using the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function and the `Command` primitive.\n\n### Basic human-in-the-loop workflow\n\nWe will create three [tasks](/oss/python/langgraph/functional-api#task):\n\n1. Append `\"bar\"`.\n2. Pause for human input. When resuming, append human input.\n3. Append `\"qux\"`.\n\n```python  theme={null}\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import Command, interrupt\n\n\n@task\ndef step_1(input_query):\n    \"\"\"Append bar.\"\"\"\n    return f\"{input_query} bar\"\n\n\n@task\ndef human_feedback(input_query):\n    \"\"\"Append user input.\"\"\"\n    feedback = interrupt(f\"Please provide feedback: {input_query}\")\n    return f\"{input_query} {feedback}\"\n\n\n@task\ndef step_3(input_query):\n    \"\"\"Append qux.\"\"\"\n    return f\"{input_query} qux\"\n```\n\nWe can now compose these tasks in an [entrypoint](/oss/python/langgraph/functional-api#entrypoint):\n\n```python  theme={null}\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(input_query):\n    result_1 = step_1(input_query).result()\n    result_2 = human_feedback(result_1).result()\n    result_3 = step_3(result_2).result()\n\n    return result_3\n```\n\n[interrupt()](/oss/python/langgraph/interrupts#pause-using-interrupt) is called inside a task, enabling a human to review and edit the output of the previous task. The results of prior tasks-- in this case `step_1`-- are persisted, so that they are not run again following the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).\n\nLet's send in a query string:\n\n```python  theme={null}\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor event in graph.stream(\"foo\", config):\n    print(event)\n    print(\"\\n\")\n```\n\nNote that we've paused with an [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) after `step_1`. The interrupt provides instructions to resume the run. To resume, we issue a [`Command`](/oss/python/langgraph/interrupts#resuming-interrupts) containing the data expected by the `human_feedback` task.\n\n```python  theme={null}\n# Continue execution\nfor event in graph.stream(Command(resume=\"baz\"), config):\n    print(event)\n    print(\"\\n\")\n```\n\nAfter resuming, the run proceeds through the remaining step and terminates as expected.\n\n### Review tool calls\n\nTo review tool calls before execution, we add a `review_tool_call` function that calls [`interrupt`](/oss/python/langgraph/interrupts#pause-using-interrupt). When this function is called, execution will be paused until we issue a command to resume it.\n\nGiven a tool call, our function will [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) for human review. At that point we can either:\n\n* Accept the tool call\n* Revise the tool call and continue\n* Generate a custom tool message (e.g., instructing the model to re-format its tool call)\n\n```python  theme={null}\nfrom typing import Union\n\ndef review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:\n    \"\"\"Review a tool call, returning a validated version.\"\"\"\n    human_review = interrupt(\n        {\n            \"question\": \"Is this correct?\",\n            \"tool_call\": tool_call,\n        }\n    )\n    review_action = human_review[\"action\"]\n    review_data = human_review.get(\"data\")\n    if review_action == \"continue\":\n        return tool_call\n    elif review_action == \"update\":\n        updated_tool_call = {**tool_call, **{\"args\": review_data}}\n        return updated_tool_call\n    elif review_action == \"feedback\":\n        return ToolMessage(\n            content=review_data, name=tool_call[\"name\"], tool_call_id=tool_call[\"id\"]\n        )\n```\n\nWe can now update our [entrypoint](/oss/python/langgraph/functional-api#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) supplied by the human. The results of prior tasks — in this case the initial model call — are persisted, so that they are not run again following the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).\n\n```python  theme={null}\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph.message import add_messages\nfrom langgraph.types import Command, interrupt\n\n\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef agent(messages, previous):\n    if previous is not None:\n        messages = add_messages(previous, messages)\n\n    model_response = call_model(messages).result()\n    while True:\n        if not model_response.tool_calls:\n            break\n\n        # Review tool calls\n        tool_results = []\n        tool_calls = []\n        for i, tool_call in enumerate(model_response.tool_calls):\n            review = review_tool_call(tool_call)\n            if isinstance(review, ToolMessage):\n                tool_results.append(review)\n            else:  # is a validated tool call\n                tool_calls.append(review)\n                if review != tool_call:\n                    model_response.tool_calls[i] = review  # update message\n\n        # Execute remaining tool calls\n        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]\n        remaining_tool_results = [fut.result() for fut in tool_result_futures]\n\n        # Append to message list\n        messages = add_messages(\n            messages,\n            [model_response, *tool_results, *remaining_tool_results],\n        )\n\n        # Call model again\n        model_response = call_model(messages).result()\n\n    # Generate final response\n    messages = add_messages(messages, model_response)\n    return entrypoint.final(value=model_response, save=messages)\n```\n\n## Short-term memory\n\nShort-term memory allows storing information across different **invocations** of the same **thread id**. See [short-term memory](/oss/python/langgraph/functional-api#short-term-memory) for more details.\n\n### Manage checkpoints\n\nYou can view and delete the information stored by the checkpointer.\n\n<a id=\"checkpoint\" />\n\n#### View thread state\n\n```python  theme={null}\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\",  # [!code highlight]\n        # optionally provide an ID for a specific checkpoint,\n        # otherwise the latest checkpoint is shown\n        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  # [!code highlight]\n\n    }\n}\ngraph.get_state(config)  # [!code highlight]\n```\n\n```\nStateSnapshot(\n    values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n    metadata={\n        'source': 'loop',\n        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n        'step': 4,\n        'parents': {},\n        'thread_id': '1'\n    },\n    created_at='2025-05-05T16:01:24.680462+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n    tasks=(),\n    interrupts=()\n)\n```\n\n<a id=\"checkpoints\" />\n\n#### View the history of the thread\n\n```python  theme={null}\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"  # [!code highlight]\n    }\n}\nlist(graph.get_state_history(config))  # [!code highlight]\n```\n\n```\n[\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:24.680462+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]},\n        next=('call_model',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863421+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=('__start__',),\n        config={...},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863173+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=(),\n        config={...},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.862295+00:00',\n        parent_config={...}\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\")]},\n        next=('call_model',),\n        config={...},\n        metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.278960+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.277497+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),),\n        interrupts=()\n    )\n]\n```\n\n### Decouple return value from saved value\n\nUse `entrypoint.final` to decouple what is returned to the caller from what is persisted in the checkpoint. This is useful when:\n\n* You want to return a computed result (e.g., a summary or status), but save a different internal value for use on the next invocation.\n* You need to control what gets passed to the previous parameter on the next run.\n\n```python  theme={null}\nfrom langgraph.func import entrypoint\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef accumulate(n: int, *, previous: int | None) -> entrypoint.final[int, int]:\n    previous = previous or 0\n    total = previous + n\n    # Return the *previous* value to the caller but save the *new* total to the checkpoint.\n    return entrypoint.final(value=previous, save=total)\n\nconfig = {\"configurable\": {\"thread_id\": \"my-thread\"}}\n\nprint(accumulate.invoke(1, config=config))  # 0\nprint(accumulate.invoke(2, config=config))  # 1\nprint(accumulate.invoke(3, config=config))  # 3\n```\n\n### Chatbot example\n\nAn example of a simple chatbot using the functional API and the [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) checkpointer.\n\nThe bot is able to remember the previous conversation and continue from where it left off.\n\n```python  theme={null}\nfrom langchain.messages import BaseMessage\nfrom langgraph.graph import add_messages\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n\n@task\ndef call_model(messages: list[BaseMessage]):\n    response = model.invoke(messages)\n    return response\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage]):\n    if previous:\n        inputs = add_messages(previous, inputs)\n\n    response = call_model(inputs).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"hi! I'm bob\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n\ninput_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n```\n\n## Long-term memory\n\n[long-term memory](/oss/python/concepts/memory#long-term-memory) allows storing information across different **thread ids**. This could be useful for learning information about a given user in one conversation and using it in another.\n\n## Workflows\n\n* [Workflows and agent](/oss/python/langgraph/workflows-agents) guide for more examples of how to build workflows using the Functional API.\n\n## Integrate with other libraries\n\n* [Add LangGraph's features to other frameworks using the functional API](/langsmith/autogen-integration): Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-functional-api.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 30135
}