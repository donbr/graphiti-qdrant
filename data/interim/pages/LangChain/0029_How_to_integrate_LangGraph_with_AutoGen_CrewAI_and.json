{
  "title": "How to integrate LangGraph with AutoGen, CrewAI, and other frameworks",
  "source_url": "https://docs.langchain.com/langsmith/autogen-integration",
  "content": "This guide shows how to integrate AutoGen agents with LangGraph to leverage features like persistence, streaming, and memory, and then deploy the integrated solution to LangSmith for scalable production use. In this guide we show how to build a LangGraph chatbot that integrates with AutoGen, but you can follow the same approach with other frameworks.\n\nIntegrating AutoGen with LangGraph provides several benefits:\n\n* Enhanced features: Add [persistence](/oss/python/langgraph/persistence), [streaming](/langsmith/streaming), [short and long-term memory](/oss/python/concepts/memory) and more to your AutoGen agents.\n* Multi-agent systems: Build [multi-agent systems](/oss/python/langchain/multi-agent) where individual agents are built with different frameworks.\n* Production deployment: Deploy your integrated solution to [LangSmith](/langsmith/home) for scalable production use.\n\n## Prerequisites\n\n* Python 3.9+\n* Autogen: `pip install autogen`\n* LangGraph: `pip install langgraph`\n* OpenAI API key\n\n## Setup\n\nSet your your environment:\n\n```python  theme={null}\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n```\n\n## 1. Define AutoGen agent\n\nCreate an AutoGen agent that can execute code. This example is adapted from AutoGen's [official tutorials](https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb):\n\n```python  theme={null}\nimport autogen\nimport os\n\nconfig_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]\n\nllm_config = {\n    \"timeout\": 600,\n    \"cache_seed\": 42,\n    \"config_list\": config_list,\n    \"temperature\": 0,\n}\n\nautogen_agent = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config,\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n    code_execution_config={\n        \"work_dir\": \"web\",\n        \"use_docker\": False,\n    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n    llm_config=llm_config,\n    system_message=\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\",\n)\n```\n\n## 2. Create the graph\n\nWe will now create a LangGraph chatbot graph that calls AutoGen agent.\n\n```python  theme={null}\nfrom langchain_core.messages import convert_to_openai_messages\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.memory import MemorySaver\n\ndef call_autogen_agent(state: MessagesState):\n    # Convert LangGraph messages to OpenAI format for AutoGen\n    messages = convert_to_openai_messages(state[\"messages\"])\n\n    # Get the last user message\n    last_message = messages[-1]\n\n    # Pass previous message history as context (excluding the last message)\n    carryover = messages[:-1] if len(messages) > 1 else []\n\n    # Initiate chat with AutoGen\n    response = user_proxy.initiate_chat(\n        autogen_agent,\n        message=last_message,\n        carryover=carryover\n    )\n\n    # Extract the final response from the agent\n    final_content = response.chat_history[-1][\"content\"]\n\n    # Return the response in LangGraph format\n    return {\"messages\": {\"role\": \"assistant\", \"content\": final_content}}\n\n# Create the graph with memory for persistence\ncheckpointer = MemorySaver()\n\n# Build the graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"autogen\", call_autogen_agent)\nbuilder.add_edge(START, \"autogen\")\n\n# Compile with checkpointer for persistence\ngraph = builder.compile(checkpointer=checkpointer)\n```\n\n```python  theme={null}\nfrom IPython.display import display, Image\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=1165c5d1a5c154b2491d6a5fca30853f\" alt=\"LangGraph chatbot with one step: START routes to autogen, where call_autogen_agent sends the latest user message (with prior context) to the AutoGen agent.\" data-og-width=\"180\" width=\"180\" data-og-height=\"134\" height=\"134\" data-path=\"langsmith/images/autogen-output.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=6a6671038776cd1784c968ee2ecf973e 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=94c98b5b118ae49006d2f56179e1dc0d 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=703b4822dfc1c3395c16dd9e7d0f1462 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=0f6b4e65d2f036d5b28dde44afbb5fd8 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=17f044c125e4a480d4bd8814c19a0949 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=841b246f7eabe796de9c4ac0af4816dd 2500w\" />\n\n## 3. Test the graph locally\n\nBefore deploying to LangSmith, you can test the graph locally:\n\n```python {highlight={2,13}} theme={null}\n# pass the thread ID to persist agent outputs for future interactions\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor chunk in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Find numbers between 10 and 30 in fibonacci sequence\",\n            }\n        ]\n    },\n    config,\n):\n    print(chunk)\n```\n\n**Output:**\n\n```\nuser_proxy (to assistant):\n\nFind numbers between 10 and 30 in fibonacci sequence\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\nTo find numbers between 10 and 30 in the Fibonacci sequence, we can generate the Fibonacci sequence and check which numbers fall within this range. Here's a plan:\n\n1. Generate Fibonacci numbers starting from 0.\n2. Continue generating until the numbers exceed 30.\n3. Collect and print the numbers that are between 10 and 30.\n\n...\n```\n\nSince we're leveraging LangGraph's [persistence](/oss/python/langgraph/persistence) features we can now continue the conversation using the same thread ID -- LangGraph will automatically pass previous history to the AutoGen agent:\n\n```python {highlight={10}} theme={null}\nfor chunk in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Multiply the last number by 3\",\n            }\n        ]\n    },\n    config,\n):\n    print(chunk)\n```\n\n**Output:**\n\n```\nuser_proxy (to assistant):\n\nMultiply the last number by 3\nContext:\nFind numbers between 10 and 30 in fibonacci sequence\nThe Fibonacci numbers between 10 and 30 are 13 and 21.\n\nThese numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1.\n\nThe sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n\nAs you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\n\nTERMINATE\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\nThe last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\n\n21 * 3 = 63\n\nTERMINATE\n\n--------------------------------------------------------------------------------\n{'call_autogen_agent': {'messages': {'role': 'assistant', 'content': 'The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\\n\\n21 * 3 = 63\\n\\nTERMINATE'}}}\n```\n\n## 4. Prepare for deployment\n\nTo deploy to LangSmith, create a file structure like the following:\n\n```\nmy-autogen-agent/\n├── agent.py          # Your main agent code\n├── requirements.txt  # Python dependencies\n└── langgraph.json   # LangGraph configuration\n```\n\n<Tabs>\n  <Tab title=\"agent.py\">\n    ```python  theme={null}\n    import os\n    import autogen\n    from langchain_core.messages import convert_to_openai_messages\n    from langgraph.graph import StateGraph, MessagesState, START\n    from langgraph.checkpoint.memory import MemorySaver\n\n    # AutoGen configuration\n    config_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]\n\n    llm_config = {\n        \"timeout\": 600,\n        \"cache_seed\": 42,\n        \"config_list\": config_list,\n        \"temperature\": 0,\n    }\n\n    # Create AutoGen agents\n    autogen_agent = autogen.AssistantAgent(\n        name=\"assistant\",\n        llm_config=llm_config,\n    )\n\n    user_proxy = autogen.UserProxyAgent(\n        name=\"user_proxy\",\n        human_input_mode=\"NEVER\",\n        max_consecutive_auto_reply=10,\n        is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n        code_execution_config={\n            \"work_dir\": \"/tmp/autogen_work\",\n            \"use_docker\": False,\n        },\n        llm_config=llm_config,\n        system_message=\"Reply TERMINATE if the task has been solved at full satisfaction.\",\n    )\n\n    def call_autogen_agent(state: MessagesState):\n        \"\"\"Node function that calls the AutoGen agent\"\"\"\n        messages = convert_to_openai_messages(state[\"messages\"])\n        last_message = messages[-1]\n        carryover = messages[:-1] if len(messages) > 1 else []\n\n        response = user_proxy.initiate_chat(\n            autogen_agent,\n            message=last_message,\n            carryover=carryover\n        )\n\n        final_content = response.chat_history[-1][\"content\"]\n        return {\"messages\": {\"role\": \"assistant\", \"content\": final_content}}\n\n    # Create and compile the graph\n    def create_graph():\n        checkpointer = MemorySaver()\n        builder = StateGraph(MessagesState)\n        builder.add_node(\"autogen\", call_autogen_agent)\n        builder.add_edge(START, \"autogen\")\n        return builder.compile(checkpointer=checkpointer)\n\n    # Export the graph for LangSmith\n    graph = create_graph()\n    ```\n  </Tab>\n\n  <Tab title=\"requirements.txt\">\n    ```\n    langgraph>=0.1.0\n    pyautogen>=0.2.0\n    langchain-core>=0.1.0\n    langchain-openai>=0.0.5\n    ```\n  </Tab>\n\n  <Tab title=\"langgraph.json\">\n    ```json  theme={null}\n    {\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"autogen_agent\": \"./agent.py:graph\"\n    },\n    \"env\": \".env\"\n    }\n    ```\n  </Tab>\n</Tabs>\n\n## 5. Deploy to LangSmith\n\nDeploy the graph with the LangSmith CLI:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U langgraph-cli\n  ```\n\n  ```bash uv theme={null}\n  uv add langgraph-cli\n  ```\n</CodeGroup>\n\n```\nlanggraph deploy --config langgraph.json\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/autogen-integration.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 11438
}