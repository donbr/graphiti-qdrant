{
  "title": "Guardrails",
  "source_url": "https://docs.langchain.com/oss/javascript/langchain/guardrails",
  "content": "Implement safety checks and content filtering for your agents\n\nGuardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent's execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.\n\nCommon use cases include:\n\n* Preventing PII leakage\n* Detecting and blocking prompt injection attacks\n* Blocking inappropriate or harmful content\n* Enforcing business rules and compliance requirements\n* Validating output quality and accuracy\n\nYou can implement guardrails using [middleware](/oss/javascript/langchain/middleware) to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img src=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1\" alt=\"Middleware flow diagram\" className=\"rounded-lg\" data-og-width=\"500\" width=\"500\" data-og-height=\"560\" height=\"560\" data-path=\"oss/images/middleware_final.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w\" />\n</div>\n\nGuardrails can be implemented using two complementary approaches:\n\n<CardGroup cols={2}>\n  <Card title=\"Deterministic guardrails\" icon=\"list-check\">\n    Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.\n  </Card>\n\n  <Card title=\"Model-based guardrails\" icon=\"brain\">\n    Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.\n  </Card>\n</CardGroup>\n\nLangChain provides both built-in guardrails (e.g., [PII detection](#pii-detection), [human-in-the-loop](#human-in-the-loop)) and a flexible middleware system for building custom guardrails using either approach.\n\n## Built-in guardrails\n\n### PII detection\n\nLangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.\n\nPII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.\n\nThe PII middleware supports multiple strategies for handling detected PII:\n\n| Strategy | Description                             | Example               |\n| -------- | --------------------------------------- | --------------------- |\n| `redact` | Replace with `[REDACTED_TYPE]`          | `[REDACTED_EMAIL]`    |\n| `mask`   | Partially obscure (e.g., last 4 digits) | `****-****-****-1234` |\n| `hash`   | Replace with deterministic hash         | `a8f5f167...`         |\n| `block`  | Raise exception when detected           | Error thrown          |\n\n```typescript  theme={null}\nimport { createAgent, piiRedactionMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [customerServiceTool, emailTool],\n  middleware: [\n    // Redact emails in user input before sending to model\n    piiRedactionMiddleware({\n      piiType: \"email\",\n      strategy: \"redact\",\n      applyToInput: true,\n    }),\n    // Mask credit cards in user input\n    piiRedactionMiddleware({\n      piiType: \"credit_card\",\n      strategy: \"mask\",\n      applyToInput: true,\n    }),\n    // Block API keys - raise error if detected\n    piiRedactionMiddleware({\n      piiType: \"api_key\",\n      detector: /sk-[a-zA-Z0-9]{32}/,\n      strategy: \"block\",\n      applyToInput: true,\n    }),\n  ],\n});\n\n// When user provides PII, it will be handled according to the strategy\nconst result = await agent.invoke({\n  messages: [{\n    role: \"user\",\n    content: \"My email is john.doe@example.com and card is 5105-1051-0510-5100\"\n  }]\n});\n```\n\n<Accordion title=\"Built-in PII types and configuration\">\n  **Built-in PII types:**\n\n  * `email` - Email addresses\n  * `credit_card` - Credit card numbers (Luhn validated)\n  * `ip` - IP addresses\n  * `mac_address` - MAC addresses\n  * `url` - URLs\n\n  **Configuration options:**\n\n  | Parameter            | Description                                                            | Default                     |\n  | -------------------- | ---------------------------------------------------------------------- | --------------------------- |\n  | `piiType`            | Type of PII to detect (built-in or custom)                             | Required                    |\n  | `strategy`           | How to handle detected PII (`\"block\"`, `\"redact\"`, `\"mask\"`, `\"hash\"`) | `\"redact\"`                  |\n  | `detector`           | Custom detector regex pattern                                          | `undefined` (uses built-in) |\n  | `applyToInput`       | Check user messages before model call                                  | `true`                      |\n  | `applyToOutput`      | Check AI messages after model call                                     | `false`                     |\n  | `applyToToolResults` | Check tool result messages after execution                             | `false`                     |\n</Accordion>\n\nSee the [middleware documentation](/oss/javascript/langchain/middleware#pii-detection) for complete details on PII detection capabilities.\n\n### Human-in-the-loop\n\nLangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.\n\nHuman-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.\n\n```typescript  theme={null}\nimport { createAgent, humanInTheLoopMiddleware } from \"langchain\";\nimport { MemorySaver, Command } from \"@langchain/langgraph\";\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [searchTool, sendEmailTool, deleteDatabaseTool],\n  middleware: [\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        // Require approval for sensitive operations\n        send_email: { allowAccept: true, allowEdit: true, allowRespond: true },\n        delete_database: { allowAccept: true, allowEdit: true, allowRespond: true },\n        // Auto-approve safe operations\n        search: false,\n      }\n    }),\n  ],\n  checkpointer: new MemorySaver(),\n});\n\n// Human-in-the-loop requires a thread ID for persistence\nconst config = { configurable: { thread_id: \"some_id\" } };\n\n// Agent will pause and wait for approval before executing sensitive tools\nlet result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Send an email to the team\" }] },\n  config\n);\n\nresult = await agent.invoke(\n  new Command({ resume: { decisions: [{ type: \"approve\" }] } }),\n  config  // Same thread ID to resume the paused conversation\n);\n```\n\n<Tip>\n  See the [human-in-the-loop documentation](/oss/javascript/langchain/human-in-the-loop) for complete details on implementing approval workflows.\n</Tip>\n\n## Custom guardrails\n\nFor more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.\n\n### Before agent guardrails\n\nUse \"before agent\" hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.\n\n```typescript  theme={null}\nimport { createMiddleware, AIMessage } from \"langchain\";\n\nconst contentFilterMiddleware = (bannedKeywords: string[]) => {\n  const keywords = bannedKeywords.map(kw => kw.toLowerCase());\n\n  return createMiddleware({\n    name: \"ContentFilterMiddleware\",\n    beforeAgent: (state) => {\n      // Get the first user message\n      if (!state.messages || state.messages.length === 0) {\n        return;\n      }\n\n      const firstMessage = state.messages[0];\n      if (firstMessage._getType() !== \"human\") {\n        return;\n      }\n\n      const content = firstMessage.content.toString().toLowerCase();\n\n      // Check for banned keywords\n      for (const keyword of keywords) {\n        if (content.includes(keyword)) {\n          // Block execution before any processing\n          return {\n            messages: [\n              new AIMessage(\n                \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n              )\n            ],\n            jumpTo: \"end\",\n          };\n        }\n      }\n\n      return;\n    },\n  });\n};\n\n// Use the custom guardrail\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [searchTool, calculatorTool],\n  middleware: [\n    contentFilterMiddleware([\"hack\", \"exploit\", \"malware\"]),\n  ],\n});\n\n// This request will be blocked before any processing\nconst result = await agent.invoke({\n  messages: [{ role: \"user\", content: \"How do I hack into a database?\" }]\n});\n```\n\n### After agent guardrails\n\nUse \"after agent\" hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.\n\n```typescript  theme={null}\nimport { createMiddleware, AIMessage, initChatModel } from \"langchain\";\n\nconst safetyGuardrailMiddleware = () => {\n  const safetyModel = initChatModel(\"gpt-4o-mini\");\n\n  return createMiddleware({\n    name: \"SafetyGuardrailMiddleware\",\n    afterAgent: async (state) => {\n      // Get the final AI response\n      if (!state.messages || state.messages.length === 0) {\n        return;\n      }\n\n      const lastMessage = state.messages[state.messages.length - 1];\n      if (lastMessage._getType() !== \"ai\") {\n        return;\n      }\n\n      // Use a model to evaluate safety\n      const safetyPrompt = `Evaluate if this response is safe and appropriate.\n      Respond with only 'SAFE' or 'UNSAFE'.\n\n      Response: ${lastMessage.content.toString()}`;\n\n      const result = await safetyModel.invoke([\n        { role: \"user\", content: safetyPrompt }\n      ]);\n\n      if (result.content.toString().includes(\"UNSAFE\")) {\n        return {\n          messages: [\n            new AIMessage(\n              \"I cannot provide that response. Please rephrase your request.\"\n            )\n          ],\n          jumpTo: \"end\",\n        };\n      }\n\n      return;\n    },\n  });\n};\n\n// Use the safety guardrail\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [searchTool, calculatorTool],\n  middleware: [safetyGuardrailMiddleware()],\n});\n\nconst result = await agent.invoke({\n  messages: [{ role: \"user\", content: \"How do I make explosives?\" }]\n});\n```\n\n### Combine multiple guardrails\n\nYou can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:\n\n```typescript  theme={null}\nimport { createAgent, piiRedactionMiddleware, humanInTheLoopMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [searchTool, sendEmailTool],\n  middleware: [\n    // Layer 1: Deterministic input filter (before agent)\n    contentFilterMiddleware([\"hack\", \"exploit\"]),\n\n    // Layer 2: PII protection (before and after model)\n    piiRedactionMiddleware({\n      piiType: \"email\",\n      strategy: \"redact\",\n      applyToInput: true,\n    }),\n    piiRedactionMiddleware({\n      piiType: \"email\",\n      strategy: \"redact\",\n      applyToOutput: true,\n    }),\n\n    // Layer 3: Human approval for sensitive tools\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        send_email: { allowAccept: true, allowEdit: true, allowRespond: true },\n      }\n    }),\n\n    // Layer 4: Model-based safety check (after agent)\n    safetyGuardrailMiddleware(),\n  ],\n});\n```\n\n## Additional resources\n\n* [Middleware documentation](/oss/javascript/langchain/middleware) - Complete guide to custom middleware\n* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) - Complete guide to custom middleware\n* [Human-in-the-loop](/oss/javascript/langchain/human-in-the-loop) - Add human review for sensitive operations\n* [Testing agents](/oss/javascript/langchain/test) - Strategies for testing safety mechanisms\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/guardrails.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 13959
}