{
  "title": "How to integrate LangGraph into your React application",
  "source_url": "https://docs.langchain.com/langsmith/use-stream-react",
  "content": "<Info>\n  **Prerequisites**\n\n  * [LangSmith](/langsmith/home)\n  * [Agent Server](/langsmith/agent-server)\n</Info>\n\nThe [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) React hook provides a seamless way to integrate LangGraph into your React applications. It handles all the complexities of streaming, state management, and branching logic, letting you focus on building great chat experiences.\n\nKey features:\n\n* Messages streaming: Handle a stream of message chunks to form a complete message\n* Automatic state management for messages, interrupts, loading states, and errors\n* Conversation branching: Create alternate conversation paths from any point in the chat history\n* UI-agnostic design: bring your own components and styling\n\nLet's explore how to use [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) in your React application.\n\nThe [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) provides a solid foundation for creating bespoke chat experiences. For pre-built chat components and interfaces, we also recommend checking out [CopilotKit](https://docs.copilotkit.ai/coagents/quickstart/langgraph) and [assistant-ui](https://www.assistant-ui.com/docs/runtimes/langgraph).\n\n## Installation\n\n```bash  theme={null}\nnpm install @langchain/langgraph-sdk @langchain/core\n```\n\n## Example\n\n```tsx  theme={null}\n\"use client\";\n\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\nexport default function App() {\n  const thread = useStream<{ messages: Message[] }>({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    messagesKey: \"messages\",\n  });\n\n  return (\n    <div>\n      <div>\n        {thread.messages.map((message) => (\n          <div key={message.id}>{message.content as string}</div>\n        ))}\n      </div>\n\n      <form\n        onSubmit={(e) => {\n          e.preventDefault();\n\n          const form = e.target as HTMLFormElement;\n          const message = new FormData(form).get(\"message\") as string;\n\n          form.reset();\n          thread.submit({ messages: [{ type: \"human\", content: message }] });\n        }}\n      >\n        <input type=\"text\" name=\"message\" />\n\n        {thread.isLoading ? (\n          <button key=\"stop\" type=\"button\" onClick={() => thread.stop()}>\n            Stop\n          </button>\n        ) : (\n          <button keytype=\"submit\">Send</button>\n        )}\n      </form>\n    </div>\n  );\n}\n```\n\n## Customizing Your UI\n\nThe [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook takes care of all the complex state management behind the scenes, providing you with simple interfaces to build your UI. Here's what you get out of the box:\n\n* Thread state management\n* Loading and error states\n* Interrupts\n* Message handling and updates\n* Branching support\n\nHere are some examples on how to use these features effectively:\n\n### Loading States\n\nThe `isLoading` property tells you when a stream is active, enabling you to:\n\n* Show a loading indicator\n* Disable input fields during processing\n* Display a cancel button\n\n```tsx  theme={null}\nexport default function App() {\n  const { isLoading, stop } = useStream<{ messages: Message[] }>({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    messagesKey: \"messages\",\n  });\n\n  return (\n    <form>\n      {isLoading && (\n        <button key=\"stop\" type=\"button\" onClick={() => stop()}>\n          Stop\n        </button>\n      )}\n    </form>\n  );\n}\n```\n\n### Resume a stream after page refresh\n\nThe [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook can automatically resume an ongoing run upon mounting by setting `reconnectOnMount: true`. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost.\n\n```tsx  theme={null}\nconst thread = useStream<{ messages: Message[] }>({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  reconnectOnMount: true,\n});\n```\n\nBy default the ID of the created run is stored in `window.sessionStorage`, which can be swapped by passing a custom storage in `reconnectOnMount` instead. The storage is used to persist the in-flight run ID for a thread (under `lg:stream:${threadId}` key).\n\n```tsx  theme={null}\nconst thread = useStream<{ messages: Message[] }>({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  reconnectOnMount: () => window.localStorage,\n});\n```\n\nYou can also manually manage the resuming process by using the run callbacks to persist the run metadata and the `joinStream` function to resume the stream. Make sure to pass `streamResumable: true` when creating the run; otherwise some events might be lost.\n\n```tsx  theme={null}\nimport type { Message } from \"@langchain/langgraph-sdk\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport { useCallback, useState, useEffect, useRef } from \"react\";\n\nexport default function App() {\n  const [threadId, onThreadId] = useSearchParam(\"threadId\");\n\n  const thread = useStream<{ messages: Message[] }>({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n\n    threadId,\n    onThreadId,\n\n    onCreated: (run) => {\n      window.sessionStorage.setItem(`resume:${run.thread_id}`, run.run_id);\n    },\n    onFinish: (_, run) => {\n      window.sessionStorage.removeItem(`resume:${run?.thread_id}`);\n    },\n  });\n\n  // Ensure that we only join the stream once per thread.\n  const joinedThreadId = useRef<string | null>(null);\n  useEffect(() => {\n    if (!threadId) return;\n\n    const resume = window.sessionStorage.getItem(`resume:${threadId}`);\n    if (resume && joinedThreadId.current !== threadId) {\n      thread.joinStream(resume);\n      joinedThreadId.current = threadId;\n    }\n  }, [threadId]);\n\n  return (\n    <form\n      onSubmit={(e) => {\n        e.preventDefault();\n        const form = e.target as HTMLFormElement;\n        const message = new FormData(form).get(\"message\") as string;\n        thread.submit(\n          { messages: [{ type: \"human\", content: message }] },\n          { streamResumable: true }\n        );\n      }}\n    >\n      <div>\n        {thread.messages.map((message) => (\n          <div key={message.id}>{message.content as string}</div>\n        ))}\n      </div>\n      <input type=\"text\" name=\"message\" />\n      <button type=\"submit\">Send</button>\n    </form>\n  );\n}\n\n// Utility method to retrieve and persist data in URL as search param\nfunction useSearchParam(key: string) {\n  const [value, setValue] = useState<string | null>(() => {\n    const params = new URLSearchParams(window.location.search);\n    return params.get(key) ?? null;\n  });\n\n  const update = useCallback(\n    (value: string | null) => {\n      setValue(value);\n\n      const url = new URL(window.location.href);\n      if (value == null) {\n        url.searchParams.delete(key);\n      } else {\n        url.searchParams.set(key, value);\n      }\n\n      window.history.pushState({}, \"\", url.toString());\n    },\n    [key]\n  );\n\n  return [value, update] as const;\n}\n```\n\n### Thread Management\n\nKeep track of conversations with built-in thread management. You can access the current thread ID and get notified when new threads are created:\n\n```tsx  theme={null}\nconst [threadId, setThreadId] = useState<string | null>(null);\n\nconst thread = useStream<{ messages: Message[] }>({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n\n  threadId: threadId,\n  onThreadId: setThreadId,\n});\n```\n\nWe recommend storing the `threadId` in your URL's query parameters to let users resume conversations after page refreshes.\n\n### Messages Handling\n\nThe [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook will keep track of the message chunks received from the server and concatenate them together to form a complete message. The completed message chunks can be retrieved via the `messages` property.\n\nBy default, the `messagesKey` is set to `messages`, where it will append the new messages chunks to `values[\"messages\"]`. If you store messages in a different key, you can change the value of `messagesKey`.\n\n```tsx  theme={null}\nimport type { Message } from \"@langchain/langgraph-sdk\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\nexport default function HomePage() {\n  const thread = useStream<{ messages: Message[] }>({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    messagesKey: \"messages\",\n  });\n\n  return (\n    <div>\n      {thread.messages.map((message) => (\n        <div key={message.id}>{message.content as string}</div>\n      ))}\n    </div>\n  );\n}\n```\n\nUnder the hood, the [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook will use the `streamMode: \"messages-tuple\"` to receive a stream of messages (i.e. individual LLM tokens) from any LangChain chat model invocations inside your graph nodes. Learn more about messages streaming in the [streaming](/langsmith/streaming#messages) guide.\n\n### Interrupts\n\nThe [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook exposes the `interrupt` property, which will be filled with the last interrupt from the thread. You can use interrupts to:\n\n* Render a confirmation UI before executing a node\n* Wait for human input, allowing agent to ask the user with clarifying questions\n\nLearn more about interrupts in the [How to handle interrupts](/oss/python/langgraph/interrupts#pause-using-interrupt) guide.\n\n```tsx  theme={null}\nconst thread = useStream<{ messages: Message[] }, { InterruptType: string }>({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  messagesKey: \"messages\",\n});\n\nif (thread.interrupt) {\n  return (\n    <div>\n      Interrupted! {thread.interrupt.value}\n      <button\n        type=\"button\"\n        onClick={() => {\n          // `resume` can be any value that the agent accepts\n          thread.submit(undefined, { command: { resume: true } });\n        }}\n      >\n        Resume\n      </button>\n    </div>\n  );\n}\n```\n\n### Branching\n\nFor each message, you can use `getMessagesMetadata()` to get the first checkpoint from which the message has been first seen. You can then create a new run from the checkpoint preceding the first seen checkpoint to create a new branch in a thread.\n\nA branch can be created in following ways:\n\n1. Edit a previous user message.\n2. Request a regeneration of a previous assistant message.\n\n```tsx  theme={null}\n\"use client\";\n\nimport type { Message } from \"@langchain/langgraph-sdk\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport { useState } from \"react\";\n\nfunction BranchSwitcher({\n  branch,\n  branchOptions,\n  onSelect,\n}: {\n  branch: string | undefined;\n  branchOptions: string[] | undefined;\n  onSelect: (branch: string) => void;\n}) {\n  if (!branchOptions || !branch) return null;\n  const index = branchOptions.indexOf(branch);\n\n  return (\n    <div className=\"flex items-center gap-2\">\n      <button\n        type=\"button\"\n        onClick={() => {\n          const prevBranch = branchOptions[index - 1];\n          if (!prevBranch) return;\n          onSelect(prevBranch);\n        }}\n      >\n        Prev\n      </button>\n      <span>\n        {index + 1} / {branchOptions.length}\n      </span>\n      <button\n        type=\"button\"\n        onClick={() => {\n          const nextBranch = branchOptions[index + 1];\n          if (!nextBranch) return;\n          onSelect(nextBranch);\n        }}\n      >\n        Next\n      </button>\n    </div>\n  );\n}\n\nfunction EditMessage({\n  message,\n  onEdit,\n}: {\n  message: Message;\n  onEdit: (message: Message) => void;\n}) {\n  const [editing, setEditing] = useState(false);\n\n  if (!editing) {\n    return (\n      <button type=\"button\" onClick={() => setEditing(true)}>\n        Edit\n      </button>\n    );\n  }\n\n  return (\n    <form\n      onSubmit={(e) => {\n        e.preventDefault();\n        const form = e.target as HTMLFormElement;\n        const content = new FormData(form).get(\"content\") as string;\n\n        form.reset();\n        onEdit({ type: \"human\", content });\n        setEditing(false);\n      }}\n    >\n      <input name=\"content\" defaultValue={message.content as string} />\n      <button type=\"submit\">Save</button>\n    </form>\n  );\n}\n\nexport default function App() {\n  const thread = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    messagesKey: \"messages\",\n  });\n\n  return (\n    <div>\n      <div>\n        {thread.messages.map((message) => {\n          const meta = thread.getMessagesMetadata(message);\n          const parentCheckpoint = meta?.firstSeenState?.parent_checkpoint;\n\n          return (\n            <div key={message.id}>\n              <div>{message.content as string}</div>\n\n              {message.type === \"human\" && (\n                <EditMessage\n                  message={message}\n                  onEdit={(message) =>\n                    thread.submit(\n                      { messages: [message] },\n                      { checkpoint: parentCheckpoint }\n                    )\n                  }\n                />\n              )}\n\n              {message.type === \"ai\" && (\n                <button\n                  type=\"button\"\n                  onClick={() =>\n                    thread.submit(undefined, { checkpoint: parentCheckpoint })\n                  }\n                >\n                  <span>Regenerate</span>\n                </button>\n              )}\n\n              <BranchSwitcher\n                branch={meta?.branch}\n                branchOptions={meta?.branchOptions}\n                onSelect={(branch) => thread.setBranch(branch)}\n              />\n            </div>\n          );\n        })}\n      </div>\n\n      <form\n        onSubmit={(e) => {\n          e.preventDefault();\n\n          const form = e.target as HTMLFormElement;\n          const message = new FormData(form).get(\"message\") as string;\n\n          form.reset();\n          thread.submit({ messages: [message] });\n        }}\n      >\n        <input type=\"text\" name=\"message\" />\n\n        {thread.isLoading ? (\n          <button key=\"stop\" type=\"button\" onClick={() => thread.stop()}>\n            Stop\n          </button>\n        ) : (\n          <button key=\"submit\" type=\"submit\">\n            Send\n          </button>\n        )}\n      </form>\n    </div>\n  );\n}\n```\n\nFor advanced use cases you can use the `experimental_branchTree` property to get the tree representation of the thread, which can be used to render branching controls for non-message based graphs.\n\n### Optimistic Updates\n\nYou can optimistically update the client state before performing a network request to the agent, allowing you to provide immediate feedback to the user, such as showing the user message immediately before the agent has seen the request.\n\n```tsx  theme={null}\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  messagesKey: \"messages\",\n});\n\nconst handleSubmit = (text: string) => {\n  const newMessage = { type: \"human\" as const, content: text };\n\n  stream.submit(\n    { messages: [newMessage] },\n    {\n      optimisticValues(prev) {\n        const prevMessages = prev.messages ?? [];\n        const newMessages = [...prevMessages, newMessage];\n        return { ...prev, messages: newMessages };\n      },\n    }\n  );\n};\n```\n\n### Cached Thread Display\n\nUse the `initialValues` option to display cached thread data immediately while the history is being loaded from the server. This improves user experience by showing cached data instantly when navigating to existing threads.\n\n```tsx  theme={null}\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\nconst CachedThreadExample = ({ threadId, cachedThreadData }) => {\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId,\n    // Show cached data immediately while history loads\n    initialValues: cachedThreadData?.values,\n    messagesKey: \"messages\",\n  });\n\n  return (\n    <div>\n      {stream.messages.map((message) => (\n        <div key={message.id}>{message.content as string}</div>\n      ))}\n    </div>\n  );\n};\n```\n\n### Optimistic Thread Creation\n\nUse the `threadId` option in `submit` function to enable optimistic UI patterns where you need to know the thread ID before the thread is actually created.\n\n```tsx  theme={null}\nimport { useState } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\nconst OptimisticThreadExample = () => {\n  const [threadId, setThreadId] = useState<string | null>(null);\n  const [optimisticThreadId] = useState(() => crypto.randomUUID());\n\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId,\n    onThreadId: setThreadId, // (3) Updated after thread has been created.\n    messagesKey: \"messages\",\n  });\n\n  const handleSubmit = (text: string) => {\n    // (1) Perform a soft navigation to /threads/${optimisticThreadId}\n    // without waiting for thread creation.\n    window.history.pushState({}, \"\", `/threads/${optimisticThreadId}`);\n\n    // (2) Submit message to create thread with the predetermined ID.\n    stream.submit(\n      { messages: [{ type: \"human\", content: text }] },\n      { threadId: optimisticThreadId }\n    );\n  };\n\n  return (\n    <div>\n      <p>Thread ID: {threadId ?? optimisticThreadId}</p>\n      {/* Rest of component */}\n    </div>\n  );\n};\n```\n\n### TypeScript\n\nThe [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook is friendly for apps written in TypeScript and you can specify types for the state to get better type safety and IDE support.\n\n```tsx  theme={null}\n// Define your types\ntype State = {\n  messages: Message[];\n  context?: Record<string, unknown>;\n};\n\n// Use them with the hook\nconst thread = useStream<State>({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  messagesKey: \"messages\",\n});\n```\n\nYou can also optionally specify types for different scenarios, such as:\n\n* `ConfigurableType`: Type for the `config.configurable` property (default: `Record<string, unknown>`)\n* `InterruptType`: Type for the interrupt value - i.e. contents of `interrupt(...)` function (default: `unknown`)\n* `CustomEventType`: Type for the custom events (default: `unknown`)\n* `UpdateType`: Type for the submit function (default: `Partial<State>`)\n\n```tsx  theme={null}\nconst thread = useStream<\n  State,\n  {\n    UpdateType: {\n      messages: Message[] | Message;\n      context?: Record<string, unknown>;\n    };\n    InterruptType: string;\n    CustomEventType: {\n      type: \"progress\" | \"debug\";\n      payload: unknown;\n    };\n    ConfigurableType: {\n      model: string;\n    };\n  }\n>({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  messagesKey: \"messages\",\n});\n```\n\nIf you're using LangGraph.js, you can also reuse your graph's annotation types. However, make sure to only import the types of the annotation schema in order to avoid importing the entire LangGraph.js runtime (i.e. via `import type { ... }` directive).\n\n```tsx  theme={null}\nimport {\n  Annotation,\n  MessagesAnnotation,\n  type StateType,\n  type UpdateType,\n} from \"@langchain/langgraph/web\";\n\nconst AgentState = Annotation.Root({\n  ...MessagesAnnotation.spec,\n  context: Annotation<string>(),\n});\n\nconst thread = useStream<\n  StateType<typeof AgentState.spec>,\n  { UpdateType: UpdateType<typeof AgentState.spec> }\n>({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  messagesKey: \"messages\",\n});\n```\n\n## Event Handling\n\nThe [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook provides several callback options to help you respond to different events:\n\n* `onError`: Called when an error occurs.\n* `onFinish`: Called when the stream is finished.\n* `onUpdateEvent`: Called when an update event is received.\n* `onCustomEvent`: Called when a custom event is received. See the [streaming](/oss/python/langgraph/streaming#stream-custom-data) guide to learn how to stream custom events.\n* `onMetadataEvent`: Called when a metadata event is received, which contains the Run ID and Thread ID.\n\n## Learn More\n\n* [JS/TS SDK Reference](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-stream-react.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 20585
}