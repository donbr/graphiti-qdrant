{
  "title": "Trace with AutoGen",
  "source_url": "https://docs.langchain.com/langsmith/trace-with-autogen",
  "content": "LangSmith can capture traces generated by [AutoGen](https://microsoft.github.io/autogen/stable/) using OpenInference's AutoGen instrumentation. This guide shows you how to automatically capture traces from your AutoGen multi-agent conversations and send them to LangSmith for monitoring and analysis.\n\n## Installation\n\nInstall the required packages using your preferred package manager:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langsmith autogen openinference-instrumentation-autogen openinference-instrumentation-openai\n  ```\n\n  ```bash uv theme={null}\n  uv add langsmith autogen openinference-instrumentation-autogen openinference-instrumentation-openai\n  ```\n</CodeGroup>\n\n<Info>\n  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.\n</Info>\n\n## Setup\n\n### 1. Configure environment variables\n\nSet your API keys and project name:\n\n<CodeGroup>\n  ```bash Shell theme={null}\n  export LANGSMITH_API_KEY=<your_langsmith_api_key>\n  export LANGSMITH_PROJECT=<your_project_name>\n  export OPENAI_API_KEY=<your_openai_api_key>\n  ```\n</CodeGroup>\n\n### 2. Configure OpenTelemetry integration\n\nIn your AutoGen application, import and configure the LangSmith OpenTelemetry integration along with the AutoGen and OpenAI instrumentors:\n\n```python  theme={null}\nfrom langsmith.integrations.otel import configure\nfrom openinference.instrumentation.autogen import AutogenInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\n# Configure LangSmith tracing\nconfigure(project_name=\"autogen-demo\")\n\n# Instrument AutoGen and OpenAI calls\nAutogenInstrumentor().instrument()\nOpenAIInstrumentor().instrument()\n```\n\n<Note>\n  You do not need to set any OpenTelemetry environment variables or configure exporters manuallyâ€”`configure()` handles everything automatically.\n</Note>\n\n### 3. Create and run your AutoGen application\n\nOnce configured, your AutoGen application will automatically send traces to LangSmith:\n\n```python  theme={null}\nimport autogen\nfrom openinference.instrumentation.autogen import AutogenInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom langsmith.integrations.otel import configure\nimport os\nimport dotenv\n\n# Load environment variables\ndotenv.load_dotenv(\".env.local\")\n\n# Configure LangSmith tracing\nconfigure(project_name=\"autogen-code-review\")\n\n# Instrument AutoGen and OpenAI\nAutogenInstrumentor().instrument()\nOpenAIInstrumentor().instrument()\n\n# Configure your agents\nconfig_list = [\n    {\n        \"model\": \"gpt-4\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n]\n\n# Create a code reviewer agent\ncode_reviewer = autogen.AssistantAgent(\n    name=\"code_reviewer\",\n    llm_config={\"config_list\": config_list},\n    system_message=\"\"\"You are an expert code reviewer. Your role is to:\n    1. Review code for bugs, security issues, and best practices\n    2. Suggest improvements and optimizations\n    3. Provide constructive feedback\n    Always be thorough but constructive in your reviews.\"\"\",\n)\n\n# Create a developer agent\ndeveloper = autogen.AssistantAgent(\n    name=\"developer\",\n    llm_config={\"config_list\": config_list},\n    system_message=\"\"\"You are a senior software developer. Your role is to:\n    1. Write clean, efficient code\n    2. Address feedback from code reviews\n    3. Explain your implementation decisions\n    4. Implement requested features and fixes\"\"\",\n)\n\n# Create a user proxy agent\nuser_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=8,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n    code_execution_config={\"work_dir\": \"workspace\"},\n    llm_config={\"config_list\": config_list},\n)\n\ndef run_code_review_session(task_description: str):\n    \"\"\"Run a multi-agent code review session.\"\"\"\n\n    # Create a group chat with the agents\n    groupchat = autogen.GroupChat(\n        agents=[user_proxy, developer, code_reviewer],\n        messages=[],\n        max_round=10\n    )\n\n    # Create a group chat manager\n    manager = autogen.GroupChatManager(\n        groupchat=groupchat,\n        llm_config={\"config_list\": config_list}\n    )\n\n    # Start the conversation\n    user_proxy.initiate_chat(\n        manager,\n        message=f\"\"\"\n        Task: {task_description}\n\n        Developer: Please implement the requested feature.\n        Code Reviewer: Please review the implementation and provide feedback.\n\n        Work together to create a high-quality solution.\n        \"\"\"\n    )\n\n    return \"Code review session completed\"\n\n# Example usage\nif __name__ == \"__main__\":\n    task = \"\"\"\n    Create a Python function that implements a binary search algorithm.\n    The function should:\n    - Take a sorted list and a target value as parameters\n    - Return the index of the target if found, or -1 if not found\n    - Include proper error handling and documentation\n    \"\"\"\n\n    result = run_code_review_session(task)\n    print(f\"Result: {result}\")\n```\n\n## Advanced usage\n\n### Custom metadata and tags\n\nYou can add custom metadata to your traces by setting span attributes in your AutoGen application:\n\n```python  theme={null}\nfrom opentelemetry import trace\n\n# Get the current tracer\ntracer = trace.get_tracer(__name__)\n\ndef run_code_review_session(task_description: str):\n    with tracer.start_as_current_span(\"autogen_code_review\") as span:\n        # Add custom metadata\n        span.set_attribute(\"langsmith.metadata.session_type\", \"code_review\")\n        span.set_attribute(\"langsmith.metadata.agent_count\", \"3\")\n        span.set_attribute(\"langsmith.metadata.task_complexity\", \"medium\")\n        span.set_attribute(\"langsmith.span.tags\", \"autogen,code-review,multi-agent\")\n\n        # Your AutoGen code here\n        groupchat = autogen.GroupChat(\n            agents=[user_proxy, developer, code_reviewer],\n            messages=[],\n            max_round=10\n        )\n\n        manager = autogen.GroupChatManager(\n            groupchat=groupchat,\n            llm_config={\"config_list\": config_list}\n        )\n\n        user_proxy.initiate_chat(manager, message=task_description)\n        return \"Session completed\"\n```\n\n### Combining with other instrumentors\n\nYou can combine AutoGen instrumentation with other instrumentors (e.g., Semantic Kernel, DSPy) by adding them and initializing them as instrumentors:\n\n```python  theme={null}\nfrom langsmith.integrations.otel import configure\nfrom openinference.instrumentation.autogen import AutogenInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom openinference.instrumentation.dspy import DSPyInstrumentor\n\n# Configure LangSmith tracing\nconfigure(project_name=\"multi-framework-app\")\n\n# Initialize multiple instrumentors\nAutogenInstrumentor().instrument()\nOpenAIInstrumentor().instrument()\nDSPyInstrumentor().instrument()\n\n# Your application code using multiple frameworks\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-autogen.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 7250
}