{
  "title": "How to evaluate an LLM application",
  "source_url": "https://docs.langchain.com/langsmith/evaluate-llm-application",
  "content": "This guide shows you how to run an evaluation on an LLM application using the LangSmith SDK.\n\n<Info>\n  [Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets)\n</Info>\n\nIn this guide we'll go over how to evaluate an application using the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method in the LangSmith SDK.\n\n<Check>\n  For larger evaluation jobs in Python we recommend using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), the asynchronous version of [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate). It is still worthwhile to read this guide first, as the two have identical interfaces, before reading the how-to guide on [running an evaluation asynchronously](/langsmith/evaluation-async).\n\n  In JS/TS evaluate() is already asynchronous so no separate method is needed.\n\n  It is also important to configure the `max_concurrency`/`maxConcurrency` arg when running large jobs. This parallelizes evaluation by effectively splitting the dataset across threads.\n</Check>\n\n## Define an application\n\nFirst we need an application to evaluate. Let's create a simple toxicity classifier for this example.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import traceable, wrappers\n  from openai import OpenAI\n\n  # Optionally wrap the OpenAI client to trace all model calls.\n  oai_client = wrappers.wrap_openai(OpenAI())\n\n  # Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.\n  @traceable\n  def toxicity_classifier(inputs: dict) -> dict:\n      instructions = (\n        \"Please review the user query below and determine if it contains any form of toxic behavior, \"\n        \"such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does \"\n        \"and 'Not toxic' if it doesn't.\"\n      )\n      messages = [\n          {\"role\": \"system\", \"content\": instructions},\n          {\"role\": \"user\", \"content\": inputs[\"text\"]},\n      ]\n      result = oai_client.chat.completions.create(\n          messages=messages, model=\"gpt-4o-mini\", temperature=0\n      )\n      return {\"class\": result.choices[0].message.content}\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { OpenAI } from \"openai\";\n  import { wrapOpenAI } from \"langsmith/wrappers\";\n  import { traceable } from \"langsmith/traceable\";\n\n  // Optionally wrap the OpenAI client to trace all model calls.\n  const oaiClient = wrapOpenAI(new OpenAI());\n\n  // Optionally add the 'traceable' wrapper to trace the inputs/outputs of this function.\n  const toxicityClassifier = traceable(\n    async (text: string) => {\n      const result = await oaiClient.chat.completions.create({\n        messages: [\n          {\n             role: \"system\",\n            content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",\n          },\n          { role: \"user\", content: text },\n        ],\n        model: \"gpt-4o-mini\",\n        temperature: 0,\n      });\n\n      return result.choices[0].message.content;\n    },\n    { name: \"toxicityClassifier\" }\n  );\n  ```\n</CodeGroup>\n\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to [this guide](/langsmith/annotate-code).\n\n## Create or select a dataset\n\nWe need a [Dataset](/langsmith/evaluation-concepts#datasets) to evaluate our application on. Our dataset will contain labeled [examples](/langsmith/evaluation-concepts#examples) of toxic and non-toxic text.\n\nRequires `langsmith>=0.3.13`\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import Client\n  ls_client = Client()\n\n  examples = [\n    {\n      \"inputs\": {\"text\": \"Shut up, idiot\"},\n      \"outputs\": {\"label\": \"Toxic\"},\n    },\n    {\n      \"inputs\": {\"text\": \"You're a wonderful person\"},\n      \"outputs\": {\"label\": \"Not toxic\"},\n    },\n    {\n      \"inputs\": {\"text\": \"This is the worst thing ever\"},\n      \"outputs\": {\"label\": \"Toxic\"},\n    },\n    {\n      \"inputs\": {\"text\": \"I had a great day today\"},\n      \"outputs\": {\"label\": \"Not toxic\"},\n    },\n    {\n      \"inputs\": {\"text\": \"Nobody likes you\"},\n      \"outputs\": {\"label\": \"Toxic\"},\n    },\n    {\n      \"inputs\": {\"text\": \"This is unacceptable. I want to speak to the manager.\"},\n      \"outputs\": {\"label\": \"Not toxic\"},\n    },\n  ]\n\n  dataset = ls_client.create_dataset(dataset_name=\"Toxic Queries\")\n  ls_client.create_examples(\n    dataset_id=dataset.id,\n    examples=examples,\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n\n  const langsmith = new Client();\n\n  // create a dataset\n  const labeledTexts = [\n    [\"Shut up, idiot\", \"Toxic\"],\n    [\"You're a wonderful person\", \"Not toxic\"],\n    [\"This is the worst thing ever\", \"Toxic\"],\n    [\"I had a great day today\", \"Not toxic\"],\n    [\"Nobody likes you\", \"Toxic\"],\n    [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],\n  ];\n\n  const [inputs, outputs] = labeledTexts.reduce<\n    [Array<{ input: string }>, Array<{ outputs: string }>]\n  >(\n    ([inputs, outputs], item) => [\n      [...inputs, { input: item[0] }],\n      [...outputs, { outputs: item[1] }],\n    ],\n    [[], []]\n  );\n\n  const datasetName = \"Toxic Queries\";\n  const toxicDataset = await langsmith.createDataset(datasetName);\n  await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });\n  ```\n</CodeGroup>\n\nFor more details on datasets, refer to the [Manage datasets](/langsmith/manage-datasets) page.\n\n## Define an evaluator\n\n<Check>\n  You can also check out LangChain's open source evaluation package [openevals](https://github.com/langchain-ai/openevals) for common pre-built evaluators.\n</Check>\n\n[Evaluators](/langsmith/evaluation-concepts#evaluators) are functions for scoring your application's outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.\n\n* Python: Requires `langsmith>=0.3.13`\n* TypeScript: Requires `langsmith>=0.2.9`\n\n<CodeGroup>\n  ```python Python theme={null}\n  def correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n      return outputs[\"class\"] == reference_outputs[\"label\"]\n  ```\n\n  ```typescript TypeScript theme={null}\n  import type { EvaluationResult } from \"langsmith/evaluation\";\n\n  function correct({\n    outputs,\n    referenceOutputs,\n  }: {\n    outputs: Record<string, any>;\n    referenceOutputs?: Record<string, any>;\n  }): EvaluationResult {\n    const score = outputs.output === referenceOutputs?.outputs;\n    return { key: \"correct\", score };\n  }\n  ```\n</CodeGroup>\n\n## Run the evaluation\n\nWe'll use the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) methods to run the evaluation.\n\nThe key arguments are:\n\n* a target function that takes an input dictionary and returns an output dictionary. The `example.inputs` field of each [Example](/langsmith/example-data-format) is what gets passed to the target function. In this case our `toxicity_classifier` is already set up to take in example inputs so we can use it directly.\n* `data` - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examples\n* `evaluators` - a list of evaluators to score the outputs of the function\n\nPython: Requires `langsmith>=0.3.13`\n\n<CodeGroup>\n  ```python Python theme={null}\n  # Can equivalently use the 'evaluate' function directly:\n  # from langsmith import evaluate; evaluate(...)\n  results = ls_client.evaluate(\n      toxicity_classifier,\n      data=dataset.name,\n      evaluators=[correct],\n      experiment_prefix=\"gpt-4o-mini, baseline\",  # optional, experiment name prefix\n      description=\"Testing the baseline system.\",  # optional, experiment description\n      max_concurrency=4, # optional, add concurrency\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { evaluate } from \"langsmith/evaluation\";\n\n  await evaluate((inputs) => toxicityClassifier(inputs[\"input\"]), {\n    data: datasetName,\n    evaluators: [correct],\n    experimentPrefix: \"gpt-4o-mini, baseline\",  // optional, experiment name prefix\n    maxConcurrency: 4, // optional, add concurrency\n  });\n  ```\n</CodeGroup>\n\n## Explore the results[​](#explore-the-results \"Direct link to Explore the results\")\n\nEach invocation of `evaluate()` creates an [Experiment](/langsmith/evaluation-concepts#experiments) which can be viewed in the LangSmith UI or queried via the SDK. Evaluation scores are stored against each actual output as feedback.\n\n*If you've annotated your code for tracing, you can open the trace of each row in a side panel view.*\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-experiment.gif?s=252d96dbd2100a691f1d3b61716fde38\" alt=\"\" data-og-width=\"1132\" width=\"1132\" data-og-height=\"720\" height=\"720\" data-path=\"langsmith/images/view-experiment.gif\" data-optimize=\"true\" data-opv=\"3\" />\n\n## Reference code[​](#reference-code \"Direct link to Reference code\")\n\n<Accordion title=\"Click to see a consolidated code snippet\">\n  <CodeGroup>\n    ```python Python theme={null}\n    from langsmith import Client, traceable, wrappers\n    from openai import OpenAI\n\n    # Step 1. Define an application\n    oai_client = wrappers.wrap_openai(OpenAI())\n\n    @traceable\n    def toxicity_classifier(inputs: dict) -> str:\n        system = (\n          \"Please review the user query below and determine if it contains any form of toxic behavior, \"\n          \"such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does \"\n          \"and 'Not toxic' if it doesn't.\"\n        )\n        messages = [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": inputs[\"text\"]},\n        ]\n        result = oai_client.chat.completions.create(\n            messages=messages, model=\"gpt-4o-mini\", temperature=0\n        )\n        return result.choices[0].message.content\n\n    # Step 2. Create a dataset\n    ls_client = Client()\n    dataset = ls_client.create_dataset(dataset_name=\"Toxic Queries\")\n    examples = [\n      {\n        \"inputs\": {\"text\": \"Shut up, idiot\"},\n        \"outputs\": {\"label\": \"Toxic\"},\n      },\n      {\n        \"inputs\": {\"text\": \"You're a wonderful person\"},\n        \"outputs\": {\"label\": \"Not toxic\"},\n      },\n      {\n        \"inputs\": {\"text\": \"This is the worst thing ever\"},\n        \"outputs\": {\"label\": \"Toxic\"},\n      },\n      {\n        \"inputs\": {\"text\": \"I had a great day today\"},\n        \"outputs\": {\"label\": \"Not toxic\"},\n      },\n      {\n        \"inputs\": {\"text\": \"Nobody likes you\"},\n        \"outputs\": {\"label\": \"Toxic\"},\n      },\n      {\n        \"inputs\": {\"text\": \"This is unacceptable. I want to speak to the manager.\"},\n        \"outputs\": {\"label\": \"Not toxic\"},\n      },\n    ]\n    ls_client.create_examples(\n      dataset_id=dataset.id,\n      examples=examples,\n    )\n\n    # Step 3. Define an evaluator\n    def correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n        return outputs[\"output\"] == reference_outputs[\"label\"]\n\n    # Step 4. Run the evaluation\n    # Client.evaluate() and evaluate() behave the same.\n    results = ls_client.evaluate(\n        toxicity_classifier,\n        data=dataset.name,\n        evaluators=[correct],\n        experiment_prefix=\"gpt-4o-mini, simple\",  # optional, experiment name prefix\n        description=\"Testing the baseline system.\",  # optional, experiment description\n        max_concurrency=4,  # optional, add concurrency\n    )\n    ```\n\n    ```typescript TypeScript theme={null}\n    import { OpenAI } from \"openai\";\n    import { Client } from \"langsmith\";\n    import { evaluate, EvaluationResult } from \"langsmith/evaluation\";\n    import type { Run, Example } from \"langsmith/schemas\";\n    import { traceable } from \"langsmith/traceable\";\n    import { wrapOpenAI } from \"langsmith/wrappers\";\n\n    const oaiClient = wrapOpenAI(new OpenAI());\n\n    const toxicityClassifier = traceable(\n      async (text: string) => {\n        const result = await oaiClient.chat.completions.create({\n          messages: [\n            {\n              role: \"system\",\n              content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",\n            },\n            { role: \"user\", content: text },\n          ],\n          model: \"gpt-4o-mini\",\n          temperature: 0,\n        });\n        return result.choices[0].message.content;\n      },\n      { name: \"toxicityClassifier\" }\n    );\n\n    const langsmith = new Client();\n\n    // create a dataset\n    const labeledTexts = [\n      [\"Shut up, idiot\", \"Toxic\"],\n      [\"You're a wonderful person\", \"Not toxic\"],\n      [\"This is the worst thing ever\", \"Toxic\"],\n      [\"I had a great day today\", \"Not toxic\"],\n      [\"Nobody likes you\", \"Toxic\"],\n      [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],\n    ];\n\n    const [inputs, outputs] = labeledTexts.reduce<\n      [Array<{ input: string }>, Array<{ outputs: string }>]\n    >(\n      ([inputs, outputs], item) => [\n        [...inputs, { input: item[0] }],\n        [...outputs, { outputs: item[1] }],\n      ],\n      [[], []]\n    );\n\n    const datasetName = \"Toxic Queries\";\n    const toxicDataset = await langsmith.createDataset(datasetName);\n    await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });\n\n    // Row-level evaluator\n    function correct({\n      outputs,\n      referenceOutputs,\n    }: {\n      outputs: Record<string, any>;\n      referenceOutputs?: Record<string, any>;\n    }): EvaluationResult {\n      const score = outputs.output === referenceOutputs?.outputs;\n      return { key: \"correct\", score };\n    }\n\n    await evaluate((inputs) => toxicityClassifier(inputs[\"input\"]), {\n      data: datasetName,\n      evaluators: [correct],\n      experimentPrefix: \"gpt-4o-mini, simple\",  // optional, experiment name prefix\n      maxConcurrency: 4, // optional, add concurrency\n    });\n    ```\n  </CodeGroup>\n</Accordion>\n\n## Related[​](#related \"Direct link to Related\")\n\n* [Run an evaluation asynchronously](/langsmith/evaluation-async)\n* [Run an evaluation via the REST API](/langsmith/run-evals-api-only)\n* [Run an evaluation from the prompt playground](/langsmith/run-evaluation-from-prompt-playground)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-llm-application.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 15321
}