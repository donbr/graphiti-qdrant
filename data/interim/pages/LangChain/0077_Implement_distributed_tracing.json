{
  "title": "Implement distributed tracing",
  "source_url": "https://docs.langchain.com/langsmith/distributed-tracing",
  "content": "Sometimes, you need to trace a request across multiple services.\n\nLangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (`langsmith-trace` and optional `baggage` for metadata/tags).\n\nExample client-server setup:\n\n* Trace starts on client\n* Continues on server\n\n## Distributed tracing in Python\n\n```python  theme={null}\n# client.py\nfrom langsmith.run_helpers import get_current_run_tree, traceable\nimport httpx\n\n@traceable\nasync def my_client_function():\n    headers = {}\n    async with httpx.AsyncClient(base_url=\"...\") as client:\n        if run_tree := get_current_run_tree():\n            # add langsmith-id to headers\n            headers.update(run_tree.to_headers())\n        return await client.post(\"/my-route\", headers=headers)\n```\n\nThen the server (or other service) can continue the trace by handling the headers appropriately. If you are using an asgi app Starlette or FastAPI, you can connect the distributed trace using LangSmith's `TracingMiddleware`.\n\n<Info>\n  The `TracingMiddleware` class was added in `langsmith==0.1.133`.\n</Info>\n\nExample using FastAPI:\n\n```python  theme={null}\nfrom langsmith import traceable\nfrom langsmith.middleware import TracingMiddleware\nfrom fastapi import FastAPI, Request\n\napp = FastAPI()  # Or Flask, Django, or any other framework\napp.add_middleware(TracingMiddleware)\n\n@traceable\nasync def some_function():\n    ...\n\n@app.post(\"/my-route\")\nasync def fake_route(request: Request):\n    return await some_function()\n```\n\nOr in Starlette:\n\n```python  theme={null}\nfrom starlette.applications import Starlette\nfrom starlette.middleware import Middleware\nfrom langsmith.middleware import TracingMiddleware\n\nroutes = ...\nmiddleware = [\n    Middleware(TracingMiddleware),\n]\napp = Starlette(..., middleware=middleware)\n```\n\nIf you are using other server frameworks, you can always \"receive\" the distributed trace by passing the headers in through `langsmith_extra`:\n\n```python  theme={null}\n# server.py\nimport langsmith as ls\nfrom fastapi import FastAPI, Request\n\n@ls.traceable\nasync def my_application():\n    ...\n\napp = FastAPI()  # Or Flask, Django, or any other framework\n\n@app.post(\"/my-route\")\nasync def fake_route(request: Request):\n    # request.headers:  {\"langsmith-trace\": \"...\"}\n    # as well as optional metadata/tags in `baggage`\n    with ls.tracing_context(parent=request.headers):\n        return await my_application()\n```\n\nThe example above uses the `tracing_context` context manager. You can also directly specify the parent run context in the `langsmith_extra` parameter of a method wrapped with `@traceable`.\n\n```python  theme={null}\n# ... same as above\n\n@app.post(\"/my-route\")\nasync def fake_route(request: Request):\n    # request.headers:  {\"langsmith-trace\": \"...\"}\n    my_application(langsmith_extra={\"parent\": request.headers})\n```\n\n## Distributed tracing in TypeScript\n\n<Note>\n  Distributed tracing in TypeScript requires `langsmith` version `>=0.1.31`\n</Note>\n\nFirst, we obtain the current run tree from the client and convert it to `langsmith-trace` and `baggage` header values, which we can pass to the server:\n\n```typescript  theme={null}\n// client.mts\nimport { getCurrentRunTree, traceable } from \"langsmith/traceable\";\n\nconst client = traceable(\n    async () => {\n        const runTree = getCurrentRunTree();\n        return await fetch(\"...\", {\n            method: \"POST\",\n            headers: runTree.toHeaders(),\n        }).then((a) => a.text());\n    },\n    { name: \"client\" }\n);\n\nawait client();\n```\n\nThen, the server converts the headers back to a run tree, which it uses to further continue the tracing.\n\nTo pass the newly created run tree to a traceable function, we can use the `withRunTree` helper, which will ensure the run tree is propagated within traceable invocations.\n\n<CodeGroup>\n  ```typescript Express.JS theme={null}\n  // server.mts\n  import { RunTree } from \"langsmith\";\n  import { traceable, withRunTree } from \"langsmith/traceable\";\n  import express from \"express\";\n  import bodyParser from \"body-parser\";\n\n      const server = traceable(\n          (text: string) => `Hello from the server! Received \"${text}\"`,\n          { name: \"server\" }\n      );\n\n      const app = express();\n      app.use(bodyParser.text());\n\n  app.post(\"/\", async (req, res) => {\n      const runTree = RunTree.fromHeaders(req.headers);\n      const result = await withRunTree(runTree, () => server(req.body));\n      res.send(result);\n  });\n  ```\n\n  ```typescript Hono theme={null}\n  // server.mts\n  import { RunTree } from \"langsmith\";\n  import { traceable, withRunTree } from \"langsmith/traceable\";\n  import { Hono } from \"hono\";\n\n      const server = traceable(\n          (text: string) => `Hello from the server! Received \"${text}\"`,\n          { name: \"server\" }\n      );\n\n      const app = new Hono();\n\n  app.post(\"/\", async (c) => {\n      const body = await c.req.text();\n      const runTree = RunTree.fromHeaders(c.req.raw.headers);\n      const result = await withRunTree(runTree, () => server(body));\n      return c.body(result);\n  });\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/distributed-tracing.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 5462
}