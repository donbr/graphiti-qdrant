{
  "title": "Build a SQL agent",
  "source_url": "https://docs.langchain.com/oss/javascript/langchain/sql-agent",
  "content": "## Overview\n\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain [agents](/oss/javascript/langchain/agents).\n\nAt a high level, the agent will:\n\n<Steps>\n  <Step title=\"Fetch the available tables and schemas from the database\" />\n\n  <Step title=\"Decide which tables are relevant to the question\" />\n\n  <Step title=\"Fetch the schemas for the relevant tables\" />\n\n  <Step title=\"Generate a query based on the question and information from the schemas\" />\n\n  <Step title=\"Double-check the query for common mistakes using an LLM\" />\n\n  <Step title=\"Execute the query and return the results\" />\n\n  <Step title=\"Correct mistakes surfaced by the database engine until the query is successful\" />\n\n  <Step title=\"Formulate a response based on the results\" />\n</Steps>\n\n<Warning>\n  Building Q\\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\n</Warning>\n\n### Concepts\n\nWe will cover the following concepts:\n\n* [Tools](/oss/javascript/langchain/tools) for reading from SQL databases\n* LangChain [agents](/oss/javascript/langchain/agents)\n* [Human-in-the-loop](/oss/javascript/langchain/human-in-the-loop) processes\n\n## Setup\n\n### Installation\n\n<CodeGroup>\n  ```bash npm theme={null}\n  npm i langchain @langchain/core typeorm sqlite3 zod\n  ```\n\n  ```bash yarn theme={null}\n  yarn add langchain @langchain/core typeorm sqlite3 zod\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add langchain @langchain/core typeorm sqlite3 zod\n  ```\n</CodeGroup>\n\n### LangSmith\n\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:\n\n```shell  theme={null}\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n## 1. Select an LLM\n\nSelect a model that supports [tool-calling](/oss/javascript/integrations/providers/overview):\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    ðŸ‘‰ Read the [OpenAI chat model integration docs](/oss/javascript/integrations/chat/openai/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/openai\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/openai\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/openai\n      ```\n\n      ```bash bun theme={null} theme={null}\n      bun add @langchain/openai\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      process.env.OPENAI_API_KEY = \"your-api-key\";\n\n      const model = await initChatModel(\"gpt-4.1\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { ChatOpenAI } from \"@langchain/openai\";\n\n      const model = new ChatOpenAI({\n        model: \"gpt-4.1\",\n        apiKey: \"your-api-key\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    ðŸ‘‰ Read the [Anthropic chat model integration docs](/oss/javascript/integrations/chat/anthropic/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/anthropic\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/anthropic\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/anthropic\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm add @langchain/anthropic\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      process.env.ANTHROPIC_API_KEY = \"your-api-key\";\n\n      const model = await initChatModel(\"claude-sonnet-4-5-20250929\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { ChatAnthropic } from \"@langchain/anthropic\";\n\n      const model = new ChatAnthropic({\n        model: \"claude-sonnet-4-5-20250929\",\n        apiKey: \"your-api-key\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    ðŸ‘‰ Read the [Azure chat model integration docs](/oss/javascript/integrations/chat/azure/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/azure\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/azure\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/azure\n      ```\n\n      ```bash bun theme={null} theme={null}\n      bun add @langchain/azure\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      process.env.AZURE_OPENAI_API_KEY = \"your-api-key\";\n      process.env.AZURE_OPENAI_ENDPOINT = \"your-endpoint\";\n      process.env.OPENAI_API_VERSION = \"your-api-version\";\n\n      const model = await initChatModel(\"azure_openai:gpt-4.1\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { AzureChatOpenAI } from \"@langchain/openai\";\n\n      const model = new AzureChatOpenAI({\n        model: \"gpt-4.1\",\n        azureOpenAIApiKey: \"your-api-key\",\n        azureOpenAIApiEndpoint: \"your-endpoint\",\n        azureOpenAIApiVersion: \"your-api-version\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    ðŸ‘‰ Read the [Google GenAI chat model integration docs](/oss/javascript/integrations/chat/google_generative_ai/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/google-genai\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/google-genai\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/google-genai\n      ```\n\n      ```bash bun theme={null} theme={null}\n      bun add @langchain/google-genai\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      process.env.GOOGLE_API_KEY = \"your-api-key\";\n\n      const model = await initChatModel(\"google-genai:gemini-2.5-flash-lite\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\n      const model = new ChatGoogleGenerativeAI({\n        model: \"gemini-2.5-flash-lite\",\n        apiKey: \"your-api-key\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Bedrock Converse\">\n    ðŸ‘‰ Read the [AWS Bedrock chat model integration docs](/oss/javascript/integrations/chat/bedrock_converse/)\n\n    <CodeGroup>\n      ```bash npm theme={null} theme={null}\n      npm install @langchain/aws\n      ```\n\n      ```bash pnpm theme={null} theme={null}\n      pnpm install @langchain/aws\n      ```\n\n      ```bash yarn theme={null} theme={null}\n      yarn add @langchain/aws\n      ```\n\n      ```bash bun theme={null} theme={null}\n      bun add @langchain/aws\n      ```\n    </CodeGroup>\n\n    <CodeGroup>\n      ```typescript initChatModel theme={null} theme={null}\n      import { initChatModel } from \"langchain\";\n\n      // Follow the steps here to configure your credentials:\n      // https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      const model = await initChatModel(\"bedrock:gpt-4.1\");\n      ```\n\n      ```typescript Model Class theme={null} theme={null}\n      import { ChatBedrockConverse } from \"@langchain/aws\";\n\n      // Follow the steps here to configure your credentials:\n      // https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      const model = new ChatBedrockConverse({\n        model: \"gpt-4.1\",\n        region: \"us-east-2\"\n      });\n      ```\n    </CodeGroup>\n  </Tab>\n</Tabs>\n\nThe output shown in the examples below used OpenAI.\n\n## 2. Configure the database\n\nYou will be creating a [SQLite database](https://www.sqlitetutorial.net/sqlite-sample-database/) for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store.\n\nFor convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket.\n\n```typescript  theme={null}\nimport fs from \"node:fs/promises\";\nimport path from \"node:path\";\n\nconst url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\";\nconst localPath = path.resolve(\"Chinook.db\");\n\nasync function resolveDbPath() {\n  if (await fs.exists(localPath)) {\n    return localPath;\n  }\n  const resp = await fetch(url);\n  if (!resp.ok) throw new Error(`Failed to download DB. Status code: ${resp.status}`);\n  const buf = Buffer.from(await resp.arrayBuffer());\n  await fs.writeFile(localPath, buf);\n  return localPath;\n}\n```\n\n## 3. Add tools for database interactions\n\nUse the `SqlDatabase` wrapper available in the `langchain/sql_db` to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```typescript  theme={null}\nimport { SqlDatabase } from \"@langchain/classic/sql_db\";\nimport { DataSource } from \"typeorm\";\n\nlet db: SqlDatabase | undefined;\nasync function getDb() {\n  if (!db) {\n    const dbPath = await resolveDbFile();\n    const datasource = new DataSource({ type: \"sqlite\", database: dbPath });\n    db = await SqlDatabase.fromDataSourceParams({ appDataSource: datasource });\n  }\n  return db;\n}\n\nasync function getSchema() {\n  const db = await getDb();\n  return await db.getTableInfo();\n}\n```\n\n## 6. Implement human-in-the-loop review\n\nIt can be prudent to check the agent's SQL queries before they are executed for any unintended actions or inefficiencies.\n\nLangChain agents feature support for built-in [human-in-the-loop middleware](/oss/javascript/langchain/human-in-the-loop) to add oversight to agent tool calls. Let's configure the agent to pause for human review on calling the `sql_db_query` tool:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver # [!code highlight]\n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n    middleware=[ # [!code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\"sql_db_query\": True}, # [!code highlight]\n            description_prefix=\"Tool execution pending approval\", # [!code highlight]\n        ), # [!code highlight]\n    ], # [!code highlight]\n    checkpointer=InMemorySaver(), # [!code highlight]\n)\n```\n\n<Note>\n  We've added a [checkpointer](/oss/javascript/langchain/short-term-memory) to our agent to allow execution to be paused and resumed. See the [human-in-the-loop guide](/oss/javascript/langchain/human-in-the-loop) for detalis on this as well as available middleware configurations.\n</Note>\n\nOn running the agent, it will now pause for review before executing the `sql_db_query` tool:\n\n```python  theme={null}\nquestion = \"Which genre on average has the longest tracks?\"\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} # [!code highlight]\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    config, # [!code highlight]\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step: # [!code highlight]\n        print(\"INTERRUPTED:\") # [!code highlight]\n        interrupt = step[\"__interrupt__\"][0] # [!code highlight]\n        for request in interrupt.value[\"action_requests\"]: # [!code highlight]\n            print(request[\"description\"]) # [!code highlight]\n    else:\n        pass\n```\n\n```\n...\n\nINTERRUPTED:\nTool execution pending approval\n\nTool: sql_db_query\nArgs: {'query': 'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;'}\n```\n\nWe can resume execution, in this case accepting the query, using [Command](/oss/javascript/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command):\n\n```python  theme={null}\nfrom langgraph.types import Command # [!code highlight]\n\nfor step in agent.stream(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), # [!code highlight]\n    config,\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step:\n        print(\"INTERRUPTED:\")\n        interrupt = step[\"__interrupt__\"][0]\n        for request in interrupt.value[\"action_requests\"]:\n            print(request[\"description\"])\n    else:\n        pass\n```\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)\n Call ID: call_7oz86Epg7lYRqi9rQHbZPS1U\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average duration of about 2,911,783 milliseconds, followed by \"Science Fiction\" and \"Drama.\"\n```\n\nRefer to the [human-in-the-loop guide](/oss/javascript/langchain/human-in-the-loop) for details.\n\n## 4. Execute SQL queries\n\nBefore running the command, do a check to check the LLM generated command in ` _safe_sql`:\n\n```typescript  theme={null}\n\nconst DENY_RE = /\\b(INSERT|UPDATE|DELETE|ALTER|DROP|CREATE|REPLACE|TRUNCATE)\\b/i;\nconst HAS_LIMIT_TAIL_RE = /\\blimit\\b\\s+\\d+(\\s*,\\s*\\d+)?\\s*;?\\s*$/i;\n\nfunction sanitizeSqlQuery(q) {\n  let query = String(q ?? \"\").trim();\n\n  // block multiple statements (allow one optional trailing ;)\n  const semis = [...query].filter((c) => c === \";\").length;\n  if (semis > 1 || (query.endsWith(\";\") && query.slice(0, -1).includes(\";\"))) {\n    throw new Error(\"multiple statements are not allowed.\")\n  }\n  query = query.replace(/;+\\s*$/g, \"\").trim();\n\n  // read-only gate\n  if (!query.toLowerCase().startsWith(\"select\")) {\n    throw new Error(\"Only SELECT statements are allowed\")\n  }\n  if (DENY_RE.test(query)) {\n    throw new Error(\"DML/DDL detected. Only read-only queries are permitted.\")\n  }\n\n  // append LIMIT only if not already present\n  if (!HAS_LIMIT_TAIL_RE.test(query)) {\n    query += \" LIMIT 5\";\n  }\n  return query;\n}\n\n```\n\nThen, use `run` from `SQLDatabase` to execute commands with an `execute_sql` tool:\n\n```typescript  theme={null}\nimport { tool } from \"langchain\"\nimport * as z from \"zod\";\n\nconst executeSql = tool(\n  async ({ query }) => {\n    const q = sanitizeSqlQuery(query);\n    try {\n      const result = await db.run(q);\n      return typeof result === \"string\" ? result : JSON.stringify(result, null, 2);\n    } catch (e) {\n      throw new Error(e?.message ?? String(e))\n    }\n  },\n  {\n    name: \"execute_sql\",\n    description: \"Execute a READ-ONLY SQLite SELECT query and return results.\",\n    schema: z.object({\n      query: z.string().describe(\"SQLite SELECT query to execute (read-only).\"),\n    }),\n  }\n);\n\n```\n\n## 5. Use `createAgent`\n\nUse `createAgent` to build a [ReAct agent](https://arxiv.org/pdf/2210.03629) with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\n\nInitialize the agent with a descriptive system prompt to customize its behavior:\n\n```typescript  theme={null}\nimport { SystemMessage } from \"langchain\";\n\nconst getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.\n\nAuthoritative schema (do not invent columns/tables):\n${await getSchema()}\n\nRules:\n- Think step-by-step.\n- When you need data, call the tool \\`execute_sql\\` with ONE SELECT query.\n- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.\n- Limit to 5 rows unless user explicitly asks otherwise.\n- If the tool returns 'Error:', revise the SQL and try again.\n- Limit the number of attempts to 5.\n- If you are not successful after 5 attempts, return a note to the user.\n- Prefer explicit column lists; avoid SELECT *.\n`);\n```\n\nNow, create an agent with the model, tools, and prompt:\n\n```typescript  theme={null}\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: [executeSql],\n  systemPrompt: getSystemPrompt,\n});\n\n```\n\n## 6. Run the agent\n\nRun the agent on a sample query and observe its behavior:\n\n```typescript  theme={null}\nconst question = \"Which genre, on average, has the longest tracks?\";\nconst stream = await agent.stream(\n  { messages: [{ role: \"user\", content: question }] },\n  { streamMode: \"values\" }\n);\nfor await (const step of stream) {\n  const message = step.messages.at(-1);\n  console.log(`${message.role}: ${JSON.stringify(message.content, null, 2)}`);\n}\n```\n\n```\nhuman: Which genre, on average, has the longest tracks?\nai:\ntool: [{\"Genre\":\"Sci Fi & Fantasy\",\"AvgMilliseconds\":2911783.0384615385}]\nai: Sci Fi & Fantasy â€” average track length â‰ˆ 48.5 minutes (about 2,911,783 ms).\n```\n\nThe agent correctly wrote a query, checked the query, and ran it to inform its final response.\n\n<Note>\n  You can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the [LangSmith trace](https://smith.langchain.com/public/653d218b-af67-4854-95ca-6abecb9b2520/r).\n</Note>\n\n#### (Optional) Use Studio\n\n[Studio](/langsmith/studio) provides a \"client side\" loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \"Tell me the scheme of the database\" or \"Show me the invoices for the 5 top customers\". You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\n\n<Accordion title=\"Run your agent in Studio\">\n  In addition to the previously mentioned packages, you will need to:\n\n  ```shell  theme={null}\n  npm i -g langgraph-cli@latest\n  ```\n\n  In directory you will run in, you will need a `langgraph.json` file with the following contents:\n\n  ```json  theme={null}\n  {\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"agent\": \"./sqlAgent.ts:agent\",\n        \"graph\": \"./sqlAgentLanggraph.ts:graph\"\n    },\n    \"env\": \".env\"\n  }\n  ```\n\n  ```typescript  theme={null}\n  import fs from \"node:fs/promises\";\n  import path from \"node:path\";\n  import { SqlDatabase } from \"@langchain/classic/sql_db\";\n  import { DataSource } from \"typeorm\";\n  import { SystemMessage, createAgent, tool } from \"langchain\"\n  import * as z from \"zod\";\n\n  const url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\";\n  const localPath = path.resolve(\"Chinook.db\");\n\n  async function resolveDbPath() {\n    if (await fs.exists(localPath)) {\n      return localPath;\n    }\n    const resp = await fetch(url);\n    if (!resp.ok) throw new Error(`Failed to download DB. Status code: ${resp.status}`);\n    const buf = Buffer.from(await resp.arrayBuffer());\n    await fs.writeFile(localPath, buf);\n    return localPath;\n  }\n\n  let db: SqlDatabase | undefined;\n  async function getDb() {\n    if (!db) {\n      const dbPath = await resolveDbPath();\n      const datasource = new DataSource({ type: \"sqlite\", database: dbPath });\n      db = await SqlDatabase.fromDataSourceParams({ appDataSource: datasource });\n    }\n    return db;\n  }\n\n  async function getSchema() {\n    const db = await getDb();\n    return await db.getTableInfo();\n  }\n\n  const DENY_RE = /\\b(INSERT|UPDATE|DELETE|ALTER|DROP|CREATE|REPLACE|TRUNCATE)\\b/i;\n  const HAS_LIMIT_TAIL_RE = /\\blimit\\b\\s+\\d+(\\s*,\\s*\\d+)?\\s*;?\\s*$/i;\n\n  function sanitizeSqlQuery(q) {\n    let query = String(q ?? \"\").trim();\n\n    // block multiple statements (allow one optional trailing ;)\n    const semis = [...query].filter((c) => c === \";\").length;\n    if (semis > 1 || (query.endsWith(\";\") && query.slice(0, -1).includes(\";\"))) {\n      throw new Error(\"multiple statements are not allowed.\")\n    }\n    query = query.replace(/;+\\s*$/g, \"\").trim();\n\n    // read-only gate\n    if (!query.toLowerCase().startsWith(\"select\")) {\n      throw new Error(\"Only SELECT statements are allowed\")\n    }\n    if (DENY_RE.test(query)) {\n      throw new Error(\"DML/DDL detected. Only read-only queries are permitted.\")\n    }\n\n    // append LIMIT only if not already present\n    if (!HAS_LIMIT_TAIL_RE.test(query)) {\n      query += \" LIMIT 5\";\n    }\n    return query;\n  }\n\n  const executeSql = tool(\n    async ({ query }) => {\n      const q = sanitizeSqlQuery(query);\n      try {\n        const result = await db.run(q);\n        return typeof result === \"string\" ? result : JSON.stringify(result, null, 2);\n      } catch (e) {\n        throw new Error(e?.message ?? String(e))\n      }\n    },\n    {\n      name: \"execute_sql\",\n      description: \"Execute a READ-ONLY SQLite SELECT query and return results.\",\n      schema: z.object({\n        query: z.string().describe(\"SQLite SELECT query to execute (read-only).\"),\n      }),\n    }\n  );\n\n  const getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.\n\n  Authoritative schema (do not invent columns/tables):\n  ${await getSchema()}\n\n  Rules:\n  - Think step-by-step.\n  - When you need data, call the tool \\`execute_sql\\` with ONE SELECT query.\n  - Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.\n  - Limit to 5 rows unless user explicitly asks otherwise.\n  - If the tool returns 'Error:', revise the SQL and try again.\n  - Limit the number of attempts to 5.\n  - If you are not successful after 5 attempts, return a note to the user.\n  - Prefer explicit column lists; avoid SELECT *.\n  `);\n\n  export const agent = createAgent({\n    model: \"gpt-5\",\n    tools: [executeSql],\n    systemPrompt: getSystemPrompt,\n  });\n  ```\n</Accordion>\n\n## Next steps\n\nFor deeper customization, check out [this tutorial](/oss/javascript/langgraph/sql-agent) for implementing a SQL agent directly using LangGraph primitives.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/sql-agent.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 23186
}