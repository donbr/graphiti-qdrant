{
  "title": "Trace with LangGraph",
  "source_url": "https://docs.langchain.com/langsmith/trace-with-langgraph",
  "content": "LangSmith smoothly integrates with LangGraph (Python and JS) to help you trace agents, whether you're using LangChain modules or other SDKs.\n\n## With LangChain\n\nIf you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.\n\nThis guide will walk through a basic example. For more detailed information on configuration, see the [Trace With LangChain](/langsmith/trace-with-langchain) guide.\n\n### 1. Installation\n\nInstall the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).\n\nFor a full list of packages available, see the [LangChain Python docs](https://python.langchain.com/docs/integrations/platforms/) and [LangChain JS docs](https://js.langchain.com/docs/integrations/platforms/).\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain_openai langgraph\n  ```\n\n  ```bash yarn theme={null}\n  yarn add @langchain/openai @langchain/langgraph\n  ```\n\n  ```bash npm theme={null}\n  npm install @langchain/openai @langchain/langgraph\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add @langchain/openai @langchain/langgraph\n  ```\n</CodeGroup>\n\n### 2. Configure your environment\n\n```bash wrap theme={null}\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n# This example uses OpenAI, but you can use any LLM provider of choice\nexport OPENAI_API_KEY=<your-openai-api-key>\n# For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.\nexport LANGSMITH_WORKSPACE_ID=<your-workspace-id>\n```\n\n<Info>\n  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`\n\n  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`\n\n  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.\n</Info>\n\n### 3. Log a trace\n\nOnce you've set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from typing import Literal\n  from langchain.messages import HumanMessage\n  from langchain_openai import ChatOpenAI\n  from langchain.tools import tool\n  from langgraph.prebuilt import ToolNode\n  from langgraph.graph import StateGraph, MessagesState\n\n  @tool\n  def search(query: str):\n      \"\"\"Call to surf the web.\"\"\"\n      if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n          return \"It's 60 degrees and foggy.\"\n      return \"It's 90 degrees and sunny.\"\n\n  tools = [search]\n  tool_node = ToolNode(tools)\n\n  model = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\n  def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n      messages = state['messages']\n      last_message = messages[-1]\n      if last_message.tool_calls:\n          return \"tools\"\n      return \"__end__\"\n\n  def call_model(state: MessagesState):\n      messages = state['messages']\n      # Invoking `model` will automatically infer the correct tracing context\n      response = model.invoke(messages)\n      return {\"messages\": [response]}\n\n  workflow = StateGraph(MessagesState)\n  workflow.add_node(\"agent\", call_model)\n  workflow.add_node(\"tools\", tool_node)\n  workflow.add_edge(\"__start__\", \"agent\")\n  workflow.add_conditional_edges(\n      \"agent\",\n      should_continue,\n  )\n  workflow.add_edge(\"tools\", 'agent')\n\n  app = workflow.compile()\n\n  final_state = app.invoke(\n      {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n      config={\"configurable\": {\"thread_id\": 42}}\n  )\n\n  final_state[\"messages\"][-1].content\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n  import { tool } from \"@langchain/core/tools\";\n  import { z } from \"zod\";\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { StateGraph, StateGraphArgs } from \"@langchain/langgraph\";\n  import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n\n  interface AgentState {\n    messages: HumanMessage[];\n  }\n\n  const graphState: StateGraphArgs<AgentState>[\"channels\"] = {\n    messages: {\n      reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),\n    },\n  };\n\n  const searchTool = tool(async ({ query }: { query: string }) => {\n    if (query.toLowerCase().includes(\"sf\") || query.toLowerCase().includes(\"san francisco\")) {\n      return \"It's 60 degrees and foggy.\"\n    }\n    return \"It's 90 degrees and sunny.\"\n  }, {\n    name: \"search\",\n    description:\n      \"Call to surf the web.\",\n    schema: z.object({\n      query: z.string().describe(\"The query to use in your search.\"),\n    }),\n  });\n\n  const tools = [searchTool];\n  const toolNode = new ToolNode<AgentState>(tools);\n\n  const model = new ChatOpenAI({\n    model: \"gpt-4o\",\n    temperature: 0,\n  }).bindTools(tools);\n\n  function shouldContinue(state: AgentState) {\n    const messages = state.messages;\n    const lastMessage = messages[messages.length - 1] as AIMessage;\n    if (lastMessage.tool_calls?.length) {\n      return \"tools\";\n    }\n    return \"__end__\";\n  }\n\n  async function callModel(state: AgentState) {\n    const messages = state.messages;\n    // Invoking `model` will automatically infer the correct tracing context\n    const response = await model.invoke(messages);\n    return { messages: [response] };\n  }\n\n  const workflow = new StateGraph<AgentState>({ channels: graphState })\n    .addNode(\"agent\", callModel)\n    .addNode(\"tools\", toolNode)\n    .addEdge(\"__start__\", \"agent\")\n    .addConditionalEdges(\"agent\", shouldContinue)\n    .addEdge(\"tools\", \"agent\");\n\n  const app = workflow.compile();\n\n  const finalState = await app.invoke(\n    { messages: [new HumanMessage(\"what is the weather in sf\")] },\n    { configurable: { thread_id: \"42\" } }\n  );\n\n  finalState.messages[finalState.messages.length - 1].content;\n  ```\n</CodeGroup>\n\nAn example trace from running the above code [looks like this](https://smith.langchain.com/public/10863294-ee79-484a-927f-0558230f1547/r):\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a589f14351fb48e721205d1e363753ea\" alt=\"Trace tree for a LangGraph run with LangChain\" data-og-width=\"3314\" width=\"3314\" data-og-height=\"1766\" height=\"1766\" data-path=\"langsmith/images/langgraph-with-langchain-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=01650d2547ba6e440a66ceb0bdeb566a 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c0d7d3f04d58edd25e9b99d2a61890ce 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf074976fbc12b0baad0e8320fd7fa47 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=627f66d7b9e2b8a054e66e8dbc7afa73 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7946baa3ad54fa851cd8a34eb31b47b0 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6b8e6a216b312f5ae3c74503fabba63 2500w\" />\n\n## Without LangChain\n\nIf you are using other SDKs or custom functions within LangGraph, you will need to [wrap or decorate them appropriately](/langsmith/annotate-code#use-traceable--traceable) (with the `@traceable` decorator in Python or the `traceable` function in JS, or something like e.g. `wrap_openai` for SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.\n\nHere's an example. You can also see this page for more information.\n\n### 1. Installation\n\nInstall the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install openai langsmith langgraph\n  ```\n\n  ```bash yarn theme={null}\n  yarn add openai langsmith @langchain/langgraph\n  ```\n\n  ```bash npm theme={null}\n  npm install openai langsmith @langchain/langgraph\n  ```\n\n  ```bash pnpm theme={null}\n  pnpm add openai langsmith @langchain/langgraph\n  ```\n</CodeGroup>\n\n### 2. Configure your environment\n\n```bash wrap theme={null}\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n# This example uses OpenAI, but you can use any LLM provider of choice\nexport OPENAI_API_KEY=<your-openai-api-key>\n```\n\n<Info>\n  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`\n\n  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`\n\n  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.\n</Info>\n\n### 3. Log a trace\n\nOnce you've set up your environment, [wrap or decorate the custom functions/SDKs](/langsmith/annotate-code#use-traceable--traceable) you want to trace. LangSmith will then infer the proper tracing config:\n\n<CodeGroup>\n  ```python Python theme={null}\n  import json\n  import openai\n  import operator\n  from langsmith import traceable\n  from langsmith.wrappers import wrap_openai\n  from typing import Annotated, Literal, TypedDict\n  from langgraph.graph import StateGraph\n\n  class State(TypedDict):\n      messages: Annotated[list, operator.add]\n\n  tool_schema = {\n      \"type\": \"function\",\n      \"function\": {\n          \"name\": \"search\",\n          \"description\": \"Call to surf the web.\",\n          \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\"query\": {\"type\": \"string\"}},\n              \"required\": [\"query\"],\n          },\n      },\n  }\n\n  # Decorating the tool function will automatically trace it with the correct context\n  @traceable(run_type=\"tool\", name=\"Search Tool\")\n  def search(query: str):\n      \"\"\"Call to surf the web.\"\"\"\n      if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n          return \"It's 60 degrees and foggy.\"\n      return \"It's 90 degrees and sunny.\"\n\n  tools = [search]\n\n  def call_tools(state):\n      function_name_to_function = {\"search\": search}\n      messages = state[\"messages\"]\n      tool_call = messages[-1][\"tool_calls\"][0]\n      function_name = tool_call[\"function\"][\"name\"]\n      function_arguments = tool_call[\"function\"][\"arguments\"]\n      arguments = json.loads(function_arguments)\n      function_response = function_name_to_function[function_name](**arguments)\n      tool_message = {\n          \"tool_call_id\": tool_call[\"id\"],\n          \"role\": \"tool\",\n          \"name\": function_name,\n          \"content\": function_response,\n      }\n      return {\"messages\": [tool_message]}\n\n  wrapped_client = wrap_openai(openai.Client())\n\n  def should_continue(state: State) -> Literal[\"tools\", \"__end__\"]:\n      messages = state[\"messages\"]\n      last_message = messages[-1]\n      if last_message[\"tool_calls\"]:\n          return \"tools\"\n      return \"__end__\"\n\n  def call_model(state: State):\n      messages = state[\"messages\"]\n      # Calling the wrapped client will automatically infer the correct tracing context\n      response = wrapped_client.chat.completions.create(\n          messages=messages, model=\"gpt-4o-mini\", tools=[tool_schema]\n      )\n      raw_tool_calls = response.choices[0].message.tool_calls\n      tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []\n      response_message = {\n          \"role\": \"assistant\",\n          \"content\": response.choices[0].message.content,\n          \"tool_calls\": tool_calls,\n      }\n      return {\"messages\": [response_message]}\n\n  workflow = StateGraph(State)\n  workflow.add_node(\"agent\", call_model)\n  workflow.add_node(\"tools\", call_tools)\n  workflow.add_edge(\"__start__\", \"agent\")\n  workflow.add_conditional_edges(\n      \"agent\",\n      should_continue,\n  )\n  workflow.add_edge(\"tools\", 'agent')\n\n  app = workflow.compile()\n\n  final_state = app.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n  )\n\n  final_state[\"messages\"][-1][\"content\"]\n  ```\n\n  ```typescript TypeScript theme={null}\n  **Note:** The below example requires `langsmith>=0.1.39` and `@langchain/langgraph>=0.0.31`\n\n  import OpenAI from \"openai\";\n  import { StateGraph } from \"@langchain/langgraph\";\n  import { wrapOpenAI } from \"langsmith/wrappers/openai\";\n  import { traceable } from \"langsmith/traceable\";\n\n  type GraphState = {\n    messages: OpenAI.ChatCompletionMessageParam[];\n  };\n\n  const wrappedClient = wrapOpenAI(new OpenAI({}));\n\n  const toolSchema: OpenAI.ChatCompletionTool = {\n    type: \"function\",\n    function: {\n      name: \"search\",\n      description: \"Use this tool to query the web.\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          query: {\n            type: \"string\",\n          },\n        },\n        required: [\"query\"],\n      }\n    }\n  };\n\n  // Wrapping the tool function will automatically trace it with the correct context\n  const search = traceable(async ({ query }: { query: string }) => {\n    if (\n      query.toLowerCase().includes(\"sf\") ||\n      query.toLowerCase().includes(\"san francisco\")\n    ) {\n      return \"It's 60 degrees and foggy.\";\n    }\n    return \"It's 90 degrees and sunny.\";\n  }, { run_type: \"tool\", name: \"Search Tool\" });\n\n  const callTools = async ({ messages }: GraphState) => {\n    const mostRecentMessage = messages[messages.length - 1];\n    const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;\n    if (toolCalls === undefined || toolCalls.length === 0) {\n      throw new Error(\"No tool calls passed to node.\");\n    }\n    const toolNameMap = {\n      search,\n    };\n    const functionName = toolCalls[0].function.name;\n    const functionArguments = JSON.parse(toolCalls[0].function.arguments);\n    const response = await toolNameMap[functionName](functionArguments);\n    const toolMessage = {\n      tool_call_id: toolCalls[0].id,\n      role: \"tool\",\n      name: functionName,\n      content: response,\n    }\n    return { messages: [toolMessage] };\n  };\n\n  const callModel = async ({ messages }: GraphState) => {\n    // Calling the wrapped client will automatically infer the correct tracing context\n    const response = await wrappedClient.chat.completions.create({\n      messages,\n      model: \"gpt-4o-mini\",\n      tools: [toolSchema],\n    });\n    const responseMessage = {\n      role: \"assistant\",\n      content: response.choices[0].message.content,\n      tool_calls: response.choices[0].message.tool_calls ?? [],\n    };\n    return { messages: [responseMessage] };\n  };\n\n  const shouldContinue = ({ messages }: GraphState) => {\n    const lastMessage =\n      messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;\n    if (\n      lastMessage?.tool_calls !== undefined &&\n      lastMessage?.tool_calls.length > 0\n    ) {\n      return \"tools\";\n    }\n    return \"__end__\";\n  }\n\n  const workflow = new StateGraph<GraphState>({\n    channels: {\n      messages: {\n        reducer: (a: any, b: any) => a.concat(b),\n      }\n    }\n  });\n\n  const graph = workflow\n    .addNode(\"model\", callModel)\n    .addNode(\"tools\", callTools)\n    .addEdge(\"__start__\", \"model\")\n    .addConditionalEdges(\"model\", shouldContinue, {\n      tools: \"tools\",\n      __end__: \"__end__\",\n    })\n    .addEdge(\"tools\", \"model\")\n    .compile();\n\n  await graph.invoke({\n    messages: [{ role: \"user\", content: \"what is the weather in sf\" }]\n  });\n  ```\n</CodeGroup>\n\nAn example trace from running the above code [looks like this](https://smith.langchain.com/public/353f27da-c221-4b67-b9ec-ede3777f3271/r):\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=abe0ae173d182563c343f6596e0ce4e2\" alt=\"Trace tree for a LangGraph run without LangChain\" data-og-width=\"3296\" width=\"3296\" data-og-height=\"1774\" height=\"1774\" data-path=\"langsmith/images/langgraph-without-langchain-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=794e9ce04677bbf721880ebb07ada7c6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6208cbec91ba187ba8f75f6cc916b3f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=418509190558d6a87363d3ba146b7722 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a3c3e2e11cbdac8c32e80e7a895b1eb0 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ea893862eb3f9bc5910649cf0ccd2abe 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2b244ca0ec1296552991428d1efffde6 2500w\" />\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langgraph.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 18452
}