{
  "title": "How to evaluate a runnable",
  "source_url": "https://docs.langchain.com/langsmith/langchain-runnable",
  "content": "<Info>\n  * `langchain`: [Python](https://python.langchain.com) and [JS/TS](https://js.langchain.com)\n  * Runnable: [Python](https://python.langchain.com/docs/concepts/runnables/) and [JS/TS](https://js.langchain.com/docs/concepts/runnables/)\n</Info>\n\n`langchain` [Runnable](https://python.langchain.com/docs/concepts/runnables/) objects (such as chat models, retrievers, chains, etc.) can be passed directly into `evaluate()` / `aevaluate()`.\n\n## Setup\n\nLet's define a simple chain to evaluate. First, install all the required packages:\n\n<CodeGroup>\n  ```bash Python theme={null}\n  pip install -U langsmith langchain[openai]\n  ```\n\n  ```bash TypeScript theme={null}\n  yarn add langsmith @langchain/openai\n  ```\n</CodeGroup>\n\nNow define a chain:\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langchain.chat_models import init_chat_model\n  from langchain_core.prompts import ChatPromptTemplate\n  from langchain_core.output_parsers import StrOutputParser\n\n  instructions = (\n      \"Please review the user query below and determine if it contains any form \"\n      \"of toxic behavior, such as insults, threats, or highly negative comments. \"\n      \"Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"\n  )\n\n  prompt = ChatPromptTemplate(\n      [(\"system\", instructions), (\"user\", \"{text}\")],\n  )\n\n  model = init_chat_model(\"gpt-4o\")\n  chain = prompt | model | StrOutputParser()\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n  import { StringOutputParser } from \"@langchain/core/output_parsers\";\n\n  const prompt = ChatPromptTemplate.fromMessages([\n    [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],\n    [\"user\", \"{text}\"]\n  ]);\n\n  const chatModel = new ChatOpenAI();\n  const outputParser = new StringOutputParser();\n  const chain = prompt.pipe(chatModel).pipe(outputParser);\n  ```\n</CodeGroup>\n\n## Evaluate\n\nTo evaluate our chain we can pass it directly to the `evaluate()` / `aevaluate()` method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form `{\"text\": \"...\"}`.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import aevaluate, Client\n\n  client = Client()\n\n  # Clone a dataset of texts with toxicity labels.\n  # Each example input has a \"text\" key and each output has a \"label\" key.\n  dataset = client.clone_public_dataset(\n      \"https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d\"\n  )\n\n  def correct(outputs: dict, reference_outputs: dict) -> bool:\n      # Since our chain outputs a string not a dict, this string\n      # gets stored under the default \"output\" key in the outputs dict:\n      actual = outputs[\"output\"]\n      expected = reference_outputs[\"label\"]\n      return actual == expected\n\n  results = await aevaluate(\n      chain,\n      data=dataset,\n      evaluators=[correct],\n      experiment_prefix=\"gpt-4o, baseline\",\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { evaluate } from \"langsmith/evaluation\";\n  import { Client } from \"langsmith\";\n\n  const langsmith = new Client();\n\n  const dataset = await client.clonePublicDataset(\n    \"https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d\"\n  )\n\n  await evaluate(chain, {\n    data: dataset.name,\n    evaluators: [correct],\n    experimentPrefix: \"gpt-4o, baseline\",\n  });\n  ```\n</CodeGroup>\n\nThe runnable is traced appropriately for each output.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b9dac41dafb9a1cbb3b90fc508f212f7\" alt=\"Runnable Evaluation\" data-og-width=\"2288\" width=\"2288\" data-og-height=\"1052\" height=\"1052\" data-path=\"langsmith/images/runnable-eval.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=39f7bda57df5d29c72729390065342c2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2bbfa58f877541adff85056d2d4910c7 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=198967ebb494d0577fac294f879f348c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=dd0758a55517d6899d445bd203bc7d03 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6fa8f6a044a0b978ef727390f18f5ce3 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=40dad8febfdaf0756c90b6326e2c4415 2500w\" />\n\n## Related\n\n* [How to evaluate a `langgraph` graph](/langsmith/evaluate-on-intermediate-steps)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langchain-runnable.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 5605
}