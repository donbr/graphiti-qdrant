{
  "title": "Messages",
  "source_url": "https://docs.langchain.com/oss/python/langchain/messages",
  "content": "Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\n\nMessages are objects that contain:\n\n* <Icon icon=\"user\" size={16} /> [**Role**](#message-types) - Identifies the message type (e.g. `system`, `user`)\n* <Icon icon=\"folder-closed\" size={16} /> [**Content**](#message-content) - Represents the actual content of the message (like text, images, audio, documents, etc.)\n* <Icon icon=\"tag\" size={16} /> [**Metadata**](#message-metadata) - Optional fields such as response information, message IDs, and token usage\n\nLangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.\n\n## Basic usage\n\nThe simplest way to use messages is to create message objects and pass them to a model when [invoking](/oss/python/langchain/models#invocation).\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\nfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nsystem_msg = SystemMessage(\"You are a helpful assistant.\")\nhuman_msg = HumanMessage(\"Hello, how are you?\")\n\n# Use with chat models\nmessages = [system_msg, human_msg]\nresponse = model.invoke(messages)  # Returns AIMessage\n```\n\n### Text prompts\n\nText prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history.\n\n```python  theme={null}\nresponse = model.invoke(\"Write a haiku about spring\")\n```\n\n**Use text prompts when:**\n\n* You have a single, standalone request\n* You don't need conversation history\n* You want minimal code complexity\n\n### Message prompts\n\nAlternatively, you can pass in a list of messages to the model by providing a list of message objects.\n\n```python  theme={null}\nfrom langchain.messages import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(\"You are a poetry expert\"),\n    HumanMessage(\"Write a haiku about spring\"),\n    AIMessage(\"Cherry blossoms bloom...\")\n]\nresponse = model.invoke(messages)\n```\n\n**Use message prompts when:**\n\n* Managing multi-turn conversations\n* Working with multimodal content (images, audio, files)\n* Including system instructions\n\n### Dictionary format\n\nYou can also specify messages directly in OpenAI chat completions format.\n\n```python  theme={null}\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a poetry expert\"},\n    {\"role\": \"user\", \"content\": \"Write a haiku about spring\"},\n    {\"role\": \"assistant\", \"content\": \"Cherry blossoms bloom...\"}\n]\nresponse = model.invoke(messages)\n```\n\n## Message types\n\n* <Icon icon=\"gear\" size={16} /> [System message](#system-message) - Tells the model how to behave and provide context for interactions\n* <Icon icon=\"user\" size={16} /> [Human message](#human-message) - Represents user input and interactions with the model\n* <Icon icon=\"robot\" size={16} /> [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata\n* <Icon icon=\"wrench\" size={16} /> [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/python/langchain/models#tool-calling)\n\n### System Message\n\nA [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses.\n\n```python Basic instructions theme={null}\nsystem_msg = SystemMessage(\"You are a helpful coding assistant.\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n```\n\n```python Detailed persona theme={null}\nfrom langchain.messages import SystemMessage, HumanMessage\n\nsystem_msg = SystemMessage(\"\"\"\nYou are a senior Python developer with expertise in web frameworks.\nAlways provide code examples and explain your reasoning.\nBe concise but thorough in your explanations.\n\"\"\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n```\n\n***\n\n### Human Message\n\nA [`HumanMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.HumanMessage) represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal [content](#message-content).\n\n#### Text content\n\n<CodeGroup>\n  ```python Message object theme={null}\n  response = model.invoke([\n    HumanMessage(\"What is machine learning?\")\n  ])\n  ```\n\n  ```python String shortcut theme={null}\n  # Using a string is a shortcut for a single HumanMessage\n  response = model.invoke(\"What is machine learning?\")\n  ```\n</CodeGroup>\n\n#### Message metadata\n\n```python Add metadata theme={null}\nhuman_msg = HumanMessage(\n    content=\"Hello!\",\n    name=\"alice\",  # Optional: identify different users\n    id=\"msg_123\",  # Optional: unique identifier for tracing\n)\n```\n\n<Note>\n  The `name` field behavior varies by provider – some use it for user identification, others ignore it. To check, refer to the model provider's [reference](https://reference.langchain.com/python/integrations/).\n</Note>\n\n***\n\n### AI Message\n\nAn [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\n\n```python  theme={null}\nresponse = model.invoke(\"Explain AI\")\nprint(type(response))  # <class 'langchain.messages.AIMessage'>\n```\n\n[`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects are returned by the model when calling it, which contains all of the associated metadata in the response.\n\nProviders weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) object and insert it into the message history as if it came from the model.\n\n```python  theme={null}\nfrom langchain.messages import AIMessage, SystemMessage, HumanMessage\n\n# Create an AI message manually (e.g., for conversation history)\nai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n\n# Add to conversation history\nmessages = [\n    SystemMessage(\"You are a helpful assistant\"),\n    HumanMessage(\"Can you help me?\"),\n    ai_msg,  # Insert as if it came from the model\n    HumanMessage(\"Great! What's 2+2?\")\n]\n\nresponse = model.invoke(messages)\n```\n\n<Accordion title=\"Attributes\">\n  <ParamField path=\"text\" type=\"string\">\n    The text content of the message.\n  </ParamField>\n\n  <ParamField path=\"content\" type=\"string | dict[]\">\n    The raw content of the message.\n  </ParamField>\n\n  <ParamField path=\"content_blocks\" type=\"ContentBlock[]\">\n    The standardized [content blocks](#message-content) of the message.\n  </ParamField>\n\n  <ParamField path=\"tool_calls\" type=\"dict[] | None\">\n    The tool calls made by the model.\n\n    Empty if no tools are called.\n  </ParamField>\n\n  <ParamField path=\"id\" type=\"string\">\n    A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)\n  </ParamField>\n\n  <ParamField path=\"usage_metadata\" type=\"dict | None\">\n    The usage metadata of the message, which can contain token counts when available.\n  </ParamField>\n\n  <ParamField path=\"response_metadata\" type=\"ResponseMetadata | None\">\n    The response metadata of the message.\n  </ParamField>\n</Accordion>\n\n#### Tool calls\n\nWhen models make [tool calls](/oss/python/langchain/models#tool-calling), they're included in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage):\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    ...\n\nmodel_with_tools = model.bind_tools([get_weather])\nresponse = model_with_tools.invoke(\"What's the weather in Paris?\")\n\nfor tool_call in response.tool_calls:\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n    print(f\"ID: {tool_call['id']}\")\n```\n\nOther structured data, such as reasoning or citations, can also appear in message [content](/oss/python/langchain/messages#message-content).\n\n#### Token usage\n\nAn [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) can hold token counts and other usage metadata in its [`usage_metadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) field:\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nresponse = model.invoke(\"Hello!\")\nresponse.usage_metadata\n```\n\n```\n{'input_tokens': 8,\n 'output_tokens': 304,\n 'total_tokens': 312,\n 'input_token_details': {'audio': 0, 'cache_read': 0},\n 'output_token_details': {'audio': 0, 'reasoning': 256}}\n```\n\nSee [`UsageMetadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) for details.\n\n#### Streaming and chunks\n\nDuring streaming, you'll receive [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects that can be combined into a full message object:\n\n```python  theme={null}\nchunks = []\nfull_message = None\nfor chunk in model.stream(\"Hi\"):\n    chunks.append(chunk)\n    print(chunk.text)\n    full_message = chunk if full_message is None else full_message + chunk\n```\n\n<Note>\n  Learn more:\n\n  * [Streaming tokens from chat models](/oss/python/langchain/models#stream)\n  * [Streaming tokens and/or steps from agents](/oss/python/langchain/streaming)\n</Note>\n\n***\n\n### Tool Message\n\nFor models that support [tool calling](/oss/python/langchain/models#tool-calling), AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.\n\n[Tools](/oss/python/langchain/tools) can generate [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) objects directly. Below, we show a simple example. Read more in the [tools guide](/oss/python/langchain/tools).\n\n```python  theme={null}\nfrom langchain.messages import AIMessage\nfrom langchain.messages import ToolMessage\n\n# After a model makes a tool call\n# (Here, we demonstrate manually creating the messages for brevity)\nai_message = AIMessage(\n    content=[],\n    tool_calls=[{\n        \"name\": \"get_weather\",\n        \"args\": {\"location\": \"San Francisco\"},\n        \"id\": \"call_123\"\n    }]\n)\n\n# Execute tool and create result message\nweather_result = \"Sunny, 72°F\"\ntool_message = ToolMessage(\n    content=weather_result,\n    tool_call_id=\"call_123\"  # Must match the call ID\n)\n\n# Continue conversation\nmessages = [\n    HumanMessage(\"What's the weather in San Francisco?\"),\n    ai_message,  # Model's tool call\n    tool_message,  # Tool execution result\n]\nresponse = model.invoke(messages)  # Model processes the result\n```\n\n<Accordion title=\"Attributes\">\n  <ParamField path=\"content\" type=\"string\" required>\n    The stringified output of the tool call.\n  </ParamField>\n\n  <ParamField path=\"tool_call_id\" type=\"string\" required>\n    The ID of the tool call that this message is responding to. Must match the ID of the tool call in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage).\n  </ParamField>\n\n  <ParamField path=\"name\" type=\"string\" required>\n    The name of the tool that was called.\n  </ParamField>\n\n  <ParamField path=\"artifact\" type=\"dict\">\n    Additional data not sent to the model but can be accessed programmatically.\n  </ParamField>\n</Accordion>\n\n<Note>\n  The `artifact` field stores supplementary data that won't be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.\n\n  <Accordion title=\"Example: Using artifact for retrieval metadata\">\n    For example, a [retrieval](/oss/python/langchain/retrieval) tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:\n\n    ```python  theme={null}\n    from langchain.messages import ToolMessage\n\n    # Sent to model\n    message_content = \"It was the best of times, it was the worst of times.\"\n\n    # Artifact available downstream\n    artifact = {\"document_id\": \"doc_123\", \"page\": 0}\n\n    tool_message = ToolMessage(\n        content=message_content,\n        tool_call_id=\"call_123\",\n        name=\"search_books\",\n        artifact=artifact,\n    )\n    ```\n\n    See the [RAG tutorial](/oss/python/langchain/rag) for an end-to-end example of building retrieval [agents](/oss/python/langchain/agents) with LangChain.\n  </Accordion>\n</Note>\n\n***\n\n## Message content\n\nYou can think of a message's content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as [multimodal](#multimodal) content and other data.\n\nSeparately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See [content blocks](#standard-content-blocks) below.\n\nLangChain chat models accept message content in the `content` attribute.\n\nThis may contain either:\n\n1. A string\n2. A list of content blocks in a provider-native format\n3. A list of [LangChain's standard content blocks](#standard-content-blocks)\n\nSee below for an example using [multimodal](#multimodal) inputs:\n\n```python  theme={null}\nfrom langchain.messages import HumanMessage\n\n# String content\nhuman_message = HumanMessage(\"Hello, how are you?\")\n\n# Provider-native format (e.g., OpenAI)\nhuman_message = HumanMessage(content=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n])\n\n# List of standard content blocks\nhuman_message = HumanMessage(content_blocks=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n])\n```\n\n<Tip>\n  Specifying `content_blocks` when initializing a message will still populate message\n  `content`, but provides a type-safe interface for doing so.\n</Tip>\n\n### Standard content blocks\n\nLangChain provides a standard representation for message content that works across providers.\n\nMessage objects implement a `content_blocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [`ChatAnthropic`](/oss/python/integrations/chat/anthropic) or [`ChatOpenAI`](/oss/python/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:\n\n<Tabs>\n  <Tab title=\"Anthropic\">\n    ```python  theme={null}\n    from langchain.messages import AIMessage\n\n    message = AIMessage(\n        content=[\n            {\"type\": \"thinking\", \"thinking\": \"...\", \"signature\": \"WaUjzkyp...\"},\n            {\"type\": \"text\", \"text\": \"...\"},\n        ],\n        response_metadata={\"model_provider\": \"anthropic\"}\n    )\n    message.content_blocks\n    ```\n\n    ```\n    [{'type': 'reasoning',\n      'reasoning': '...',\n      'extras': {'signature': 'WaUjzkyp...'}},\n     {'type': 'text', 'text': '...'}]\n    ```\n  </Tab>\n\n  <Tab title=\"OpenAI\">\n    ```python  theme={null}\n    from langchain.messages import AIMessage\n\n    message = AIMessage(\n        content=[\n            {\n                \"type\": \"reasoning\",\n                \"id\": \"rs_abc123\",\n                \"summary\": [\n                    {\"type\": \"summary_text\", \"text\": \"summary 1\"},\n                    {\"type\": \"summary_text\", \"text\": \"summary 2\"},\n                ],\n            },\n            {\"type\": \"text\", \"text\": \"...\", \"id\": \"msg_abc123\"},\n        ],\n        response_metadata={\"model_provider\": \"openai\"}\n    )\n    message.content_blocks\n    ```\n\n    ```\n    [{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 1'},\n     {'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 2'},\n     {'type': 'text', 'text': '...', 'id': 'msg_abc123'}]\n    ```\n  </Tab>\n</Tabs>\n\nSee the [integrations guides](/oss/python/integrations/providers/overview) to get started with the\ninference provider of your choice.\n\n<Note>\n  **Serializing standard content**\n\n  If an application outside of LangChain needs access to the standard content block\n  representation, you can opt-in to storing content blocks in message content.\n\n  To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or,\n  initialize any chat model with `output_version=\"v1\"`:\n\n  ```python  theme={null}\n  from langchain.chat_models import init_chat_model\n\n  model = init_chat_model(\"gpt-5-nano\", output_version=\"v1\")\n  ```\n</Note>\n\n### Multimodal\n\n**Multimodality** refers to the ability to work with data that comes in different\nforms, such as text, audio, images, and video. LangChain includes standard types\nfor these data that can be used across providers.\n\n[Chat models](/oss/python/langchain/models) can accept multimodal data as input and generate\nit as output. Below we show short examples of input messages featuring multimodal data.\n\n<Note>\n  Extra keys can be included top-level in the content block or nested in `\"extras\": {\"key\": value}`.\n\n  [OpenAI](/oss/python/integrations/chat/openai#pdfs) and [AWS Bedrock Converse](/oss/python/integrations/chat/bedrock),\n  for example, require a filename for PDFs. See the [provider page](/oss/python/integrations/providers/overview)\n  for your chosen model for specifics.\n</Note>\n\n<CodeGroup>\n  ```python Image input theme={null}\n  # From URL\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n          {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n      ]\n  }\n\n  # From base64 data\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n          {\n              \"type\": \"image\",\n              \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n              \"mime_type\": \"image/jpeg\",\n          },\n      ]\n  }\n\n  # From provider-managed File ID\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n          {\"type\": \"image\", \"file_id\": \"file-abc123\"},\n      ]\n  }\n  ```\n\n  ```python PDF document input theme={null}\n  # From URL\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n          {\"type\": \"file\", \"url\": \"https://example.com/path/to/document.pdf\"},\n      ]\n  }\n\n  # From base64 data\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n          {\n              \"type\": \"file\",\n              \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n              \"mime_type\": \"application/pdf\",\n          },\n      ]\n  }\n\n  # From provider-managed File ID\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n          {\"type\": \"file\", \"file_id\": \"file-abc123\"},\n      ]\n  }\n  ```\n\n  ```python Audio input theme={null}\n  # From base64 data\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this audio.\"},\n          {\n              \"type\": \"audio\",\n              \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n              \"mime_type\": \"audio/wav\",\n          },\n      ]\n  }\n\n  # From provider-managed File ID\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this audio.\"},\n          {\"type\": \"audio\", \"file_id\": \"file-abc123\"},\n      ]\n  }\n  ```\n\n  ```python Video input theme={null}\n  # From base64 data\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this video.\"},\n          {\n              \"type\": \"video\",\n              \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n              \"mime_type\": \"video/mp4\",\n          },\n      ]\n  }\n\n  # From provider-managed File ID\n  message = {\n      \"role\": \"user\",\n      \"content\": [\n          {\"type\": \"text\", \"text\": \"Describe the content of this video.\"},\n          {\"type\": \"video\", \"file_id\": \"file-abc123\"},\n      ]\n  }\n  ```\n</CodeGroup>\n\n<Warning>\n  Not all models support all file types. Check the model provider's [reference](https://reference.langchain.com/python/integrations/) for supported formats and size limits.\n</Warning>\n\n### Content block reference\n\nContent blocks are represented (either when creating a message or accessing the `content_blocks` property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:\n\n<AccordionGroup>\n  <Accordion title=\"Core\" icon=\"cube\">\n    <AccordionGroup>\n      <Accordion title=\"TextContentBlock\" icon=\"text\">\n        **Purpose:** Standard text output\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"text\"`\n        </ParamField>\n\n        <ParamField body=\"text\" type=\"string\" required>\n          The text content\n        </ParamField>\n\n        <ParamField body=\"annotations\" type=\"object[]\">\n          List of annotations for the text\n        </ParamField>\n\n        <ParamField body=\"extras\" type=\"object\">\n          Additional provider-specific data\n        </ParamField>\n\n        **Example:**\n\n        ```python  theme={null}\n        {\n            \"type\": \"text\",\n            \"text\": \"Hello world\",\n            \"annotations\": []\n        }\n        ```\n      </Accordion>\n\n      <Accordion title=\"ReasoningContentBlock\" icon=\"brain\">\n        **Purpose:** Model reasoning steps\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"reasoning\"`\n        </ParamField>\n\n        <ParamField body=\"reasoning\" type=\"string\">\n          The reasoning content\n        </ParamField>\n\n        <ParamField body=\"extras\" type=\"object\">\n          Additional provider-specific data\n        </ParamField>\n\n        **Example:**\n\n        ```python  theme={null}\n        {\n            \"type\": \"reasoning\",\n            \"reasoning\": \"The user is asking about...\",\n            \"extras\": {\"signature\": \"abc123\"},\n        }\n        ```\n      </Accordion>\n    </AccordionGroup>\n  </Accordion>\n\n  <Accordion title=\"Multimodal\" icon=\"images\">\n    <AccordionGroup>\n      <Accordion title=\"ImageContentBlock\" icon=\"image\">\n        **Purpose:** Image data\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"image\"`\n        </ParamField>\n\n        <ParamField body=\"url\" type=\"string\">\n          URL pointing to the image location.\n        </ParamField>\n\n        <ParamField body=\"base64\" type=\"string\">\n          Base64-encoded image data.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Reference ID to an externally stored image (e.g., in a provider's file system or in a bucket).\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"AudioContentBlock\" icon=\"volume-high\">\n        **Purpose:** Audio data\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"audio\"`\n        </ParamField>\n\n        <ParamField body=\"url\" type=\"string\">\n          URL pointing to the audio location.\n        </ParamField>\n\n        <ParamField body=\"base64\" type=\"string\">\n          Base64-encoded audio data.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Reference ID to an externally stored audio file (e.g., in a provider's file system or in a bucket).\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#audio) (e.g., `audio/mpeg`, `audio/wav`)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"VideoContentBlock\" icon=\"video\">\n        **Purpose:** Video data\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"video\"`\n        </ParamField>\n\n        <ParamField body=\"url\" type=\"string\">\n          URL pointing to the video location.\n        </ParamField>\n\n        <ParamField body=\"base64\" type=\"string\">\n          Base64-encoded video data.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Reference ID to an externally stored video file (e.g., in a provider's file system or in a bucket).\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#video) (e.g., `video/mp4`, `video/webm`)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"FileContentBlock\" icon=\"file\">\n        **Purpose:** Generic files (PDF, etc)\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"file\"`\n        </ParamField>\n\n        <ParamField body=\"url\" type=\"string\">\n          URL pointing to the file location.\n        </ParamField>\n\n        <ParamField body=\"base64\" type=\"string\">\n          Base64-encoded file data.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Reference ID to an externally stored file (e.g., in a provider's file system or in a bucket).\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) (e.g., `application/pdf`)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"PlainTextContentBlock\" icon=\"align-left\">\n        **Purpose:** Document text (`.txt`, `.md`)\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"text-plain\"`\n        </ParamField>\n\n        <ParamField body=\"text\" type=\"string\">\n          The text content\n        </ParamField>\n\n        <ParamField body=\"mime_type\" type=\"string\">\n          [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) of the text (e.g., `text/plain`, `text/markdown`)\n        </ParamField>\n      </Accordion>\n    </AccordionGroup>\n  </Accordion>\n\n  <Accordion title=\"Tool Calling\" icon=\"wrench\">\n    <AccordionGroup>\n      <Accordion title=\"ToolCall\" icon=\"function\">\n        **Purpose:** Function calls\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"tool_call\"`\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\" required>\n          Name of the tool to call\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"object\" required>\n          Arguments to pass to the tool\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\" required>\n          Unique identifier for this tool call\n        </ParamField>\n\n        **Example:**\n\n        ```python  theme={null}\n        {\n            \"type\": \"tool_call\",\n            \"name\": \"search\",\n            \"args\": {\"query\": \"weather\"},\n            \"id\": \"call_123\"\n        }\n        ```\n      </Accordion>\n\n      <Accordion title=\"ToolCallChunk\" icon=\"puzzle-piece\">\n        **Purpose:** Streaming tool call fragments\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"tool_call_chunk\"`\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\">\n          Name of the tool being called\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"string\">\n          Partial tool arguments (may be incomplete JSON)\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Tool call identifier\n        </ParamField>\n\n        <ParamField body=\"index\" type=\"number | string\">\n          Position of this chunk in the stream\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"InvalidToolCall\" icon=\"triangle-exclamation\">\n        **Purpose:** Malformed calls, intended to catch JSON parsing errors.\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"invalid_tool_call\"`\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\">\n          Name of the tool that failed to be called\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"object\">\n          Arguments to pass to the tool\n        </ParamField>\n\n        <ParamField body=\"error\" type=\"string\">\n          Description of what went wrong\n        </ParamField>\n      </Accordion>\n    </AccordionGroup>\n  </Accordion>\n\n  <Accordion title=\"Server-Side Tool Execution\" icon=\"server\">\n    <AccordionGroup>\n      <Accordion title=\"ServerToolCall\" icon=\"wrench\">\n        **Purpose:** Tool call that is executed server-side.\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"server_tool_call\"`\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\" required>\n          An identifier associated with the tool call.\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\" required>\n          The name of the tool to be called.\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"string\" required>\n          Partial tool arguments (may be incomplete JSON)\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"ServerToolCallChunk\" icon=\"puzzle-piece\">\n        **Purpose:** Streaming server-side tool call fragments\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"server_tool_call_chunk\"`\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          An identifier associated with the tool call.\n        </ParamField>\n\n        <ParamField body=\"name\" type=\"string\">\n          Name of the tool being called\n        </ParamField>\n\n        <ParamField body=\"args\" type=\"string\">\n          Partial tool arguments (may be incomplete JSON)\n        </ParamField>\n\n        <ParamField body=\"index\" type=\"number | string\">\n          Position of this chunk in the stream\n        </ParamField>\n      </Accordion>\n\n      <Accordion title=\"ServerToolResult\" icon=\"box-open\">\n        **Purpose:** Search results\n\n        <ParamField body=\"type\" type=\"string\" required>\n          Always `\"server_tool_result\"`\n        </ParamField>\n\n        <ParamField body=\"tool_call_id\" type=\"string\" required>\n          Identifier of the corresponding server tool call.\n        </ParamField>\n\n        <ParamField body=\"id\" type=\"string\">\n          Identifier associated with the server tool result.\n        </ParamField>\n\n        <ParamField body=\"status\" type=\"string\" required>\n          Execution status of the server-side tool. `\"success\"` or `\"error\"`.\n        </ParamField>\n\n        <ParamField body=\"output\">\n          Output of the executed tool.\n        </ParamField>\n      </Accordion>\n    </AccordionGroup>\n  </Accordion>\n\n  <Accordion title=\"Provider-Specific Blocks\" icon=\"plug\">\n    <Accordion title=\"NonStandardContentBlock\" icon=\"asterisk\">\n      **Purpose:** Provider-specific escape hatch\n\n      <ParamField body=\"type\" type=\"string\" required>\n        Always `\"non_standard\"`\n      </ParamField>\n\n      <ParamField body=\"value\" type=\"object\" required>\n        Provider-specific data structure\n      </ParamField>\n\n      **Usage:** For experimental or provider-unique features\n    </Accordion>\n\n    Additional provider-specific content types may be found within the [reference documentation](/oss/python/integrations/providers/overview) of each model provider.\n  </Accordion>\n</AccordionGroup>\n\n<Tip>\n  View the canonical type definitions in the [API reference](https://reference.langchain.com/python/langchain/messages).\n</Tip>\n\n<Info>\n  Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.\n\n  Content blocks are not a replacement for the [`content`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content) property, but rather a new property that can be used to access the content of a message in a standardized format.\n</Info>\n\n## Use with chat models\n\n[Chat models](/oss/python/langchain/models) accept a sequence of message objects as input and return an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.\n\nRefer to the below guides to learn more:\n\n* Built-in features for [persisting and managing conversation histories](/oss/python/langchain/short-term-memory)\n* Strategies for managing context windows, including [trimming and summarizing messages](/oss/python/langchain/short-term-memory#common-patterns)\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/messages.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 34145
}