{
  "title": "MCP endpoint in Agent Server",
  "source_url": "https://docs.langchain.com/langsmith/server-mcp",
  "content": "The Model Context Protocol (MCP) is an open protocol for describing tools and data sources in a model-agnostic format, enabling LLMs to discover and use them via a structured API.\n\n[Agent Server](/langsmith/agent-server) implements MCP using the [Streamable HTTP transport](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/transports/#streamable-http). This allows LangGraph **agents** to be exposed as **MCP tools**, making them usable with any MCP-compliant client supporting Streamable HTTP.\n\nThe MCP endpoint is available at `/mcp` on [Agent Server](/langsmith/agent-server).\n\nYou can set up [custom authentication middleware](/langsmith/custom-auth) to authenticate a user with an MCP server to get access to user-scoped tools within your LangSmith deployment.\n\nAn example architecture for this flow:\n\n```mermaid  theme={null}\nsequenceDiagram\n  %% Actors\n  participant ClientApp as Client\n  participant AuthProv  as Auth Provider\n  participant LangGraph as LangGraph Backend\n  participant SecretStore as Secret Store\n  participant MCPServer as MCP Server\n\n  %% Platform login / AuthN\n  ClientApp  ->> AuthProv: 1. Login (username / password)\n  AuthProv   -->> ClientApp: 2. Return token\n  ClientApp  ->> LangGraph: 3. Request with token\n\n  Note over LangGraph: 4. Validate token (@auth.authenticate)\n  LangGraph  -->> AuthProv: 5. Fetch user info\n  AuthProv   -->> LangGraph: 6. Confirm validity\n\n  %% Fetch user tokens from secret store\n  LangGraph  ->> SecretStore: 6a. Fetch user tokens\n  SecretStore -->> LangGraph: 6b. Return tokens\n\n  Note over LangGraph: 7. Apply access control (@auth.on.*)\n\n  %% MCP round-trip\n  Note over LangGraph: 8. Build MCP client with user token\n  LangGraph  ->> MCPServer: 9. Call MCP tool (with header)\n  Note over MCPServer: 10. MCP validates header and runs tool\n  MCPServer  -->> LangGraph: 11. Tool response\n\n  %% Return to caller\n  LangGraph  -->> ClientApp: 12. Return resources / tool output\n```\n\n## Requirements\n\nTo use MCP, ensure you have the following dependencies installed:\n\n* `langgraph-api >= 0.2.3`\n* `langgraph-sdk >= 0.1.61`\n\nInstall them with:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install \"langgraph-api>=0.2.3\" \"langgraph-sdk>=0.1.61\"\n  ```\n\n  ```bash uv theme={null}\n  uv add \"langgraph-api>=0.2.3\" \"langgraph-sdk>=0.1.61\"\n  ```\n</CodeGroup>\n\n## Usage overview\n\nTo enable MCP:\n\n* Upgrade to use langgraph-api>=0.2.3. If you are deploying LangSmith, this will be done for you automatically if you create a new revision.\n* MCP tools (agents) will be automatically exposed.\n* Connect with any MCP-compliant client that supports Streamable HTTP.\n\n### Client\n\nUse an MCP-compliant client to connect to the Agent Server. The following examples show how to connect using different programming languages.\n\n<Tabs>\n  <Tab title=\"JavaScript/TypeScript\">\n    ```bash  theme={null}\n    npm install @modelcontextprotocol/sdk\n    ```\n\n    > **Note**\n    > Replace `serverUrl` with your Agent Server URL and configure authentication headers as needed.\n\n    ```js  theme={null}\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StreamableHTTPClientTransport } from \"@modelcontextprotocol/sdk/client/streamableHttp.js\";\n\n    // Connects to the LangGraph MCP endpoint\n    async function connectClient(url) {\n        const baseUrl = new URL(url);\n        const client = new Client({\n            name: 'streamable-http-client',\n            version: '1.0.0'\n        });\n\n        const transport = new StreamableHTTPClientTransport(baseUrl);\n        await client.connect(transport);\n\n        console.log(\"Connected using Streamable HTTP transport\");\n        console.log(JSON.stringify(await client.listTools(), null, 2));\n        return client;\n    }\n\n    const serverUrl = \"http://localhost:2024/mcp\";\n\n    connectClient(serverUrl)\n        .then(() => {\n            console.log(\"Client connected successfully\");\n        })\n        .catch(error => {\n            console.error(\"Failed to connect client:\", error);\n        });\n    ```\n  </Tab>\n\n  <Tab title=\"Python\">\n    Install the adapter with:\n\n    ```bash  theme={null}\n    pip install langchain-mcp-adapters\n    ```\n\n    Here is an example of how to connect to a remote MCP endpoint and use an agent as a tool:\n\n    ```python  theme={null}\n    # Create server parameters for stdio connection\n    from mcp import ClientSession\n    from mcp.client.streamable_http import streamablehttp_client\n    import asyncio\n\n    from langchain_mcp_adapters.tools import load_mcp_tools\n    from langchain.agents import create_agent\n\n\n    server_params = {\n        \"url\": \"https://mcp-finance-agent.xxx.us.langgraph.app/mcp\",\n        \"headers\": {\n            \"X-Api-Key\":\"lsv2_pt_your_api_key\"\n        }\n    }\n\n    async def main():\n        async with streamablehttp_client(**server_params) as (read, write, _):\n            async with ClientSession(read, write) as session:\n                # Initialize the connection\n                await session.initialize()\n\n                # Load the remote graph as if it was a tool\n                tools = await load_mcp_tools(session)\n\n                # Create and run a react agent with the tools\n                agent = create_agent(\"gpt-4.1\", tools)\n\n                # Invoke the agent with a message\n                agent_response = await agent.ainvoke({\"messages\": \"What can the finance agent do for me?\"})\n                print(agent_response)\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n  </Tab>\n</Tabs>\n\n## Expose an agent as MCP tool\n\nWhen deployed, your agent will appear as a tool in the MCP endpoint\nwith this configuration:\n\n* **Tool name**: The agent's name.\n* **Tool description**: The agent's description.\n* **Tool input schema**: The agent's input schema.\n\n### Setting name and description\n\nYou can set the name and description of your agent in `langgraph.json`:\n\n```json  theme={null}\n{\n    \"graphs\": {\n        \"my_agent\": {\n            \"path\": \"./my_agent/agent.py:graph\",\n            \"description\": \"A description of what the agent does\"\n        }\n    },\n    \"env\": \".env\"\n}\n```\n\nAfter deployment, you can update the name and description using the LangGraph SDK.\n\n### Schema\n\nDefine clear, minimal input and output schemas to avoid exposing unnecessary internal complexity to the LLM.\n\nThe default [MessagesState](/oss/python/langgraph/graph-api#messagesstate) uses `AnyMessage`, which supports many message types but is too general for direct LLM exposure.\n\nInstead, define **custom agents or workflows** that use explicitly typed input and output structures.\n\nFor example, a workflow answering documentation questions might look like this:\n\n```python  theme={null}\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# Define input schema\nclass InputState(TypedDict):\n    question: str\n\n# Define output schema\nclass OutputState(TypedDict):\n    answer: str\n\n# Combine input and output\nclass OverallState(InputState, OutputState):\n    pass\n\n# Define the processing node\ndef answer_node(state: InputState):\n    # Replace with actual logic and do something useful\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\n\n# Build the graph with explicit schemas\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\nbuilder.add_node(answer_node)\nbuilder.add_edge(START, \"answer_node\")\nbuilder.add_edge(\"answer_node\", END)\ngraph = builder.compile()\n\n# Run the graph\nprint(graph.invoke({\"question\": \"hi\"}))\n```\n\nFor more details, see the [low-level concepts guide](/oss/python/langgraph/graph-api#state).\n\n## Use user-scoped MCP tools in your deployment\n\n<Tip>\n  **Prerequisites**\n  You have added your own [custom auth middleware](/langsmith/custom-auth) that populates the `langgraph_auth_user` object, making it accessible through configurable context for every node in your graph.\n</Tip>\n\nTo make user-scoped tools available to your LangSmith deployment, start with implementing a snippet like the following:\n\n```python  theme={null}\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\ndef mcp_tools_node(state, config):\n    user = config[\"configurable\"].get(\"langgraph_auth_user\")\n         , user[\"github_token\"], user[\"email\"], etc.\n\n    client = MultiServerMCPClient({\n        \"github\": {\n            \"transport\": \"streamable_http\", # (1)\n            \"url\": \"https://my-github-mcp-server/mcp\", # (2)\n            \"headers\": {\n                \"Authorization\": f\"Bearer {user['github_token']}\"\n            }\n        }\n    })\n    tools = await client.get_tools() # (3)\n\n    # Your tool-calling logic here\n\n    tool_messages = ...\n    return {\"messages\": tool_messages}\n```\n\n1. MCP only supports adding headers to requests made to `streamable_http` and `sse` `transport` servers.\n2. Your MCP server URL.\n3. Get available tools from your MCP server.\n\n*This can also be done by [rebuilding your graph at runtime](/langsmith/graph-rebuild) to have a different configuration for a new run*\n\n## Session behavior\n\nThe current LangGraph MCP implementation does not support sessions. Each `/mcp` request is stateless and independent.\n\n## Authentication\n\nThe `/mcp` endpoint uses the same authentication as the rest of the LangGraph API. Refer to the [authentication guide](/langsmith/auth) for setup details.\n\n## Disable MCP\n\nTo disable the MCP endpoint, set `disable_mcp` to `true` in your `langgraph.json` configuration file:\n\n```json  theme={null}\n{\n  \"http\": {\n    \"disable_mcp\": true\n  }\n}\n```\n\nThis will prevent the server from exposing the `/mcp` endpoint.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/server-mcp.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 9918
}