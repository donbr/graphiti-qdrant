{
  "title": "Trace generator functions",
  "source_url": "https://docs.langchain.com/langsmith/trace-generator-functions",
  "content": "In most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user.\n\nLangSmith's tracing functionality natively supports streamed outputs via `generator` functions. Below is an example.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import traceable\n  @traceable\n  def my_generator():\n    for chunk in [\"Hello\", \"World\", \"!\"]:\n        yield chunk\n  # Stream to the user\n  for output in my_generator():\n    print(output)\n  # It also works with async functions\n  import asyncio\n  @traceable\n  async def my_async_generator():\n    for chunk in [\"Hello\", \"World\", \"!\"]:\n        yield chunk\n  # Stream to the user\n  async def main():\n    async for output in my_async_generator():\n        print(output)\n  asyncio.run(main())\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { traceable } from \"langsmith/traceable\";\n  const myGenerator = traceable(function* () {\n    for (const chunk of [\"Hello\", \"World\", \"!\"]) {\n        yield chunk;\n    }\n  });\n  for (const output of myGenerator()) {\n    console.log(output);\n  }\n  ```\n</CodeGroup>\n\n## Aggregate Results[\u000b](#aggregate-results \"Direct link to Aggregate Results\")\n\nBy default, the `outputs` of the traced function are aggregated into a single array in LangSmith. If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the `aggregate` option (`reduce_fn` in python). This is especially useful for aggregating streamed LLM outputs.\n\n<Note>\n  Aggregating outputs **only** impacts the traced representation of the outputs. It doesn not alter the values returned by your function.\n</Note>\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith import traceable\n  def concatenate_strings(outputs: list):\n    return \"\".join(outputs)\n  @traceable(reduce_fn=concatenate_strings)\n  def my_generator():\n    for chunk in [\"Hello\", \"World\", \"!\"]:\n        yield chunk\n  # Stream to the user\n  for output in my_generator():\n    print(output)\n  # It also works with async functions\n  import asyncio\n  @traceable(reduce_fn=concatenate_strings)\n  async def my_async_generator():\n    for chunk in [\"Hello\", \"World\", \"!\"]:\n        yield chunk\n  # Stream to the user\n  async def main():\n    async for output in my_async_generator():\n        print(output)\n  asyncio.run(main())\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { traceable } from \"langsmith/traceable\";\n  const concatenateStrings = (outputs: string[]) => outputs.join(\"\");\n  const myGenerator = traceable(function* () {\n    for (const chunk of [\"Hello\", \"World\", \"!\"]) {\n        yield chunk;\n    }\n  }, { aggregator: concatenateStrings });\n  for (const output of await myGenerator()) {\n    console.log(output);\n  }\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-generator-functions.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 3136
}