{
  "title": "Prevent logging of sensitive data in traces",
  "source_url": "https://docs.langchain.com/langsmith/mask-inputs-outputs",
  "content": "In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.\n\nIf you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application:\n\n```bash  theme={null}\nLANGSMITH_HIDE_INPUTS=true\nLANGSMITH_HIDE_OUTPUTS=true\n```\n\nThis works for both the LangSmith SDK (Python and TypeScript) and LangChain.\n\nYou can also customize and override this behavior for a given `Client` instance. This can be done by setting the `hide_inputs` and `hide_outputs` parameters on the `Client` object (`hideInputs` and `hideOutputs` in TypeScript).\n\nFor the example below, we will simply return an empty object for both `hide_inputs` and `hide_outputs`, but you can customize this to your needs.\n\n<CodeGroup>\n  ```python Python theme={null}\n  import openai\n  from langsmith import Client\n  from langsmith.wrappers import wrap_openai\n\n  openai_client = wrap_openai(openai.Client())\n  langsmith_client = Client(\n      hide_inputs=lambda inputs: {}, hide_outputs=lambda outputs: {}\n  )\n\n  # The trace produced will have its metadata present, but the inputs will be hidden\n  openai_client.chat.completions.create(\n      model=\"gpt-4o-mini\",\n      messages=[\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\"role\": \"user\", \"content\": \"Hello!\"},\n      ],\n      langsmith_extra={\"client\": langsmith_client},\n  )\n\n  # The trace produced will not have hidden inputs and outputs\n  openai_client.chat.completions.create(\n      model=\"gpt-4o-mini\",\n      messages=[\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\"role\": \"user\", \"content\": \"Hello!\"},\n      ],\n  )\n  ```\n\n  ```typescript TypeScript theme={null}\n  import OpenAI from \"openai\";\n  import { Client } from \"langsmith\";\n  import { wrapOpenAI } from \"langsmith/wrappers\";\n\n  const langsmithClient = new Client({\n      hideInputs: (inputs) => ({}),\n      hideOutputs: (outputs) => ({}),\n  });\n\n  // The trace produced will have its metadata present, but the inputs will be hidden\n  const filteredOAIClient = wrapOpenAI(new OpenAI(), {\n      client: langsmithClient,\n  });\n  await filteredOAIClient.chat.completions.create({\n      model: \"gpt-4o-mini\",\n      messages: [\n          { role: \"system\", content: \"You are a helpful assistant.\" },\n          { role: \"user\", content: \"Hello!\" },\n      ],\n  });\n\n  const openaiClient = wrapOpenAI(new OpenAI());\n  // The trace produced will not have hidden inputs and outputs\n  await openaiClient.chat.completions.create({\n      model: \"gpt-4o-mini\",\n      messages: [\n          { role: \"system\", content: \"You are a helpful assistant.\" },\n          { role: \"user\", content: \"Hello!\" },\n      ],\n  });\n  ```\n</CodeGroup>\n\n## Rule-based masking of inputs and outputs\n\n<Info>\n  This feature is available in the following LangSmith SDK versions:\n\n  * Python: 0.1.81 and above\n  * TypeScript: 0.1.33 and above\n</Info>\n\nTo mask specific data in inputs and outputs, you can use the `create_anonymizer` / `createAnonymizer` function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.\n\nThe anonymizer will be skipped for inputs if `LANGSMITH_HIDE_INPUTS = true`. Same applies for outputs if `LANGSMITH_HIDE_OUTPUTS = true`.\n\nHowever, if inputs or outputs are to be sent to client, the `anonymizer` method will take precedence over functions found in `hide_inputs` and `hide_outputs`. By default, the `create_anonymizer` will only look at maximum of 10 nesting levels deep, which can be configured via the `max_depth` parameter.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith.anonymizer import create_anonymizer\n  from langsmith import Client, traceable\n  import re\n\n  # create anonymizer from list of regex patterns and replacement values\n  anonymizer = create_anonymizer([\n      { \"pattern\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\", \"replace\": \"<email-address>\" },\n      { \"pattern\": r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\", \"replace\": \"<UUID>\" }\n  ])\n\n  # or create anonymizer from a function\n  email_pattern = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\")\n  uuid_pattern = re.compile(r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\")\n  anonymizer = create_anonymizer(\n      lambda text: email_pattern.sub(\"<email-address>\", uuid_pattern.sub(\"<UUID>\", text))\n  )\n\n  client = Client(anonymizer=anonymizer)\n\n  @traceable(client=client)\n  def main(inputs: dict) -> dict:\n      ...\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { createAnonymizer } from \"langsmith/anonymizer\"\n  import { traceable } from \"langsmith/traceable\"\n  import { Client } from \"langsmith\"\n\n  // create anonymizer from list of regex patterns and replacement values\n  const anonymizer = createAnonymizer([\n      { pattern: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g, replace: \"<email>\" },\n      { pattern: /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g, replace: \"<uuid>\" }\n  ])\n\n  // or create anonymizer from a function\n  const anonymizer = createAnonymizer((value) => value.replace(\"...\", \"<value>\"))\n\n  const client = new Client({ anonymizer })\n\n  const main = traceable(async (inputs: any) => {\n      // ...\n  }, { client })\n  ```\n</CodeGroup>\n\nPlease note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing.\n\n<Note>\n  Improving the performance of `anonymizer` API is on our roadmap! If you are encountering performance issues, please contact us at [support@langchain.dev](mailto:support@langchain.dev).\n</Note>\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ac9ba9a6729029a7fa38da03e1466a1a\" alt=\"\" data-og-width=\"1708\" width=\"1708\" data-og-height=\"717\" height=\"717\" data-path=\"langsmith/images/hide-inputs-outputs.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7ded12c0345f47d55e9802083c5032d0 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8cbe74d09660d8c65e8a75dd78cdb24e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8cb8c0b5c926e46522b9539b0262ee7a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9b8ef244796fad943ec76b0aa5733f80 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=87f35d63f42f05c49a220d5b8a87787a 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5173e30032c065646c13e9b9c6a95fb5 2500w\" />\n\nOlder versions of LangSmith SDKs can use the `hide_inputs` and `hide_outputs` parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well.\n\n<CodeGroup>\n  ```python Python theme={null}\n  import re\n  from langsmith import Client, traceable\n\n  # Define the regex patterns for email addresses and UUIDs\n  EMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\"\n  UUID_REGEX = r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\"\n\n  def replace_sensitive_data(data, depth=10):\n      if depth == 0:\n          return data\n      if isinstance(data, dict):\n          return {k: replace_sensitive_data(v, depth-1) for k, v in data.items()}\n      elif isinstance(data, list):\n          return [replace_sensitive_data(item, depth-1) for item in data]\n      elif isinstance(data, str):\n          data = re.sub(EMAIL_REGEX, \"<email-address>\", data)\n          data = re.sub(UUID_REGEX, \"<UUID>\", data)\n          return data\n      else:\n          return data\n\n  client = Client(\n      hide_inputs=lambda inputs: replace_sensitive_data(inputs),\n      hide_outputs=lambda outputs: replace_sensitive_data(outputs)\n  )\n\n  inputs = {\"role\": \"user\", \"content\": \"Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.\"}\n  outputs = {\"role\": \"assistant\", \"content\": \"Hi! I've noted your email as user@example.com and your ID as 123e4567-e89b-12d3-a456-426614174000.\"}\n\n  @traceable(client=client)\n  def child(inputs: dict) -> dict:\n      return outputs\n\n  @traceable(client=client)\n  def parent(inputs: dict) -> dict:\n      child_outputs = child(inputs)\n      return child_outputs\n\n  parent(inputs)\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n  import { traceable } from \"langsmith/traceable\";\n\n  // Define the regex patterns for email addresses and UUIDs\n  const EMAIL_REGEX = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g;\n  const UUID_REGEX = /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g;\n\n  function replaceSensitiveData(data: any, depth: number = 10): any {\n      if (depth === 0) return data;\n      if (typeof data === \"object\" && !Array.isArray(data)) {\n          const result: Record<string, any> = {};\n          for (const [key, value] of Object.entries(data)) {\n              result[key] = replaceSensitiveData(value, depth - 1);\n          }\n          return result;\n      } else if (Array.isArray(data)) {\n          return data.map(item => replaceSensitiveData(item, depth - 1));\n      } else if (typeof data === \"string\") {\n          return data.replace(EMAIL_REGEX, \"<email-address>\").replace(UUID_REGEX, \"<UUID>\");\n      } else {\n          return data;\n      }\n  }\n\n  const langsmithClient = new Client({\n      hideInputs: (inputs) => replaceSensitiveData(inputs),\n      hideOutputs: (outputs) => replaceSensitiveData(outputs)\n  });\n\n  const inputs = {\n      role: \"user\",\n      content: \"Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.\"\n  };\n  const outputs = {\n      role: \"assistant\",\n      content: \"Hi! I've noted your email as <email-address> and your ID as <UUID>.\"\n  };\n\n  const child = traceable(async (inputs: any) => {\n      return outputs;\n  }, { name: \"child\", client: langsmithClient });\n\n  const parent = traceable(async (inputs: any) => {\n      const childOutputs = await child(inputs);\n      return childOutputs;\n  }, { name: \"parent\", client: langsmithClient });\n\n  await parent(inputs);\n  ```\n</CodeGroup>\n\n## Processing Inputs & Outputs for a Single Function\n\n<Info>\n  The `process_outputs` parameter is available in LangSmith SDK version 0.1.98 and above for Python.\n</Info>\n\nIn addition to client-level input and output processing, LangSmith provides function-level processing through the `process_inputs` and `process_outputs` parameters of the `@traceable` decorator.\n\nThese parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function.\n\nHere's an example of how to use `process_inputs` and `process_outputs`:\n\n```python  theme={null}\nfrom langsmith import traceable\n\ndef process_inputs(inputs: dict) -> dict:\n    # inputs is a dictionary where keys are argument names and values are the provided arguments\n    # Return a new dictionary with processed inputs\n    return {\n        \"processed_key\": inputs.get(\"my_cool_key\", \"default\"),\n        \"length\": len(inputs.get(\"my_cool_key\", \"\"))\n    }\n\ndef process_outputs(output: Any) -> dict:\n    # output is the direct return value of the function\n    # Transform the output into a dictionary\n    # In this case, \"output\" will be an integer\n    return {\"processed_output\": str(output)}\n\n@traceable(process_inputs=process_inputs, process_outputs=process_outputs)\ndef my_function(my_cool_key: str) -> int:\n    # Function implementation\n    return len(my_cool_key)\n\nresult = my_function(\"example\")\n```\n\nIn this example, `process_inputs` creates a new dictionary with processed input data, and `process_outputs` transforms the output into a specific format before logging to LangSmith.\n\n<Warning>\n  It's recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data.\n</Warning>\n\nFor asynchronous functions, the usage is similar:\n\n```python  theme={null}\n@traceable(process_inputs=process_inputs, process_outputs=process_outputs)\nasync def async_function(key: str) -> int:\n    # Async implementation\n    return len(key)\n```\n\nThese function-level processors take precedence over client-level processors (`hide_inputs` and `hide_outputs`) when both are defined.\n\n## Quick starts\n\nYou can combine rule-based masking with various anonymizers to scrub sensitive information from inputs and outputs. In this how-to-guide, we'll cover working with regex, Microsoft Presidio, and Amazon Comprehend.\n\n### Regex\n\n<Info>\n  The implementation below is not exhaustive and may miss some formats or edge cases. Test any implementation thoroughly before using it in production.\n</Info>\n\nYou can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.\n\n```python  theme={null}\nimport re\nimport openai\nfrom langsmith import Client\nfrom langsmith.wrappers import wrap_openai\n\n# Define regex patterns for various PII\nSSN_PATTERN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')\nCREDIT_CARD_PATTERN = re.compile(r'\\b(?:\\d[ -]*?){13,16}\\b')\nEMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')\nPHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\nFULL_NAME_PATTERN = re.compile(r'\\b([A-Z][a-z]*\\s[A-Z][a-z]*)\\b')\n\ndef regex_anonymize(text):\n    \"\"\"\n    Anonymize sensitive information in the text using regex patterns.\n    Args:\n        text (str): The input text to be anonymized.\n    Returns:\n        str: The anonymized text.\n    \"\"\"\n    # Replace sensitive information with placeholders\n    text = SSN_PATTERN.sub('[REDACTED SSN]', text)\n    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)\n    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)\n    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)\n    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)\n    return text\n\ndef recursive_anonymize(data, depth=10):\n    \"\"\"\n    Recursively traverse the data structure and anonymize sensitive information.\n    Args:\n        data (any): The input data to be anonymized.\n        depth (int): The current recursion depth to prevent excessive recursion.\n    Returns:\n        any: The anonymized data.\n    \"\"\"\n    if depth == 0:\n        return data\n    if isinstance(data, dict):\n        anonymized_dict = {}\n        for k, v in data.items():\n            anonymized_value = recursive_anonymize(v, depth - 1)\n            anonymized_dict[k] = anonymized_value\n        return anonymized_dict\n    elif isinstance(data, list):\n        anonymized_list = []\n        for item in data:\n            anonymized_item = recursive_anonymize(item, depth - 1)\n            anonymized_list.append(anonymized_item)\n        return anonymized_list\n    elif isinstance(data, str):\n        anonymized_data = regex_anonymize(data)\n        return anonymized_data\n    else:\n        return data\n\nopenai_client = wrap_openai(openai.Client())\n\n# Initialize the LangSmith client with the anonymization functions\nlangsmith_client = Client(\n    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize\n)\n\n# The trace produced will have its metadata present, but the inputs and outputs will be anonymized\nresponse_with_anonymization = openai_client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},\n    ],\n    langsmith_extra={\"client\": langsmith_client},\n)\n\n# The trace produced will not have anonymized inputs and outputs\nresponse_without_anonymization = openai_client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},\n    ],\n)\n```\n\nThe anonymized run will look like this in LangSmith: <img src=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-anonymized.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d42154587a9440675d2d506e1124b3fe\" alt=\"Anonymized run\" data-og-width=\"3178\" width=\"3178\" data-og-height=\"1836\" height=\"1836\" data-path=\"langsmith/images/regex-anonymized.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-anonymized.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0d0da550b34ec564879292413f36b492 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-anonymized.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3160769f89669b892d97ce42b69d2e9b 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-anonymized.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=22d315f029d39b4b2094551077455399 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-anonymized.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3a949b36dd27bc4491177bb6020df5f9 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-anonymized.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=101541bc73fff2c554e9922107119837 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-anonymized.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=38ff424147bb73093b40d7a87767f41f 2500w\" />\n\nThe non-anonymized run will look like this in LangSmith: <img src=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-not-anonymized.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=98da1cf86195ebb3be2119cddbf1ae0d\" alt=\"Non-anonymized run\" data-og-width=\"3176\" width=\"3176\" data-og-height=\"1830\" height=\"1830\" data-path=\"langsmith/images/regex-not-anonymized.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-not-anonymized.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=871939d7d51881a70cb8d9d6334d47cb 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-not-anonymized.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=00c968ee9e50c1588e2dae1bc157e26c 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-not-anonymized.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b37c8e4de393f885634c24407bb26bbe 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-not-anonymized.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=90800846fe864309521405f18d918d76 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-not-anonymized.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=f2b6161b2bbb45fcb3b1a02b55368fca 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regex-not-anonymized.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c01f709dba7c6ebbf3e28c2cdc94c2cc 2500w\" />\n\n### Microsoft Presidio\n\n<Info>\n  The implementation below provides a general example of how to anonymize sensitive information in messages exchanged between a user and an LLM. It is not exhaustive and does not account for all cases. Test any implementation thoroughly before using it in production.\n</Info>\n\nMicrosoft Presidio is a data protection and de-identification SDK. The implementation below uses Presidio to anonymize inputs and outputs before they are sent to LangSmith. For up to date information, please refer to Presidio's [official documentation](https://microsoft.github.io/presidio/).\n\nTo use Presidio and its spaCy model, install the following:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install presidio-analyzer\n  pip install presidio-anonymizer\n  python -m spacy download en_core_web_lg\n  ```\n\n  ```bash uv theme={null}\n  uv add presidio-analyzer\n  uv add presidio-anonymizer\n  python -m spacy download en_core_web_lg\n  ```\n</CodeGroup>\n\nAlso, install OpenAI:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install openai\n  ```\n\n  ```bash uv theme={null}\n  uv add openai\n  ```\n</CodeGroup>\n\n```python  theme={null}\nimport openai\nfrom langsmith import Client\nfrom langsmith.wrappers import wrap_openai\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_analyzer import AnalyzerEngine\n\nanonymizer = AnonymizerEngine()\nanalyzer = AnalyzerEngine()\n\ndef presidio_anonymize(data):\n    \"\"\"\n    Anonymize sensitive information sent by the user or returned by the model.\n    Args:\n        data (any): The data to be anonymized.\n    Returns:\n        any: The anonymized data.\n    \"\"\"\n    message_list = (\n        data.get('messages') or [data.get('choices', [{}])[0].get('message')]\n    )\n    if not message_list or not all(isinstance(msg, dict) and msg for msg in message_list):\n        return data\n\n    for message in message_list:\n        content = message.get('content', '')\n        if not content.strip():\n            print(\"Empty content detected. Skipping anonymization.\")\n            continue\n\n        results = analyzer.analyze(\n            text=content,\n            entities=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"US_SSN\"],\n            language='en'\n        )\n        anonymized_result = anonymizer.anonymize(\n            text=content,\n            analyzer_results=results\n        )\n        message['content'] = anonymized_result.text\n\n    return data\n\nopenai_client = wrap_openai(openai.Client())\n\n# initialize the langsmith client with the anonymization functions\nlangsmith_client = Client(\n  hide_inputs=presidio_anonymize, hide_outputs=presidio_anonymize\n)\n\n# The trace produced will have its metadata present, but the inputs and outputs will be anonymized\nresponse_with_anonymization = openai_client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\"},\n  ],\n  langsmith_extra={\"client\": langsmith_client},\n)\n\n# The trace produced will not have anonymized inputs and outputs\nresponse_without_anonymization = openai_client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\"},\n  ],\n)\n```\n\nThe anonymized run will look like this in LangSmith: <img src=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-anonymized.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=100d741a3fb32d0c1f251186f57568ac\" alt=\"Anonymized run\" data-og-width=\"3174\" width=\"3174\" data-og-height=\"1830\" height=\"1830\" data-path=\"langsmith/images/presidio-anonymized.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-anonymized.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=3f2f159c048e309e5b78e85ca4dfa75f 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-anonymized.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=e78dc405ee0019523ffb150dc0b56eae 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-anonymized.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=edefbf42e4749b8d26fca7e87b627659 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-anonymized.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=69a3b82526f4a31ce449322c100ece2f 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-anonymized.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9de36ba2545c911c9908b5dbbb8b30c3 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-anonymized.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ca15dafdf66dd0e7dd13eb898d76ee7f 2500w\" />\n\nThe non-anonymized run will look like this in LangSmith: <img src=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-not-anonymized.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2ef3e46591877ffdf23c0c240ec389c2\" alt=\"Non-anonymized run\" data-og-width=\"3176\" width=\"3176\" data-og-height=\"1684\" height=\"1684\" data-path=\"langsmith/images/presidio-not-anonymized.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-not-anonymized.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7792f325a64b7f64ff78db535b275c61 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-not-anonymized.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=85d81aa5511eae3fc961b97a0e857e3c 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-not-anonymized.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=be4fceb44186435053c2e7eea51ceba5 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-not-anonymized.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6a29ce75afa17e6a7c09fb591e17aa97 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-not-anonymized.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4acda7489155c3a97d732b6788243fa9 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/presidio-not-anonymized.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a5f962c936c4dfe97bdb31fb0b51c445 2500w\" />\n\n### Amazon Comprehend\n\n<Info>\n  The implementation below provides a general example of how to anonymize sensitive information in messages exchanged between a user and an LLM. It is not exhaustive and does not account for all cases. Test any implementation thoroughly before using it in production.\n</Info>\n\nComprehend is a natural language processing service that can detect personally identifiable information. The implementation below uses Comprehend to anonymize inputs and outputs before they are sent to LangSmith. For up to date information, please refer to Comprehend's [official documentation](https://docs.aws.amazon.com/comprehend/latest/APIReference/API_DetectPiiEntities.html).\n\nTo use Comprehend, install [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html):\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install boto3\n  ```\n\n  ```bash uv theme={null}\n  uv add boto3\n  ```\n</CodeGroup>\n\nAlso, install OpenAI:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install openai\n  ```\n\n  ```bash uv theme={null}\n  uv add openai\n  ```\n</CodeGroup>\n\nYou will need to set up credentials in AWS and authenticate using the AWS CLI. Follow the instructions [here](https://docs.aws.amazon.com/comprehend/latest/dg/setting-up.html).\n\n```python  theme={null}\nimport openai\nimport boto3\nfrom langsmith import Client\nfrom langsmith.wrappers import wrap_openai\n\ncomprehend = boto3.client('comprehend', region_name='us-east-1')\n\ndef redact_pii_entities(text, entities):\n    \"\"\"\n    Redact PII entities in the text based on the detected entities.\n    Args:\n        text (str): The original text containing PII.\n        entities (list): A list of detected PII entities.\n    Returns:\n        str: The text with PII entities redacted.\n    \"\"\"\n    sorted_entities = sorted(entities, key=lambda x: x['BeginOffset'], reverse=True)\n    redacted_text = text\n    for entity in sorted_entities:\n        begin = entity['BeginOffset']\n        end = entity['EndOffset']\n        entity_type = entity['Type']\n        # Define the redaction placeholder based on entity type\n        placeholder = f\"[{entity_type}]\"\n        # Replace the PII in the text with the placeholder\n        redacted_text = redacted_text[:begin] + placeholder + redacted_text[end:]\n    return redacted_text\n\ndef detect_pii(text):\n    \"\"\"\n    Detect PII entities in the given text using AWS Comprehend.\n    Args:\n        text (str): The text to analyze.\n    Returns:\n        list: A list of detected PII entities.\n    \"\"\"\n    try:\n        response = comprehend.detect_pii_entities(\n            Text=text,\n            LanguageCode='en',\n        )\n        entities = response.get('Entities', [])\n        return entities\n    except Exception as e:\n        print(f\"Error detecting PII: {e}\")\n        return []\n\ndef comprehend_anonymize(data):\n    \"\"\"\n    Anonymize sensitive information sent by the user or returned by the model.\n    Args:\n        data (any): The input data to be anonymized.\n    Returns:\n        any: The anonymized data.\n    \"\"\"\n    message_list = (\n        data.get('messages') or [data.get('choices', [{}])[0].get('message')]\n    )\n    if not message_list or not all(isinstance(msg, dict) and msg for msg in message_list):\n        return data\n\n    for message in message_list:\n        content = message.get('content', '')\n        if not content.strip():\n            print(\"Empty content detected. Skipping anonymization.\")\n            continue\n\n        entities = detect_pii(content)\n        if entities:\n            anonymized_text = redact_pii_entities(content, entities)\n            message['content'] = anonymized_text\n        else:\n            print(\"No PII detected. Content remains unchanged.\")\n\n    return data\n\nopenai_client = wrap_openai(openai.Client())\n\n# initialize the langsmith client with the anonymization functions\nlangsmith_client = Client(\n  hide_inputs=comprehend_anonymize, hide_outputs=comprehend_anonymize\n)\n\n# The trace produced will have its metadata present, but the inputs and outputs will be anonymized\nresponse_with_anonymization = openai_client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\"},\n  ],\n  langsmith_extra={\"client\": langsmith_client},\n)\n\n# The trace produced will not have anonymized inputs and outputs\nresponse_without_anonymization = openai_client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\"},\n  ],\n)\n```\n\nThe anonymized run will look like this in LangSmith: <img src=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cea182d95ef02e614a6f1bbd7e3a2657\" alt=\"Anonymized run\" data-og-width=\"3180\" width=\"3180\" data-og-height=\"1616\" height=\"1616\" data-path=\"langsmith/images/aws-comprehend-anonymized.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d3c5a665e2ee726ad6dacf89ade8daea 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=77bccbc4ba3bcde3bd771866e44ce535 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=527a6563672cb66d28bf7ae3272c0c5e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=108afc60434f26addae7525049850aac 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e0216a9846c6e7ff041bcccdb23ce98a 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cd4119b7e81420f54db86cad58db5426 2500w\" />\n\nThe non-anonymized run will look like this in LangSmith: <img src=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ec61e5c8d78268b5b34b6b9c184871cc\" alt=\"Non-anonymized run\" data-og-width=\"3180\" width=\"3180\" data-og-height=\"1648\" height=\"1648\" data-path=\"langsmith/images/aws-comprehend-not-anonymized.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=34ad34770c55fcea58caee9dfa7f856e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=089c86d993f3dc8a5998bfc2adc8a75d 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7fcd25c38a94d366762cf74629dd07c7 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b82a670a0e25bc656475cccea5d1a50d 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c422ed47282a844783713e5dc291a7b0 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a5a62a5bd72b2cf49c9495d14d3059fa 2500w\" />\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/mask-inputs-outputs.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 35451
}