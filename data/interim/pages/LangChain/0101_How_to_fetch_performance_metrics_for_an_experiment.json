{
  "title": "How to fetch performance metrics for an experiment",
  "source_url": "https://docs.langchain.com/langsmith/fetch-perf-metrics-experiment",
  "content": "<Check>\n  Tracing projects and experiments use the same underlying data structure in our backend, which is called a \"session.\"\n\n  You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.\n\n  We are working on unifying the terminology across our documentation and APIs.\n</Check>\n\nWhen you run an experiment using `evaluate` with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the `read_project`/`readProject` methods.\n\nThe payload for experiment details includes the following values:\n\n```json  theme={null}\n{\n  \"start_time\": \"2024-06-06T01:02:51.299960\",\n  \"end_time\": \"2024-06-06T01:03:04.557530+00:00\",\n  \"extra\": {\n    \"metadata\": {\n      \"git\": {\n        \"tags\": null,\n        \"dirty\": true,\n        \"branch\": \"ankush/agent-eval\",\n        \"commit\": \"...\",\n        \"repo_name\": \"...\",\n        \"remote_url\": \"...\",\n        \"author_name\": \"Ankush Gola\",\n        \"commit_time\": \"...\",\n        \"author_email\": \"...\"\n      },\n      \"revision_id\": null,\n      \"dataset_splits\": [\"base\"],\n      \"dataset_version\": \"2024-06-05T04:57:01.535578+00:00\",\n      \"num_repetitions\": 3\n    }\n  },\n  \"name\": \"SQL Database Agent-ae9ad229\",\n  \"description\": null,\n  \"default_dataset_id\": null,\n  \"reference_dataset_id\": \"...\",\n  \"id\": \"...\",\n  \"run_count\": 9,\n  \"latency_p50\": 7.896,\n  \"latency_p99\": 13.09332,\n  \"first_token_p50\": null,\n  \"first_token_p99\": null,\n  \"total_tokens\": 35573,\n  \"prompt_tokens\": 32711,\n  \"completion_tokens\": 2862,\n  \"total_cost\": 0.206485,\n  \"prompt_cost\": 0.163555,\n  \"completion_cost\": 0.04293,\n  \"tenant_id\": \"...\",\n  \"last_run_start_time\": \"2024-06-06T01:02:51.366397\",\n  \"last_run_start_time_live\": null,\n  \"feedback_stats\": {\n    \"cot contextual accuracy\": {\n      \"n\": 9,\n      \"avg\": 0.6666666666666666,\n      \"values\": {\n        \"CORRECT\": 6,\n        \"INCORRECT\": 3\n      }\n    }\n  },\n  \"session_feedback_stats\": {},\n  \"run_facets\": [],\n  \"error_rate\": 0,\n  \"streaming_rate\": 0,\n  \"test_run_number\": 11\n}\n```\n\nFrom here, you can extract performance metrics such as:\n\n* `latency_p50`: The 50th percentile latency in seconds.\n* `latency_p99`: The 99th percentile latency in seconds.\n* `total_tokens`: The total number of tokens used.\n* `prompt_tokens`: The number of prompt tokens used.\n* `completion_tokens`: The number of completion tokens used.\n* `total_cost`: The total cost of the experiment.\n* `prompt_cost`: The cost of the prompt tokens.\n* `completion_cost`: The cost of the completion tokens.\n* `feedback_stats`: The feedback statistics for the experiment.\n* `error_rate`: The error rate for the experiment.\n* `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).\n* `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).\n\nHere is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.\n\nFirst, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the [how-to guide](/langsmith/evaluate-llm-application) on evaluation for more details.\n\n```python  theme={null}\nfrom langsmith import Client\n\nclient = Client()\n\n# Create a dataset\ndataset_name = \"HelloDataset\"\ndataset = client.create_dataset(dataset_name=dataset_name)\n\nexamples = [\n    {\n        \"inputs\": {\"input\": \"Harrison\"},\n        \"outputs\": {\"expected\": \"Hello Harrison\"},\n    },\n    {\n        \"inputs\": {\"input\": \"Ankush\"},\n        \"outputs\": {\"expected\": \"Hello Ankush\"},\n    },\n]\n\nclient.create_examples(dataset_id=dataset.id, examples=examples)\n```\n\nNext, we will create an experiment, retrieve the experiment name from the result of `evaluate`, then fetch the performance metrics for the experiment.\n\n<CodeGroup>\n  ```python Python theme={null}\n  from langsmith.schemas import Example, Run\n  dataset_name = \"HelloDataset\"\n\n  def foo_label(root_run: Run, example: Example) -> dict:\n      return {\"score\": 1, \"key\": \"foo\"}\n\n  from langsmith import evaluate\n\n  results = evaluate(\n      lambda inputs: \"Hello \" + inputs[\"input\"],\n      data=dataset_name,\n      evaluators=[foo_label],\n      experiment_prefix=\"Hello\",\n  )\n\n  resp = client.read_project(project_name=results.experiment_name, include_stats=True)\n  print(resp.json(indent=2))\n  ```\n\n  ```typescript TypeScript theme={null}\n  import { Client } from \"langsmith\";\n  import { evaluate } from \"langsmith/evaluation\";\n  import type { EvaluationResult } from \"langsmith/evaluation\";\n  import type { Run, Example } from \"langsmith/schemas\";\n\n  // Row-level evaluator\n  function fooLabel(rootRun: Run, example: Example): EvaluationResult {\n      return {score: 1, key: \"foo\"};\n  }\n\n  const client = new Client();\n\n  const results = await evaluate(\n      (inputs) => {\n          return { output: \"Hello \" + inputs.input };\n      },\n      {\n          data: \"HelloDataset\",\n          experimentPrefix: \"Hello\",\n          evaluators: [fooLabel],\n      }\n  );\n\n  const resp = await client.readProject({\n      projectName: results.experimentName,\n      includeStats: true\n  })\n  console.log(JSON.stringify(resp, null, 2))\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/fetch-perf-metrics-experiment.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 5619
}