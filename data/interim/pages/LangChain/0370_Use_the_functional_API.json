{
  "title": "Use the functional API",
  "source_url": "https://docs.langchain.com/oss/javascript/langgraph/use-functional-api",
  "content": "The [**Functional API**](/oss/javascript/langgraph/functional-api) allows you to add LangGraph's key features — [persistence](/oss/javascript/langgraph/persistence), [memory](/oss/javascript/langgraph/add-memory), [human-in-the-loop](/oss/javascript/langgraph/interrupts), and [streaming](/oss/javascript/langgraph/streaming) — to your applications with minimal changes to your existing code.\n\n<Tip>\n  For conceptual information on the functional API, see [Functional API](/oss/javascript/langgraph/functional-api).\n</Tip>\n\n## Creating a simple workflow\n\nWhen defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.\n\n```typescript  theme={null}\nconst checkpointer = new MemorySaver();\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"myWorkflow\" },\n  async (inputs: { value: number; anotherValue: number }) => {\n    const value = inputs.value;\n    const anotherValue = inputs.anotherValue;\n    // ...\n  }\n);\n\nawait myWorkflow.invoke({ value: 1, anotherValue: 2 });\n```\n\n<Accordion title=\"Extended example: simple workflow\">\n  ```typescript  theme={null}\n  import { v4 as uuidv4 } from \"uuid\";\n  import { entrypoint, task, MemorySaver } from \"@langchain/langgraph\";\n\n  // Task that checks if a number is even\n  const isEven = task(\"isEven\", async (number: number) => {\n    return number % 2 === 0;\n  });\n\n  // Task that formats a message\n  const formatMessage = task(\"formatMessage\", async (isEven: boolean) => {\n    return isEven ? \"The number is even.\" : \"The number is odd.\";\n  });\n\n  // Create a checkpointer for persistence\n  const checkpointer = new MemorySaver();\n\n  const workflow = entrypoint(\n    { checkpointer, name: \"workflow\" },\n    async (inputs: { number: number }) => {\n      // Simple workflow to classify a number\n      const even = await isEven(inputs.number);\n      return await formatMessage(even);\n    }\n  );\n\n  // Run the workflow with a unique thread ID\n  const config = { configurable: { thread_id: uuidv4() } };\n  const result = await workflow.invoke({ number: 7 }, config);\n  console.log(result);\n  ```\n</Accordion>\n\n<Accordion title=\"Extended example: Compose an essay with an LLM\">\n  This example demonstrates how to use the `@task` and `@entrypoint` decorators\n  syntactically. Given that a checkpointer is provided, the workflow results will\n  be persisted in the checkpointer.\n\n  ```typescript  theme={null}\n  import { v4 as uuidv4 } from \"uuid\";\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { entrypoint, task, MemorySaver } from \"@langchain/langgraph\";\n\n  const model = new ChatOpenAI({ model: \"gpt-3.5-turbo\" });\n\n  // Task: generate essay using an LLM\n  const composeEssay = task(\"composeEssay\", async (topic: string) => {\n    // Generate an essay about the given topic\n    const response = await model.invoke([\n      { role: \"system\", content: \"You are a helpful assistant that writes essays.\" },\n      { role: \"user\", content: `Write an essay about ${topic}.` }\n    ]);\n    return response.content as string;\n  });\n\n  // Create a checkpointer for persistence\n  const checkpointer = new MemorySaver();\n\n  const workflow = entrypoint(\n    { checkpointer, name: \"workflow\" },\n    async (topic: string) => {\n      // Simple workflow that generates an essay with an LLM\n      return await composeEssay(topic);\n    }\n  );\n\n  // Execute the workflow\n  const config = { configurable: { thread_id: uuidv4() } };\n  const result = await workflow.invoke(\"the history of flight\", config);\n  console.log(result);\n  ```\n</Accordion>\n\n## Parallel execution\n\nTasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).\n\n```typescript  theme={null}\nconst addOne = task(\"addOne\", async (number: number) => {\n  return number + 1;\n});\n\nconst graph = entrypoint(\n  { checkpointer, name: \"graph\" },\n  async (numbers: number[]) => {\n    return await Promise.all(numbers.map(addOne));\n  }\n);\n```\n\n<Accordion title=\"Extended example: parallel LLM calls\">\n  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.\n\n  ```typescript  theme={null}\n  import { v4 as uuidv4 } from \"uuid\";\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { entrypoint, task, MemorySaver } from \"@langchain/langgraph\";\n\n  // Initialize the LLM model\n  const model = new ChatOpenAI({ model: \"gpt-3.5-turbo\" });\n\n  // Task that generates a paragraph about a given topic\n  const generateParagraph = task(\"generateParagraph\", async (topic: string) => {\n    const response = await model.invoke([\n      { role: \"system\", content: \"You are a helpful assistant that writes educational paragraphs.\" },\n      { role: \"user\", content: `Write a paragraph about ${topic}.` }\n    ]);\n    return response.content as string;\n  });\n\n  // Create a checkpointer for persistence\n  const checkpointer = new MemorySaver();\n\n  const workflow = entrypoint(\n    { checkpointer, name: \"workflow\" },\n    async (topics: string[]) => {\n      // Generates multiple paragraphs in parallel and combines them\n      const paragraphs = await Promise.all(topics.map(generateParagraph));\n      return paragraphs.join(\"\\n\\n\");\n    }\n  );\n\n  // Run the workflow\n  const config = { configurable: { thread_id: uuidv4() } };\n  const result = await workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config);\n  console.log(result);\n  ```\n\n  This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.\n</Accordion>\n\n## Calling graphs\n\nThe **Functional API** and the [**Graph API**](/oss/javascript/langgraph/graph-api) can be used together in the same application as they share the same underlying runtime.\n\n```typescript  theme={null}\nimport { entrypoint } from \"@langchain/langgraph\";\nimport { StateGraph } from \"@langchain/langgraph\";\n\nconst builder = new StateGraph(/* ... */);\n// ...\nconst someGraph = builder.compile();\n\nconst someWorkflow = entrypoint(\n  { name: \"someWorkflow\" },\n  async (someInput: Record<string, any>) => {\n    // Call a graph defined using the graph API\n    const result1 = await someGraph.invoke(/* ... */);\n    // Call another graph defined using the graph API\n    const result2 = await anotherGraph.invoke(/* ... */);\n    return {\n      result1,\n      result2,\n    };\n  }\n);\n```\n\n<Accordion title=\"Extended example: calling a simple graph from the functional API\">\n  ```typescript  theme={null}\n  import { v4 as uuidv4 } from \"uuid\";\n  import { entrypoint, MemorySaver } from \"@langchain/langgraph\";\n  import { StateGraph } from \"@langchain/langgraph\";\n  import * as z from \"zod\";\n\n  // Define the shared state type\n  const State = z.object({\n    foo: z.number(),\n  });\n\n  // Build the graph using the Graph API\n  const builder = new StateGraph(State)\n    .addNode(\"double\", (state) => {\n      return { foo: state.foo * 2 };\n    })\n    .addEdge(\"__start__\", \"double\");\n  const graph = builder.compile();\n\n  // Define the functional API workflow\n  const checkpointer = new MemorySaver();\n\n  const workflow = entrypoint(\n    { checkpointer, name: \"workflow\" },\n    async (x: number) => {\n      const result = await graph.invoke({ foo: x });\n      return { bar: result.foo };\n    }\n  );\n\n  // Execute the workflow\n  const config = { configurable: { thread_id: uuidv4() } };\n  console.log(await workflow.invoke(5, config)); // Output: { bar: 10 }\n  ```\n</Accordion>\n\n## Call other entrypoints\n\nYou can call other **entrypoints** from within an **entrypoint** or a **task**.\n\n```typescript  theme={null}\n// Will automatically use the checkpointer from the parent entrypoint\nconst someOtherWorkflow = entrypoint(\n  { name: \"someOtherWorkflow\" },\n  async (inputs: { value: number }) => {\n    return inputs.value;\n  }\n);\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"myWorkflow\" },\n  async (inputs: { value: number }) => {\n    const value = await someOtherWorkflow.invoke({ value: 1 });\n    return value;\n  }\n);\n```\n\n<Accordion title=\"Extended example: calling another entrypoint\">\n  ```typescript  theme={null}\n  import { v4 as uuidv4 } from \"uuid\";\n  import { entrypoint, MemorySaver } from \"@langchain/langgraph\";\n\n  // Initialize a checkpointer\n  const checkpointer = new MemorySaver();\n\n  // A reusable sub-workflow that multiplies a number\n  const multiply = entrypoint(\n    { name: \"multiply\" },\n    async (inputs: { a: number; b: number }) => {\n      return inputs.a * inputs.b;\n    }\n  );\n\n  // Main workflow that invokes the sub-workflow\n  const main = entrypoint(\n    { checkpointer, name: \"main\" },\n    async (inputs: { x: number; y: number }) => {\n      const result = await multiply.invoke({ a: inputs.x, b: inputs.y });\n      return { product: result };\n    }\n  );\n\n  // Execute the main workflow\n  const config = { configurable: { thread_id: uuidv4() } };\n  console.log(await main.invoke({ x: 6, y: 7 }, config)); // Output: { product: 42 }\n  ```\n</Accordion>\n\n## Streaming\n\nThe **Functional API** uses the same streaming mechanism as the **Graph API**. Please\nread the [**streaming guide**](/oss/javascript/langgraph/streaming) section for more details.\n\nExample of using the streaming API to stream both updates and custom data.\n\n```typescript  theme={null}\nimport {\n  entrypoint,\n  MemorySaver,\n  LangGraphRunnableConfig,\n} from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst main = entrypoint(\n  { checkpointer, name: \"main\" },\n  async (\n    inputs: { x: number },\n    config: LangGraphRunnableConfig\n  ): Promise<number> => {\n    config.writer?.(\"Started processing\");   // [!code highlight]\n    const result = inputs.x * 2;\n    config.writer?.(`Result is ${result}`);   // [!code highlight]\n    return result;\n  }\n);\n\nconst config = { configurable: { thread_id: \"abc\" } };\n\n  // [!code highlight]\nfor await (const [mode, chunk] of await main.stream(\n  { x: 5 },\n  { streamMode: [\"custom\", \"updates\"], ...config }   // [!code highlight]\n)) {\n  console.log(`${mode}: ${JSON.stringify(chunk)}`);\n}\n```\n\n1. Emit custom data before computation begins.\n2. Emit another custom message after computing the result.\n3. Use `.stream()` to process streamed output.\n4. Specify which streaming modes to use.\n\n```\nupdates: {\"addOne\": 2}\nupdates: {\"addTwo\": 3}\ncustom: \"hello\"\ncustom: \"world\"\nupdates: {\"main\": 5}\n```\n\n## Retry policy\n\n```typescript  theme={null}\nimport {\n  MemorySaver,\n  entrypoint,\n  task,\n  RetryPolicy,\n} from \"@langchain/langgraph\";\n\n// This variable is just used for demonstration purposes to simulate a network failure.\n// It's not something you will have in your actual code.\nlet attempts = 0;\n\n// Let's configure the RetryPolicy to retry on ValueError.\n// The default RetryPolicy is optimized for retrying specific network errors.\nconst retryPolicy: RetryPolicy = { retryOn: (error) => error instanceof Error };\n\nconst getInfo = task(\n  {\n    name: \"getInfo\",\n    retry: retryPolicy,\n  },\n  () => {\n    attempts += 1;\n\n    if (attempts < 2) {\n      throw new Error(\"Failure\");\n    }\n    return \"OK\";\n  }\n);\n\nconst checkpointer = new MemorySaver();\n\nconst main = entrypoint(\n  { checkpointer, name: \"main\" },\n  async (inputs: Record<string, any>) => {\n    return await getInfo();\n  }\n);\n\nconst config = {\n  configurable: {\n    thread_id: \"1\",\n  },\n};\n\nawait main.invoke({ any_input: \"foobar\" }, config);\n```\n\n```\n'OK'\n```\n\n## Caching Tasks\n\n```typescript  theme={null}\nimport {\n  InMemoryCache,\n  entrypoint,\n  task,\n  CachePolicy,\n} from \"@langchain/langgraph\";\n\nconst slowAdd = task(\n  {\n    name: \"slowAdd\",\n    cache: { ttl: 120 },   // [!code highlight]\n  },\n  async (x: number) => {\n    await new Promise((resolve) => setTimeout(resolve, 1000));\n    return x * 2;\n  }\n);\n\nconst main = entrypoint(\n  { cache: new InMemoryCache(), name: \"main\" },\n  async (inputs: { x: number }) => {\n    const result1 = await slowAdd(inputs.x);\n    const result2 = await slowAdd(inputs.x);\n    return { result1, result2 };\n  }\n);\n\nfor await (const chunk of await main.stream(\n  { x: 5 },\n  { streamMode: \"updates\" }\n)) {\n  console.log(chunk);\n}\n\n//> { slowAdd: 10 }\n//> { slowAdd: 10, '__metadata__': { cached: true } }\n//> { main: { result1: 10, result2: 10 } }\n```\n\n1. `ttl` is specified in seconds. The cache will be invalidated after this time.\n\n## Resuming after an error\n\n```typescript  theme={null}\nimport { entrypoint, task, MemorySaver } from \"@langchain/langgraph\";\n\n// This variable is just used for demonstration purposes to simulate a network failure.\n// It's not something you will have in your actual code.\nlet attempts = 0;\n\nconst getInfo = task(\"getInfo\", async () => {\n  /**\n   * Simulates a task that fails once before succeeding.\n   * Throws an exception on the first attempt, then returns \"OK\" on subsequent tries.\n   */\n  attempts += 1;\n\n  if (attempts < 2) {\n    throw new Error(\"Failure\"); // Simulate a failure on the first attempt\n  }\n  return \"OK\";\n});\n\n// Initialize an in-memory checkpointer for persistence\nconst checkpointer = new MemorySaver();\n\nconst slowTask = task(\"slowTask\", async () => {\n  /**\n   * Simulates a slow-running task by introducing a 1-second delay.\n   */\n  await new Promise((resolve) => setTimeout(resolve, 1000));\n  return \"Ran slow task.\";\n});\n\nconst main = entrypoint(\n  { checkpointer, name: \"main\" },\n  async (inputs: Record<string, any>) => {\n    /**\n     * Main workflow function that runs the slowTask and getInfo tasks sequentially.\n     *\n     * Parameters:\n     * - inputs: Record<string, any> containing workflow input values.\n     *\n     * The workflow first executes `slowTask` and then attempts to execute `getInfo`,\n     * which will fail on the first invocation.\n     */\n    const slowTaskResult = await slowTask(); // Blocking call to slowTask\n    await getInfo(); // Exception will be raised here on the first attempt\n    return slowTaskResult;\n  }\n);\n\n// Workflow execution configuration with a unique thread identifier\nconst config = {\n  configurable: {\n    thread_id: \"1\", // Unique identifier to track workflow execution\n  },\n};\n\n// This invocation will take ~1 second due to the slowTask execution\ntry {\n  // First invocation will raise an exception due to the `getInfo` task failing\n  await main.invoke({ any_input: \"foobar\" }, config);\n} catch (err) {\n  // Handle the failure gracefully\n}\n```\n\nWhen we resume execution, we won't need to re-run the `slowTask` as its result is already saved in the checkpoint.\n\n```typescript  theme={null}\nawait main.invoke(null, config);\n```\n\n```\n'Ran slow task.'\n```\n\n## Human-in-the-loop\n\nThe functional API supports [human-in-the-loop](/oss/javascript/langgraph/interrupts) workflows using the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) function and the `Command` primitive.\n\n### Basic human-in-the-loop workflow\n\nWe will create three [tasks](/oss/javascript/langgraph/functional-api#task):\n\n1. Append `\"bar\"`.\n2. Pause for human input. When resuming, append human input.\n3. Append `\"qux\"`.\n\n```typescript  theme={null}\nimport { entrypoint, task, interrupt, Command } from \"@langchain/langgraph\";\n\nconst step1 = task(\"step1\", async (inputQuery: string) => {\n  // Append bar\n  return `${inputQuery} bar`;\n});\n\nconst humanFeedback = task(\"humanFeedback\", async (inputQuery: string) => {\n  // Append user input\n  const feedback = interrupt(`Please provide feedback: ${inputQuery}`);\n  return `${inputQuery} ${feedback}`;\n});\n\nconst step3 = task(\"step3\", async (inputQuery: string) => {\n  // Append qux\n  return `${inputQuery} qux`;\n});\n```\n\nWe can now compose these tasks in an [entrypoint](/oss/javascript/langgraph/functional-api#entrypoint):\n\n```typescript  theme={null}\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst graph = entrypoint(\n  { checkpointer, name: \"graph\" },\n  async (inputQuery: string) => {\n    const result1 = await step1(inputQuery);\n    const result2 = await humanFeedback(result1);\n    const result3 = await step3(result2);\n\n    return result3;\n  }\n);\n```\n\n[interrupt()](/oss/javascript/langgraph/interrupts#pause-using-interrupt) is called inside a task, enabling a human to review and edit the output of the previous task. The results of prior tasks-- in this case `step_1`-- are persisted, so that they are not run again following the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html).\n\nLet's send in a query string:\n\n```typescript  theme={null}\nconst config = { configurable: { thread_id: \"1\" } };\n\nfor await (const event of await graph.stream(\"foo\", config)) {\n  console.log(event);\n  console.log(\"\\n\");\n}\n```\n\nNote that we've paused with an [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) after `step_1`. The interrupt provides instructions to resume the run. To resume, we issue a [`Command`](/oss/javascript/langgraph/interrupts#resuming-interrupts) containing the data expected by the `human_feedback` task.\n\n```typescript  theme={null}\n// Continue execution\nfor await (const event of await graph.stream(\n  new Command({ resume: \"baz\" }),\n  config\n)) {\n  console.log(event);\n  console.log(\"\\n\");\n}\n```\n\nAfter resuming, the run proceeds through the remaining step and terminates as expected.\n\n### Review tool calls\n\nTo review tool calls before execution, we add a `review_tool_call` function that calls [`interrupt`](/oss/javascript/langgraph/interrupts#pause-using-interrupt). When this function is called, execution will be paused until we issue a command to resume it.\n\nGiven a tool call, our function will [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html) for human review. At that point we can either:\n\n* Accept the tool call\n* Revise the tool call and continue\n* Generate a custom tool message (e.g., instructing the model to re-format its tool call)\n\n```typescript  theme={null}\nimport { ToolCall } from \"@langchain/core/messages/tool\";\nimport { ToolMessage } from \"@langchain/core/messages\";\n\nfunction reviewToolCall(toolCall: ToolCall): ToolCall | ToolMessage {\n  // Review a tool call, returning a validated version\n  const humanReview = interrupt({\n    question: \"Is this correct?\",\n    tool_call: toolCall,\n  });\n\n  const reviewAction = humanReview.action;\n  const reviewData = humanReview.data;\n\n  if (reviewAction === \"continue\") {\n    return toolCall;\n  } else if (reviewAction === \"update\") {\n    const updatedToolCall = { ...toolCall, args: reviewData };\n    return updatedToolCall;\n  } else if (reviewAction === \"feedback\") {\n    return new ToolMessage({\n      content: reviewData,\n      name: toolCall.name,\n      tool_call_id: toolCall.id,\n    });\n  }\n\n  throw new Error(`Unknown review action: ${reviewAction}`);\n}\n```\n\nWe can now update our [entrypoint](/oss/javascript/langgraph/functional-api#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) supplied by the human. The results of prior tasks — in this case the initial model call — are persisted, so that they are not run again following the [`interrupt`](https://reference.langchain.com/javascript/functions/_langchain_langgraph.index.interrupt.html).\n\n```typescript  theme={null}\nimport {\n  MemorySaver,\n  entrypoint,\n  interrupt,\n  Command,\n  addMessages,\n} from \"@langchain/langgraph\";\nimport { ToolMessage, AIMessage, BaseMessage } from \"@langchain/core/messages\";\n\nconst checkpointer = new MemorySaver();\n\nconst agent = entrypoint(\n  { checkpointer, name: \"agent\" },\n  async (\n    messages: BaseMessage[],\n    previous?: BaseMessage[]\n  ): Promise<BaseMessage> => {\n    if (previous !== undefined) {\n      messages = addMessages(previous, messages);\n    }\n\n    let modelResponse = await callModel(messages);\n    while (true) {\n      if (!modelResponse.tool_calls?.length) {\n        break;\n      }\n\n      // Review tool calls\n      const toolResults: ToolMessage[] = [];\n      const toolCalls: ToolCall[] = [];\n\n      for (let i = 0; i < modelResponse.tool_calls.length; i++) {\n        const review = reviewToolCall(modelResponse.tool_calls[i]);\n        if (review instanceof ToolMessage) {\n          toolResults.push(review);\n        } else {\n          // is a validated tool call\n          toolCalls.push(review);\n          if (review !== modelResponse.tool_calls[i]) {\n            modelResponse.tool_calls[i] = review; // update message\n          }\n        }\n      }\n\n      // Execute remaining tool calls\n      const remainingToolResults = await Promise.all(\n        toolCalls.map((toolCall) => callTool(toolCall))\n      );\n\n      // Append to message list\n      messages = addMessages(messages, [\n        modelResponse,\n        ...toolResults,\n        ...remainingToolResults,\n      ]);\n\n      // Call model again\n      modelResponse = await callModel(messages);\n    }\n\n    // Generate final response\n    messages = addMessages(messages, modelResponse);\n    return entrypoint.final({ value: modelResponse, save: messages });\n  }\n);\n```\n\n## Short-term memory\n\nShort-term memory allows storing information across different **invocations** of the same **thread id**. See [short-term memory](/oss/javascript/langgraph/functional-api#short-term-memory) for more details.\n\n### Manage checkpoints\n\nYou can view and delete the information stored by the checkpointer.\n\n<a id=\"checkpoint\" />\n\n#### View thread state\n\n```typescript  theme={null}\nconst config = {\n  configurable: {\n    thread_id: \"1\",  // [!code highlight]\n    // optionally provide an ID for a specific checkpoint,\n    // otherwise the latest checkpoint is shown\n    // checkpoint_id: \"1f029ca3-1f5b-6704-8004-820c16b69a5a\" [!code highlight]\n  },\n};\nawait graph.getState(config);  // [!code highlight]\n```\n\n```\nStateSnapshot {\n  values: {\n    messages: [\n      HumanMessage { content: \"hi! I'm bob\" },\n      AIMessage { content: \"Hi Bob! How are you doing today?\" },\n      HumanMessage { content: \"what's my name?\" },\n      AIMessage { content: \"Your name is Bob.\" }\n    ]\n  },\n  next: [],\n  config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },\n  metadata: {\n    source: 'loop',\n    writes: { call_model: { messages: AIMessage { content: \"Your name is Bob.\" } } },\n    step: 4,\n    parents: {},\n    thread_id: '1'\n  },\n  createdAt: '2025-05-05T16:01:24.680462+00:00',\n  parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },\n  tasks: [],\n  interrupts: []\n}\n```\n\n<a id=\"checkpoints\" />\n\n#### View the history of the thread\n\n```typescript  theme={null}\nconst config = {\n  configurable: {\n    thread_id: \"1\",  // [!code highlight]\n  },\n};\nconst history = [];  // [!code highlight]\nfor await (const state of graph.getStateHistory(config)) {\n  history.push(state);\n}\n```\n\n```\n[\n  StateSnapshot {\n    values: {\n      messages: [\n        HumanMessage { content: \"hi! I'm bob\" },\n        AIMessage { content: \"Hi Bob! How are you doing today? Is there anything I can help you with?\" },\n        HumanMessage { content: \"what's my name?\" },\n        AIMessage { content: \"Your name is Bob.\" }\n      ]\n    },\n    next: [],\n    config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },\n    metadata: { source: 'loop', writes: { call_model: { messages: AIMessage { content: \"Your name is Bob.\" } } }, step: 4, parents: {}, thread_id: '1' },\n    createdAt: '2025-05-05T16:01:24.680462+00:00',\n    parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },\n    tasks: [],\n    interrupts: []\n  },\n  // ... more state snapshots\n]\n```\n\n### Decouple return value from saved value\n\nUse `entrypoint.final` to decouple what is returned to the caller from what is persisted in the checkpoint. This is useful when:\n\n* You want to return a computed result (e.g., a summary or status), but save a different internal value for use on the next invocation.\n* You need to control what gets passed to the previous parameter on the next run.\n\n```typescript  theme={null}\nimport { entrypoint, MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst accumulate = entrypoint(\n  { checkpointer, name: \"accumulate\" },\n  async (n: number, previous?: number) => {\n    const prev = previous || 0;\n    const total = prev + n;\n    // Return the *previous* value to the caller but save the *new* total to the checkpoint.\n    return entrypoint.final({ value: prev, save: total });\n  }\n);\n\nconst config = { configurable: { thread_id: \"my-thread\" } };\n\nconsole.log(await accumulate.invoke(1, config)); // 0\nconsole.log(await accumulate.invoke(2, config)); // 1\nconsole.log(await accumulate.invoke(3, config)); // 3\n```\n\n### Chatbot example\n\nAn example of a simple chatbot using the functional API and the [`InMemorySaver`](https://reference.langchain.com/javascript/classes/_langchain_langgraph-checkpoint.MemorySaver.html) checkpointer.\n\nThe bot is able to remember the previous conversation and continue from where it left off.\n\n```typescript  theme={null}\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport {\n  addMessages,\n  entrypoint,\n  task,\n  MemorySaver,\n} from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({ model: \"claude-sonnet-4-5-20250929\" });\n\nconst callModel = task(\n  \"callModel\",\n  async (messages: BaseMessage[]): Promise<BaseMessage> => {\n    const response = await model.invoke(messages);\n    return response;\n  }\n);\n\nconst checkpointer = new MemorySaver();\n\nconst workflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (\n    inputs: BaseMessage[],\n    previous?: BaseMessage[]\n  ): Promise<BaseMessage> => {\n    let messages = inputs;\n    if (previous) {\n      messages = addMessages(previous, inputs);\n    }\n\n    const response = await callModel(messages);\n    return entrypoint.final({\n      value: response,\n      save: addMessages(messages, response),\n    });\n  }\n);\n\nconst config = { configurable: { thread_id: \"1\" } };\nconst inputMessage = { role: \"user\", content: \"hi! I'm bob\" };\n\nfor await (const chunk of await workflow.stream([inputMessage], {\n  ...config,\n  streamMode: \"values\",\n})) {\n  console.log(chunk.content);\n}\n\nconst inputMessage2 = { role: \"user\", content: \"what's my name?\" };\nfor await (const chunk of await workflow.stream([inputMessage2], {\n  ...config,\n  streamMode: \"values\",\n})) {\n  console.log(chunk.content);\n}\n```\n\n## Long-term memory\n\n[long-term memory](/oss/javascript/concepts/memory#long-term-memory) allows storing information across different **thread ids**. This could be useful for learning information about a given user in one conversation and using it in another.\n\n## Workflows\n\n* [Workflows and agent](/oss/javascript/langgraph/workflows-agents) guide for more examples of how to build workflows using the Functional API.\n\n## Integrate with other libraries\n\n* [Add LangGraph's features to other frameworks using the functional API](/langsmith/autogen-integration): Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-functional-api.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 27959
}