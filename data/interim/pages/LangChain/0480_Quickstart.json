{
  "title": "Quickstart",
  "source_url": "https://docs.langchain.com/oss/python/langgraph/quickstart",
  "content": "This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\n\n* [Use the Graph API](#use-the-graph-api) if you prefer to define your agent as a graph of nodes and edges.\n* [Use the Functional API](#use-the-functional-api) if you prefer to define your agent as a single function.\n\nFor conceptual information, see [Graph API overview](/oss/python/langgraph/graph-api) and [Functional API overview](/oss/python/langgraph/functional-api).\n\n<Info>\n  For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.\n</Info>\n\n<Tabs>\n  <Tab title=\"Use the Graph API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```python  theme={null}\n    from langchain.tools import tool\n    from langchain.chat_models import init_chat_model\n\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0\n    )\n\n\n    # Define tools\n    @tool\n    def multiply(a: int, b: int) -> int:\n        \"\"\"Multiply `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a * b\n\n\n    @tool\n    def add(a: int, b: int) -> int:\n        \"\"\"Adds `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a + b\n\n\n    @tool\n    def divide(a: int, b: int) -> float:\n        \"\"\"Divide `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a / b\n\n\n    # Augment the LLM with tools\n    tools = [add, multiply, divide]\n    tools_by_name = {tool.name: tool for tool in tools}\n    model_with_tools = model.bind_tools(tools)\n    ```\n\n    ## 2. Define state\n\n    The graph's state is used to store the messages and the number of LLM calls.\n\n    <Tip>\n      State in LangGraph persists throughout the agent's execution.\n\n      The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.\n    </Tip>\n\n    ```python  theme={null}\n    from langchain.messages import AnyMessage\n    from typing_extensions import TypedDict, Annotated\n    import operator\n\n\n    class MessagesState(TypedDict):\n        messages: Annotated[list[AnyMessage], operator.add]\n        llm_calls: int\n    ```\n\n    ## 3. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    ```python  theme={null}\n    from langchain.messages import SystemMessage\n\n\n    def llm_call(state: dict):\n        \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n        return {\n            \"messages\": [\n                model_with_tools.invoke(\n                    [\n                        SystemMessage(\n                            content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                        )\n                    ]\n                    + state[\"messages\"]\n                )\n            ],\n            \"llm_calls\": state.get('llm_calls', 0) + 1\n        }\n    ```\n\n    ## 4. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```python  theme={null}\n    from langchain.messages import ToolMessage\n\n\n    def tool_node(state: dict):\n        \"\"\"Performs the tool call\"\"\"\n\n        result = []\n        for tool_call in state[\"messages\"][-1].tool_calls:\n            tool = tools_by_name[tool_call[\"name\"]]\n            observation = tool.invoke(tool_call[\"args\"])\n            result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n        return {\"messages\": result}\n    ```\n\n    ## 5. Define end logic\n\n    The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.\n\n    ```python  theme={null}\n    from typing import Literal\n    from langgraph.graph import StateGraph, START, END\n\n\n    def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n        \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n\n        # If the LLM makes a tool call, then perform an action\n        if last_message.tool_calls:\n            return \"tool_node\"\n\n        # Otherwise, we stop (reply to the user)\n        return END\n    ```\n\n    ## 6. Build and compile the agent\n\n    The agent is built using the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) class and compiled using the [`compile`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.compile) method.\n\n    ```python  theme={null}\n    # Build workflow\n    agent_builder = StateGraph(MessagesState)\n\n    # Add nodes\n    agent_builder.add_node(\"llm_call\", llm_call)\n    agent_builder.add_node(\"tool_node\", tool_node)\n\n    # Add edges to connect nodes\n    agent_builder.add_edge(START, \"llm_call\")\n    agent_builder.add_conditional_edges(\n        \"llm_call\",\n        should_continue,\n        [\"tool_node\", END]\n    )\n    agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n    # Compile the agent\n    agent = agent_builder.compile()\n\n    # Show the agent\n    from IPython.display import Image, display\n    display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n    # Invoke\n    from langchain.messages import HumanMessage\n    messages = [HumanMessage(content=\"Add 3 and 4.\")]\n    messages = agent.invoke({\"messages\": messages})\n    for m in messages[\"messages\"]:\n        m.pretty_print()\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Graph API.\n\n    <Accordion title=\"Full code example\">\n      ```python  theme={null}\n      # Step 1: Define tools and model\n\n      from langchain.tools import tool\n      from langchain.chat_models import init_chat_model\n\n\n      model = init_chat_model(\n          \"claude-sonnet-4-5-20250929\",\n          temperature=0\n      )\n\n\n      # Define tools\n      @tool\n      def multiply(a: int, b: int) -> int:\n          \"\"\"Multiply `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a * b\n\n\n      @tool\n      def add(a: int, b: int) -> int:\n          \"\"\"Adds `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a + b\n\n\n      @tool\n      def divide(a: int, b: int) -> float:\n          \"\"\"Divide `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a / b\n\n\n      # Augment the LLM with tools\n      tools = [add, multiply, divide]\n      tools_by_name = {tool.name: tool for tool in tools}\n      model_with_tools = model.bind_tools(tools)\n\n      # Step 2: Define state\n\n      from langchain.messages import AnyMessage\n      from typing_extensions import TypedDict, Annotated\n      import operator\n\n\n      class MessagesState(TypedDict):\n          messages: Annotated[list[AnyMessage], operator.add]\n          llm_calls: int\n\n      # Step 3: Define model node\n      from langchain.messages import SystemMessage\n\n\n      def llm_call(state: dict):\n          \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n          return {\n              \"messages\": [\n                  model_with_tools.invoke(\n                      [\n                          SystemMessage(\n                              content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                          )\n                      ]\n                      + state[\"messages\"]\n                  )\n              ],\n              \"llm_calls\": state.get('llm_calls', 0) + 1\n          }\n\n\n      # Step 4: Define tool node\n\n      from langchain.messages import ToolMessage\n\n\n      def tool_node(state: dict):\n          \"\"\"Performs the tool call\"\"\"\n\n          result = []\n          for tool_call in state[\"messages\"][-1].tool_calls:\n              tool = tools_by_name[tool_call[\"name\"]]\n              observation = tool.invoke(tool_call[\"args\"])\n              result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n          return {\"messages\": result}\n\n      # Step 5: Define logic to determine whether to end\n\n      from typing import Literal\n      from langgraph.graph import StateGraph, START, END\n\n\n      # Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n      def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n          \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n          messages = state[\"messages\"]\n          last_message = messages[-1]\n\n          # If the LLM makes a tool call, then perform an action\n          if last_message.tool_calls:\n              return \"tool_node\"\n\n          # Otherwise, we stop (reply to the user)\n          return END\n\n      # Step 6: Build agent\n\n      # Build workflow\n      agent_builder = StateGraph(MessagesState)\n\n      # Add nodes\n      agent_builder.add_node(\"llm_call\", llm_call)\n      agent_builder.add_node(\"tool_node\", tool_node)\n\n      # Add edges to connect nodes\n      agent_builder.add_edge(START, \"llm_call\")\n      agent_builder.add_conditional_edges(\n          \"llm_call\",\n          should_continue,\n          [\"tool_node\", END]\n      )\n      agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n      # Compile the agent\n      agent = agent_builder.compile()\n\n\n      from IPython.display import Image, display\n      # Show the agent\n      display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n      # Invoke\n      from langchain.messages import HumanMessage\n      messages = [HumanMessage(content=\"Add 3 and 4.\")]\n      messages = agent.invoke({\"messages\": messages})\n      for m in messages[\"messages\"]:\n          m.pretty_print()\n\n      ```\n    </Accordion>\n  </Tab>\n\n  <Tab title=\"Use the Functional API\">\n    ## 1. Define tools and model\n\n    In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.\n\n    ```python  theme={null}\n    from langchain.tools import tool\n    from langchain.chat_models import init_chat_model\n\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0\n    )\n\n\n    # Define tools\n    @tool\n    def multiply(a: int, b: int) -> int:\n        \"\"\"Multiply `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a * b\n\n\n    @tool\n    def add(a: int, b: int) -> int:\n        \"\"\"Adds `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a + b\n\n\n    @tool\n    def divide(a: int, b: int) -> float:\n        \"\"\"Divide `a` and `b`.\n\n        Args:\n            a: First int\n            b: Second int\n        \"\"\"\n        return a / b\n\n\n    # Augment the LLM with tools\n    tools = [add, multiply, divide]\n    tools_by_name = {tool.name: tool for tool in tools}\n    model_with_tools = model.bind_tools(tools)\n\n    from langgraph.graph import add_messages\n    from langchain.messages import (\n        SystemMessage,\n        HumanMessage,\n        ToolCall,\n    )\n    from langchain_core.messages import BaseMessage\n    from langgraph.func import entrypoint, task\n    ```\n\n    ## 2. Define model node\n\n    The model node is used to call the LLM and decide whether to call a tool or not.\n\n    <Tip>\n      The [`@task`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.task) decorator marks a function as a task that can be executed as part of the agent. Tasks can be called synchronously or asynchronously within your entrypoint function.\n    </Tip>\n\n    ```python  theme={null}\n    @task\n    def call_llm(messages: list[BaseMessage]):\n        \"\"\"LLM decides whether to call a tool or not\"\"\"\n        return model_with_tools.invoke(\n            [\n                SystemMessage(\n                    content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                )\n            ]\n            + messages\n        )\n    ```\n\n    ## 3. Define tool node\n\n    The tool node is used to call the tools and return the results.\n\n    ```python  theme={null}\n    @task\n    def call_tool(tool_call: ToolCall):\n        \"\"\"Performs the tool call\"\"\"\n        tool = tools_by_name[tool_call[\"name\"]]\n        return tool.invoke(tool_call)\n\n    ```\n\n    ## 4. Define agent\n\n    The agent is built using the [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) function.\n\n    <Note>\n      In the Functional API, instead of defining nodes and edges explicitly, you write standard control flow logic (loops, conditionals) within a single function.\n    </Note>\n\n    ```python  theme={null}\n    @entrypoint()\n    def agent(messages: list[BaseMessage]):\n        model_response = call_llm(messages).result()\n\n        while True:\n            if not model_response.tool_calls:\n                break\n\n            # Execute tools\n            tool_result_futures = [\n                call_tool(tool_call) for tool_call in model_response.tool_calls\n            ]\n            tool_results = [fut.result() for fut in tool_result_futures]\n            messages = add_messages(messages, [model_response, *tool_results])\n            model_response = call_llm(messages).result()\n\n        messages = add_messages(messages, model_response)\n        return messages\n\n    # Invoke\n    messages = [HumanMessage(content=\"Add 3 and 4.\")]\n    for chunk in agent.stream(messages, stream_mode=\"updates\"):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Functional API.\n\n    <Accordion title=\"Full code example\" icon=\"code\">\n      ```python  theme={null}\n      # Step 1: Define tools and model\n\n      from langchain.tools import tool\n      from langchain.chat_models import init_chat_model\n\n\n      model = init_chat_model(\n          \"claude-sonnet-4-5-20250929\",\n          temperature=0\n      )\n\n\n      # Define tools\n      @tool\n      def multiply(a: int, b: int) -> int:\n          \"\"\"Multiply `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a * b\n\n\n      @tool\n      def add(a: int, b: int) -> int:\n          \"\"\"Adds `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a + b\n\n\n      @tool\n      def divide(a: int, b: int) -> float:\n          \"\"\"Divide `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a / b\n\n\n      # Augment the LLM with tools\n      tools = [add, multiply, divide]\n      tools_by_name = {tool.name: tool for tool in tools}\n      model_with_tools = model.bind_tools(tools)\n\n      from langgraph.graph import add_messages\n      from langchain.messages import (\n          SystemMessage,\n          HumanMessage,\n          ToolCall,\n      )\n      from langchain_core.messages import BaseMessage\n      from langgraph.func import entrypoint, task\n\n\n      # Step 2: Define model node\n\n      @task\n      def call_llm(messages: list[BaseMessage]):\n          \"\"\"LLM decides whether to call a tool or not\"\"\"\n          return model_with_tools.invoke(\n              [\n                  SystemMessage(\n                      content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                  )\n              ]\n              + messages\n          )\n\n\n      # Step 3: Define tool node\n\n      @task\n      def call_tool(tool_call: ToolCall):\n          \"\"\"Performs the tool call\"\"\"\n          tool = tools_by_name[tool_call[\"name\"]]\n          return tool.invoke(tool_call)\n\n\n      # Step 4: Define agent\n\n      @entrypoint()\n      def agent(messages: list[BaseMessage]):\n          model_response = call_llm(messages).result()\n\n          while True:\n              if not model_response.tool_calls:\n                  break\n\n              # Execute tools\n              tool_result_futures = [\n                  call_tool(tool_call) for tool_call in model_response.tool_calls\n              ]\n              tool_results = [fut.result() for fut in tool_result_futures]\n              messages = add_messages(messages, [model_response, *tool_results])\n              model_response = call_llm(messages).result()\n\n          messages = add_messages(messages, model_response)\n          return messages\n\n      # Invoke\n      messages = [HumanMessage(content=\"Add 3 and 4.\")]\n      for chunk in agent.stream(messages, stream_mode=\"updates\"):\n          print(chunk)\n          print(\"\\n\")\n      ```\n    </Accordion>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/quickstart.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 17519
}