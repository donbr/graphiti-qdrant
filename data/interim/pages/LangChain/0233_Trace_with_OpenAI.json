{
  "title": "Trace with OpenAI",
  "source_url": "https://docs.langchain.com/langsmith/trace-openai",
  "content": "The `wrap_openai`/`wrapOpenAI` methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. Also note that the wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.\n\n<Note>\n  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_openai` or `wrapOpenAI`. This allows you to toggle tracing on and off without changing your code.\n\n  Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).\n\n  If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.\n\n  By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).\n</Note>\n\n<CodeGroup>\n  ```python Python theme={null}\n  import openai\n  from langsmith import traceable\n  from langsmith.wrappers import wrap_openai\n\n  client = wrap_openai(openai.Client())\n\n  @traceable(run_type=\"tool\", name=\"Retrieve Context\")\n  def my_tool(question: str) -> str:\n    return \"During this morning's meeting, we solved all world conflict.\"\n\n  @traceable(name=\"Chat Pipeline\")\n  def chat_pipeline(question: str):\n    context = my_tool(question)\n    messages = [\n        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n    ]\n    chat_completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\", messages=messages\n    )\n    return chat_completion.choices[0].message.content\n\n  chat_pipeline(\"Can you summarize this morning's meetings?\")\n  ```\n\n  ```typescript TypeScript theme={null}\n  import OpenAI from \"openai\";\n  import { traceable } from \"langsmith/traceable\";\n  import { wrapOpenAI } from \"langsmith/wrappers\";\n\n  const client = wrapOpenAI(new OpenAI());\n\n  const myTool = traceable(async (question: string) => {\n    return \"During this morning's meeting, we solved all world conflict.\";\n  }, { name: \"Retrieve Context\", run_type: \"tool\" });\n\n  const chatPipeline = traceable(async (question: string) => {\n    const context = await myTool(question);\n    const messages = [\n        {\n            role: \"system\",\n            content:\n                \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",\n        },\n        { role: \"user\", content: `Question: ${question} Context: ${context}` },\n    ];\n    const chatCompletion = await client.chat.completions.create({\n        model: \"gpt-4o-mini\",\n        messages: messages,\n    });\n    return chatCompletion.choices[0].message.content;\n  }, { name: \"Chat Pipeline\" });\n\n  await chatPipeline(\"Can you summarize this morning's meetings?\");\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-openai.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 3566
}