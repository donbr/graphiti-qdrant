{
  "title": "Build a SQL agent",
  "source_url": "https://docs.langchain.com/oss/python/langchain/sql-agent",
  "content": "## Overview\n\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain [agents](/oss/python/langchain/agents).\n\nAt a high level, the agent will:\n\n<Steps>\n  <Step title=\"Fetch the available tables and schemas from the database\" />\n\n  <Step title=\"Decide which tables are relevant to the question\" />\n\n  <Step title=\"Fetch the schemas for the relevant tables\" />\n\n  <Step title=\"Generate a query based on the question and information from the schemas\" />\n\n  <Step title=\"Double-check the query for common mistakes using an LLM\" />\n\n  <Step title=\"Execute the query and return the results\" />\n\n  <Step title=\"Correct mistakes surfaced by the database engine until the query is successful\" />\n\n  <Step title=\"Formulate a response based on the results\" />\n</Steps>\n\n<Warning>\n  Building Q\\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\n</Warning>\n\n### Concepts\n\nWe will cover the following concepts:\n\n* [Tools](/oss/python/langchain/tools) for reading from SQL databases\n* LangChain [agents](/oss/python/langchain/agents)\n* [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) processes\n\n## Setup\n\n### Installation\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install langchain  langgraph  langchain-community\n  ```\n</CodeGroup>\n\n### LangSmith\n\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:\n\n```shell  theme={null}\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n## 1. Select an LLM\n\nSelect a model that supports [tool-calling](/oss/python/integrations/providers/overview):\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    ðŸ‘‰ Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)\n\n    ```shell theme={null} theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null} theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null} theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    ðŸ‘‰ Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)\n\n    ```shell theme={null} theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null} theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null} theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    ðŸ‘‰ Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell theme={null} theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null} theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null} theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    ðŸ‘‰ Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell theme={null} theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null} theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null} theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"AWS Bedrock\">\n    ðŸ‘‰ Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)\n\n    ```shell theme={null} theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null} theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null} theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    </CodeGroup>\n\n    <Tab title=\"HuggingFace\">\n      ðŸ‘‰ Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)\n\n      ```shell theme={null} theme={null}\n      pip install -U \"langchain[huggingface]\"\n      ```\n\n      <CodeGroup>\n        ```python init_chat_model theme={null} theme={null}\n        import os\n        from langchain.chat_models import init_chat_model\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        model = init_chat_model(\n            \"microsoft/Phi-3-mini-4k-instruct\",\n            model_provider=\"huggingface\",\n            temperature=0.7,\n            max_tokens=1024,\n        )\n        ```\n\n        ```python Model Class theme={null} theme={null}\n        import os\n        from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n        llm = HuggingFaceEndpoint(\n            repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n            temperature=0.7,\n            max_length=1024,\n        )\n        model = ChatHuggingFace(llm=llm)\n        ```\n      </CodeGroup>\n    </Tab>\n  </Tab>\n</Tabs>\n\nThe output shown in the examples below used OpenAI.\n\n## 2. Configure the database\n\nYou will be creating a [SQLite database](https://www.sqlitetutorial.net/sqlite-sample-database/) for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store.\n\nFor convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket.\n\n```python  theme={null}\nimport requests, pathlib\n\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\nlocal_path = pathlib.Path(\"Chinook.db\")\n\nif local_path.exists():\n    print(f\"{local_path} already exists, skipping download.\")\nelse:\n    response = requests.get(url)\n    if response.status_code == 200:\n        local_path.write_bytes(response.content)\n        print(f\"File downloaded and saved as {local_path}\")\n    else:\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\n```\n\nWe will use a handy SQL database wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```python  theme={null}\nfrom langchain_community.utilities import SQLDatabase\n\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\nprint(f\"Dialect: {db.dialect}\")\nprint(f\"Available tables: {db.get_usable_table_names()}\")\nprint(f'Sample output: {db.run(\"SELECT * FROM Artist LIMIT 5;\")}')\n```\n\n```\nDialect: sqlite\nAvailable tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\nSample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\n```\n\n## 3. Add tools for database interactions\n\nUse the `SQLDatabase` wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```python  theme={null}\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\n\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\n\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(f\"{tool.name}: {tool.description}\\n\")\n```\n\n```\nsql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\n\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\n\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\n\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n```\n\n## 4. Use `create_agent`\n\nUse [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) to build a [ReAct agent](https://arxiv.org/pdf/2210.03629) with minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\n\nInitialize the agent with a descriptive system prompt to customize its behavior:\n\n```python  theme={null}\nsystem_prompt = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\n\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\n\nThen you should query the schema of the most relevant tables.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n```\n\nNow, create an agent with the model, tools, and prompt:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n)\n```\n\n## 5. Run the agent\n\nRun the agent on a sample query and observe its behavior:\n\n```python  theme={null}\nquestion = \"Which genre on average has the longest tracks?\"\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================ Human Message =================================\n\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_list_tables (call_BQsWg8P65apHc8BTJ1NPDvnM)\n Call ID: call_BQsWg8P65apHc8BTJ1NPDvnM\n  Args:\n================================= Tool Message =================================\nName: sql_db_list_tables\n\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_schema (call_i89tjKECFSeERbuACYm4w0cU)\n Call ID: call_i89tjKECFSeERbuACYm4w0cU\n  Args:\n    table_names: Track, Genre\n================================= Tool Message =================================\nName: sql_db_schema\n\n\nCREATE TABLE \"Genre\" (\n\t\"GenreId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(120),\n\tPRIMARY KEY (\"GenreId\")\n)\n\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\n\n\nCREATE TABLE \"Track\" (\n\t\"TrackId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(200) NOT NULL,\n\t\"AlbumId\" INTEGER,\n\t\"MediaTypeId\" INTEGER NOT NULL,\n\t\"GenreId\" INTEGER,\n\t\"Composer\" NVARCHAR(220),\n\t\"Milliseconds\" INTEGER NOT NULL,\n\t\"Bytes\" INTEGER,\n\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\n\tPRIMARY KEY (\"TrackId\"),\n\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\n\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\n\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query_checker (call_G64yYm6R6UauiVPCXJZMA49b)\n Call ID: call_G64yYm6R6UauiVPCXJZMA49b\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query_checker\n\nSELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_AnO3SrhD0ODJBxh6dHMwvHwZ)\n Call ID: call_AnO3SrhD0ODJBxh6dHMwvHwZ\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nOn average, the genre with the longest tracks is \"Sci Fi & Fantasy\" with an average track length of approximately 2,911,783 milliseconds. This is followed by \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\n```\n\nThe agent correctly wrote a query, checked the query, and ran it to inform its final response.\n\n<Note>\n  You can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the [LangSmith trace](https://smith.langchain.com/public/cd2ce887-388a-4bb1-a29d-48208ce50d15/r).\n</Note>\n\n### (Optional) Use Studio\n\n[Studio](/langsmith/studio) provides a \"client side\" loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \"Tell me the scheme of the database\" or \"Show me the invoices for the 5 top customers\". You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\n\n<Accordion title=\"Run your agent in Studio\">\n  In addition to the previously mentioned packages, you will need to:\n\n  ```shell  theme={null}\n  pip install -U langgraph-cli[inmem]>=0.4.0\n  ```\n\n  In directory you will run in, you will need a `langgraph.json` file with the following contents:\n\n  ```json  theme={null}\n  {\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"agent\": \"./sql_agent.py:agent\",\n        \"graph\": \"./sql_agent_langgraph.py:graph\"\n    },\n    \"env\": \".env\"\n  }\n  ```\n\n  Create a file `sql_agent.py` and insert this:\n\n  ```python  theme={null}\n  #sql_agent.py for studio\n  import pathlib\n\n  from langchain.agents import create_agent\n  from langchain.chat_models import init_chat_model\n  from langchain_community.agent_toolkits import SQLDatabaseToolkit\n  from langchain_community.utilities import SQLDatabase\n  import requests\n\n\n  # Initialize an LLM\n  model = init_chat_model(\"gpt-4.1\")\n\n  # Get the database, store it locally\n  url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\n  local_path = pathlib.Path(\"Chinook.db\")\n\n  if local_path.exists():\n      print(f\"{local_path} already exists, skipping download.\")\n  else:\n      response = requests.get(url)\n      if response.status_code == 200:\n          local_path.write_bytes(response.content)\n          print(f\"File downloaded and saved as {local_path}\")\n      else:\n          print(f\"Failed to download the file. Status code: {response.status_code}\")\n\n  db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\n  # Create the tools\n  toolkit = SQLDatabaseToolkit(db=db, llm=model)\n\n  tools = toolkit.get_tools()\n\n  for tool in tools:\n      print(f\"{tool.name}: {tool.description}\\n\")\n\n  # Use create_agent\n  system_prompt = \"\"\"\n  You are an agent designed to interact with a SQL database.\n  Given an input question, create a syntactically correct {dialect} query to run,\n  then look at the results of the query and return the answer. Unless the user\n  specifies a specific number of examples they wish to obtain, always limit your\n  query to at most {top_k} results.\n\n  You can order the results by a relevant column to return the most interesting\n  examples in the database. Never query for all the columns from a specific table,\n  only ask for the relevant columns given the question.\n\n  You MUST double check your query before executing it. If you get an error while\n  executing a query, rewrite the query and try again.\n\n  DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\n  database.\n\n  To start you should ALWAYS look at the tables in the database to see what you\n  can query. Do NOT skip this step.\n\n  Then you should query the schema of the most relevant tables.\n  \"\"\".format(\n      dialect=db.dialect,\n      top_k=5,\n  )\n\n  agent = create_agent(\n      model,\n      tools,\n      system_prompt=system_prompt,\n  )\n  ```\n</Accordion>\n\n## 6. Implement human-in-the-loop review\n\nIt can be prudent to check the agent's SQL queries before they are executed for any unintended actions or inefficiencies.\n\nLangChain agents feature support for built-in [human-in-the-loop middleware](/oss/python/langchain/human-in-the-loop) to add oversight to agent tool calls. Let's configure the agent to pause for human review on calling the `sql_db_query` tool:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver # [!code highlight]\n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n    middleware=[ # [!code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\"sql_db_query\": True}, # [!code highlight]\n            description_prefix=\"Tool execution pending approval\", # [!code highlight]\n        ), # [!code highlight]\n    ], # [!code highlight]\n    checkpointer=InMemorySaver(), # [!code highlight]\n)\n```\n\n<Note>\n  We've added a [checkpointer](/oss/python/langchain/short-term-memory) to our agent to allow execution to be paused and resumed. See the [human-in-the-loop guide](/oss/python/langchain/human-in-the-loop) for detalis on this as well as available middleware configurations.\n</Note>\n\nOn running the agent, it will now pause for review before executing the `sql_db_query` tool:\n\n```python  theme={null}\nquestion = \"Which genre on average has the longest tracks?\"\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} # [!code highlight]\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    config, # [!code highlight]\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step: # [!code highlight]\n        print(\"INTERRUPTED:\") # [!code highlight]\n        interrupt = step[\"__interrupt__\"][0] # [!code highlight]\n        for request in interrupt.value[\"action_requests\"]: # [!code highlight]\n            print(request[\"description\"]) # [!code highlight]\n    else:\n        pass\n```\n\n```\n...\n\nINTERRUPTED:\nTool execution pending approval\n\nTool: sql_db_query\nArgs: {'query': 'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;'}\n```\n\nWe can resume execution, in this case accepting the query, using [Command](/oss/python/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command):\n\n```python  theme={null}\nfrom langgraph.types import Command # [!code highlight]\n\nfor step in agent.stream(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), # [!code highlight]\n    config,\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step:\n        print(\"INTERRUPTED:\")\n        interrupt = step[\"__interrupt__\"][0]\n        for request in interrupt.value[\"action_requests\"]:\n            print(request[\"description\"])\n    else:\n        pass\n```\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)\n Call ID: call_7oz86Epg7lYRqi9rQHbZPS1U\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average duration of about 2,911,783 milliseconds, followed by \"Science Fiction\" and \"Drama.\"\n```\n\nRefer to the [human-in-the-loop guide](/oss/python/langchain/human-in-the-loop) for details.\n\n## Next steps\n\nFor deeper customization, check out [this tutorial](/oss/python/langgraph/sql-agent) for implementing a SQL agent directly using LangGraph primitives.\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/sql-agent.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 24586
}