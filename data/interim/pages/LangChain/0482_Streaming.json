{
  "title": "Streaming",
  "source_url": "https://docs.langchain.com/oss/python/langgraph/streaming",
  "content": "LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\nWhat's possible with LangGraph streaming:\n\n* <Icon icon=\"share-nodes\" size={16} /> [**Stream graph state**](#stream-graph-state) — get state updates / values with `updates` and `values` modes.\n* <Icon icon=\"square-poll-horizontal\" size={16} /> [**Stream subgraph outputs**](#stream-subgraph-outputs) — include outputs from both the parent graph and any nested subgraphs.\n* <Icon icon=\"square-binary\" size={16} /> [**Stream LLM tokens**](#messages) — capture token streams from anywhere: inside nodes, subgraphs, or tools.\n* <Icon icon=\"table\" size={16} /> [**Stream custom data**](#stream-custom-data) — send custom updates or progress signals directly from tool functions.\n* <Icon icon=\"layer-plus\" size={16} /> [**Use multiple streaming modes**](#stream-multiple-modes) — choose from `values` (full state), `updates` (state deltas), `messages` (LLM tokens + metadata), `custom` (arbitrary user data), or `debug` (detailed traces).\n\n## Supported stream modes\n\nPass one or more of the following stream modes as a list to the [`stream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods:\n\n| Mode       | Description                                                                                                                                                                         |\n| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `values`   | Streams the full value of the state after each step of the graph.                                                                                                                   |\n| `updates`  | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |\n| `custom`   | Streams custom data from inside your graph nodes.                                                                                                                                   |\n| `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.                                                                                                |\n| `debug`    | Streams as much information as possible throughout the execution of the graph.                                                                                                      |\n\n## Basic usage example\n\nLangGraph graphs expose the [`stream`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) (sync) and [`astream`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.astream) (async) methods to yield streamed outputs as iterators.\n\n```python  theme={null}\nfor chunk in graph.stream(inputs, stream_mode=\"updates\"):\n    print(chunk)\n```\n\n<Accordion title=\"Extended example: streaming updates\">\n  ```python  theme={null}\n  from typing import TypedDict\n  from langgraph.graph import StateGraph, START, END\n\n  class State(TypedDict):\n      topic: str\n      joke: str\n\n  def refine_topic(state: State):\n      return {\"topic\": state[\"topic\"] + \" and cats\"}\n\n  def generate_joke(state: State):\n      return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\n  graph = (\n      StateGraph(State)\n      .add_node(refine_topic)\n      .add_node(generate_joke)\n      .add_edge(START, \"refine_topic\")\n      .add_edge(\"refine_topic\", \"generate_joke\")\n      .add_edge(\"generate_joke\", END)\n      .compile()\n  )\n\n  # The stream() method returns an iterator that yields streamed outputs\n  for chunk in graph.stream(  # [!code highlight]\n      {\"topic\": \"ice cream\"},\n      # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\n      # Other stream modes are also available. See supported stream modes for details\n      stream_mode=\"updates\",  # [!code highlight]\n  ):\n      print(chunk)\n  ```\n\n  ```output  theme={null}\n  {'refineTopic': {'topic': 'ice cream and cats'}}\n  {'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}\n  ```\n</Accordion>\n\n## Stream multiple modes\n\nYou can pass a list as the `stream_mode` parameter to stream multiple modes at once.\n\nThe streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.\n\n```python  theme={null}\nfor mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\n    print(chunk)\n```\n\n## Stream graph state\n\nUse the stream modes `updates` and `values` to stream the state of the graph as it executes.\n\n* `updates` streams the **updates** to the state after each step of the graph.\n* `values` streams the **full value** of the state after each step of the graph.\n\n```python  theme={null}\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n  topic: str\n  joke: str\n\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n  StateGraph(State)\n  .add_node(refine_topic)\n  .add_node(generate_joke)\n  .add_edge(START, \"refine_topic\")\n  .add_edge(\"refine_topic\", \"generate_joke\")\n  .add_edge(\"generate_joke\", END)\n  .compile()\n)\n```\n\n<Tabs>\n  <Tab title=\"updates\">\n    Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.\n\n    ```python  theme={null}\n    for chunk in graph.stream(\n        {\"topic\": \"ice cream\"},\n        stream_mode=\"updates\",  # [!code highlight]\n    ):\n        print(chunk)\n    ```\n  </Tab>\n\n  <Tab title=\"values\">\n    Use this to stream the **full state** of the graph after each step.\n\n    ```python  theme={null}\n    for chunk in graph.stream(\n        {\"topic\": \"ice cream\"},\n        stream_mode=\"values\",  # [!code highlight]\n    ):\n        print(chunk)\n    ```\n  </Tab>\n</Tabs>\n\n## Stream subgraph outputs\n\nTo include outputs from [subgraphs](/oss/python/langgraph/use-subgraphs) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\n\nThe outputs will be streamed as tuples `(namespace, data)`, where `namespace` is a tuple with the path to the node where a subgraph is invoked, e.g. `(\"parent_node:<task_id>\", \"child_node:<task_id>\")`.\n\n```python  theme={null}\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    # Set subgraphs=True to stream outputs from subgraphs\n    subgraphs=True,  # [!code highlight]\n    stream_mode=\"updates\",\n):\n    print(chunk)\n```\n\n<Accordion title=\"Extended example: streaming from subgraphs\">\n  ```python  theme={null}\n  from langgraph.graph import START, StateGraph\n  from typing import TypedDict\n\n  # Define subgraph\n  class SubgraphState(TypedDict):\n      foo: str  # note that this key is shared with the parent graph state\n      bar: str\n\n  def subgraph_node_1(state: SubgraphState):\n      return {\"bar\": \"bar\"}\n\n  def subgraph_node_2(state: SubgraphState):\n      return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\n  subgraph_builder = StateGraph(SubgraphState)\n  subgraph_builder.add_node(subgraph_node_1)\n  subgraph_builder.add_node(subgraph_node_2)\n  subgraph_builder.add_edge(START, \"subgraph_node_1\")\n  subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n  subgraph = subgraph_builder.compile()\n\n  # Define parent graph\n  class ParentState(TypedDict):\n      foo: str\n\n  def node_1(state: ParentState):\n      return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n  builder = StateGraph(ParentState)\n  builder.add_node(\"node_1\", node_1)\n  builder.add_node(\"node_2\", subgraph)\n  builder.add_edge(START, \"node_1\")\n  builder.add_edge(\"node_1\", \"node_2\")\n  graph = builder.compile()\n\n  for chunk in graph.stream(\n      {\"foo\": \"foo\"},\n      stream_mode=\"updates\",\n      # Set subgraphs=True to stream outputs from subgraphs\n      subgraphs=True,  # [!code highlight]\n  ):\n      print(chunk)\n  ```\n\n  ```\n  ((), {'node_1': {'foo': 'hi! foo'}})\n  (('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})\n  (('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n  ((), {'node_2': {'foo': 'hi! foobar'}})\n  ```\n\n  **Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\n</Accordion>\n\n<a id=\"debug\" />\n\n### Debugging\n\nUse the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\n\n```python  theme={null}\nfor chunk in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"debug\",  # [!code highlight]\n):\n    print(chunk)\n```\n\n<a id=\"messages\" />\n\n## LLM tokens\n\nUse the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.\n\nThe streamed output from [`messages` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:\n\n* `message_chunk`: the token or message segment from the LLM.\n* `metadata`: a dictionary containing details about the graph node and LLM invocation.\n\n> If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.\n\n<Warning>\n  **Manual config required for async in Python \\< 3.11**\n  When using Python \\< 3.11 with async code, you must explicitly pass [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) to `ainvoke()` to enable proper streaming. See [Async with Python \\< 3.11](#async) for details or upgrade to Python 3.11+.\n</Warning>\n\n```python  theme={null}\nfrom dataclasses import dataclass\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START\n\n\n@dataclass\nclass MyState:\n    topic: str\n    joke: str = \"\"\n\n\nmodel = init_chat_model(model=\"gpt-4o-mini\")\n\ndef call_model(state: MyState):\n    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\n    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream\n    model_response = model.invoke(  # [!code highlight]\n        [\n            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\n        ]\n    )\n    return {\"joke\": model_response.content}\n\ngraph = (\n    StateGraph(MyState)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)\n\n# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor message_chunk, metadata in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"messages\",  # [!code highlight]\n):\n    if message_chunk.content:\n        print(message_chunk.content, end=\"|\", flush=True)\n```\n\n#### Filter by LLM invocation\n\nYou can associate `tags` with LLM invocations to filter the streamed tokens by LLM invocation.\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\n\n# model_1 is tagged with \"joke\"\nmodel_1 = init_chat_model(model=\"gpt-4o-mini\", tags=['joke'])\n# model_2 is tagged with \"poem\"\nmodel_2 = init_chat_model(model=\"gpt-4o-mini\", tags=['poem'])\n\ngraph = ... # define a graph that uses these LLMs\n\n# The stream_mode is set to \"messages\" to stream LLM tokens\n# The metadata contains information about the LLM invocation, including the tags\nasync for msg, metadata in graph.astream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"messages\",  # [!code highlight]\n):\n    # Filter the streamed tokens by the tags field in the metadata to only include\n    # the tokens from the LLM invocation with the \"joke\" tag\n    if metadata[\"tags\"] == [\"joke\"]:\n        print(msg.content, end=\"|\", flush=True)\n```\n\n<Accordion title=\"Extended example: filtering by tags\">\n  ```python  theme={null}\n  from typing import TypedDict\n\n  from langchain.chat_models import init_chat_model\n  from langgraph.graph import START, StateGraph\n\n  # The joke_model is tagged with \"joke\"\n  joke_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"joke\"])\n  # The poem_model is tagged with \"poem\"\n  poem_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"poem\"])\n\n\n  class State(TypedDict):\n        topic: str\n        joke: str\n        poem: str\n\n\n  async def call_model(state, config):\n        topic = state[\"topic\"]\n        print(\"Writing joke...\")\n        # Note: Passing the config through explicitly is required for python < 3.11\n        # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n        # The config is passed through explicitly to ensure the context vars are propagated correctly\n        # This is required for Python < 3.11 when using async code. Please see the async section for more details\n        joke_response = await joke_model.ainvoke(\n              [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n              config,\n        )\n        print(\"\\n\\nWriting poem...\")\n        poem_response = await poem_model.ainvoke(\n              [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n              config,\n        )\n        return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n\n\n  graph = (\n        StateGraph(State)\n        .add_node(call_model)\n        .add_edge(START, \"call_model\")\n        .compile()\n  )\n\n  # The stream_mode is set to \"messages\" to stream LLM tokens\n  # The metadata contains information about the LLM invocation, including the tags\n  async for msg, metadata in graph.astream(\n        {\"topic\": \"cats\"},\n        stream_mode=\"messages\",\n  ):\n      if metadata[\"tags\"] == [\"joke\"]:\n          print(msg.content, end=\"|\", flush=True)\n  ```\n</Accordion>\n\n#### Filter by node\n\nTo stream tokens only from specific nodes, use `stream_mode=\"messages\"` and filter the outputs by the `langgraph_node` field in the streamed metadata:\n\n```python  theme={null}\n# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor msg, metadata in graph.stream(\n    inputs,\n    stream_mode=\"messages\",  # [!code highlight]\n):\n    # Filter the streamed tokens by the langgraph_node field in the metadata\n    # to only include the tokens from the specified node\n    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\":\n        ...\n```\n\n<Accordion title=\"Extended example: streaming LLM tokens from specific nodes\">\n  ```python  theme={null}\n  from typing import TypedDict\n  from langgraph.graph import START, StateGraph\n  from langchain_openai import ChatOpenAI\n\n  model = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\n  class State(TypedDict):\n        topic: str\n        joke: str\n        poem: str\n\n\n  def write_joke(state: State):\n        topic = state[\"topic\"]\n        joke_response = model.invoke(\n              [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n        )\n        return {\"joke\": joke_response.content}\n\n\n  def write_poem(state: State):\n        topic = state[\"topic\"]\n        poem_response = model.invoke(\n              [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n        )\n        return {\"poem\": poem_response.content}\n\n\n  graph = (\n        StateGraph(State)\n        .add_node(write_joke)\n        .add_node(write_poem)\n        # write both the joke and the poem concurrently\n        .add_edge(START, \"write_joke\")\n        .add_edge(START, \"write_poem\")\n        .compile()\n  )\n\n  # The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n  # where message_chunk is the token streamed by the LLM and metadata is a dictionary\n  # with information about the graph node where the LLM was called and other information\n  for msg, metadata in graph.stream(\n      {\"topic\": \"cats\"},\n      stream_mode=\"messages\",  # [!code highlight]\n  ):\n      # Filter the streamed tokens by the langgraph_node field in the metadata\n      # to only include the tokens from the write_poem node\n      if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\n          print(msg.content, end=\"|\", flush=True)\n  ```\n</Accordion>\n\n## Stream custom data\n\nTo send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:\n\n1. Use [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) to access the stream writer and emit custom data.\n2. Set `stream_mode=\"custom\"` when calling `.stream()` or `.astream()` to get the custom data in the stream. You can combine multiple modes (e.g., `[\"updates\", \"custom\"]`), but at least one must be `\"custom\"`.\n\n<Warning>\n  **No [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) in async for Python \\< 3.11**\n  In async code running on Python \\< 3.11, [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) will not work.\n  Instead, add a `writer` parameter to your node or tool and pass it manually.\n  See [Async with Python \\< 3.11](#async) for usage examples.\n</Warning>\n\n<Tabs>\n  <Tab title=\"node\">\n    ```python  theme={null}\n    from typing import TypedDict\n    from langgraph.config import get_stream_writer\n    from langgraph.graph import StateGraph, START\n\n    class State(TypedDict):\n        query: str\n        answer: str\n\n    def node(state: State):\n        # Get the stream writer to send custom data\n        writer = get_stream_writer()\n        # Emit a custom key-value pair (e.g., progress update)\n        writer({\"custom_key\": \"Generating custom data inside node\"})\n        return {\"answer\": \"some data\"}\n\n    graph = (\n        StateGraph(State)\n        .add_node(node)\n        .add_edge(START, \"node\")\n        .compile()\n    )\n\n    inputs = {\"query\": \"example\"}\n\n    # Set stream_mode=\"custom\" to receive the custom data in the stream\n    for chunk in graph.stream(inputs, stream_mode=\"custom\"):\n        print(chunk)\n    ```\n  </Tab>\n\n  <Tab title=\"tool\">\n    ```python  theme={null}\n    from langchain.tools import tool\n    from langgraph.config import get_stream_writer\n\n    @tool\n    def query_database(query: str) -> str:\n        \"\"\"Query the database.\"\"\"\n        # Access the stream writer to send custom data\n        writer = get_stream_writer()  # [!code highlight]\n        # Emit a custom key-value pair (e.g., progress update)\n        writer({\"data\": \"Retrieved 0/100 records\", \"type\": \"progress\"})  # [!code highlight]\n        # perform query\n        # Emit another custom key-value pair\n        writer({\"data\": \"Retrieved 100/100 records\", \"type\": \"progress\"})\n        return \"some-answer\"\n\n\n    graph = ... # define a graph that uses this tool\n\n    # Set stream_mode=\"custom\" to receive the custom data in the stream\n    for chunk in graph.stream(inputs, stream_mode=\"custom\"):\n        print(chunk)\n    ```\n  </Tab>\n</Tabs>\n\n## Use with any LLM\n\nYou can use `stream_mode=\"custom\"` to stream data from **any LLM API** — even if that API does **not** implement the LangChain chat model interface.\n\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\n\n```python  theme={null}\nfrom langgraph.config import get_stream_writer\n\ndef call_arbitrary_model(state):\n    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\n    # Get the stream writer to send custom data\n    writer = get_stream_writer()  # [!code highlight]\n    # Assume you have a streaming client that yields chunks\n    # Generate LLM tokens using your custom streaming client\n    for chunk in your_custom_streaming_client(state[\"topic\"]):\n        # Use the writer to send custom data to the stream\n        writer({\"custom_llm_chunk\": chunk})  # [!code highlight]\n    return {\"result\": \"completed\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_arbitrary_model)\n    # Add other nodes and edges as needed\n    .compile()\n)\n# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor chunk in graph.stream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"custom\",  # [!code highlight]\n\n):\n    # The chunk will contain the custom data streamed from the llm\n    print(chunk)\n```\n\n<Accordion title=\"Extended example: streaming arbitrary chat model\">\n  ```python  theme={null}\n  import operator\n  import json\n\n  from typing import TypedDict\n  from typing_extensions import Annotated\n  from langgraph.graph import StateGraph, START\n\n  from openai import AsyncOpenAI\n\n  openai_client = AsyncOpenAI()\n  model_name = \"gpt-4o-mini\"\n\n\n  async def stream_tokens(model_name: str, messages: list[dict]):\n      response = await openai_client.chat.completions.create(\n          messages=messages, model=model_name, stream=True\n      )\n      role = None\n      async for chunk in response:\n          delta = chunk.choices[0].delta\n\n          if delta.role is not None:\n              role = delta.role\n\n          if delta.content:\n              yield {\"role\": role, \"content\": delta.content}\n\n\n  # this is our tool\n  async def get_items(place: str) -> str:\n      \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n      writer = get_stream_writer()\n      response = \"\"\n      async for msg_chunk in stream_tokens(\n          model_name,\n          [\n              {\n                  \"role\": \"user\",\n                  \"content\": (\n                      \"Can you tell me what kind of items \"\n                      f\"i might find in the following place: '{place}'. \"\n                      \"List at least 3 such items separating them by a comma. \"\n                      \"And include a brief description of each item.\"\n                  ),\n              }\n          ],\n      ):\n          response += msg_chunk[\"content\"]\n          writer(msg_chunk)\n\n      return response\n\n\n  class State(TypedDict):\n      messages: Annotated[list[dict], operator.add]\n\n\n  # this is the tool-calling graph node\n  async def call_tool(state: State):\n      ai_message = state[\"messages\"][-1]\n      tool_call = ai_message[\"tool_calls\"][-1]\n\n      function_name = tool_call[\"function\"][\"name\"]\n      if function_name != \"get_items\":\n          raise ValueError(f\"Tool {function_name} not supported\")\n\n      function_arguments = tool_call[\"function\"][\"arguments\"]\n      arguments = json.loads(function_arguments)\n\n      function_response = await get_items(**arguments)\n      tool_message = {\n          \"tool_call_id\": tool_call[\"id\"],\n          \"role\": \"tool\",\n          \"name\": function_name,\n          \"content\": function_response,\n      }\n      return {\"messages\": [tool_message]}\n\n\n  graph = (\n      StateGraph(State)\n      .add_node(call_tool)\n      .add_edge(START, \"call_tool\")\n      .compile()\n  )\n  ```\n\n  Let's invoke the graph with an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) that includes a tool call:\n\n  ```python  theme={null}\n  inputs = {\n      \"messages\": [\n          {\n              \"content\": None,\n              \"role\": \"assistant\",\n              \"tool_calls\": [\n                  {\n                      \"id\": \"1\",\n                      \"function\": {\n                          \"arguments\": '{\"place\":\"bedroom\"}',\n                          \"name\": \"get_items\",\n                      },\n                      \"type\": \"function\",\n                  }\n              ],\n          }\n      ]\n  }\n\n  async for chunk in graph.astream(\n      inputs,\n      stream_mode=\"custom\",\n  ):\n      print(chunk[\"content\"], end=\"|\", flush=True)\n  ```\n</Accordion>\n\n## Disable streaming for specific chat models\n\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\nmodels that do not support it.\n\nSet `disable_streaming=True` when initializing the model.\n\n<Tabs>\n  <Tab title=\"init_chat_model\">\n    ```python  theme={null}\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        # Set disable_streaming=True to disable streaming for the chat model\n        disable_streaming=True  # [!code highlight]\n\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Chat model interface\">\n    ```python  theme={null}\n    from langchain_openai import ChatOpenAI\n\n    # Set disable_streaming=True to disable streaming for the chat model\n    model = ChatOpenAI(model=\"o1-preview\", disable_streaming=True)\n    ```\n  </Tab>\n</Tabs>\n\n<a id=\"async\" />\n\n### Async with Python \\< 3.11\n\nIn Python versions \\< 3.11, [asyncio tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) do not support the `context` parameter.\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph's streaming mechanisms in two key ways:\n\n1. You **must** explicitly pass [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) into async LLM calls (e.g., `ainvoke()`), as callbacks are not automatically propagated.\n2. You **cannot** use [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) in async nodes or tools — you must pass a `writer` argument directly.\n\n<Accordion title=\"Extended example: async LLM call with manual config\">\n  ```python  theme={null}\n  from typing import TypedDict\n  from langgraph.graph import START, StateGraph\n  from langchain.chat_models import init_chat_model\n\n  model = init_chat_model(model=\"gpt-4o-mini\")\n\n  class State(TypedDict):\n      topic: str\n      joke: str\n\n  # Accept config as an argument in the async node function\n  async def call_model(state, config):\n      topic = state[\"topic\"]\n      print(\"Generating joke...\")\n      # Pass config to model.ainvoke() to ensure proper context propagation\n      joke_response = await model.ainvoke(  # [!code highlight]\n          [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n          config,\n      )\n      return {\"joke\": joke_response.content}\n\n  graph = (\n      StateGraph(State)\n      .add_node(call_model)\n      .add_edge(START, \"call_model\")\n      .compile()\n  )\n\n  # Set stream_mode=\"messages\" to stream LLM tokens\n  async for chunk, metadata in graph.astream(\n      {\"topic\": \"ice cream\"},\n      stream_mode=\"messages\",  # [!code highlight]\n  ):\n      if chunk.content:\n          print(chunk.content, end=\"|\", flush=True)\n  ```\n</Accordion>\n\n<Accordion title=\"Extended example: async custom streaming with stream writer\">\n  ```python  theme={null}\n  from typing import TypedDict\n  from langgraph.types import StreamWriter\n\n  class State(TypedDict):\n        topic: str\n        joke: str\n\n  # Add writer as an argument in the function signature of the async node or tool\n  # LangGraph will automatically pass the stream writer to the function\n  async def generate_joke(state: State, writer: StreamWriter):  # [!code highlight]\n        writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\n        return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\n  graph = (\n        StateGraph(State)\n        .add_node(generate_joke)\n        .add_edge(START, \"generate_joke\")\n        .compile()\n  )\n\n  # Set stream_mode=\"custom\" to receive the custom data in the stream  # [!code highlight]\n  async for chunk in graph.astream(\n        {\"topic\": \"ice cream\"},\n        stream_mode=\"custom\",\n  ):\n        print(chunk)\n  ```\n</Accordion>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "content_length": 28756
}