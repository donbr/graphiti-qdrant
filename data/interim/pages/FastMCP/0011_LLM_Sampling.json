{
  "title": "LLM Sampling",
  "source_url": "https://gofastmcp.com/clients/sampling",
  "content": "Handle server-initiated LLM sampling requests.\n\nexport const VersionBadge = ({version}) => {\n  return <code className=\"version-badge-container\">\n            <p className=\"version-badge\">\n                <span className=\"version-badge-label\">New in version:</span>Â \n                <code className=\"version-badge-version\">{version}</code>\n            </p>\n        </code>;\n};\n\n<VersionBadge version=\"2.0.0\" />\n\nMCP servers can request LLM completions from clients. The client handles these requests through a sampling handler callback.\n\n## Sampling Handler\n\nProvide a `sampling_handler` function when creating the client:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nfrom fastmcp import Client\nfrom fastmcp.client.sampling import (\n    SamplingMessage,\n    SamplingParams,\n    RequestContext,\n)\n\nasync def sampling_handler(\n    messages: list[SamplingMessage],\n    params: SamplingParams,\n    context: RequestContext\n) -> str:\n    # Your LLM integration logic here\n    # Extract text from messages and generate a response\n    return \"Generated response based on the messages\"\n\nclient = Client(\n    \"my_mcp_server.py\",\n    sampling_handler=sampling_handler,\n)\n```\n\n### Handler Parameters\n\nThe sampling handler receives three parameters:\n\n<Card icon=\"code\" title=\"Sampling Handler Parameters\">\n  <ResponseField name=\"SamplingMessage\" type=\"Sampling Message Object\">\n    <Expandable title=\"attributes\">\n      <ResponseField name=\"role\" type=\"Literal[&#x22;user&#x22;, &#x22;assistant&#x22;]\">\n        The role of the message.\n      </ResponseField>\n\n      <ResponseField name=\"content\" type=\"TextContent | ImageContent | AudioContent\">\n        The content of the message.\n\n        TextContent is most common, and has a `.text` attribute.\n      </ResponseField>\n    </Expandable>\n  </ResponseField>\n\n  <ResponseField name=\"SamplingParams\" type=\"Sampling Parameters Object\">\n    <Expandable title=\"attributes\">\n      <ResponseField name=\"messages\" type=\"list[SamplingMessage]\">\n        The messages to sample from\n      </ResponseField>\n\n      <ResponseField name=\"modelPreferences\" type=\"ModelPreferences | None\">\n        The server's preferences for which model to select. The client MAY ignore\n        these preferences.\n\n        <Expandable title=\"attributes\">\n          <ResponseField name=\"hints\" type=\"list[ModelHint] | None\">\n            The hints to use for model selection.\n          </ResponseField>\n\n          <ResponseField name=\"costPriority\" type=\"float | None\">\n            The cost priority for model selection.\n          </ResponseField>\n\n          <ResponseField name=\"speedPriority\" type=\"float | None\">\n            The speed priority for model selection.\n          </ResponseField>\n\n          <ResponseField name=\"intelligencePriority\" type=\"float | None\">\n            The intelligence priority for model selection.\n          </ResponseField>\n        </Expandable>\n      </ResponseField>\n\n      <ResponseField name=\"systemPrompt\" type=\"str | None\">\n        An optional system prompt the server wants to use for sampling.\n      </ResponseField>\n\n      <ResponseField name=\"includeContext\" type=\"IncludeContext | None\">\n        A request to include context from one or more MCP servers (including the caller), to\n        be attached to the prompt.\n      </ResponseField>\n\n      <ResponseField name=\"temperature\" type=\"float | None\">\n        The sampling temperature.\n      </ResponseField>\n\n      <ResponseField name=\"maxTokens\" type=\"int\">\n        The maximum number of tokens to sample.\n      </ResponseField>\n\n      <ResponseField name=\"stopSequences\" type=\"list[str] | None\">\n        The stop sequences to use for sampling.\n      </ResponseField>\n\n      <ResponseField name=\"metadata\" type=\"dict[str, Any] | None\">\n        Optional metadata to pass through to the LLM provider.\n      </ResponseField>\n    </Expandable>\n  </ResponseField>\n\n  <ResponseField name=\"RequestContext\" type=\"Request Context Object\">\n    <Expandable title=\"attributes\">\n      <ResponseField name=\"request_id\" type=\"RequestId\">\n        Unique identifier for the MCP request\n      </ResponseField>\n    </Expandable>\n  </ResponseField>\n</Card>\n\n## Basic Example\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nfrom fastmcp import Client\nfrom fastmcp.client.sampling import SamplingMessage, SamplingParams, RequestContext\n\nasync def basic_sampling_handler(\n    messages: list[SamplingMessage],\n    params: SamplingParams,\n    context: RequestContext\n) -> str:\n    # Extract message content\n    conversation = []\n    for message in messages:\n        content = message.content.text if hasattr(message.content, 'text') else str(message.content)\n        conversation.append(f\"{message.role}: {content}\")\n\n    # Use the system prompt if provided\n    system_prompt = params.systemPrompt or \"You are a helpful assistant.\"\n\n    # Here you would integrate with your preferred LLM service\n    # This is just a placeholder response\n    return f\"Response based on conversation: {' | '.join(conversation)}\"\n\nclient = Client(\n    \"my_mcp_server.py\",\n    sampling_handler=basic_sampling_handler\n)\n```\n\n<Note>\n  If the client doesn't provide a sampling handler, servers can optionally configure a fallback handler. See [Server Sampling](/servers/sampling#sampling-fallback-handler) for details.\n</Note>",
  "content_length": 5325
}