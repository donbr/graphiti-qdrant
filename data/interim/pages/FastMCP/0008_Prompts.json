{
  "title": "Prompts",
  "source_url": "https://gofastmcp.com/clients/prompts",
  "content": "Use server-side prompt templates with automatic argument serialization.\n\nexport const VersionBadge = ({version}) => {\n  return <code className=\"version-badge-container\">\n            <p className=\"version-badge\">\n                <span className=\"version-badge-label\">New in version:</span>Â \n                <code className=\"version-badge-version\">{version}</code>\n            </p>\n        </code>;\n};\n\n<VersionBadge version=\"2.0.0\" />\n\nPrompts are reusable message templates exposed by MCP servers. They can accept arguments to generate personalized message sequences for LLM interactions.\n\n## Listing Prompts\n\nUse `list_prompts()` to retrieve all available prompt templates:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:\n    prompts = await client.list_prompts()\n    # prompts -> list[mcp.types.Prompt]\n    \n    for prompt in prompts:\n        print(f\"Prompt: {prompt.name}\")\n        print(f\"Description: {prompt.description}\")\n        if prompt.arguments:\n            print(f\"Arguments: {[arg.name for arg in prompt.arguments]}\")\n        # Access tags and other metadata\n        if hasattr(prompt, '_meta') and prompt._meta:\n            fastmcp_meta = prompt._meta.get('_fastmcp', {})\n            print(f\"Tags: {fastmcp_meta.get('tags', [])}\")\n```\n\n### Filtering by Tags\n\n<VersionBadge version=\"2.11.0\" />\n\nYou can use the `meta` field to filter prompts based on their tags:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:\n    prompts = await client.list_prompts()\n    \n    # Filter prompts by tag\n    analysis_prompts = [\n        prompt for prompt in prompts \n        if hasattr(prompt, '_meta') and prompt._meta and\n           prompt._meta.get('_fastmcp', {}) and\n           'analysis' in prompt._meta.get('_fastmcp', {}).get('tags', [])\n    ]\n    \n    print(f\"Found {len(analysis_prompts)} analysis prompts\")\n```\n\n<Note>\n  The `_meta` field is part of the standard MCP specification. FastMCP servers include tags and other metadata within a `_fastmcp` namespace (e.g., `_meta._fastmcp.tags`) to avoid conflicts with user-defined metadata. This behavior can be controlled with the server's `include_fastmcp_meta` setting - when disabled, the `_fastmcp` namespace won't be included. Other MCP server implementations may not provide this metadata structure.\n</Note>\n\n## Using Prompts\n\n### Basic Usage\n\nRequest a rendered prompt using `get_prompt()` with the prompt name and arguments:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:\n    # Simple prompt without arguments\n    result = await client.get_prompt(\"welcome_message\")\n    # result -> mcp.types.GetPromptResult\n    \n    # Access the generated messages\n    for message in result.messages:\n        print(f\"Role: {message.role}\")\n        print(f\"Content: {message.content}\")\n```\n\n### Prompts with Arguments\n\nPass arguments as a dictionary to customize the prompt:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:\n    # Prompt with simple arguments\n    result = await client.get_prompt(\"user_greeting\", {\n        \"name\": \"Alice\",\n        \"role\": \"administrator\"\n    })\n    \n    # Access the personalized messages\n    for message in result.messages:\n        print(f\"Generated message: {message.content}\")\n```\n\n## Automatic Argument Serialization\n\n<VersionBadge version=\"2.9.0\" />\n\nFastMCP automatically serializes complex arguments to JSON strings as required by the MCP specification. This allows you to pass typed objects directly:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nfrom dataclasses import dataclass\n\n@dataclass\nclass UserData:\n    name: str\n    age: int\n\nasync with client:\n    # Complex arguments are automatically serialized\n    result = await client.get_prompt(\"analyze_user\", {\n        \"user\": UserData(name=\"Alice\", age=30),     # Automatically serialized to JSON\n        \"preferences\": {\"theme\": \"dark\"},           # Dict serialized to JSON string\n        \"scores\": [85, 92, 78],                     # List serialized to JSON string\n        \"simple_name\": \"Bob\"                        # Strings passed through unchanged\n    })\n```\n\nThe client handles serialization using `pydantic_core.to_json()` for consistent formatting. FastMCP servers can automatically deserialize these JSON strings back to the expected types.\n\n### Serialization Examples\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:\n    result = await client.get_prompt(\"data_analysis\", {\n        # These will be automatically serialized to JSON strings:\n        \"config\": {\n            \"format\": \"csv\",\n            \"include_headers\": True,\n            \"delimiter\": \",\"\n        },\n        \"filters\": [\n            {\"field\": \"age\", \"operator\": \">\", \"value\": 18},\n            {\"field\": \"status\", \"operator\": \"==\", \"value\": \"active\"}\n        ],\n        # This remains a string:\n        \"report_title\": \"Monthly Analytics Report\"\n    })\n```\n\n## Working with Prompt Results\n\nThe `get_prompt()` method returns a `GetPromptResult` object containing a list of messages:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:\n    result = await client.get_prompt(\"conversation_starter\", {\"topic\": \"climate\"})\n    \n    # Access individual messages\n    for i, message in enumerate(result.messages):\n        print(f\"Message {i + 1}:\")\n        print(f\"  Role: {message.role}\")\n        print(f\"  Content: {message.content.text if hasattr(message.content, 'text') else message.content}\")\n```\n\n## Raw MCP Protocol Access\n\nFor access to the complete MCP protocol objects, use the `*_mcp` methods:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:\n    # Raw MCP method returns full protocol object\n    prompts_result = await client.list_prompts_mcp()\n    # prompts_result -> mcp.types.ListPromptsResult\n    \n    prompt_result = await client.get_prompt_mcp(\"example_prompt\", {\"arg\": \"value\"})\n    # prompt_result -> mcp.types.GetPromptResult\n```\n\n## Multi-Server Clients\n\nWhen using multi-server clients, prompts are accessible without prefixing (unlike tools):\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:  # Multi-server client\n    # Prompts from any server are directly accessible\n    result1 = await client.get_prompt(\"weather_prompt\", {\"city\": \"London\"})\n    result2 = await client.get_prompt(\"assistant_prompt\", {\"query\": \"help\"})\n```\n\n## Common Prompt Patterns\n\n### System Messages\n\nMany prompts generate system messages for LLM configuration:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:\n    result = await client.get_prompt(\"system_configuration\", {\n        \"role\": \"helpful assistant\",\n        \"expertise\": \"python programming\"\n    })\n    \n    # Typically returns messages with role=\"system\"\n    system_message = result.messages[0]\n    print(f\"System prompt: {system_message.content}\")\n```\n\n### Conversation Templates\n\nPrompts can generate multi-turn conversation templates:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync with client:\n    result = await client.get_prompt(\"interview_template\", {\n        \"candidate_name\": \"Alice\",\n        \"position\": \"Senior Developer\"\n    })\n    \n    # Multiple messages for a conversation flow\n    for message in result.messages:\n        print(f\"{message.role}: {message.content}\")\n```\n\n<Tip>\n  Prompt arguments and their expected types depend on the specific prompt implementation. Check the server's documentation or use `list_prompts()` to see available arguments for each prompt.\n</Tip>",
  "content_length": 7693
}