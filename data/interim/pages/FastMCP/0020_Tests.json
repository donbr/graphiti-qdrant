{
  "title": "Tests",
  "source_url": "https://gofastmcp.com/development/tests",
  "content": "Testing patterns and requirements for FastMCP\n\nexport const VersionBadge = ({version}) => {\n  return <code className=\"version-badge-container\">\n            <p className=\"version-badge\">\n                <span className=\"version-badge-label\">New in version:</span>Â \n                <code className=\"version-badge-version\">{version}</code>\n            </p>\n        </code>;\n};\n\nGood tests are the foundation of reliable software. In FastMCP, we treat tests as first-class documentation that demonstrates how features work while protecting against regressions. Every new capability needs comprehensive tests that demonstrate correctness.\n\n## FastMCP Tests\n\n### Running Tests\n\n```bash  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\n# Run all tests\nuv run pytest\n\n# Run specific test file\nuv run pytest tests/server/test_auth.py\n\n# Run with coverage\nuv run pytest --cov=fastmcp\n\n# Skip integration tests for faster runs\nuv run pytest -m \"not integration\"\n\n# Skip tests that spawn processes\nuv run pytest -m \"not integration and not client_process\"\n```\n\nTests should complete in under 1 second unless marked as integration tests. This speed encourages running them frequently, catching issues early.\n\n### Test Organization\n\nOur test organization mirrors the `src/` directory structure, creating a predictable mapping between code and tests. When you're working on `src/fastmcp/server/auth.py`, you'll find its tests in `tests/server/test_auth.py`. In rare cases tests are split further - for example, the OpenAPI tests are so comprehensive they're split across multiple files.\n\n### Test Markers\n\nWe use pytest markers to categorize tests that require special resources or take longer to run:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\n@pytest.mark.integration\nasync def test_github_api_integration():\n    \"\"\"Test GitHub API integration with real service.\"\"\"\n    token = os.getenv(\"FASTMCP_GITHUB_TOKEN\")\n    if not token:\n        pytest.skip(\"FASTMCP_GITHUB_TOKEN not available\")\n    \n    # Test against real GitHub API\n    client = GitHubClient(token)\n    repos = await client.list_repos(\"jlowin\")\n    assert \"fastmcp\" in [repo.name for repo in repos]\n\n@pytest.mark.client_process\nasync def test_stdio_transport():\n    \"\"\"Test STDIO transport with separate process.\"\"\"\n    # This spawns a subprocess\n    async with Client(\"python examples/simple_echo.py\") as client:\n        result = await client.call_tool(\"echo\", {\"message\": \"test\"})\n        assert result.content[0].text == \"test\"\n```\n\n## Writing Tests\n\n### Test Requirements\n\nFollowing these practices creates maintainable, debuggable test suites that serve as both documentation and regression protection.\n\n#### Single Behavior Per Test\n\nEach test should verify exactly one behavior. When it fails, you need to know immediately what broke. A test that checks five things gives you five potential failure points to investigate. A test that checks one thing points directly to the problem.\n\n<CodeGroup>\n  ```python Good: Atomic Test theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\n  async def test_tool_registration():\n      \"\"\"Test that tools are properly registered with the server.\"\"\"\n      mcp = FastMCP(\"test-server\")\n      \n      @mcp.tool\n      def add(a: int, b: int) -> int:\n          return a + b\n      \n      tools = mcp.list_tools()\n      assert len(tools) == 1\n      assert tools[0].name == \"add\"\n  ```\n\n  ```python Bad: Multi-Behavior Test theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\n  async def test_server_functionality():\n      \"\"\"Test multiple server features at once.\"\"\"\n      mcp = FastMCP(\"test-server\")\n      \n      # Tool registration\n      @mcp.tool\n      def add(a: int, b: int) -> int:\n          return a + b\n      \n      # Resource creation\n      @mcp.resource(\"config://app\")\n      def get_config():\n          return {\"version\": \"1.0\"}\n      \n      # Authentication setup\n      mcp.auth = BearerTokenProvider({\"token\": \"user\"})\n      \n      # What exactly are we testing? If this fails, what broke?\n      assert mcp.list_tools()\n      assert mcp.list_resources()\n      assert mcp.auth is not None\n  ```\n</CodeGroup>\n\n#### Self-Contained Setup\n\nEvery test must create its own setup. Tests should be runnable in any order, in parallel, or in isolation. When a test fails, you should be able to run just that test to reproduce the issue.\n\n<CodeGroup>\n  ```python Good: Self-Contained theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\n  async def test_tool_execution_with_error():\n      \"\"\"Test that tool errors are properly handled.\"\"\"\n      mcp = FastMCP(\"test-server\")\n      \n      @mcp.tool\n      def divide(a: int, b: int) -> float:\n          if b == 0:\n              raise ValueError(\"Cannot divide by zero\")\n          return a / b\n      \n      async with Client(mcp) as client:\n          with pytest.raises(Exception):\n              await client.call_tool(\"divide\", {\"a\": 10, \"b\": 0})\n  ```\n\n  ```python Bad: Test Dependencies theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\n  # Global state that tests depend on\n  test_server = None\n\n  def test_setup_server():\n      \"\"\"Setup for other tests.\"\"\"\n      global test_server\n      test_server = FastMCP(\"shared-server\")\n\n  def test_server_works():\n      \"\"\"Test server functionality.\"\"\"\n      # Depends on test_setup_server running first\n      assert test_server is not None\n  ```\n</CodeGroup>\n\n#### Clear Intent\n\nTest names and assertions should make the verified behavior obvious. A developer reading your test should understand what feature it validates and how that feature should behave.\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nasync def test_authenticated_tool_requires_valid_token():\n    \"\"\"Test that authenticated users can access protected tools.\"\"\"\n    mcp = FastMCP(\"test-server\")\n    mcp.auth = BearerTokenProvider({\"secret-token\": \"test-user\"})\n    \n    @mcp.tool\n    def protected_action() -> str:\n        return \"success\"\n    \n    async with Client(mcp, auth=BearerAuth(\"secret-token\")) as client:\n        result = await client.call_tool(\"protected_action\", {})\n        assert result.content[0].text == \"success\"\n```\n\n#### Using Fixtures\n\nUse fixtures to create reusable data, server configurations, or other resources for your tests. Note that you should **not** open FastMCP clients in your fixtures as it can create hard-to-diagnose issues with event loops.\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nimport pytest\nfrom fastmcp import FastMCP, Client\n\n@pytest.fixture\ndef weather_server():\n    server = FastMCP(\"WeatherServer\")\n    \n    @server.tool\n    def get_temperature(city: str) -> dict:\n        temps = {\"NYC\": 72, \"LA\": 85, \"Chicago\": 68}\n        return {\"city\": city, \"temp\": temps.get(city, 70)}\n    \n    return server\n\nasync def test_temperature_tool(weather_server):\n    async with Client(weather_server) as client:\n        result = await client.call_tool(\"get_temperature\", {\"city\": \"LA\"})\n        assert result.data == {\"city\": \"LA\", \"temp\": 85}\n```\n\n#### Effective Assertions\n\nAssertions should be specific and provide context on failure. When a test fails during CI, the assertion message should tell you exactly what went wrong.\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\n# Basic assertion - minimal context on failure\nassert result.status == \"success\"\n\n# Better - explains what was expected\nassert result.status == \"success\", f\"Expected successful operation, got {result.status}: {result.error}\"\n```\n\nTry not to have too many assertions in a single test unless you truly need to check various aspects of the same behavior. In general, assertions of different behaviors should be in separate tests.\n\n#### Inline Snapshots\n\nFastMCP uses `inline-snapshot` for testing complex data structures. On first run of `pytest --inline-snapshot=create` with an empty `snapshot()`, pytest will auto-populate the expected value. To update snapshots after intentional changes, run `pytest --inline-snapshot=fix`. This is particularly useful for testing JSON schemas and API responses.\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nfrom inline_snapshot import snapshot\n\nasync def test_tool_schema_generation():\n    \"\"\"Test that tool schemas are generated correctly.\"\"\"\n    mcp = FastMCP(\"test-server\")\n    \n    @mcp.tool\n    def calculate_tax(amount: float, rate: float = 0.1) -> dict:\n        \"\"\"Calculate tax on an amount.\"\"\"\n        return {\"amount\": amount, \"tax\": amount * rate, \"total\": amount * (1 + rate)}\n    \n    tools = mcp.list_tools()\n    schema = tools[0].inputSchema\n    \n    # First run: snapshot() is empty, gets auto-populated\n    # Subsequent runs: compares against stored snapshot\n    assert schema == snapshot({\n        \"type\": \"object\", \n        \"properties\": {\n            \"amount\": {\"type\": \"number\"}, \n            \"rate\": {\"type\": \"number\", \"default\": 0.1}\n        }, \n        \"required\": [\"amount\"]\n    })\n```\n\n### In-Memory Testing\n\nFastMCP uses in-memory transport for testing, where servers and clients communicate directly. The majority of functionality can be tested in a deterministic fashion this way. We use more complex setups only when testing transports themselves.\n\nThe in-memory transport runs the real MCP protocol implementation without network overhead. Instead of deploying your server or managing network connections, you pass your server instance directly to the client. Everything runs in the same Python process - you can set breakpoints anywhere and step through with your debugger.\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nfrom fastmcp import FastMCP, Client\n\n# Create your server\nserver = FastMCP(\"WeatherServer\")\n\n@server.tool\ndef get_temperature(city: str) -> dict:\n    \"\"\"Get current temperature for a city\"\"\"\n    temps = {\"NYC\": 72, \"LA\": 85, \"Chicago\": 68}\n    return {\"city\": city, \"temp\": temps.get(city, 70)}\n\nasync def test_weather_operations():\n    # Pass server directly - no deployment needed\n    async with Client(server) as client:\n        result = await client.call_tool(\"get_temperature\", {\"city\": \"NYC\"})\n        assert result.data == {\"city\": \"NYC\", \"temp\": 72}\n```\n\nThis pattern makes tests deterministic and fast - typically completing in milliseconds rather than seconds.\n\n### Mocking External Dependencies\n\nFastMCP servers are standard Python objects, so you can mock external dependencies using your preferred approach:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nfrom unittest.mock import AsyncMock\n\nasync def test_database_tool():\n    server = FastMCP(\"DataServer\")\n    \n    # Mock the database\n    mock_db = AsyncMock()\n    mock_db.fetch_users.return_value = [\n        {\"id\": 1, \"name\": \"Alice\"},\n        {\"id\": 2, \"name\": \"Bob\"}\n    ]\n    \n    @server.tool\n    async def list_users() -> list:\n        return await mock_db.fetch_users()\n    \n    async with Client(server) as client:\n        result = await client.call_tool(\"list_users\", {})\n        assert len(result.data) == 2\n        assert result.data[0][\"name\"] == \"Alice\"\n        mock_db.fetch_users.assert_called_once()\n```\n\n### Testing Network Transports\n\nWhile in-memory testing covers most unit testing needs, you'll occasionally need to test actual network transports like HTTP or SSE. FastMCP provides two approaches: in-process async servers (preferred), and separate subprocess servers (for special cases).\n\n#### In-Process Network Testing (Preferred)\n\n<VersionBadge version=\"2.13.0\" />\n\nFor most network transport tests, use `run_server_async` as an async context manager. This runs the server as a task in the same process, providing fast, deterministic tests with full debugger support:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nimport pytest\nfrom fastmcp import FastMCP, Client\nfrom fastmcp.client.transports import StreamableHttpTransport\nfrom fastmcp.utilities.tests import run_server_async\n\ndef create_test_server() -> FastMCP:\n    \"\"\"Create a test server instance.\"\"\"\n    server = FastMCP(\"TestServer\")\n\n    @server.tool\n    def greet(name: str) -> str:\n        return f\"Hello, {name}!\"\n\n    return server\n\n@pytest.fixture\nasync def http_server() -> str:\n    \"\"\"Start server in-process for testing.\"\"\"\n    server = create_test_server()\n    async with run_server_async(server) as url:\n        yield url\n\nasync def test_http_transport(http_server: str):\n    \"\"\"Test actual HTTP transport behavior.\"\"\"\n    async with Client(\n        transport=StreamableHttpTransport(http_server)\n    ) as client:\n        result = await client.ping()\n        assert result is True\n\n        greeting = await client.call_tool(\"greet\", {\"name\": \"World\"})\n        assert greeting.data == \"Hello, World!\"\n```\n\nThe `run_server_async` context manager automatically handles server lifecycle and cleanup. This approach is faster than subprocess-based testing and provides better error messages.\n\n#### Subprocess Testing (Special Cases)\n\nFor tests that require complete process isolation (like STDIO transport or testing subprocess behavior), use `run_server_in_process`:\n\n```python  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\nimport pytest\nfrom fastmcp.utilities.tests import run_server_in_process\nfrom fastmcp import FastMCP, Client\nfrom fastmcp.client.transports import StreamableHttpTransport\n\ndef run_server(host: str, port: int) -> None:\n    \"\"\"Function to run in subprocess.\"\"\"\n    server = FastMCP(\"TestServer\")\n    \n    @server.tool\n    def greet(name: str) -> str:\n        return f\"Hello, {name}!\"\n    \n    server.run(host=host, port=port)\n\n@pytest.fixture\nasync def http_server():\n    \"\"\"Fixture that runs server in subprocess.\"\"\"\n    with run_server_in_process(run_server, transport=\"http\") as url:\n        yield f\"{url}/mcp\"\n\nasync def test_http_transport(http_server: str):\n    \"\"\"Test actual HTTP transport behavior.\"\"\"\n    async with Client(\n        transport=StreamableHttpTransport(http_server)\n    ) as client:\n        result = await client.ping()\n        assert result is True\n```\n\nThe `run_server_in_process` utility handles server lifecycle, port allocation, and cleanup automatically. Use this only when subprocess isolation is truly necessary, as it's slower and harder to debug than in-process testing. FastMCP uses the `client_process` marker to isolate these tests in CI.\n\n### Documentation Testing\n\nDocumentation requires the same validation as code. The `just docs` command launches a local Mintlify server that renders your documentation exactly as users will see it:\n\n```bash  theme={\"theme\":{\"light\":\"snazzy-light\",\"dark\":\"dark-plus\"}}\n# Start local documentation server with hot reload\njust docs\n\n# Or run Mintlify directly\nmintlify dev\n```\n\nThe local server watches for changes and automatically refreshes. This preview catches formatting issues and helps you see documentation as users will experience it.",
  "content_length": 14916
}