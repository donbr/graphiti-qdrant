{
  "title": "Sampling",
  "source_url": "https://modelcontextprotocol.io/specification/2025-11-25/client/sampling",
  "content": "<div id=\"enable-section-numbers\" />\n\n<Info>**Protocol Revision**: 2025-11-25</Info>\n\nThe Model Context Protocol (MCP) provides a standardized way for servers to request LLM\nsampling (\"completions\" or \"generations\") from language models via clients. This flow\nallows clients to maintain control over model access, selection, and permissions while\nenabling servers to leverage AI capabilities—with no server API keys necessary.\nServers can request text, audio, or image-based interactions and optionally include\ncontext from MCP servers in their prompts.\n\n## User Interaction Model\n\nSampling in MCP allows servers to implement agentic behaviors, by enabling LLM calls to\noccur *nested* inside other MCP server features.\n\nImplementations are free to expose sampling through any interface pattern that suits\ntheir needs—the protocol itself does not mandate any specific user interaction\nmodel.\n\n<Warning>\n  For trust & safety and security, there **SHOULD** always\n  be a human in the loop with the ability to deny sampling requests.\n\n  Applications **SHOULD**:\n\n  * Provide UI that makes it easy and intuitive to review sampling requests\n  * Allow users to view and edit prompts before sending\n  * Present generated responses for review before delivery\n</Warning>\n\n## Tools in Sampling\n\nServers can request that the client's LLM use tools during sampling by providing a `tools` array and optional `toolChoice` configuration in their sampling requests. This enables servers to implement agentic behaviors where the LLM can call tools, receive results, and continue the conversation - all within a single sampling request flow.\n\nClients **MUST** declare support for tool use via the `sampling.tools` capability to receive tool-enabled sampling requests. Servers **MUST NOT** send tool-enabled sampling requests to Clients that have not declared support for tool use via the `sampling.tools` capability.\n\n## Capabilities\n\nClients that support sampling **MUST** declare the `sampling` capability during\n[initialization](/specification/2025-11-25/basic/lifecycle#initialization):\n\n**Basic sampling:**\n\n```json  theme={null}\n{\n  \"capabilities\": {\n    \"sampling\": {}\n  }\n}\n```\n\n**With tool use support:**\n\n```json  theme={null}\n{\n  \"capabilities\": {\n    \"sampling\": {\n      \"tools\": {}\n    }\n  }\n}\n```\n\n**With context inclusion support (soft-deprecated):**\n\n```json  theme={null}\n{\n  \"capabilities\": {\n    \"sampling\": {\n      \"context\": {}\n    }\n  }\n}\n```\n\n<Note>\n  The `includeContext` parameter values `\"thisServer\"` and `\"allServers\"` are\n  soft-deprecated. Servers **SHOULD** avoid using these values (e.g. can just\n  omit `includeContext` since it defaults to `\"none\"`), and **SHOULD NOT** use\n  them unless the client declares `sampling.context` capability. These values\n  may be removed in future spec releases.\n</Note>\n\n## Protocol Messages\n\n### Creating Messages\n\nTo request a language model generation, servers send a `sampling/createMessage` request:\n\n**Request:**\n\n```json  theme={null}\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"method\": \"sampling/createMessage\",\n  \"params\": {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": {\n          \"type\": \"text\",\n          \"text\": \"What is the capital of France?\"\n        }\n      }\n    ],\n    \"modelPreferences\": {\n      \"hints\": [\n        {\n          \"name\": \"claude-3-sonnet\"\n        }\n      ],\n      \"intelligencePriority\": 0.8,\n      \"speedPriority\": 0.5\n    },\n    \"systemPrompt\": \"You are a helpful assistant.\",\n    \"maxTokens\": 100\n  }\n}\n```\n\n**Response:**\n\n```json  theme={null}\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"result\": {\n    \"role\": \"assistant\",\n    \"content\": {\n      \"type\": \"text\",\n      \"text\": \"The capital of France is Paris.\"\n    },\n    \"model\": \"claude-3-sonnet-20240307\",\n    \"stopReason\": \"endTurn\"\n  }\n}\n```\n\n### Sampling with Tools\n\nThe following diagram illustrates the complete flow of sampling with tools, including the multi-turn tool loop:\n\n```mermaid  theme={null}\nsequenceDiagram\n    participant Server\n    participant Client\n    participant User\n    participant LLM\n\n    Note over Server,Client: Initial request with tools\n    Server->>Client: sampling/createMessage<br/>(messages + tools)\n\n    Note over Client,User: Human-in-the-loop review\n    Client->>User: Present request for approval\n    User-->>Client: Approve/modify\n\n    Client->>LLM: Forward request with tools\n    LLM-->>Client: Response with tool_use<br/>(stopReason: \"toolUse\")\n\n    Client->>User: Present tool calls for review\n    User-->>Client: Approve tool calls\n    Client-->>Server: Return tool_use response\n\n    Note over Server: Execute tool(s)\n    Server->>Server: Run get_weather(\"Paris\")<br/>Run get_weather(\"London\")\n\n    Note over Server,Client: Continue with tool results\n    Server->>Client: sampling/createMessage<br/>(history + tool_results + tools)\n\n    Client->>User: Present continuation\n    User-->>Client: Approve\n\n    Client->>LLM: Forward with tool results\n    LLM-->>Client: Final text response<br/>(stopReason: \"endTurn\")\n\n    Client->>User: Present response\n    User-->>Client: Approve\n    Client-->>Server: Return final response\n\n    Note over Server: Server processes result<br/>(may continue conversation...)\n```\n\nTo request LLM generation with tool use capabilities, servers include `tools` and optionally `toolChoice` in the request:\n\n**Request (Server -> Client):**\n\n```json  theme={null}\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"method\": \"sampling/createMessage\",\n  \"params\": {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": {\n          \"type\": \"text\",\n          \"text\": \"What's the weather like in Paris and London?\"\n        }\n      }\n    ],\n    \"tools\": [\n      {\n        \"name\": \"get_weather\",\n        \"description\": \"Get current weather for a city\",\n        \"inputSchema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"city\": {\n              \"type\": \"string\",\n              \"description\": \"City name\"\n            }\n          },\n          \"required\": [\"city\"]\n        }\n      }\n    ],\n    \"toolChoice\": {\n      \"mode\": \"auto\"\n    },\n    \"maxTokens\": 1000\n  }\n}\n```\n\n**Response (Client -> Server):**\n\n```json  theme={null}\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"result\": {\n    \"role\": \"assistant\",\n    \"content\": [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"call_abc123\",\n        \"name\": \"get_weather\",\n        \"input\": {\n          \"city\": \"Paris\"\n        }\n      },\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"call_def456\",\n        \"name\": \"get_weather\",\n        \"input\": {\n          \"city\": \"London\"\n        }\n      }\n    ],\n    \"model\": \"claude-3-sonnet-20240307\",\n    \"stopReason\": \"toolUse\"\n  }\n}\n```\n\n### Multi-turn Tool Loop\n\nAfter receiving tool use requests from the LLM, the server typically:\n\n1. Executes the requested tool uses.\n2. Sends a new sampling request with the tool results appended\n3. Receives the LLM's response (which might contain new tool uses)\n4. Repeats as many times as needed (server might cap the maximum number of iterations, and e.g. pass `toolChoice: {mode: \"none\"}` on the last iteration to force a final result)\n\n**Follow-up request (Server -> Client) with tool results:**\n\n```json  theme={null}\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"method\": \"sampling/createMessage\",\n  \"params\": {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": {\n          \"type\": \"text\",\n          \"text\": \"What's the weather like in Paris and London?\"\n        }\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": [\n          {\n            \"type\": \"tool_use\",\n            \"id\": \"call_abc123\",\n            \"name\": \"get_weather\",\n            \"input\": { \"city\": \"Paris\" }\n          },\n          {\n            \"type\": \"tool_use\",\n            \"id\": \"call_def456\",\n            \"name\": \"get_weather\",\n            \"input\": { \"city\": \"London\" }\n          }\n        ]\n      },\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"tool_result\",\n            \"toolUseId\": \"call_abc123\",\n            \"content\": [\n              {\n                \"type\": \"text\",\n                \"text\": \"Weather in Paris: 18°C, partly cloudy\"\n              }\n            ]\n          },\n          {\n            \"type\": \"tool_result\",\n            \"toolUseId\": \"call_def456\",\n            \"content\": [\n              {\n                \"type\": \"text\",\n                \"text\": \"Weather in London: 15°C, rainy\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"tools\": [\n      {\n        \"name\": \"get_weather\",\n        \"description\": \"Get current weather for a city\",\n        \"inputSchema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"city\": { \"type\": \"string\" }\n          },\n          \"required\": [\"city\"]\n        }\n      }\n    ],\n    \"maxTokens\": 1000\n  }\n}\n```\n\n**Final response (Client -> Server):**\n\n```json  theme={null}\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"result\": {\n    \"role\": \"assistant\",\n    \"content\": {\n      \"type\": \"text\",\n      \"text\": \"Based on the current weather data:\\n\\n- **Paris**: 18°C and partly cloudy - quite pleasant!\\n- **London**: 15°C and rainy - you'll want an umbrella.\\n\\nParis has slightly warmer and drier conditions today.\"\n    },\n    \"model\": \"claude-3-sonnet-20240307\",\n    \"stopReason\": \"endTurn\"\n  }\n}\n```\n\n## Message Content Constraints\n\n### Tool Result Messages\n\nWhen a user message contains tool results (type: \"tool\\_result\"), it **MUST** contain ONLY tool results. Mixing tool results with other content types (text, image, audio) in the same message is not allowed.\n\nThis constraint ensures compatibility with provider APIs that use dedicated roles for tool results (e.g., OpenAI's \"tool\" role, Gemini's \"function\" role).\n\n**Valid - single tool result:**\n\n```json  theme={null}\n{\n  \"role\": \"user\",\n  \"content\": {\n    \"type\": \"tool_result\",\n    \"toolUseId\": \"call_123\",\n    \"content\": [{ \"type\": \"text\", \"text\": \"Result data\" }]\n  }\n}\n```\n\n**Valid - multiple tool results:**\n\n```json  theme={null}\n{\n  \"role\": \"user\",\n  \"content\": [\n    {\n      \"type\": \"tool_result\",\n      \"toolUseId\": \"call_123\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Result 1\" }]\n    },\n    {\n      \"type\": \"tool_result\",\n      \"toolUseId\": \"call_456\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Result 2\" }]\n    }\n  ]\n}\n```\n\n**Invalid - mixed content:**\n\n```json  theme={null}\n{\n  \"role\": \"user\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Here are the results:\"\n    },\n    {\n      \"type\": \"tool_result\",\n      \"toolUseId\": \"call_123\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Result data\" }]\n    }\n  ]\n}\n```\n\n### Tool Use and Result Balance\n\nWhen using tool use in sampling, every assistant message containing `ToolUseContent` blocks **MUST** be followed by a user message that consists entirely of `ToolResultContent` blocks, with each tool use (e.g. with `id: $id`) matched by a corresponding tool result (with `toolUseId: $id`), before any other message.\n\nThis requirement ensures:\n\n* Tool uses are always resolved before the conversation continues\n* Provider APIs can concurrently process multiple tool uses and fetch their results in parallel\n* The conversation maintains a consistent request-response pattern\n\n**Example valid sequence:**\n\n1. User message: \"What's the weather like in Paris and London?\"\n2. Assistant message: `ToolUseContent` (`id: \"call_abc123\", name: \"get_weather\", input: {city: \"Paris\"}`) + `ToolUseContent` (`id: \"call_def456\", name: \"get_weather\", input: {city: \"London\"}`)\n3. User message: `ToolResultContent` (`toolUseId: \"call_abc123\", content: \"18°C, partly cloudy\"`) + `ToolResultContent` (`toolUseId: \"call_def456\", content: \"15°C, rainy\"`)\n4. Assistant message: Text response comparing the weather in both cities\n\n**Invalid sequence - missing tool result:**\n\n1. User message: \"What's the weather like in Paris and London?\"\n2. Assistant message: `ToolUseContent` (`id: \"call_abc123\", name: \"get_weather\", input: {city: \"Paris\"}`) + `ToolUseContent` (`id: \"call_def456\", name: \"get_weather\", input: {city: \"London\"}`)\n3. User message: `ToolResultContent` (`toolUseId: \"call_abc123\", content: \"18°C, partly cloudy\"`) ← Missing result for call\\_def456\n4. Assistant message: Text response (invalid - not all tool uses were resolved)\n\n## Cross-API Compatibility\n\nThe sampling specification is designed to work across multiple LLM provider APIs (Claude, OpenAI, Gemini, etc.). Key design decisions for compatibility:\n\n### Message Roles\n\nMCP uses two roles: \"user\" and \"assistant\".\n\nTool use requests are sent in CreateMessageResult with the \"assistant\" role.\nTool results are sent back in messages with the \"user\" role.\nMessages with tool results cannot contain other kinds of content.\n\n### Tool Choice Modes\n\n`CreateMessageRequest.params.toolChoice` controls the tool use ability of the model:\n\n* `{mode: \"auto\"}`: Model decides whether to use tools (default)\n* `{mode: \"required\"}`: Model MUST use at least one tool before completing\n* `{mode: \"none\"}`: Model MUST NOT use any tools\n\n### Parallel Tool Use\n\nMCP allows models to make multiple tool use requests in parallel (returning an array of `ToolUseContent`). All major provider APIs support this:\n\n* **Claude**: Supports parallel tool use natively\n* **OpenAI**: Supports parallel tool calls (can be disabled with `parallel_tool_calls: false`)\n* **Gemini**: Supports parallel function calls natively\n\nImplementations wrapping providers that support disabling parallel tool use MAY expose this as an extension, but it is not part of the core MCP specification.\n\n## Message Flow\n\n```mermaid  theme={null}\nsequenceDiagram\n    participant Server\n    participant Client\n    participant User\n    participant LLM\n\n    Note over Server,Client: Server initiates sampling\n    Server->>Client: sampling/createMessage\n\n    Note over Client,User: Human-in-the-loop review\n    Client->>User: Present request for approval\n    User-->>Client: Review and approve/modify\n\n    Note over Client,LLM: Model interaction\n    Client->>LLM: Forward approved request\n    LLM-->>Client: Return generation\n\n    Note over Client,User: Response review\n    Client->>User: Present response for approval\n    User-->>Client: Review and approve/modify\n\n    Note over Server,Client: Complete request\n    Client-->>Server: Return approved response\n```\n\n## Data Types\n\n### Messages\n\nSampling messages can contain:\n\n#### Text Content\n\n```json  theme={null}\n{\n  \"type\": \"text\",\n  \"text\": \"The message content\"\n}\n```\n\n#### Image Content\n\n```json  theme={null}\n{\n  \"type\": \"image\",\n  \"data\": \"base64-encoded-image-data\",\n  \"mimeType\": \"image/jpeg\"\n}\n```\n\n#### Audio Content\n\n```json  theme={null}\n{\n  \"type\": \"audio\",\n  \"data\": \"base64-encoded-audio-data\",\n  \"mimeType\": \"audio/wav\"\n}\n```\n\n### Model Preferences\n\nModel selection in MCP requires careful abstraction since servers and clients may use\ndifferent AI providers with distinct model offerings. A server cannot simply request a\nspecific model by name since the client may not have access to that exact model or may\nprefer to use a different provider's equivalent model.\n\nTo solve this, MCP implements a preference system that combines abstract capability\npriorities with optional model hints:\n\n#### Capability Priorities\n\nServers express their needs through three normalized priority values (0-1):\n\n* `costPriority`: How important is minimizing costs? Higher values prefer cheaper models.\n* `speedPriority`: How important is low latency? Higher values prefer faster models.\n* `intelligencePriority`: How important are advanced capabilities? Higher values prefer\n  more capable models.\n\n#### Model Hints\n\nWhile priorities help select models based on characteristics, `hints` allow servers to\nsuggest specific models or model families:\n\n* Hints are treated as substrings that can match model names flexibly\n* Multiple hints are evaluated in order of preference\n* Clients **MAY** map hints to equivalent models from different providers\n* Hints are advisory—clients make final model selection\n\nFor example:\n\n```json  theme={null}\n{\n  \"hints\": [\n    { \"name\": \"claude-3-sonnet\" }, // Prefer Sonnet-class models\n    { \"name\": \"claude\" } // Fall back to any Claude model\n  ],\n  \"costPriority\": 0.3, // Cost is less important\n  \"speedPriority\": 0.8, // Speed is very important\n  \"intelligencePriority\": 0.5 // Moderate capability needs\n}\n```\n\nThe client processes these preferences to select an appropriate model from its available\noptions. For instance, if the client doesn't have access to Claude models but has Gemini,\nit might map the sonnet hint to `gemini-1.5-pro` based on similar capabilities.\n\n## Error Handling\n\nClients **SHOULD** return errors for common failure cases:\n\n* User rejected sampling request: `-1`\n* Tool result missing in request: `-32602` (Invalid params)\n* Tool results mixed with other content: `-32602` (Invalid params)\n\nExample errors:\n\n```json  theme={null}\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 3,\n  \"error\": {\n    \"code\": -1,\n    \"message\": \"User rejected sampling request\"\n  }\n}\n```\n\n```json  theme={null}\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 4,\n  \"error\": {\n    \"code\": -32602,\n    \"message\": \"Tool result missing in request\"\n  }\n}\n```\n\n## Security Considerations\n\n1. Clients **SHOULD** implement user approval controls\n2. Both parties **SHOULD** validate message content\n3. Clients **SHOULD** respect model preference hints\n4. Clients **SHOULD** implement rate limiting\n5. Both parties **MUST** handle sensitive data appropriately\n\nWhen tools are used in sampling, additional security considerations apply:\n\n6. Servers **MUST** ensure that when replying to a `stopReason: \"toolUse\"`, each `ToolUseContent` item is responded to with a `ToolResultContent` item with a matching `toolUseId`, and that the user message contains only tool results (no other content types)\n7. Both parties **SHOULD** implement iteration limits for tool loops",
  "content_length": 17812
}