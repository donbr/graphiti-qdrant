{
  "title": "`pydantic_evals.reporting`",
  "source_url": null,
  "content": "### ReportCase\n\nBases: `Generic[InputsT, OutputT, MetadataT]`\n\nA single case in an evaluation report.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\n@dataclass(kw_only=True)\nclass ReportCase(Generic[InputsT, OutputT, MetadataT]):\n    \"\"\"A single case in an evaluation report.\"\"\"\n\n    name: str\n    \"\"\"The name of the [case][pydantic_evals.Case].\"\"\"\n    inputs: InputsT\n    \"\"\"The inputs to the task, from [`Case.inputs`][pydantic_evals.Case.inputs].\"\"\"\n    metadata: MetadataT | None\n    \"\"\"Any metadata associated with the case, from [`Case.metadata`][pydantic_evals.Case.metadata].\"\"\"\n    expected_output: OutputT | None\n    \"\"\"The expected output of the task, from [`Case.expected_output`][pydantic_evals.Case.expected_output].\"\"\"\n    output: OutputT\n    \"\"\"The output of the task execution.\"\"\"\n\n    metrics: dict[str, float | int]\n    attributes: dict[str, Any]\n\n    scores: dict[str, EvaluationResult[int | float]]\n    labels: dict[str, EvaluationResult[str]]\n    assertions: dict[str, EvaluationResult[bool]]\n\n    task_duration: float\n    total_duration: float  # includes evaluator execution time\n\n    trace_id: str | None = None\n    \"\"\"The trace ID of the case span.\"\"\"\n    span_id: str | None = None\n    \"\"\"The span ID of the case span.\"\"\"\n\n    evaluator_failures: list[EvaluatorFailure] = field(default_factory=list)\n\n```\n\n#### name\n\n```python\nname: str\n\n```\n\nThe name of the case.\n\n#### inputs\n\n```python\ninputs: InputsT\n\n```\n\nThe inputs to the task, from Case.inputs.\n\n#### metadata\n\n```python\nmetadata: MetadataT | None\n\n```\n\nAny metadata associated with the case, from Case.metadata.\n\n#### expected_output\n\n```python\nexpected_output: OutputT | None\n\n```\n\nThe expected output of the task, from Case.expected_output.\n\n#### output\n\n```python\noutput: OutputT\n\n```\n\nThe output of the task execution.\n\n#### trace_id\n\n```python\ntrace_id: str | None = None\n\n```\n\nThe trace ID of the case span.\n\n#### span_id\n\n```python\nspan_id: str | None = None\n\n```\n\nThe span ID of the case span.\n\n### ReportCaseFailure\n\nBases: `Generic[InputsT, OutputT, MetadataT]`\n\nA single case in an evaluation report that failed due to an error during task execution.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\n@dataclass(kw_only=True)\nclass ReportCaseFailure(Generic[InputsT, OutputT, MetadataT]):\n    \"\"\"A single case in an evaluation report that failed due to an error during task execution.\"\"\"\n\n    name: str\n    \"\"\"The name of the [case][pydantic_evals.Case].\"\"\"\n    inputs: InputsT\n    \"\"\"The inputs to the task, from [`Case.inputs`][pydantic_evals.Case.inputs].\"\"\"\n    metadata: MetadataT | None\n    \"\"\"Any metadata associated with the case, from [`Case.metadata`][pydantic_evals.Case.metadata].\"\"\"\n    expected_output: OutputT | None\n    \"\"\"The expected output of the task, from [`Case.expected_output`][pydantic_evals.Case.expected_output].\"\"\"\n\n    error_message: str\n    \"\"\"The message of the exception that caused the failure.\"\"\"\n    error_stacktrace: str\n    \"\"\"The stacktrace of the exception that caused the failure.\"\"\"\n\n    trace_id: str | None = None\n    \"\"\"The trace ID of the case span.\"\"\"\n    span_id: str | None = None\n    \"\"\"The span ID of the case span.\"\"\"\n\n```\n\n#### name\n\n```python\nname: str\n\n```\n\nThe name of the case.\n\n#### inputs\n\n```python\ninputs: InputsT\n\n```\n\nThe inputs to the task, from Case.inputs.\n\n#### metadata\n\n```python\nmetadata: MetadataT | None\n\n```\n\nAny metadata associated with the case, from Case.metadata.\n\n#### expected_output\n\n```python\nexpected_output: OutputT | None\n\n```\n\nThe expected output of the task, from Case.expected_output.\n\n#### error_message\n\n```python\nerror_message: str\n\n```\n\nThe message of the exception that caused the failure.\n\n#### error_stacktrace\n\n```python\nerror_stacktrace: str\n\n```\n\nThe stacktrace of the exception that caused the failure.\n\n#### trace_id\n\n```python\ntrace_id: str | None = None\n\n```\n\nThe trace ID of the case span.\n\n#### span_id\n\n```python\nspan_id: str | None = None\n\n```\n\nThe span ID of the case span.\n\n### ReportCaseAggregate\n\nBases: `BaseModel`\n\nA synthetic case that summarizes a set of cases.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\nclass ReportCaseAggregate(BaseModel):\n    \"\"\"A synthetic case that summarizes a set of cases.\"\"\"\n\n    name: str\n\n    scores: dict[str, float | int]\n    labels: dict[str, dict[str, float]]\n    metrics: dict[str, float | int]\n    assertions: float | None\n    task_duration: float\n    total_duration: float\n\n    @staticmethod\n    def average(cases: list[ReportCase]) -> ReportCaseAggregate:\n        \"\"\"Produce a synthetic \"summary\" case by averaging quantitative attributes.\"\"\"\n        num_cases = len(cases)\n        if num_cases == 0:\n            return ReportCaseAggregate(\n                name='Averages',\n                scores={},\n                labels={},\n                metrics={},\n                assertions=None,\n                task_duration=0.0,\n                total_duration=0.0,\n            )\n\n        def _scores_averages(scores_by_name: list[dict[str, int | float | bool]]) -> dict[str, float]:\n            counts_by_name: dict[str, int] = defaultdict(int)\n            sums_by_name: dict[str, float] = defaultdict(float)\n            for sbn in scores_by_name:\n                for name, score in sbn.items():\n                    counts_by_name[name] += 1\n                    sums_by_name[name] += score\n            return {name: sums_by_name[name] / counts_by_name[name] for name in sums_by_name}\n\n        def _labels_averages(labels_by_name: list[dict[str, str]]) -> dict[str, dict[str, float]]:\n            counts_by_name: dict[str, int] = defaultdict(int)\n            sums_by_name: dict[str, dict[str, float]] = defaultdict(lambda: defaultdict(float))\n            for lbn in labels_by_name:\n                for name, label in lbn.items():\n                    counts_by_name[name] += 1\n                    sums_by_name[name][label] += 1\n            return {\n                name: {value: count / counts_by_name[name] for value, count in sums_by_name[name].items()}\n                for name in sums_by_name\n            }\n\n        average_task_duration = sum(case.task_duration for case in cases) / num_cases\n        average_total_duration = sum(case.total_duration for case in cases) / num_cases\n\n        # average_assertions: dict[str, float] = _scores_averages([{k: v.value for k, v in case.scores.items()} for case in cases])\n        average_scores: dict[str, float] = _scores_averages(\n            [{k: v.value for k, v in case.scores.items()} for case in cases]\n        )\n        average_labels: dict[str, dict[str, float]] = _labels_averages(\n            [{k: v.value for k, v in case.labels.items()} for case in cases]\n        )\n        average_metrics: dict[str, float] = _scores_averages([case.metrics for case in cases])\n\n        average_assertions: float | None = None\n        n_assertions = sum(len(case.assertions) for case in cases)\n        if n_assertions > 0:\n            n_passing = sum(1 for case in cases for assertion in case.assertions.values() if assertion.value)\n            average_assertions = n_passing / n_assertions\n\n        return ReportCaseAggregate(\n            name='Averages',\n            scores=average_scores,\n            labels=average_labels,\n            metrics=average_metrics,\n            assertions=average_assertions,\n            task_duration=average_task_duration,\n            total_duration=average_total_duration,\n        )\n\n```\n\n#### average\n\n```python\naverage(cases: list[ReportCase]) -> ReportCaseAggregate\n\n```\n\nProduce a synthetic \"summary\" case by averaging quantitative attributes.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\n@staticmethod\ndef average(cases: list[ReportCase]) -> ReportCaseAggregate:\n    \"\"\"Produce a synthetic \"summary\" case by averaging quantitative attributes.\"\"\"\n    num_cases = len(cases)\n    if num_cases == 0:\n        return ReportCaseAggregate(\n            name='Averages',\n            scores={},\n            labels={},\n            metrics={},\n            assertions=None,\n            task_duration=0.0,\n            total_duration=0.0,\n        )\n\n    def _scores_averages(scores_by_name: list[dict[str, int | float | bool]]) -> dict[str, float]:\n        counts_by_name: dict[str, int] = defaultdict(int)\n        sums_by_name: dict[str, float] = defaultdict(float)\n        for sbn in scores_by_name:\n            for name, score in sbn.items():\n                counts_by_name[name] += 1\n                sums_by_name[name] += score\n        return {name: sums_by_name[name] / counts_by_name[name] for name in sums_by_name}\n\n    def _labels_averages(labels_by_name: list[dict[str, str]]) -> dict[str, dict[str, float]]:\n        counts_by_name: dict[str, int] = defaultdict(int)\n        sums_by_name: dict[str, dict[str, float]] = defaultdict(lambda: defaultdict(float))\n        for lbn in labels_by_name:\n            for name, label in lbn.items():\n                counts_by_name[name] += 1\n                sums_by_name[name][label] += 1\n        return {\n            name: {value: count / counts_by_name[name] for value, count in sums_by_name[name].items()}\n            for name in sums_by_name\n        }\n\n    average_task_duration = sum(case.task_duration for case in cases) / num_cases\n    average_total_duration = sum(case.total_duration for case in cases) / num_cases\n\n    # average_assertions: dict[str, float] = _scores_averages([{k: v.value for k, v in case.scores.items()} for case in cases])\n    average_scores: dict[str, float] = _scores_averages(\n        [{k: v.value for k, v in case.scores.items()} for case in cases]\n    )\n    average_labels: dict[str, dict[str, float]] = _labels_averages(\n        [{k: v.value for k, v in case.labels.items()} for case in cases]\n    )\n    average_metrics: dict[str, float] = _scores_averages([case.metrics for case in cases])\n\n    average_assertions: float | None = None\n    n_assertions = sum(len(case.assertions) for case in cases)\n    if n_assertions > 0:\n        n_passing = sum(1 for case in cases for assertion in case.assertions.values() if assertion.value)\n        average_assertions = n_passing / n_assertions\n\n    return ReportCaseAggregate(\n        name='Averages',\n        scores=average_scores,\n        labels=average_labels,\n        metrics=average_metrics,\n        assertions=average_assertions,\n        task_duration=average_task_duration,\n        total_duration=average_total_duration,\n    )\n\n```\n\n### EvaluationReport\n\nBases: `Generic[InputsT, OutputT, MetadataT]`\n\nA report of the results of evaluating a model on a set of cases.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\n@dataclass(kw_only=True)\nclass EvaluationReport(Generic[InputsT, OutputT, MetadataT]):\n    \"\"\"A report of the results of evaluating a model on a set of cases.\"\"\"\n\n    name: str\n    \"\"\"The name of the report.\"\"\"\n\n    cases: list[ReportCase[InputsT, OutputT, MetadataT]]\n    \"\"\"The cases in the report.\"\"\"\n    failures: list[ReportCaseFailure[InputsT, OutputT, MetadataT]] = field(default_factory=list)\n    \"\"\"The failures in the report. These are cases where task execution raised an exception.\"\"\"\n\n    experiment_metadata: dict[str, Any] | None = None\n    \"\"\"Metadata associated with the specific experiment represented by this report.\"\"\"\n    trace_id: str | None = None\n    \"\"\"The trace ID of the evaluation.\"\"\"\n    span_id: str | None = None\n    \"\"\"The span ID of the evaluation.\"\"\"\n\n    def averages(self) -> ReportCaseAggregate | None:\n        if self.cases:\n            return ReportCaseAggregate.average(self.cases)\n        return None\n\n    def render(\n        self,\n        width: int | None = None,\n        baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,\n        *,\n        include_input: bool = False,\n        include_metadata: bool = False,\n        include_expected_output: bool = False,\n        include_output: bool = False,\n        include_durations: bool = True,\n        include_total_duration: bool = False,\n        include_removed_cases: bool = False,\n        include_averages: bool = True,\n        include_errors: bool = True,\n        include_error_stacktrace: bool = False,\n        include_evaluator_failures: bool = True,\n        input_config: RenderValueConfig | None = None,\n        metadata_config: RenderValueConfig | None = None,\n        output_config: RenderValueConfig | None = None,\n        score_configs: dict[str, RenderNumberConfig] | None = None,\n        label_configs: dict[str, RenderValueConfig] | None = None,\n        metric_configs: dict[str, RenderNumberConfig] | None = None,\n        duration_config: RenderNumberConfig | None = None,\n        include_reasons: bool = False,\n    ) -> str:\n        \"\"\"Render this report to a nicely-formatted string, optionally comparing it to a baseline report.\n\n        If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.\n        \"\"\"\n        io_file = StringIO()\n        console = Console(width=width, file=io_file)\n        self.print(\n            width=width,\n            baseline=baseline,\n            console=console,\n            include_input=include_input,\n            include_metadata=include_metadata,\n            include_expected_output=include_expected_output,\n            include_output=include_output,\n            include_durations=include_durations,\n            include_total_duration=include_total_duration,\n            include_removed_cases=include_removed_cases,\n            include_averages=include_averages,\n            include_errors=include_errors,\n            include_error_stacktrace=include_error_stacktrace,\n            include_evaluator_failures=include_evaluator_failures,\n            input_config=input_config,\n            metadata_config=metadata_config,\n            output_config=output_config,\n            score_configs=score_configs,\n            label_configs=label_configs,\n            metric_configs=metric_configs,\n            duration_config=duration_config,\n            include_reasons=include_reasons,\n        )\n        return io_file.getvalue()\n\n    def print(\n        self,\n        width: int | None = None,\n        baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,\n        *,\n        console: Console | None = None,\n        include_input: bool = False,\n        include_metadata: bool = False,\n        include_expected_output: bool = False,\n        include_output: bool = False,\n        include_durations: bool = True,\n        include_total_duration: bool = False,\n        include_removed_cases: bool = False,\n        include_averages: bool = True,\n        include_errors: bool = True,\n        include_error_stacktrace: bool = False,\n        include_evaluator_failures: bool = True,\n        input_config: RenderValueConfig | None = None,\n        metadata_config: RenderValueConfig | None = None,\n        output_config: RenderValueConfig | None = None,\n        score_configs: dict[str, RenderNumberConfig] | None = None,\n        label_configs: dict[str, RenderValueConfig] | None = None,\n        metric_configs: dict[str, RenderNumberConfig] | None = None,\n        duration_config: RenderNumberConfig | None = None,\n        include_reasons: bool = False,\n    ) -> None:\n        \"\"\"Print this report to the console, optionally comparing it to a baseline report.\n\n        If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.\n        \"\"\"\n        if console is None:  # pragma: no branch\n            console = Console(width=width)\n\n        metadata_panel = self._metadata_panel(baseline=baseline)\n        renderable: RenderableType = self.console_table(\n            baseline=baseline,\n            include_input=include_input,\n            include_metadata=include_metadata,\n            include_expected_output=include_expected_output,\n            include_output=include_output,\n            include_durations=include_durations,\n            include_total_duration=include_total_duration,\n            include_removed_cases=include_removed_cases,\n            include_averages=include_averages,\n            include_evaluator_failures=include_evaluator_failures,\n            input_config=input_config,\n            metadata_config=metadata_config,\n            output_config=output_config,\n            score_configs=score_configs,\n            label_configs=label_configs,\n            metric_configs=metric_configs,\n            duration_config=duration_config,\n            include_reasons=include_reasons,\n            with_title=not metadata_panel,\n        )\n        # Wrap table with experiment metadata panel if present\n        if metadata_panel:\n            renderable = Group(metadata_panel, renderable)\n        console.print(renderable)\n        if include_errors and self.failures:  # pragma: no cover\n            failures_table = self.failures_table(\n                include_input=include_input,\n                include_metadata=include_metadata,\n                include_expected_output=include_expected_output,\n                include_error_message=True,\n                include_error_stacktrace=include_error_stacktrace,\n                input_config=input_config,\n                metadata_config=metadata_config,\n            )\n            console.print(failures_table, style='red')\n\n    # TODO(DavidM): in v2, change the return type here to RenderableType\n    def console_table(\n        self,\n        baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,\n        *,\n        include_input: bool = False,\n        include_metadata: bool = False,\n        include_expected_output: bool = False,\n        include_output: bool = False,\n        include_durations: bool = True,\n        include_total_duration: bool = False,\n        include_removed_cases: bool = False,\n        include_averages: bool = True,\n        include_evaluator_failures: bool = True,\n        input_config: RenderValueConfig | None = None,\n        metadata_config: RenderValueConfig | None = None,\n        output_config: RenderValueConfig | None = None,\n        score_configs: dict[str, RenderNumberConfig] | None = None,\n        label_configs: dict[str, RenderValueConfig] | None = None,\n        metric_configs: dict[str, RenderNumberConfig] | None = None,\n        duration_config: RenderNumberConfig | None = None,\n        include_reasons: bool = False,\n        with_title: bool = True,\n    ) -> Table:\n        \"\"\"Return a table containing the data from this report.\n\n        If a baseline is provided, returns a diff between this report and the baseline report.\n        Optionally include input and output details.\n        \"\"\"\n        renderer = EvaluationRenderer(\n            include_input=include_input,\n            include_metadata=include_metadata,\n            include_expected_output=include_expected_output,\n            include_output=include_output,\n            include_durations=include_durations,\n            include_total_duration=include_total_duration,\n            include_removed_cases=include_removed_cases,\n            include_averages=include_averages,\n            include_error_message=False,\n            include_error_stacktrace=False,\n            include_evaluator_failures=include_evaluator_failures,\n            input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},\n            metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},\n            output_config=output_config or _DEFAULT_VALUE_CONFIG,\n            score_configs=score_configs or {},\n            label_configs=label_configs or {},\n            metric_configs=metric_configs or {},\n            duration_config=duration_config or _DEFAULT_DURATION_CONFIG,\n            include_reasons=include_reasons,\n        )\n        if baseline is None:\n            return renderer.build_table(self, with_title=with_title)\n        else:\n            return renderer.build_diff_table(self, baseline, with_title=with_title)\n\n    def _metadata_panel(\n        self, baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None\n    ) -> RenderableType | None:\n        \"\"\"Wrap a table with an experiment metadata panel if metadata exists.\n\n        Args:\n            table: The table to wrap\n            baseline: Optional baseline report for diff metadata\n\n        Returns:\n            Either the table unchanged or a Group with Panel and Table\n        \"\"\"\n        if baseline is None:\n            # Single report - show metadata if present\n            if self.experiment_metadata:\n                metadata_text = Text()\n                items = list(self.experiment_metadata.items())\n                for i, (key, value) in enumerate(items):\n                    metadata_text.append(f'{key}: {value}', style='dim')\n                    if i < len(items) - 1:\n                        metadata_text.append('\\n')\n                return Panel(\n                    metadata_text,\n                    title=f'Evaluation Summary: {self.name}',\n                    title_align='left',\n                    border_style='dim',\n                    padding=(0, 1),\n                    expand=False,\n                )\n        else:\n            # Diff report - show metadata diff if either has metadata\n            if self.experiment_metadata or baseline.experiment_metadata:\n                diff_name = baseline.name if baseline.name == self.name else f'{baseline.name} → {self.name}'\n                metadata_text = Text()\n                lines_styles: list[tuple[str, str]] = []\n                if baseline.experiment_metadata and self.experiment_metadata:\n                    # Collect all keys from both\n                    all_keys = sorted(set(baseline.experiment_metadata.keys()) | set(self.experiment_metadata.keys()))\n                    for key in all_keys:\n                        baseline_val = baseline.experiment_metadata.get(key)\n                        report_val = self.experiment_metadata.get(key)\n                        if baseline_val == report_val:\n                            lines_styles.append((f'{key}: {report_val}', 'dim'))\n                        elif baseline_val is None:\n                            lines_styles.append((f'+ {key}: {report_val}', 'green'))\n                        elif report_val is None:\n                            lines_styles.append((f'- {key}: {baseline_val}', 'red'))\n                        else:\n                            lines_styles.append((f'{key}: {baseline_val} → {report_val}', 'yellow'))\n                elif self.experiment_metadata:\n                    lines_styles = [(f'+ {k}: {v}', 'green') for k, v in self.experiment_metadata.items()]\n                else:  # baseline.experiment_metadata only\n                    assert baseline.experiment_metadata is not None\n                    lines_styles = [(f'- {k}: {v}', 'red') for k, v in baseline.experiment_metadata.items()]\n\n                for i, (line, style) in enumerate(lines_styles):\n                    metadata_text.append(line, style=style)\n                    if i < len(lines_styles) - 1:\n                        metadata_text.append('\\n')\n\n                return Panel(\n                    metadata_text,\n                    title=f'Evaluation Diff: {diff_name}',\n                    title_align='left',\n                    border_style='dim',\n                    padding=(0, 1),\n                    expand=False,\n                )\n\n        return None\n\n    # TODO(DavidM): in v2, change the return type here to RenderableType\n    def failures_table(\n        self,\n        *,\n        include_input: bool = False,\n        include_metadata: bool = False,\n        include_expected_output: bool = False,\n        include_error_message: bool = True,\n        include_error_stacktrace: bool = True,\n        input_config: RenderValueConfig | None = None,\n        metadata_config: RenderValueConfig | None = None,\n    ) -> Table:\n        \"\"\"Return a table containing the failures in this report.\"\"\"\n        renderer = EvaluationRenderer(\n            include_input=include_input,\n            include_metadata=include_metadata,\n            include_expected_output=include_expected_output,\n            include_output=False,\n            include_durations=False,\n            include_total_duration=False,\n            include_removed_cases=False,\n            include_averages=False,\n            input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},\n            metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},\n            output_config=_DEFAULT_VALUE_CONFIG,\n            score_configs={},\n            label_configs={},\n            metric_configs={},\n            duration_config=_DEFAULT_DURATION_CONFIG,\n            include_reasons=False,\n            include_error_message=include_error_message,\n            include_error_stacktrace=include_error_stacktrace,\n            include_evaluator_failures=False,  # Not applicable for failures table\n        )\n        return renderer.build_failures_table(self)\n\n    def __str__(self) -> str:  # pragma: lax no cover\n        \"\"\"Return a string representation of the report.\"\"\"\n        return self.render()\n\n```\n\n#### name\n\n```python\nname: str\n\n```\n\nThe name of the report.\n\n#### cases\n\n```python\ncases: list[ReportCase[InputsT, OutputT, MetadataT]]\n\n```\n\nThe cases in the report.\n\n#### failures\n\n```python\nfailures: list[\n    ReportCaseFailure[InputsT, OutputT, MetadataT]\n] = field(default_factory=list)\n\n```\n\nThe failures in the report. These are cases where task execution raised an exception.\n\n#### experiment_metadata\n\n```python\nexperiment_metadata: dict[str, Any] | None = None\n\n```\n\nMetadata associated with the specific experiment represented by this report.\n\n#### trace_id\n\n```python\ntrace_id: str | None = None\n\n```\n\nThe trace ID of the evaluation.\n\n#### span_id\n\n```python\nspan_id: str | None = None\n\n```\n\nThe span ID of the evaluation.\n\n#### render\n\n```python\nrender(\n    width: int | None = None,\n    baseline: (\n        EvaluationReport[InputsT, OutputT, MetadataT] | None\n    ) = None,\n    *,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_output: bool = False,\n    include_durations: bool = True,\n    include_total_duration: bool = False,\n    include_removed_cases: bool = False,\n    include_averages: bool = True,\n    include_errors: bool = True,\n    include_error_stacktrace: bool = False,\n    include_evaluator_failures: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n    output_config: RenderValueConfig | None = None,\n    score_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    label_configs: (\n        dict[str, RenderValueConfig] | None\n    ) = None,\n    metric_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    duration_config: RenderNumberConfig | None = None,\n    include_reasons: bool = False\n) -> str\n\n```\n\nRender this report to a nicely-formatted string, optionally comparing it to a baseline report.\n\nIf you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\ndef render(\n    self,\n    width: int | None = None,\n    baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,\n    *,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_output: bool = False,\n    include_durations: bool = True,\n    include_total_duration: bool = False,\n    include_removed_cases: bool = False,\n    include_averages: bool = True,\n    include_errors: bool = True,\n    include_error_stacktrace: bool = False,\n    include_evaluator_failures: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n    output_config: RenderValueConfig | None = None,\n    score_configs: dict[str, RenderNumberConfig] | None = None,\n    label_configs: dict[str, RenderValueConfig] | None = None,\n    metric_configs: dict[str, RenderNumberConfig] | None = None,\n    duration_config: RenderNumberConfig | None = None,\n    include_reasons: bool = False,\n) -> str:\n    \"\"\"Render this report to a nicely-formatted string, optionally comparing it to a baseline report.\n\n    If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.\n    \"\"\"\n    io_file = StringIO()\n    console = Console(width=width, file=io_file)\n    self.print(\n        width=width,\n        baseline=baseline,\n        console=console,\n        include_input=include_input,\n        include_metadata=include_metadata,\n        include_expected_output=include_expected_output,\n        include_output=include_output,\n        include_durations=include_durations,\n        include_total_duration=include_total_duration,\n        include_removed_cases=include_removed_cases,\n        include_averages=include_averages,\n        include_errors=include_errors,\n        include_error_stacktrace=include_error_stacktrace,\n        include_evaluator_failures=include_evaluator_failures,\n        input_config=input_config,\n        metadata_config=metadata_config,\n        output_config=output_config,\n        score_configs=score_configs,\n        label_configs=label_configs,\n        metric_configs=metric_configs,\n        duration_config=duration_config,\n        include_reasons=include_reasons,\n    )\n    return io_file.getvalue()\n\n```\n\n#### print\n\n```python\nprint(\n    width: int | None = None,\n    baseline: (\n        EvaluationReport[InputsT, OutputT, MetadataT] | None\n    ) = None,\n    *,\n    console: Console | None = None,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_output: bool = False,\n    include_durations: bool = True,\n    include_total_duration: bool = False,\n    include_removed_cases: bool = False,\n    include_averages: bool = True,\n    include_errors: bool = True,\n    include_error_stacktrace: bool = False,\n    include_evaluator_failures: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n    output_config: RenderValueConfig | None = None,\n    score_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    label_configs: (\n        dict[str, RenderValueConfig] | None\n    ) = None,\n    metric_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    duration_config: RenderNumberConfig | None = None,\n    include_reasons: bool = False\n) -> None\n\n```\n\nPrint this report to the console, optionally comparing it to a baseline report.\n\nIf you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\ndef print(\n    self,\n    width: int | None = None,\n    baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,\n    *,\n    console: Console | None = None,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_output: bool = False,\n    include_durations: bool = True,\n    include_total_duration: bool = False,\n    include_removed_cases: bool = False,\n    include_averages: bool = True,\n    include_errors: bool = True,\n    include_error_stacktrace: bool = False,\n    include_evaluator_failures: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n    output_config: RenderValueConfig | None = None,\n    score_configs: dict[str, RenderNumberConfig] | None = None,\n    label_configs: dict[str, RenderValueConfig] | None = None,\n    metric_configs: dict[str, RenderNumberConfig] | None = None,\n    duration_config: RenderNumberConfig | None = None,\n    include_reasons: bool = False,\n) -> None:\n    \"\"\"Print this report to the console, optionally comparing it to a baseline report.\n\n    If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.\n    \"\"\"\n    if console is None:  # pragma: no branch\n        console = Console(width=width)\n\n    metadata_panel = self._metadata_panel(baseline=baseline)\n    renderable: RenderableType = self.console_table(\n        baseline=baseline,\n        include_input=include_input,\n        include_metadata=include_metadata,\n        include_expected_output=include_expected_output,\n        include_output=include_output,\n        include_durations=include_durations,\n        include_total_duration=include_total_duration,\n        include_removed_cases=include_removed_cases,\n        include_averages=include_averages,\n        include_evaluator_failures=include_evaluator_failures,\n        input_config=input_config,\n        metadata_config=metadata_config,\n        output_config=output_config,\n        score_configs=score_configs,\n        label_configs=label_configs,\n        metric_configs=metric_configs,\n        duration_config=duration_config,\n        include_reasons=include_reasons,\n        with_title=not metadata_panel,\n    )\n    # Wrap table with experiment metadata panel if present\n    if metadata_panel:\n        renderable = Group(metadata_panel, renderable)\n    console.print(renderable)\n    if include_errors and self.failures:  # pragma: no cover\n        failures_table = self.failures_table(\n            include_input=include_input,\n            include_metadata=include_metadata,\n            include_expected_output=include_expected_output,\n            include_error_message=True,\n            include_error_stacktrace=include_error_stacktrace,\n            input_config=input_config,\n            metadata_config=metadata_config,\n        )\n        console.print(failures_table, style='red')\n\n```\n\n#### console_table\n\n```python\nconsole_table(\n    baseline: (\n        EvaluationReport[InputsT, OutputT, MetadataT] | None\n    ) = None,\n    *,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_output: bool = False,\n    include_durations: bool = True,\n    include_total_duration: bool = False,\n    include_removed_cases: bool = False,\n    include_averages: bool = True,\n    include_evaluator_failures: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n    output_config: RenderValueConfig | None = None,\n    score_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    label_configs: (\n        dict[str, RenderValueConfig] | None\n    ) = None,\n    metric_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    duration_config: RenderNumberConfig | None = None,\n    include_reasons: bool = False,\n    with_title: bool = True\n) -> Table\n\n```\n\nReturn a table containing the data from this report.\n\nIf a baseline is provided, returns a diff between this report and the baseline report. Optionally include input and output details.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\ndef console_table(\n    self,\n    baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,\n    *,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_output: bool = False,\n    include_durations: bool = True,\n    include_total_duration: bool = False,\n    include_removed_cases: bool = False,\n    include_averages: bool = True,\n    include_evaluator_failures: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n    output_config: RenderValueConfig | None = None,\n    score_configs: dict[str, RenderNumberConfig] | None = None,\n    label_configs: dict[str, RenderValueConfig] | None = None,\n    metric_configs: dict[str, RenderNumberConfig] | None = None,\n    duration_config: RenderNumberConfig | None = None,\n    include_reasons: bool = False,\n    with_title: bool = True,\n) -> Table:\n    \"\"\"Return a table containing the data from this report.\n\n    If a baseline is provided, returns a diff between this report and the baseline report.\n    Optionally include input and output details.\n    \"\"\"\n    renderer = EvaluationRenderer(\n        include_input=include_input,\n        include_metadata=include_metadata,\n        include_expected_output=include_expected_output,\n        include_output=include_output,\n        include_durations=include_durations,\n        include_total_duration=include_total_duration,\n        include_removed_cases=include_removed_cases,\n        include_averages=include_averages,\n        include_error_message=False,\n        include_error_stacktrace=False,\n        include_evaluator_failures=include_evaluator_failures,\n        input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},\n        metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},\n        output_config=output_config or _DEFAULT_VALUE_CONFIG,\n        score_configs=score_configs or {},\n        label_configs=label_configs or {},\n        metric_configs=metric_configs or {},\n        duration_config=duration_config or _DEFAULT_DURATION_CONFIG,\n        include_reasons=include_reasons,\n    )\n    if baseline is None:\n        return renderer.build_table(self, with_title=with_title)\n    else:\n        return renderer.build_diff_table(self, baseline, with_title=with_title)\n\n```\n\n#### failures_table\n\n```python\nfailures_table(\n    *,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_error_message: bool = True,\n    include_error_stacktrace: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None\n) -> Table\n\n```\n\nReturn a table containing the failures in this report.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\ndef failures_table(\n    self,\n    *,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_error_message: bool = True,\n    include_error_stacktrace: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n) -> Table:\n    \"\"\"Return a table containing the failures in this report.\"\"\"\n    renderer = EvaluationRenderer(\n        include_input=include_input,\n        include_metadata=include_metadata,\n        include_expected_output=include_expected_output,\n        include_output=False,\n        include_durations=False,\n        include_total_duration=False,\n        include_removed_cases=False,\n        include_averages=False,\n        input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},\n        metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},\n        output_config=_DEFAULT_VALUE_CONFIG,\n        score_configs={},\n        label_configs={},\n        metric_configs={},\n        duration_config=_DEFAULT_DURATION_CONFIG,\n        include_reasons=False,\n        include_error_message=include_error_message,\n        include_error_stacktrace=include_error_stacktrace,\n        include_evaluator_failures=False,  # Not applicable for failures table\n    )\n    return renderer.build_failures_table(self)\n\n```\n\n#### __str__\n\n```python\n__str__() -> str\n\n```\n\nReturn a string representation of the report.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\ndef __str__(self) -> str:  # pragma: lax no cover\n    \"\"\"Return a string representation of the report.\"\"\"\n    return self.render()\n\n```\n\n### RenderValueConfig\n\nBases: `TypedDict`\n\nA configuration for rendering a values in an Evaluation report.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\nclass RenderValueConfig(TypedDict, total=False):\n    \"\"\"A configuration for rendering a values in an Evaluation report.\"\"\"\n\n    value_formatter: str | Callable[[Any], str]\n    diff_checker: Callable[[Any, Any], bool] | None\n    diff_formatter: Callable[[Any, Any], str | None] | None\n    diff_style: str\n\n```\n\n### RenderNumberConfig\n\nBases: `TypedDict`\n\nA configuration for rendering a particular score or metric in an Evaluation report.\n\nSee the implementation of `_RenderNumber` for more clarity on how these parameters affect the rendering.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\nclass RenderNumberConfig(TypedDict, total=False):\n    \"\"\"A configuration for rendering a particular score or metric in an Evaluation report.\n\n    See the implementation of `_RenderNumber` for more clarity on how these parameters affect the rendering.\n    \"\"\"\n\n    value_formatter: str | Callable[[float | int], str]\n    \"\"\"The logic to use for formatting values.\n\n    * If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures.\n    * You can also use a custom string format spec, e.g. '{:.3f}'\n    * You can also use a custom function, e.g. lambda x: f'{x:.3f}'\n    \"\"\"\n    diff_formatter: str | Callable[[float | int, float | int], str | None] | None\n    \"\"\"The logic to use for formatting details about the diff.\n\n    The strings produced by the value_formatter will always be included in the reports, but the diff_formatter is\n    used to produce additional text about the difference between the old and new values, such as the absolute or\n    relative difference.\n\n    * If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four\n        significant figures, and will include the percentage change.\n    * You can also use a custom string format spec, e.g. '{:+.3f}'\n    * You can also use a custom function, e.g. lambda x: f'{x:+.3f}'.\n        If this function returns None, no extra diff text will be added.\n    * You can also use None to never generate extra diff text.\n    \"\"\"\n    diff_atol: float\n    \"\"\"The absolute tolerance for considering a difference \"significant\".\n\n    A difference is \"significant\" if `abs(new - old) < self.diff_atol + self.diff_rtol * abs(old)`.\n\n    If a difference is not significant, it will not have the diff styles applied. Note that we still show\n    both the rendered before and after values in the diff any time they differ, even if the difference is not\n    significant. (If the rendered values are exactly the same, we only show the value once.)\n\n    If not provided, use 1e-6.\n    \"\"\"\n    diff_rtol: float\n    \"\"\"The relative tolerance for considering a difference \"significant\".\n\n    See the description of `diff_atol` for more details about what makes a difference \"significant\".\n\n    If not provided, use 0.001 if all values are ints, otherwise 0.05.\n    \"\"\"\n    diff_increase_style: str\n    \"\"\"The style to apply to diffed values that have a significant increase.\n\n    See the description of `diff_atol` for more details about what makes a difference \"significant\".\n\n    If not provided, use green for scores and red for metrics. You can also use arbitrary `rich` styles, such as \"bold red\".\n    \"\"\"\n    diff_decrease_style: str\n    \"\"\"The style to apply to diffed values that have significant decrease.\n\n    See the description of `diff_atol` for more details about what makes a difference \"significant\".\n\n    If not provided, use red for scores and green for metrics. You can also use arbitrary `rich` styles, such as \"bold red\".\n    \"\"\"\n\n```\n\n#### value_formatter\n\n```python\nvalue_formatter: str | Callable[[float | int], str]\n\n```\n\nThe logic to use for formatting values.\n\n- If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures.\n- You can also use a custom string format spec, e.g. '{:.3f}'\n- You can also use a custom function, e.g. lambda x: f'{x:.3f}'\n\n#### diff_formatter\n\n```python\ndiff_formatter: (\n    str\n    | Callable[[float | int, float | int], str | None]\n    | None\n)\n\n```\n\nThe logic to use for formatting details about the diff.\n\nThe strings produced by the value_formatter will always be included in the reports, but the diff_formatter is used to produce additional text about the difference between the old and new values, such as the absolute or relative difference.\n\n- If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures, and will include the percentage change.\n- You can also use a custom string format spec, e.g. '{:+.3f}'\n- You can also use a custom function, e.g. lambda x: f'{x:+.3f}'. If this function returns None, no extra diff text will be added.\n- You can also use None to never generate extra diff text.\n\n#### diff_atol\n\n```python\ndiff_atol: float\n\n```\n\nThe absolute tolerance for considering a difference \"significant\".\n\nA difference is \"significant\" if `abs(new - old) < self.diff_atol + self.diff_rtol * abs(old)`.\n\nIf a difference is not significant, it will not have the diff styles applied. Note that we still show both the rendered before and after values in the diff any time they differ, even if the difference is not significant. (If the rendered values are exactly the same, we only show the value once.)\n\nIf not provided, use 1e-6.\n\n#### diff_rtol\n\n```python\ndiff_rtol: float\n\n```\n\nThe relative tolerance for considering a difference \"significant\".\n\nSee the description of `diff_atol` for more details about what makes a difference \"significant\".\n\nIf not provided, use 0.001 if all values are ints, otherwise 0.05.\n\n#### diff_increase_style\n\n```python\ndiff_increase_style: str\n\n```\n\nThe style to apply to diffed values that have a significant increase.\n\nSee the description of `diff_atol` for more details about what makes a difference \"significant\".\n\nIf not provided, use green for scores and red for metrics. You can also use arbitrary `rich` styles, such as \"bold red\".\n\n#### diff_decrease_style\n\n```python\ndiff_decrease_style: str\n\n```\n\nThe style to apply to diffed values that have significant decrease.\n\nSee the description of `diff_atol` for more details about what makes a difference \"significant\".\n\nIf not provided, use red for scores and green for metrics. You can also use arbitrary `rich` styles, such as \"bold red\".\n\n### EvaluationRenderer\n\nA class for rendering an EvalReport or the diff between two EvalReports.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\n@dataclass(kw_only=True)\nclass EvaluationRenderer:\n    \"\"\"A class for rendering an EvalReport or the diff between two EvalReports.\"\"\"\n\n    # Columns to include\n    include_input: bool\n    include_metadata: bool\n    include_expected_output: bool\n    include_output: bool\n    include_durations: bool\n    include_total_duration: bool\n\n    # Rows to include\n    include_removed_cases: bool\n    include_averages: bool\n\n    input_config: RenderValueConfig\n    metadata_config: RenderValueConfig\n    output_config: RenderValueConfig\n    score_configs: dict[str, RenderNumberConfig]\n    label_configs: dict[str, RenderValueConfig]\n    metric_configs: dict[str, RenderNumberConfig]\n    duration_config: RenderNumberConfig\n\n    # Data to include\n    include_reasons: bool  # only applies to reports, not to diffs\n\n    include_error_message: bool\n    include_error_stacktrace: bool\n    include_evaluator_failures: bool\n\n    def include_scores(self, report: EvaluationReport, baseline: EvaluationReport | None = None):\n        return any(case.scores for case in self._all_cases(report, baseline))\n\n    def include_labels(self, report: EvaluationReport, baseline: EvaluationReport | None = None):\n        return any(case.labels for case in self._all_cases(report, baseline))\n\n    def include_metrics(self, report: EvaluationReport, baseline: EvaluationReport | None = None):\n        return any(case.metrics for case in self._all_cases(report, baseline))\n\n    def include_assertions(self, report: EvaluationReport, baseline: EvaluationReport | None = None):\n        return any(case.assertions for case in self._all_cases(report, baseline))\n\n    def include_evaluator_failures_column(self, report: EvaluationReport, baseline: EvaluationReport | None = None):\n        return self.include_evaluator_failures and any(\n            case.evaluator_failures for case in self._all_cases(report, baseline)\n        )\n\n    def _all_cases(self, report: EvaluationReport, baseline: EvaluationReport | None) -> list[ReportCase]:\n        if not baseline:\n            return report.cases\n        else:\n            return report.cases + self._baseline_cases_to_include(report, baseline)\n\n    def _baseline_cases_to_include(self, report: EvaluationReport, baseline: EvaluationReport) -> list[ReportCase]:\n        if self.include_removed_cases:\n            return baseline.cases\n        report_case_names = {case.name for case in report.cases}\n        return [case for case in baseline.cases if case.name in report_case_names]\n\n    def _get_case_renderer(\n        self, report: EvaluationReport, baseline: EvaluationReport | None = None\n    ) -> ReportCaseRenderer:\n        input_renderer = _ValueRenderer.from_config(self.input_config)\n        metadata_renderer = _ValueRenderer.from_config(self.metadata_config)\n        output_renderer = _ValueRenderer.from_config(self.output_config)\n        score_renderers = self._infer_score_renderers(report, baseline)\n        label_renderers = self._infer_label_renderers(report, baseline)\n        metric_renderers = self._infer_metric_renderers(report, baseline)\n        duration_renderer = _NumberRenderer.infer_from_config(\n            self.duration_config, 'duration', [x.task_duration for x in self._all_cases(report, baseline)]\n        )\n\n        return ReportCaseRenderer(\n            include_input=self.include_input,\n            include_metadata=self.include_metadata,\n            include_expected_output=self.include_expected_output,\n            include_output=self.include_output,\n            include_scores=self.include_scores(report, baseline),\n            include_labels=self.include_labels(report, baseline),\n            include_metrics=self.include_metrics(report, baseline),\n            include_assertions=self.include_assertions(report, baseline),\n            include_reasons=self.include_reasons,\n            include_durations=self.include_durations,\n            include_total_duration=self.include_total_duration,\n            include_error_message=self.include_error_message,\n            include_error_stacktrace=self.include_error_stacktrace,\n            include_evaluator_failures=self.include_evaluator_failures_column(report, baseline),\n            input_renderer=input_renderer,\n            metadata_renderer=metadata_renderer,\n            output_renderer=output_renderer,\n            score_renderers=score_renderers,\n            label_renderers=label_renderers,\n            metric_renderers=metric_renderers,\n            duration_renderer=duration_renderer,\n        )\n\n    # TODO(DavidM): in v2, change the return type here to RenderableType\n    def build_table(self, report: EvaluationReport, *, with_title: bool = True) -> Table:\n        \"\"\"Build a table for the report.\n\n        Args:\n            report: The evaluation report to render\n            with_title: Whether to include the title in the table (default True)\n\n        Returns:\n            A Rich Table object\n        \"\"\"\n        case_renderer = self._get_case_renderer(report)\n\n        title = f'Evaluation Summary: {report.name}' if with_title else ''\n        table = case_renderer.build_base_table(title)\n\n        for case in report.cases:\n            table.add_row(*case_renderer.build_row(case))\n\n        if self.include_averages:  # pragma: no branch\n            average = report.averages()\n            if average:  # pragma: no branch\n                table.add_row(*case_renderer.build_aggregate_row(average))\n\n        return table\n\n    # TODO(DavidM): in v2, change the return type here to RenderableType\n    def build_diff_table(\n        self, report: EvaluationReport, baseline: EvaluationReport, *, with_title: bool = True\n    ) -> Table:\n        \"\"\"Build a diff table comparing report to baseline.\n\n        Args:\n            report: The evaluation report to compare\n            baseline: The baseline report to compare against\n            with_title: Whether to include the title in the table (default True)\n\n        Returns:\n            A Rich Table object\n        \"\"\"\n        report_cases = report.cases\n        baseline_cases = self._baseline_cases_to_include(report, baseline)\n\n        report_cases_by_id = {case.name: case for case in report_cases}\n        baseline_cases_by_id = {case.name: case for case in baseline_cases}\n\n        diff_cases: list[tuple[ReportCase, ReportCase]] = []\n        removed_cases: list[ReportCase] = []\n        added_cases: list[ReportCase] = []\n\n        for case_id in sorted(set(baseline_cases_by_id.keys()) | set(report_cases_by_id.keys())):\n            maybe_baseline_case = baseline_cases_by_id.get(case_id)\n            maybe_report_case = report_cases_by_id.get(case_id)\n            if maybe_baseline_case and maybe_report_case:\n                diff_cases.append((maybe_baseline_case, maybe_report_case))\n            elif maybe_baseline_case:\n                removed_cases.append(maybe_baseline_case)\n            elif maybe_report_case:\n                added_cases.append(maybe_report_case)\n            else:  # pragma: no cover\n                assert False, 'This should be unreachable'\n\n        case_renderer = self._get_case_renderer(report, baseline)\n        diff_name = baseline.name if baseline.name == report.name else f'{baseline.name} → {report.name}'\n\n        title = f'Evaluation Diff: {diff_name}' if with_title else ''\n        table = case_renderer.build_base_table(title)\n\n        for baseline_case, new_case in diff_cases:\n            table.add_row(*case_renderer.build_diff_row(new_case, baseline_case))\n        for case in added_cases:\n            row = case_renderer.build_row(case)\n            row[0] = f'[green]+ Added Case[/]\\n{row[0]}'\n            table.add_row(*row)\n        for case in removed_cases:\n            row = case_renderer.build_row(case)\n            row[0] = f'[red]- Removed Case[/]\\n{row[0]}'\n            table.add_row(*row)\n\n        if self.include_averages:  # pragma: no branch\n            report_average = ReportCaseAggregate.average(report_cases)\n            baseline_average = ReportCaseAggregate.average(baseline_cases)\n            table.add_row(*case_renderer.build_diff_aggregate_row(report_average, baseline_average))\n\n        return table\n\n    # TODO(DavidM): in v2, change the return type here to RenderableType\n    def build_failures_table(self, report: EvaluationReport) -> Table:\n        case_renderer = self._get_case_renderer(report)\n        table = case_renderer.build_failures_table('Case Failures')\n        for case in report.failures:\n            table.add_row(*case_renderer.build_failure_row(case))\n\n        return table\n\n    def _infer_score_renderers(\n        self, report: EvaluationReport, baseline: EvaluationReport | None\n    ) -> dict[str, _NumberRenderer]:\n        all_cases = self._all_cases(report, baseline)\n\n        values_by_name: dict[str, list[float | int]] = {}\n        for case in all_cases:\n            for k, score in case.scores.items():\n                values_by_name.setdefault(k, []).append(score.value)\n\n        all_renderers: dict[str, _NumberRenderer] = {}\n        for name, values in values_by_name.items():\n            merged_config = _DEFAULT_NUMBER_CONFIG.copy()\n            merged_config.update(self.score_configs.get(name, {}))\n            all_renderers[name] = _NumberRenderer.infer_from_config(merged_config, 'score', values)\n        return all_renderers\n\n    def _infer_label_renderers(\n        self, report: EvaluationReport, baseline: EvaluationReport | None\n    ) -> dict[str, _ValueRenderer]:\n        all_cases = self._all_cases(report, baseline)\n        all_names: set[str] = set()\n        for case in all_cases:\n            for k in case.labels:\n                all_names.add(k)\n\n        all_renderers: dict[str, _ValueRenderer] = {}\n        for name in all_names:\n            merged_config = _DEFAULT_VALUE_CONFIG.copy()\n            merged_config.update(self.label_configs.get(name, {}))\n            all_renderers[name] = _ValueRenderer.from_config(merged_config)\n        return all_renderers\n\n    def _infer_metric_renderers(\n        self, report: EvaluationReport, baseline: EvaluationReport | None\n    ) -> dict[str, _NumberRenderer]:\n        all_cases = self._all_cases(report, baseline)\n\n        values_by_name: dict[str, list[float | int]] = {}\n        for case in all_cases:\n            for k, v in case.metrics.items():\n                values_by_name.setdefault(k, []).append(v)\n\n        all_renderers: dict[str, _NumberRenderer] = {}\n        for name, values in values_by_name.items():\n            merged_config = _DEFAULT_NUMBER_CONFIG.copy()\n            merged_config.update(self.metric_configs.get(name, {}))\n            all_renderers[name] = _NumberRenderer.infer_from_config(merged_config, 'metric', values)\n        return all_renderers\n\n    def _infer_duration_renderer(\n        self, report: EvaluationReport, baseline: EvaluationReport | None\n    ) -> _NumberRenderer:  # pragma: no cover\n        all_cases = self._all_cases(report, baseline)\n        all_durations = [x.task_duration for x in all_cases]\n        if self.include_total_duration:\n            all_durations += [x.total_duration for x in all_cases]\n        return _NumberRenderer.infer_from_config(self.duration_config, 'duration', all_durations)\n\n```\n\n#### build_table\n\n```python\nbuild_table(\n    report: EvaluationReport, *, with_title: bool = True\n) -> Table\n\n```\n\nBuild a table for the report.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `report` | `EvaluationReport` | The evaluation report to render | *required* | | `with_title` | `bool` | Whether to include the title in the table (default True) | `True` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `Table` | A Rich Table object |\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\ndef build_table(self, report: EvaluationReport, *, with_title: bool = True) -> Table:\n    \"\"\"Build a table for the report.\n\n    Args:\n        report: The evaluation report to render\n        with_title: Whether to include the title in the table (default True)\n\n    Returns:\n        A Rich Table object\n    \"\"\"\n    case_renderer = self._get_case_renderer(report)\n\n    title = f'Evaluation Summary: {report.name}' if with_title else ''\n    table = case_renderer.build_base_table(title)\n\n    for case in report.cases:\n        table.add_row(*case_renderer.build_row(case))\n\n    if self.include_averages:  # pragma: no branch\n        average = report.averages()\n        if average:  # pragma: no branch\n            table.add_row(*case_renderer.build_aggregate_row(average))\n\n    return table\n\n```\n\n#### build_diff_table\n\n```python\nbuild_diff_table(\n    report: EvaluationReport,\n    baseline: EvaluationReport,\n    *,\n    with_title: bool = True\n) -> Table\n\n```\n\nBuild a diff table comparing report to baseline.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `report` | `EvaluationReport` | The evaluation report to compare | *required* | | `baseline` | `EvaluationReport` | The baseline report to compare against | *required* | | `with_title` | `bool` | Whether to include the title in the table (default True) | `True` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `Table` | A Rich Table object |\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n```python\ndef build_diff_table(\n    self, report: EvaluationReport, baseline: EvaluationReport, *, with_title: bool = True\n) -> Table:\n    \"\"\"Build a diff table comparing report to baseline.\n\n    Args:\n        report: The evaluation report to compare\n        baseline: The baseline report to compare against\n        with_title: Whether to include the title in the table (default True)\n\n    Returns:\n        A Rich Table object\n    \"\"\"\n    report_cases = report.cases\n    baseline_cases = self._baseline_cases_to_include(report, baseline)\n\n    report_cases_by_id = {case.name: case for case in report_cases}\n    baseline_cases_by_id = {case.name: case for case in baseline_cases}\n\n    diff_cases: list[tuple[ReportCase, ReportCase]] = []\n    removed_cases: list[ReportCase] = []\n    added_cases: list[ReportCase] = []\n\n    for case_id in sorted(set(baseline_cases_by_id.keys()) | set(report_cases_by_id.keys())):\n        maybe_baseline_case = baseline_cases_by_id.get(case_id)\n        maybe_report_case = report_cases_by_id.get(case_id)\n        if maybe_baseline_case and maybe_report_case:\n            diff_cases.append((maybe_baseline_case, maybe_report_case))\n        elif maybe_baseline_case:\n            removed_cases.append(maybe_baseline_case)\n        elif maybe_report_case:\n            added_cases.append(maybe_report_case)\n        else:  # pragma: no cover\n            assert False, 'This should be unreachable'\n\n    case_renderer = self._get_case_renderer(report, baseline)\n    diff_name = baseline.name if baseline.name == report.name else f'{baseline.name} → {report.name}'\n\n    title = f'Evaluation Diff: {diff_name}' if with_title else ''\n    table = case_renderer.build_base_table(title)\n\n    for baseline_case, new_case in diff_cases:\n        table.add_row(*case_renderer.build_diff_row(new_case, baseline_case))\n    for case in added_cases:\n        row = case_renderer.build_row(case)\n        row[0] = f'[green]+ Added Case[/]\\n{row[0]}'\n        table.add_row(*row)\n    for case in removed_cases:\n        row = case_renderer.build_row(case)\n        row[0] = f'[red]- Removed Case[/]\\n{row[0]}'\n        table.add_row(*row)\n\n    if self.include_averages:  # pragma: no branch\n        report_average = ReportCaseAggregate.average(report_cases)\n        baseline_average = ReportCaseAggregate.average(baseline_cases)\n        table.add_row(*case_renderer.build_diff_aggregate_row(report_average, baseline_average))\n\n    return table\n\n```",
  "content_length": 62132
}