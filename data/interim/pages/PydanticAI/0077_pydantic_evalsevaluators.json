{
  "title": "`pydantic_evals.evaluators`",
  "source_url": null,
  "content": "### Contains\n\nBases: `Evaluator[object, object, object]`\n\nCheck if the output contains the expected output.\n\nFor strings, checks if expected_output is a substring of output. For lists/tuples, checks if expected_output is in output. For dicts, checks if all key-value pairs in expected_output are in output.\n\nNote: case_sensitive only applies when both the value and output are strings.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n```python\n@dataclass(repr=False)\nclass Contains(Evaluator[object, object, object]):\n    \"\"\"Check if the output contains the expected output.\n\n    For strings, checks if expected_output is a substring of output.\n    For lists/tuples, checks if expected_output is in output.\n    For dicts, checks if all key-value pairs in expected_output are in output.\n\n    Note: case_sensitive only applies when both the value and output are strings.\n    \"\"\"\n\n    value: Any\n    case_sensitive: bool = True\n    as_strings: bool = False\n    evaluation_name: str | None = field(default=None)\n\n    def evaluate(\n        self,\n        ctx: EvaluatorContext[object, object, object],\n    ) -> EvaluationReason:\n        # Convert objects to strings if requested\n        failure_reason: str | None = None\n        as_strings = self.as_strings or (isinstance(self.value, str) and isinstance(ctx.output, str))\n        if as_strings:\n            output_str = str(ctx.output)\n            expected_str = str(self.value)\n\n            if not self.case_sensitive:\n                output_str = output_str.lower()\n                expected_str = expected_str.lower()\n\n            failure_reason: str | None = None\n            if expected_str not in output_str:\n                output_trunc = _truncated_repr(output_str, max_length=100)\n                expected_trunc = _truncated_repr(expected_str, max_length=100)\n                failure_reason = f'Output string {output_trunc} does not contain expected string {expected_trunc}'\n            return EvaluationReason(value=failure_reason is None, reason=failure_reason)\n\n        try:\n            # Handle different collection types\n            if isinstance(ctx.output, dict):\n                if isinstance(self.value, dict):\n                    # Cast to Any to avoid type checking issues\n                    output_dict = cast(dict[Any, Any], ctx.output)  # pyright: ignore[reportUnknownMemberType]\n                    expected_dict = cast(dict[Any, Any], self.value)  # pyright: ignore[reportUnknownMemberType]\n                    for k in expected_dict:\n                        if k not in output_dict:\n                            k_trunc = _truncated_repr(k, max_length=30)\n                            failure_reason = f'Output dictionary does not contain expected key {k_trunc}'\n                            break\n                        elif output_dict[k] != expected_dict[k]:\n                            k_trunc = _truncated_repr(k, max_length=30)\n                            output_v_trunc = _truncated_repr(output_dict[k], max_length=100)\n                            expected_v_trunc = _truncated_repr(expected_dict[k], max_length=100)\n                            failure_reason = f'Output dictionary has different value for key {k_trunc}: {output_v_trunc} != {expected_v_trunc}'\n                            break\n                else:\n                    if self.value not in ctx.output:  # pyright: ignore[reportUnknownMemberType]\n                        output_trunc = _truncated_repr(ctx.output, max_length=200)  # pyright: ignore[reportUnknownMemberType]\n                        failure_reason = f'Output {output_trunc} does not contain provided value as a key'\n            elif self.value not in ctx.output:  # pyright: ignore[reportOperatorIssue]  # will be handled by except block\n                output_trunc = _truncated_repr(ctx.output, max_length=200)\n                failure_reason = f'Output {output_trunc} does not contain provided value'\n        except (TypeError, ValueError) as e:\n            failure_reason = f'Containment check failed: {e}'\n\n        return EvaluationReason(value=failure_reason is None, reason=failure_reason)\n\n```\n\n### Equals\n\nBases: `Evaluator[object, object, object]`\n\nCheck if the output exactly equals the provided value.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n```python\n@dataclass(repr=False)\nclass Equals(Evaluator[object, object, object]):\n    \"\"\"Check if the output exactly equals the provided value.\"\"\"\n\n    value: Any\n    evaluation_name: str | None = field(default=None)\n\n    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool:\n        return ctx.output == self.value\n\n```\n\n### EqualsExpected\n\nBases: `Evaluator[object, object, object]`\n\nCheck if the output exactly equals the expected output.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n```python\n@dataclass(repr=False)\nclass EqualsExpected(Evaluator[object, object, object]):\n    \"\"\"Check if the output exactly equals the expected output.\"\"\"\n\n    evaluation_name: str | None = field(default=None)\n\n    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool | dict[str, bool]:\n        if ctx.expected_output is None:\n            return {}  # Only compare if expected output is provided\n        return ctx.output == ctx.expected_output\n\n```\n\n### HasMatchingSpan\n\nBases: `Evaluator[object, object, object]`\n\nCheck if the span tree contains a span that matches the specified query.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n```python\n@dataclass(repr=False)\nclass HasMatchingSpan(Evaluator[object, object, object]):\n    \"\"\"Check if the span tree contains a span that matches the specified query.\"\"\"\n\n    query: SpanQuery\n    evaluation_name: str | None = field(default=None)\n\n    def evaluate(\n        self,\n        ctx: EvaluatorContext[object, object, object],\n    ) -> bool:\n        return ctx.span_tree.any(self.query)\n\n```\n\n### IsInstance\n\nBases: `Evaluator[object, object, object]`\n\nCheck if the output is an instance of a type with the given name.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n```python\n@dataclass(repr=False)\nclass IsInstance(Evaluator[object, object, object]):\n    \"\"\"Check if the output is an instance of a type with the given name.\"\"\"\n\n    type_name: str\n    evaluation_name: str | None = field(default=None)\n\n    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> EvaluationReason:\n        output = ctx.output\n        for cls in type(output).__mro__:\n            if cls.__name__ == self.type_name or cls.__qualname__ == self.type_name:\n                return EvaluationReason(value=True)\n\n        reason = f'output is of type {type(output).__name__}'\n        if type(output).__qualname__ != type(output).__name__:\n            reason += f' (qualname: {type(output).__qualname__})'\n        return EvaluationReason(value=False, reason=reason)\n\n```\n\n### LLMJudge\n\nBases: `Evaluator[object, object, object]`\n\nJudge whether the output of a language model meets the criteria of a provided rubric.\n\nIf you do not specify a model, it uses the default model for judging. This starts as 'openai:gpt-4o', but can be overridden by calling set_default_judge_model.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n```python\n@dataclass(repr=False)\nclass LLMJudge(Evaluator[object, object, object]):\n    \"\"\"Judge whether the output of a language model meets the criteria of a provided rubric.\n\n    If you do not specify a model, it uses the default model for judging. This starts as 'openai:gpt-4o', but can be\n    overridden by calling [`set_default_judge_model`][pydantic_evals.evaluators.llm_as_a_judge.set_default_judge_model].\n    \"\"\"\n\n    rubric: str\n    model: models.Model | models.KnownModelName | None = None\n    include_input: bool = False\n    include_expected_output: bool = False\n    model_settings: ModelSettings | None = None\n    score: OutputConfig | Literal[False] = False\n    assertion: OutputConfig | Literal[False] = field(default_factory=lambda: OutputConfig(include_reason=True))\n\n    async def evaluate(\n        self,\n        ctx: EvaluatorContext[object, object, object],\n    ) -> EvaluatorOutput:\n        if self.include_input:\n            if self.include_expected_output:\n                from .llm_as_a_judge import judge_input_output_expected\n\n                grading_output = await judge_input_output_expected(\n                    ctx.inputs, ctx.output, ctx.expected_output, self.rubric, self.model, self.model_settings\n                )\n            else:\n                from .llm_as_a_judge import judge_input_output\n\n                grading_output = await judge_input_output(\n                    ctx.inputs, ctx.output, self.rubric, self.model, self.model_settings\n                )\n        else:\n            if self.include_expected_output:\n                from .llm_as_a_judge import judge_output_expected\n\n                grading_output = await judge_output_expected(\n                    ctx.output, ctx.expected_output, self.rubric, self.model, self.model_settings\n                )\n            else:\n                from .llm_as_a_judge import judge_output\n\n                grading_output = await judge_output(ctx.output, self.rubric, self.model, self.model_settings)\n\n        output: dict[str, EvaluationScalar | EvaluationReason] = {}\n        include_both = self.score is not False and self.assertion is not False\n        evaluation_name = self.get_default_evaluation_name()\n\n        if self.score is not False:\n            default_name = f'{evaluation_name}_score' if include_both else evaluation_name\n            _update_combined_output(output, grading_output.score, grading_output.reason, self.score, default_name)\n\n        if self.assertion is not False:\n            default_name = f'{evaluation_name}_pass' if include_both else evaluation_name\n            _update_combined_output(output, grading_output.pass_, grading_output.reason, self.assertion, default_name)\n\n        return output\n\n    def build_serialization_arguments(self):\n        result = super().build_serialization_arguments()\n        # always serialize the model as a string when present; use its name if it's a KnownModelName\n        if (model := result.get('model')) and isinstance(model, models.Model):  # pragma: no branch\n            result['model'] = f'{model.system}:{model.model_name}'\n\n        # Note: this may lead to confusion if you try to serialize-then-deserialize with a custom model.\n        # I expect that is rare enough to be worth not solving yet, but common enough that we probably will want to\n        # solve it eventually. I'm imagining some kind of model registry, but don't want to work out the details yet.\n        return result\n\n```\n\n### MaxDuration\n\nBases: `Evaluator[object, object, object]`\n\nCheck if the execution time is under the specified maximum.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n```python\n@dataclass(repr=False)\nclass MaxDuration(Evaluator[object, object, object]):\n    \"\"\"Check if the execution time is under the specified maximum.\"\"\"\n\n    seconds: float | timedelta\n\n    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool:\n        duration = timedelta(seconds=ctx.duration)\n        seconds = self.seconds\n        if not isinstance(seconds, timedelta):\n            seconds = timedelta(seconds=seconds)\n        return duration <= seconds\n\n```\n\n### OutputConfig\n\nBases: `TypedDict`\n\nConfiguration for the score and assertion outputs of the LLMJudge evaluator.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n```python\nclass OutputConfig(TypedDict, total=False):\n    \"\"\"Configuration for the score and assertion outputs of the LLMJudge evaluator.\"\"\"\n\n    evaluation_name: str\n    include_reason: bool\n\n```\n\n### EvaluatorContext\n\nBases: `Generic[InputsT, OutputT, MetadataT]`\n\nContext for evaluating a task execution.\n\nAn instance of this class is the sole input to all Evaluators. It contains all the information needed to evaluate the task execution, including inputs, outputs, metadata, and telemetry data.\n\nEvaluators use this context to access the task inputs, actual output, expected output, and other information when evaluating the result of the task execution.\n\nExample:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        # Use the context to access task inputs, outputs, and expected outputs\n        return ctx.output == ctx.expected_output\n\n```\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/context.py`\n\n````python\n@dataclass(kw_only=True)\nclass EvaluatorContext(Generic[InputsT, OutputT, MetadataT]):\n    \"\"\"Context for evaluating a task execution.\n\n    An instance of this class is the sole input to all Evaluators. It contains all the information\n    needed to evaluate the task execution, including inputs, outputs, metadata, and telemetry data.\n\n    Evaluators use this context to access the task inputs, actual output, expected output, and other\n    information when evaluating the result of the task execution.\n\n    Example:\n    ```python\n    from dataclasses import dataclass\n\n    from pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n    @dataclass\n    class ExactMatch(Evaluator):\n        def evaluate(self, ctx: EvaluatorContext) -> bool:\n            # Use the context to access task inputs, outputs, and expected outputs\n            return ctx.output == ctx.expected_output\n    ```\n    \"\"\"\n\n    name: str | None\n    \"\"\"The name of the case.\"\"\"\n    inputs: InputsT\n    \"\"\"The inputs provided to the task for this case.\"\"\"\n    metadata: MetadataT | None\n    \"\"\"Metadata associated with the case, if provided. May be None if no metadata was specified.\"\"\"\n    expected_output: OutputT | None\n    \"\"\"The expected output for the case, if provided. May be None if no expected output was specified.\"\"\"\n\n    output: OutputT\n    \"\"\"The actual output produced by the task for this case.\"\"\"\n    duration: float\n    \"\"\"The duration of the task run for this case.\"\"\"\n    _span_tree: SpanTree | SpanTreeRecordingError = field(repr=False)\n    \"\"\"The span tree for the task run for this case.\n\n    This will be `None` if `logfire.configure` has not been called.\n    \"\"\"\n\n    attributes: dict[str, Any]\n    \"\"\"Attributes associated with the task run for this case.\n\n    These can be set by calling `pydantic_evals.dataset.set_eval_attribute` in any code executed\n    during the evaluation task.\"\"\"\n    metrics: dict[str, int | float]\n    \"\"\"Metrics associated with the task run for this case.\n\n    These can be set by calling `pydantic_evals.dataset.increment_eval_metric` in any code executed\n    during the evaluation task.\"\"\"\n\n    @property\n    def span_tree(self) -> SpanTree:\n        \"\"\"Get the `SpanTree` for this task execution.\n\n        The span tree is a graph where each node corresponds to an OpenTelemetry span recorded during the task\n        execution, including timing information and any custom spans created during execution.\n\n        Returns:\n            The span tree for the task execution.\n\n        Raises:\n            SpanTreeRecordingError: If spans were not captured during execution of the task, e.g. due to not having\n                the necessary dependencies installed.\n        \"\"\"\n        if isinstance(self._span_tree, SpanTreeRecordingError):\n            # In this case, there was a reason we couldn't record the SpanTree. We raise that now\n            raise self._span_tree\n        return self._span_tree\n\n````\n\n#### name\n\n```python\nname: str | None\n\n```\n\nThe name of the case.\n\n#### inputs\n\n```python\ninputs: InputsT\n\n```\n\nThe inputs provided to the task for this case.\n\n#### metadata\n\n```python\nmetadata: MetadataT | None\n\n```\n\nMetadata associated with the case, if provided. May be None if no metadata was specified.\n\n#### expected_output\n\n```python\nexpected_output: OutputT | None\n\n```\n\nThe expected output for the case, if provided. May be None if no expected output was specified.\n\n#### output\n\n```python\noutput: OutputT\n\n```\n\nThe actual output produced by the task for this case.\n\n#### duration\n\n```python\nduration: float\n\n```\n\nThe duration of the task run for this case.\n\n#### attributes\n\n```python\nattributes: dict[str, Any]\n\n```\n\nAttributes associated with the task run for this case.\n\nThese can be set by calling `pydantic_evals.dataset.set_eval_attribute` in any code executed during the evaluation task.\n\n#### metrics\n\n```python\nmetrics: dict[str, int | float]\n\n```\n\nMetrics associated with the task run for this case.\n\nThese can be set by calling `pydantic_evals.dataset.increment_eval_metric` in any code executed during the evaluation task.\n\n#### span_tree\n\n```python\nspan_tree: SpanTree\n\n```\n\nGet the `SpanTree` for this task execution.\n\nThe span tree is a graph where each node corresponds to an OpenTelemetry span recorded during the task execution, including timing information and any custom spans created during execution.\n\nReturns:\n\n| Type | Description | | --- | --- | | `SpanTree` | The span tree for the task execution. |\n\nRaises:\n\n| Type | Description | | --- | --- | | `SpanTreeRecordingError` | If spans were not captured during execution of the task, e.g. due to not having the necessary dependencies installed. |\n\n### EvaluationReason\n\nThe result of running an evaluator with an optional explanation.\n\nContains a scalar value and an optional \"reason\" explaining the value.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `value` | `EvaluationScalar` | The scalar result of the evaluation (boolean, integer, float, or string). | *required* | | `reason` | `str | None` | An optional explanation of the evaluation result. | `None` |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\n@dataclass\nclass EvaluationReason:\n    \"\"\"The result of running an evaluator with an optional explanation.\n\n    Contains a scalar value and an optional \"reason\" explaining the value.\n\n    Args:\n        value: The scalar result of the evaluation (boolean, integer, float, or string).\n        reason: An optional explanation of the evaluation result.\n    \"\"\"\n\n    value: EvaluationScalar\n    reason: str | None = None\n\n```\n\n### EvaluationResult\n\nBases: `Generic[EvaluationScalarT]`\n\nThe details of an individual evaluation result.\n\nContains the name, value, reason, and source evaluator for a single evaluation.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str` | The name of the evaluation. | *required* | | `value` | `EvaluationScalarT` | The scalar result of the evaluation. | *required* | | `reason` | `str | None` | An optional explanation of the evaluation result. | *required* | | `source` | `EvaluatorSpec` | The spec of the evaluator that produced this result. | *required* |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\n@dataclass\nclass EvaluationResult(Generic[EvaluationScalarT]):\n    \"\"\"The details of an individual evaluation result.\n\n    Contains the name, value, reason, and source evaluator for a single evaluation.\n\n    Args:\n        name: The name of the evaluation.\n        value: The scalar result of the evaluation.\n        reason: An optional explanation of the evaluation result.\n        source: The spec of the evaluator that produced this result.\n    \"\"\"\n\n    name: str\n    value: EvaluationScalarT\n    reason: str | None\n    source: EvaluatorSpec\n\n    def downcast(self, *value_types: type[T]) -> EvaluationResult[T] | None:\n        \"\"\"Attempt to downcast this result to a more specific type.\n\n        Args:\n            *value_types: The types to check the value against.\n\n        Returns:\n            A downcast version of this result if the value is an instance of one of the given types,\n            otherwise None.\n        \"\"\"\n        # Check if value matches any of the target types, handling bool as a special case\n        for value_type in value_types:\n            if isinstance(self.value, value_type):\n                # Only match bool with explicit bool type\n                if isinstance(self.value, bool) and value_type is not bool:\n                    continue\n                return cast(EvaluationResult[T], self)\n        return None\n\n```\n\n#### downcast\n\n```python\ndowncast(\n    *value_types: type[T],\n) -> EvaluationResult[T] | None\n\n```\n\nAttempt to downcast this result to a more specific type.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `*value_types` | `type[T]` | The types to check the value against. | `()` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `EvaluationResult[T] | None` | A downcast version of this result if the value is an instance of one of the given types, | | `EvaluationResult[T] | None` | otherwise None. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\ndef downcast(self, *value_types: type[T]) -> EvaluationResult[T] | None:\n    \"\"\"Attempt to downcast this result to a more specific type.\n\n    Args:\n        *value_types: The types to check the value against.\n\n    Returns:\n        A downcast version of this result if the value is an instance of one of the given types,\n        otherwise None.\n    \"\"\"\n    # Check if value matches any of the target types, handling bool as a special case\n    for value_type in value_types:\n        if isinstance(self.value, value_type):\n            # Only match bool with explicit bool type\n            if isinstance(self.value, bool) and value_type is not bool:\n                continue\n            return cast(EvaluationResult[T], self)\n    return None\n\n```\n\n### Evaluator\n\nBases: `Generic[InputsT, OutputT, MetadataT]`\n\nBase class for all evaluators.\n\nEvaluators can assess the performance of a task in a variety of ways, as a function of the EvaluatorContext.\n\nSubclasses must implement the `evaluate` method. Note it can be defined with either `def` or `async def`.\n\nExample:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output\n\n```\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n````python\n@dataclass(repr=False)\nclass Evaluator(Generic[InputsT, OutputT, MetadataT], metaclass=_StrictABCMeta):\n    \"\"\"Base class for all evaluators.\n\n    Evaluators can assess the performance of a task in a variety of ways, as a function of the EvaluatorContext.\n\n    Subclasses must implement the `evaluate` method. Note it can be defined with either `def` or `async def`.\n\n    Example:\n    ```python\n    from dataclasses import dataclass\n\n    from pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n    @dataclass\n    class ExactMatch(Evaluator):\n        def evaluate(self, ctx: EvaluatorContext) -> bool:\n            return ctx.output == ctx.expected_output\n    ```\n    \"\"\"\n\n    __pydantic_config__ = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def get_serialization_name(cls) -> str:\n        \"\"\"Return the 'name' of this Evaluator to use during serialization.\n\n        Returns:\n            The name of the Evaluator, which is typically the class name.\n        \"\"\"\n        return cls.__name__\n\n    @classmethod\n    @deprecated('`name` has been renamed, use `get_serialization_name` instead.')\n    def name(cls) -> str:\n        \"\"\"`name` has been renamed, use `get_serialization_name` instead.\"\"\"\n        return cls.get_serialization_name()\n\n    def get_default_evaluation_name(self) -> str:\n        \"\"\"Return the default name to use in reports for the output of this evaluator.\n\n        By default, if the evaluator has an attribute called `evaluation_name` of type string, that will be used.\n        Otherwise, the serialization name of the evaluator (which is usually the class name) will be used.\n\n        This can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information.\n\n        Note that evaluators that return a mapping of results will always use the keys of that mapping as the names\n        of the associated evaluation results.\n        \"\"\"\n        evaluation_name = getattr(self, 'evaluation_name', None)\n        if isinstance(evaluation_name, str):\n            # If the evaluator has an attribute `name` of type string, use that\n            return evaluation_name\n\n        return self.get_serialization_name()\n\n    @abstractmethod\n    def evaluate(\n        self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]\n    ) -> EvaluatorOutput | Awaitable[EvaluatorOutput]:  # pragma: no cover\n        \"\"\"Evaluate the task output in the given context.\n\n        This is the main evaluation method that subclasses must implement. It can be either synchronous\n        or asynchronous, returning either an EvaluatorOutput directly or an Awaitable[EvaluatorOutput].\n\n        Args:\n            ctx: The context containing the inputs, outputs, and metadata for evaluation.\n\n        Returns:\n            The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping\n            of evaluation names to either of those. Can be returned either synchronously or as an\n            awaitable for asynchronous evaluation.\n        \"\"\"\n        raise NotImplementedError('You must implement `evaluate`.')\n\n    def evaluate_sync(self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]) -> EvaluatorOutput:\n        \"\"\"Run the evaluator synchronously, handling both sync and async implementations.\n\n        This method ensures synchronous execution by running any async evaluate implementation\n        to completion using run_until_complete.\n\n        Args:\n            ctx: The context containing the inputs, outputs, and metadata for evaluation.\n\n        Returns:\n            The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping\n            of evaluation names to either of those.\n        \"\"\"\n        output = self.evaluate(ctx)\n        if inspect.iscoroutine(output):  # pragma: no cover\n            return get_event_loop().run_until_complete(output)\n        else:\n            return cast(EvaluatorOutput, output)\n\n    async def evaluate_async(self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]) -> EvaluatorOutput:\n        \"\"\"Run the evaluator asynchronously, handling both sync and async implementations.\n\n        This method ensures asynchronous execution by properly awaiting any async evaluate\n        implementation. For synchronous implementations, it returns the result directly.\n\n        Args:\n            ctx: The context containing the inputs, outputs, and metadata for evaluation.\n\n        Returns:\n            The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping\n            of evaluation names to either of those.\n        \"\"\"\n        # Note: If self.evaluate is synchronous, but you need to prevent this from blocking, override this method with:\n        # return await anyio.to_thread.run_sync(self.evaluate, ctx)\n        output = self.evaluate(ctx)\n        if inspect.iscoroutine(output):\n            return await output\n        else:\n            return cast(EvaluatorOutput, output)\n\n    @model_serializer(mode='plain')\n    def serialize(self, info: SerializationInfo) -> Any:\n        \"\"\"Serialize this Evaluator to a JSON-serializable form.\n\n        Returns:\n            A JSON-serializable representation of this evaluator as an EvaluatorSpec.\n        \"\"\"\n        return to_jsonable_python(\n            self.as_spec(),\n            context=info.context,\n            serialize_unknown=True,\n        )\n\n    def as_spec(self) -> EvaluatorSpec:\n        raw_arguments = self.build_serialization_arguments()\n\n        arguments: None | tuple[Any,] | dict[str, Any]\n        if len(raw_arguments) == 0:\n            arguments = None\n        elif len(raw_arguments) == 1:\n            arguments = (next(iter(raw_arguments.values())),)\n        else:\n            arguments = raw_arguments\n\n        return EvaluatorSpec(name=self.get_serialization_name(), arguments=arguments)\n\n    def build_serialization_arguments(self) -> dict[str, Any]:\n        \"\"\"Build the arguments for serialization.\n\n        Evaluators are serialized for inclusion as the \"source\" in an `EvaluationResult`.\n        If you want to modify how the evaluator is serialized for that or other purposes, you can override this method.\n\n        Returns:\n            A dictionary of arguments to be used during serialization.\n        \"\"\"\n        raw_arguments: dict[str, Any] = {}\n        for field in fields(self):\n            value = getattr(self, field.name)\n            # always exclude defaults:\n            if field.default is not MISSING:\n                if value == field.default:\n                    continue\n            if field.default_factory is not MISSING:\n                if value == field.default_factory():  # pragma: no branch\n                    continue\n            raw_arguments[field.name] = value\n        return raw_arguments\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n````\n\n#### get_serialization_name\n\n```python\nget_serialization_name() -> str\n\n```\n\nReturn the 'name' of this Evaluator to use during serialization.\n\nReturns:\n\n| Type | Description | | --- | --- | | `str` | The name of the Evaluator, which is typically the class name. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\n@classmethod\ndef get_serialization_name(cls) -> str:\n    \"\"\"Return the 'name' of this Evaluator to use during serialization.\n\n    Returns:\n        The name of the Evaluator, which is typically the class name.\n    \"\"\"\n    return cls.__name__\n\n```\n\n#### name\n\n```python\nname() -> str\n\n```\n\nDeprecated\n\n`name` has been renamed, use `get_serialization_name` instead.\n\n`name` has been renamed, use `get_serialization_name` instead.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\n@classmethod\n@deprecated('`name` has been renamed, use `get_serialization_name` instead.')\ndef name(cls) -> str:\n    \"\"\"`name` has been renamed, use `get_serialization_name` instead.\"\"\"\n    return cls.get_serialization_name()\n\n```\n\n#### get_default_evaluation_name\n\n```python\nget_default_evaluation_name() -> str\n\n```\n\nReturn the default name to use in reports for the output of this evaluator.\n\nBy default, if the evaluator has an attribute called `evaluation_name` of type string, that will be used. Otherwise, the serialization name of the evaluator (which is usually the class name) will be used.\n\nThis can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information.\n\nNote that evaluators that return a mapping of results will always use the keys of that mapping as the names of the associated evaluation results.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\ndef get_default_evaluation_name(self) -> str:\n    \"\"\"Return the default name to use in reports for the output of this evaluator.\n\n    By default, if the evaluator has an attribute called `evaluation_name` of type string, that will be used.\n    Otherwise, the serialization name of the evaluator (which is usually the class name) will be used.\n\n    This can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information.\n\n    Note that evaluators that return a mapping of results will always use the keys of that mapping as the names\n    of the associated evaluation results.\n    \"\"\"\n    evaluation_name = getattr(self, 'evaluation_name', None)\n    if isinstance(evaluation_name, str):\n        # If the evaluator has an attribute `name` of type string, use that\n        return evaluation_name\n\n    return self.get_serialization_name()\n\n```\n\n#### evaluate\n\n```python\nevaluate(\n    ctx: EvaluatorContext[InputsT, OutputT, MetadataT],\n) -> EvaluatorOutput | Awaitable[EvaluatorOutput]\n\n```\n\nEvaluate the task output in the given context.\n\nThis is the main evaluation method that subclasses must implement. It can be either synchronous or asynchronous, returning either an EvaluatorOutput directly or an Awaitable[EvaluatorOutput].\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `ctx` | `EvaluatorContext[InputsT, OutputT, MetadataT]` | The context containing the inputs, outputs, and metadata for evaluation. | *required* |\n\nReturns:\n\n| Type | Description | | --- | --- | | `EvaluatorOutput | Awaitable[EvaluatorOutput]` | The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping | | `EvaluatorOutput | Awaitable[EvaluatorOutput]` | of evaluation names to either of those. Can be returned either synchronously or as an | | `EvaluatorOutput | Awaitable[EvaluatorOutput]` | awaitable for asynchronous evaluation. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\n@abstractmethod\ndef evaluate(\n    self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]\n) -> EvaluatorOutput | Awaitable[EvaluatorOutput]:  # pragma: no cover\n    \"\"\"Evaluate the task output in the given context.\n\n    This is the main evaluation method that subclasses must implement. It can be either synchronous\n    or asynchronous, returning either an EvaluatorOutput directly or an Awaitable[EvaluatorOutput].\n\n    Args:\n        ctx: The context containing the inputs, outputs, and metadata for evaluation.\n\n    Returns:\n        The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping\n        of evaluation names to either of those. Can be returned either synchronously or as an\n        awaitable for asynchronous evaluation.\n    \"\"\"\n    raise NotImplementedError('You must implement `evaluate`.')\n\n```\n\n#### evaluate_sync\n\n```python\nevaluate_sync(\n    ctx: EvaluatorContext[InputsT, OutputT, MetadataT],\n) -> EvaluatorOutput\n\n```\n\nRun the evaluator synchronously, handling both sync and async implementations.\n\nThis method ensures synchronous execution by running any async evaluate implementation to completion using run_until_complete.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `ctx` | `EvaluatorContext[InputsT, OutputT, MetadataT]` | The context containing the inputs, outputs, and metadata for evaluation. | *required* |\n\nReturns:\n\n| Type | Description | | --- | --- | | `EvaluatorOutput` | The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping | | `EvaluatorOutput` | of evaluation names to either of those. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\ndef evaluate_sync(self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]) -> EvaluatorOutput:\n    \"\"\"Run the evaluator synchronously, handling both sync and async implementations.\n\n    This method ensures synchronous execution by running any async evaluate implementation\n    to completion using run_until_complete.\n\n    Args:\n        ctx: The context containing the inputs, outputs, and metadata for evaluation.\n\n    Returns:\n        The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping\n        of evaluation names to either of those.\n    \"\"\"\n    output = self.evaluate(ctx)\n    if inspect.iscoroutine(output):  # pragma: no cover\n        return get_event_loop().run_until_complete(output)\n    else:\n        return cast(EvaluatorOutput, output)\n\n```\n\n#### evaluate_async\n\n```python\nevaluate_async(\n    ctx: EvaluatorContext[InputsT, OutputT, MetadataT],\n) -> EvaluatorOutput\n\n```\n\nRun the evaluator asynchronously, handling both sync and async implementations.\n\nThis method ensures asynchronous execution by properly awaiting any async evaluate implementation. For synchronous implementations, it returns the result directly.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `ctx` | `EvaluatorContext[InputsT, OutputT, MetadataT]` | The context containing the inputs, outputs, and metadata for evaluation. | *required* |\n\nReturns:\n\n| Type | Description | | --- | --- | | `EvaluatorOutput` | The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping | | `EvaluatorOutput` | of evaluation names to either of those. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\nasync def evaluate_async(self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]) -> EvaluatorOutput:\n    \"\"\"Run the evaluator asynchronously, handling both sync and async implementations.\n\n    This method ensures asynchronous execution by properly awaiting any async evaluate\n    implementation. For synchronous implementations, it returns the result directly.\n\n    Args:\n        ctx: The context containing the inputs, outputs, and metadata for evaluation.\n\n    Returns:\n        The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping\n        of evaluation names to either of those.\n    \"\"\"\n    # Note: If self.evaluate is synchronous, but you need to prevent this from blocking, override this method with:\n    # return await anyio.to_thread.run_sync(self.evaluate, ctx)\n    output = self.evaluate(ctx)\n    if inspect.iscoroutine(output):\n        return await output\n    else:\n        return cast(EvaluatorOutput, output)\n\n```\n\n#### serialize\n\n```python\nserialize(info: SerializationInfo) -> Any\n\n```\n\nSerialize this Evaluator to a JSON-serializable form.\n\nReturns:\n\n| Type | Description | | --- | --- | | `Any` | A JSON-serializable representation of this evaluator as an EvaluatorSpec. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\n@model_serializer(mode='plain')\ndef serialize(self, info: SerializationInfo) -> Any:\n    \"\"\"Serialize this Evaluator to a JSON-serializable form.\n\n    Returns:\n        A JSON-serializable representation of this evaluator as an EvaluatorSpec.\n    \"\"\"\n    return to_jsonable_python(\n        self.as_spec(),\n        context=info.context,\n        serialize_unknown=True,\n    )\n\n```\n\n#### build_serialization_arguments\n\n```python\nbuild_serialization_arguments() -> dict[str, Any]\n\n```\n\nBuild the arguments for serialization.\n\nEvaluators are serialized for inclusion as the \"source\" in an `EvaluationResult`. If you want to modify how the evaluator is serialized for that or other purposes, you can override this method.\n\nReturns:\n\n| Type | Description | | --- | --- | | `dict[str, Any]` | A dictionary of arguments to be used during serialization. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\ndef build_serialization_arguments(self) -> dict[str, Any]:\n    \"\"\"Build the arguments for serialization.\n\n    Evaluators are serialized for inclusion as the \"source\" in an `EvaluationResult`.\n    If you want to modify how the evaluator is serialized for that or other purposes, you can override this method.\n\n    Returns:\n        A dictionary of arguments to be used during serialization.\n    \"\"\"\n    raw_arguments: dict[str, Any] = {}\n    for field in fields(self):\n        value = getattr(self, field.name)\n        # always exclude defaults:\n        if field.default is not MISSING:\n            if value == field.default:\n                continue\n        if field.default_factory is not MISSING:\n            if value == field.default_factory():  # pragma: no branch\n                continue\n        raw_arguments[field.name] = value\n    return raw_arguments\n\n```\n\n### EvaluatorFailure\n\nRepresents a failure raised during the execution of an evaluator.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n```python\n@dataclass\nclass EvaluatorFailure:\n    \"\"\"Represents a failure raised during the execution of an evaluator.\"\"\"\n\n    name: str\n    error_message: str\n    error_stacktrace: str\n    source: EvaluatorSpec\n\n```\n\n### EvaluatorOutput\n\n```python\nEvaluatorOutput = (\n    EvaluationScalar\n    | EvaluationReason\n    | Mapping[str, EvaluationScalar | EvaluationReason]\n)\n\n```\n\nType for the output of an evaluator, which can be a scalar, an EvaluationReason, or a mapping of names to either.\n\n### EvaluatorSpec\n\nBases: `BaseModel`\n\nThe specification of an evaluator to be run.\n\nThis class is used to represent evaluators in a serializable format, supporting various short forms for convenience when defining evaluators in YAML or JSON dataset files.\n\nIn particular, each of the following forms is supported for specifying an evaluator with name `MyEvaluator`: * `'MyEvaluator'` - Just the (string) name of the Evaluator subclass is used if its `__init__` takes no arguments * `{'MyEvaluator': first_arg}` - A single argument is passed as the first positional argument to `MyEvaluator.__init__` * `{'MyEvaluator': {k1: v1, k2: v2}}` - Multiple kwargs are passed to `MyEvaluator.__init__`\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/spec.py`\n\n```python\nclass EvaluatorSpec(BaseModel):\n    \"\"\"The specification of an evaluator to be run.\n\n    This class is used to represent evaluators in a serializable format, supporting various\n    short forms for convenience when defining evaluators in YAML or JSON dataset files.\n\n    In particular, each of the following forms is supported for specifying an evaluator with name `MyEvaluator`:\n    * `'MyEvaluator'` - Just the (string) name of the Evaluator subclass is used if its `__init__` takes no arguments\n    * `{'MyEvaluator': first_arg}` - A single argument is passed as the first positional argument to `MyEvaluator.__init__`\n    * `{'MyEvaluator': {k1: v1, k2: v2}}` - Multiple kwargs are passed to `MyEvaluator.__init__`\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the evaluator class; should be the value returned by `EvaluatorClass.get_serialization_name()`\"\"\"\n\n    arguments: None | tuple[Any] | dict[str, Any]\n    \"\"\"The arguments to pass to the evaluator's constructor.\n\n    Can be None (no arguments), a tuple (a single positional argument), or a dict (keyword arguments).\n    \"\"\"\n\n    @property\n    def args(self) -> tuple[Any, ...]:\n        \"\"\"Get the positional arguments for the evaluator.\n\n        Returns:\n            A tuple of positional arguments if arguments is a tuple, otherwise an empty tuple.\n        \"\"\"\n        if isinstance(self.arguments, tuple):\n            return self.arguments\n        return ()\n\n    @property\n    def kwargs(self) -> dict[str, Any]:\n        \"\"\"Get the keyword arguments for the evaluator.\n\n        Returns:\n            A dictionary of keyword arguments if arguments is a dict, otherwise an empty dict.\n        \"\"\"\n        if isinstance(self.arguments, dict):\n            return self.arguments\n        return {}\n\n    @model_validator(mode='wrap')\n    @classmethod\n    def deserialize(cls, value: Any, handler: ModelWrapValidatorHandler[EvaluatorSpec]) -> EvaluatorSpec:\n        \"\"\"Deserialize an EvaluatorSpec from various formats.\n\n        This validator handles the various short forms of evaluator specifications,\n        converting them to a consistent EvaluatorSpec instance.\n\n        Args:\n            value: The value to deserialize.\n            handler: The validator handler.\n\n        Returns:\n            The deserialized EvaluatorSpec.\n\n        Raises:\n            ValidationError: If the value cannot be deserialized.\n        \"\"\"\n        try:\n            result = handler(value)\n            return result\n        except ValidationError as exc:\n            try:\n                deserialized = _SerializedEvaluatorSpec.model_validate(value)\n            except ValidationError:\n                raise exc  # raise the original error\n            return deserialized.to_evaluator_spec()\n\n    @model_serializer(mode='wrap')\n    def serialize(self, handler: SerializerFunctionWrapHandler, info: SerializationInfo) -> Any:\n        \"\"\"Serialize using the appropriate short-form if possible.\n\n        Returns:\n            The serialized evaluator specification, using the shortest form possible:\n            - Just the name if there are no arguments\n            - {name: first_arg} if there's a single positional argument\n            - {name: {kwargs}} if there are multiple (keyword) arguments\n        \"\"\"\n        if isinstance(info.context, dict) and info.context.get('use_short_form'):  # pyright: ignore[reportUnknownMemberType]\n            if self.arguments is None:\n                return self.name\n            elif isinstance(self.arguments, tuple):\n                return {self.name: self.arguments[0]}\n            else:\n                return {self.name: self.arguments}\n        else:\n            return handler(self)\n\n```\n\n#### name\n\n```python\nname: str\n\n```\n\nThe name of the evaluator class; should be the value returned by `EvaluatorClass.get_serialization_name()`\n\n#### arguments\n\n```python\narguments: None | tuple[Any] | dict[str, Any]\n\n```\n\nThe arguments to pass to the evaluator's constructor.\n\nCan be None (no arguments), a tuple (a single positional argument), or a dict (keyword arguments).\n\n#### args\n\n```python\nargs: tuple[Any, ...]\n\n```\n\nGet the positional arguments for the evaluator.\n\nReturns:\n\n| Type | Description | | --- | --- | | `tuple[Any, ...]` | A tuple of positional arguments if arguments is a tuple, otherwise an empty tuple. |\n\n#### kwargs\n\n```python\nkwargs: dict[str, Any]\n\n```\n\nGet the keyword arguments for the evaluator.\n\nReturns:\n\n| Type | Description | | --- | --- | | `dict[str, Any]` | A dictionary of keyword arguments if arguments is a dict, otherwise an empty dict. |\n\n#### deserialize\n\n```python\ndeserialize(\n    value: Any,\n    handler: ModelWrapValidatorHandler[EvaluatorSpec],\n) -> EvaluatorSpec\n\n```\n\nDeserialize an EvaluatorSpec from various formats.\n\nThis validator handles the various short forms of evaluator specifications, converting them to a consistent EvaluatorSpec instance.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `value` | `Any` | The value to deserialize. | *required* | | `handler` | `ModelWrapValidatorHandler[EvaluatorSpec]` | The validator handler. | *required* |\n\nReturns:\n\n| Type | Description | | --- | --- | | `EvaluatorSpec` | The deserialized EvaluatorSpec. |\n\nRaises:\n\n| Type | Description | | --- | --- | | `ValidationError` | If the value cannot be deserialized. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/spec.py`\n\n```python\n@model_validator(mode='wrap')\n@classmethod\ndef deserialize(cls, value: Any, handler: ModelWrapValidatorHandler[EvaluatorSpec]) -> EvaluatorSpec:\n    \"\"\"Deserialize an EvaluatorSpec from various formats.\n\n    This validator handles the various short forms of evaluator specifications,\n    converting them to a consistent EvaluatorSpec instance.\n\n    Args:\n        value: The value to deserialize.\n        handler: The validator handler.\n\n    Returns:\n        The deserialized EvaluatorSpec.\n\n    Raises:\n        ValidationError: If the value cannot be deserialized.\n    \"\"\"\n    try:\n        result = handler(value)\n        return result\n    except ValidationError as exc:\n        try:\n            deserialized = _SerializedEvaluatorSpec.model_validate(value)\n        except ValidationError:\n            raise exc  # raise the original error\n        return deserialized.to_evaluator_spec()\n\n```\n\n#### serialize\n\n```python\nserialize(\n    handler: SerializerFunctionWrapHandler,\n    info: SerializationInfo,\n) -> Any\n\n```\n\nSerialize using the appropriate short-form if possible.\n\nReturns:\n\n| Type | Description | | --- | --- | | `Any` | The serialized evaluator specification, using the shortest form possible: | | `Any` | Just the name if there are no arguments | | `Any` | {name: first_arg} if there's a single positional argument | | `Any` | {name: {kwargs}} if there are multiple (keyword) arguments |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/spec.py`\n\n```python\n@model_serializer(mode='wrap')\ndef serialize(self, handler: SerializerFunctionWrapHandler, info: SerializationInfo) -> Any:\n    \"\"\"Serialize using the appropriate short-form if possible.\n\n    Returns:\n        The serialized evaluator specification, using the shortest form possible:\n        - Just the name if there are no arguments\n        - {name: first_arg} if there's a single positional argument\n        - {name: {kwargs}} if there are multiple (keyword) arguments\n    \"\"\"\n    if isinstance(info.context, dict) and info.context.get('use_short_form'):  # pyright: ignore[reportUnknownMemberType]\n        if self.arguments is None:\n            return self.name\n        elif isinstance(self.arguments, tuple):\n            return {self.name: self.arguments[0]}\n        else:\n            return {self.name: self.arguments}\n    else:\n        return handler(self)\n\n```\n\n### GradingOutput\n\nBases: `BaseModel`\n\nThe output of a grading operation.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n```python\nclass GradingOutput(BaseModel, populate_by_name=True):\n    \"\"\"The output of a grading operation.\"\"\"\n\n    reason: str\n    pass_: bool = Field(validation_alias='pass', serialization_alias='pass')\n    score: float\n\n```\n\n### judge_output\n\n```python\njudge_output(\n    output: Any,\n    rubric: str,\n    model: Model | KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput\n\n```\n\nJudge the output of a model based on a rubric.\n\nIf the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the `set_default_judge_model` function.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n```python\nasync def judge_output(\n    output: Any,\n    rubric: str,\n    model: models.Model | models.KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput:\n    \"\"\"Judge the output of a model based on a rubric.\n\n    If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',\n    but this can be changed using the `set_default_judge_model` function.\n    \"\"\"\n    user_prompt = _build_prompt(output=output, rubric=rubric)\n    return (\n        await _judge_output_agent.run(user_prompt, model=model or _default_model, model_settings=model_settings)\n    ).output\n\n```\n\n### judge_input_output\n\n```python\njudge_input_output(\n    inputs: Any,\n    output: Any,\n    rubric: str,\n    model: Model | KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput\n\n```\n\nJudge the output of a model based on the inputs and a rubric.\n\nIf the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the `set_default_judge_model` function.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n```python\nasync def judge_input_output(\n    inputs: Any,\n    output: Any,\n    rubric: str,\n    model: models.Model | models.KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput:\n    \"\"\"Judge the output of a model based on the inputs and a rubric.\n\n    If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',\n    but this can be changed using the `set_default_judge_model` function.\n    \"\"\"\n    user_prompt = _build_prompt(inputs=inputs, output=output, rubric=rubric)\n\n    return (\n        await _judge_input_output_agent.run(user_prompt, model=model or _default_model, model_settings=model_settings)\n    ).output\n\n```\n\n### judge_input_output_expected\n\n```python\njudge_input_output_expected(\n    inputs: Any,\n    output: Any,\n    expected_output: Any,\n    rubric: str,\n    model: Model | KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput\n\n```\n\nJudge the output of a model based on the inputs and a rubric.\n\nIf the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the `set_default_judge_model` function.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n```python\nasync def judge_input_output_expected(\n    inputs: Any,\n    output: Any,\n    expected_output: Any,\n    rubric: str,\n    model: models.Model | models.KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput:\n    \"\"\"Judge the output of a model based on the inputs and a rubric.\n\n    If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',\n    but this can be changed using the `set_default_judge_model` function.\n    \"\"\"\n    user_prompt = _build_prompt(inputs=inputs, output=output, rubric=rubric, expected_output=expected_output)\n\n    return (\n        await _judge_input_output_expected_agent.run(\n            user_prompt, model=model or _default_model, model_settings=model_settings\n        )\n    ).output\n\n```\n\n### judge_output_expected\n\n```python\njudge_output_expected(\n    output: Any,\n    expected_output: Any,\n    rubric: str,\n    model: Model | KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput\n\n```\n\nJudge the output of a model based on the expected output, output, and a rubric.\n\nIf the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the `set_default_judge_model` function.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n```python\nasync def judge_output_expected(\n    output: Any,\n    expected_output: Any,\n    rubric: str,\n    model: models.Model | models.KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput:\n    \"\"\"Judge the output of a model based on the expected output, output, and a rubric.\n\n    If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',\n    but this can be changed using the `set_default_judge_model` function.\n    \"\"\"\n    user_prompt = _build_prompt(output=output, rubric=rubric, expected_output=expected_output)\n    return (\n        await _judge_output_expected_agent.run(\n            user_prompt, model=model or _default_model, model_settings=model_settings\n        )\n    ).output\n\n```\n\n### set_default_judge_model\n\n```python\nset_default_judge_model(\n    model: Model | KnownModelName,\n) -> None\n\n```\n\nSet the default model used for judging.\n\nThis model is used if `None` is passed to the `model` argument of `judge_output` and `judge_input_output`.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n```python\ndef set_default_judge_model(model: models.Model | models.KnownModelName) -> None:\n    \"\"\"Set the default model used for judging.\n\n    This model is used if `None` is passed to the `model` argument of `judge_output` and `judge_input_output`.\n    \"\"\"\n    global _default_model\n    _default_model = model\n\n```",
  "content_length": 54882
}