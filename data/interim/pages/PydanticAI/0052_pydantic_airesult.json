{
  "title": "`pydantic_ai.result`",
  "source_url": null,
  "content": "### StreamedRunResult\n\nBases: `Generic[AgentDepsT, OutputDataT]`\n\nResult of a streamed run that returns structured data via a tool call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\n@dataclass(init=False)\nclass StreamedRunResult(Generic[AgentDepsT, OutputDataT]):\n    \"\"\"Result of a streamed run that returns structured data via a tool call.\"\"\"\n\n    _all_messages: list[_messages.ModelMessage]\n    _new_message_index: int\n\n    _stream_response: AgentStream[AgentDepsT, OutputDataT] | None = None\n    _on_complete: Callable[[], Awaitable[None]] | None = None\n\n    _run_result: AgentRunResult[OutputDataT] | None = None\n\n    is_complete: bool = field(default=False, init=False)\n    \"\"\"Whether the stream has all been received.\n\n    This is set to `True` when one of\n    [`stream_output`][pydantic_ai.result.StreamedRunResult.stream_output],\n    [`stream_text`][pydantic_ai.result.StreamedRunResult.stream_text],\n    [`stream_responses`][pydantic_ai.result.StreamedRunResult.stream_responses] or\n    [`get_output`][pydantic_ai.result.StreamedRunResult.get_output] completes.\n    \"\"\"\n\n    @overload\n    def __init__(\n        self,\n        all_messages: list[_messages.ModelMessage],\n        new_message_index: int,\n        stream_response: AgentStream[AgentDepsT, OutputDataT] | None,\n        on_complete: Callable[[], Awaitable[None]] | None,\n    ) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        all_messages: list[_messages.ModelMessage],\n        new_message_index: int,\n        *,\n        run_result: AgentRunResult[OutputDataT],\n    ) -> None: ...\n\n    def __init__(\n        self,\n        all_messages: list[_messages.ModelMessage],\n        new_message_index: int,\n        stream_response: AgentStream[AgentDepsT, OutputDataT] | None = None,\n        on_complete: Callable[[], Awaitable[None]] | None = None,\n        run_result: AgentRunResult[OutputDataT] | None = None,\n    ) -> None:\n        self._all_messages = all_messages\n        self._new_message_index = new_message_index\n\n        self._stream_response = stream_response\n        self._on_complete = on_complete\n        self._run_result = run_result\n\n    def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return the history of _messages.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of messages.\n        \"\"\"\n        # this is a method to be consistent with the other methods\n        if output_tool_return_content is not None:\n            raise NotImplementedError('Setting output tool return content is not supported for this result type.')\n        return self._all_messages\n\n    def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n        \"\"\"Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResult.all_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(\n            self.all_messages(output_tool_return_content=output_tool_return_content)\n        )\n\n    def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return new messages associated with this run.\n\n        Messages from older runs are excluded.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of new messages.\n        \"\"\"\n        return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :]\n\n    def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n        \"\"\"Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResult.new_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the new messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(\n            self.new_messages(output_tool_return_content=output_tool_return_content)\n        )\n\n    @deprecated('`StreamedRunResult.stream` is deprecated, use `stream_output` instead.')\n    async def stream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:\n        async for output in self.stream_output(debounce_by=debounce_by):\n            yield output\n\n    async def stream_output(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:\n        \"\"\"Stream the output as an async iterable.\n\n        The pydantic validator for structured data will be called in\n        [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation)\n        on each iteration.\n\n        Args:\n            debounce_by: by how much (if at all) to debounce/group the output chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured outputs to reduce the overhead of\n                performing validation as each token is received.\n\n        Returns:\n            An async iterable of the response data.\n        \"\"\"\n        if self._run_result is not None:\n            yield self._run_result.output\n            await self._marked_completed()\n        elif self._stream_response is not None:\n            async for output in self._stream_response.stream_output(debounce_by=debounce_by):\n                yield output\n            await self._marked_completed(self.response)\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    async def stream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> AsyncIterator[str]:\n        \"\"\"Stream the text result as an async iterable.\n\n        !!! note\n            Result validators will NOT be called on the text result if `delta=True`.\n\n        Args:\n            delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\n                up to the current point.\n            debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured responses to reduce the overhead of\n                performing validation as each token is received.\n        \"\"\"\n        if self._run_result is not None:  # pragma: no cover\n            # We can't really get here, as `_run_result` is only set in `run_stream` when `CallToolsNode` produces `DeferredToolRequests` output\n            # as a result of a tool function raising `CallDeferred` or `ApprovalRequired`.\n            # That'll change if we ever support something like `raise EndRun(output: OutputT)` where `OutputT` could be `str`.\n            if not isinstance(self._run_result.output, str):\n                raise exceptions.UserError('stream_text() can only be used with text responses')\n            yield self._run_result.output\n            await self._marked_completed()\n        elif self._stream_response is not None:\n            async for text in self._stream_response.stream_text(delta=delta, debounce_by=debounce_by):\n                yield text\n            await self._marked_completed(self.response)\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    @deprecated('`StreamedRunResult.stream_structured` is deprecated, use `stream_responses` instead.')\n    async def stream_structured(\n        self, *, debounce_by: float | None = 0.1\n    ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:\n        async for msg, last in self.stream_responses(debounce_by=debounce_by):\n            yield msg, last\n\n    async def stream_responses(\n        self, *, debounce_by: float | None = 0.1\n    ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:\n        \"\"\"Stream the response as an async iterable of Structured LLM Messages.\n\n        Args:\n            debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured responses to reduce the overhead of\n                performing validation as each token is received.\n\n        Returns:\n            An async iterable of the structured response message and whether that is the last message.\n        \"\"\"\n        if self._run_result is not None:\n            yield self.response, True\n            await self._marked_completed()\n        elif self._stream_response is not None:\n            # if the message currently has any parts with content, yield before streaming\n            async for msg in self._stream_response.stream_responses(debounce_by=debounce_by):\n                yield msg, False\n\n            msg = self.response\n            yield msg, True\n\n            await self._marked_completed(msg)\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    async def get_output(self) -> OutputDataT:\n        \"\"\"Stream the whole response, validate and return it.\"\"\"\n        if self._run_result is not None:\n            output = self._run_result.output\n            await self._marked_completed()\n            return output\n        elif self._stream_response is not None:\n            output = await self._stream_response.get_output()\n            await self._marked_completed(self.response)\n            return output\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    @property\n    def response(self) -> _messages.ModelResponse:\n        \"\"\"Return the current state of the response.\"\"\"\n        if self._run_result is not None:\n            return self._run_result.response\n        elif self._stream_response is not None:\n            return self._stream_response.get()\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    # TODO (v2): Make this a property\n    def usage(self) -> RunUsage:\n        \"\"\"Return the usage of the whole run.\n\n        !!! note\n            This won't return the full usage until the stream is finished.\n        \"\"\"\n        if self._run_result is not None:\n            return self._run_result.usage()\n        elif self._stream_response is not None:\n            return self._stream_response.usage()\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    # TODO (v2): Make this a property\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        if self._run_result is not None:\n            return self._run_result.timestamp()\n        elif self._stream_response is not None:\n            return self._stream_response.timestamp()\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    @property\n    def run_id(self) -> str:\n        \"\"\"The unique identifier for the agent run.\"\"\"\n        if self._run_result is not None:\n            return self._run_result.run_id\n        elif self._stream_response is not None:\n            return self._stream_response.run_id\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    @deprecated('`validate_structured_output` is deprecated, use `validate_response_output` instead.')\n    async def validate_structured_output(\n        self, message: _messages.ModelResponse, *, allow_partial: bool = False\n    ) -> OutputDataT:\n        return await self.validate_response_output(message, allow_partial=allow_partial)\n\n    async def validate_response_output(\n        self, message: _messages.ModelResponse, *, allow_partial: bool = False\n    ) -> OutputDataT:\n        \"\"\"Validate a structured result message.\"\"\"\n        if self._run_result is not None:\n            return self._run_result.output\n        elif self._stream_response is not None:\n            return await self._stream_response.validate_response_output(message, allow_partial=allow_partial)\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    async def _marked_completed(self, message: _messages.ModelResponse | None = None) -> None:\n        self.is_complete = True\n        if message is not None:\n            if self._stream_response:  # pragma: no branch\n                message.run_id = self._stream_response.run_id\n            self._all_messages.append(message)\n        if self._on_complete is not None:\n            await self._on_complete()\n\n```\n\n#### is_complete\n\n```python\nis_complete: bool = field(default=False, init=False)\n\n```\n\nWhether the stream has all been received.\n\nThis is set to `True` when one of stream_output, stream_text, stream_responses or get_output completes.\n\n#### all_messages\n\n```python\nall_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n\n```\n\nReturn the history of \\_messages.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `list[ModelMessage]` | List of messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n    \"\"\"Return the history of _messages.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        List of messages.\n    \"\"\"\n    # this is a method to be consistent with the other methods\n    if output_tool_return_content is not None:\n        raise NotImplementedError('Setting output tool return content is not supported for this result type.')\n    return self._all_messages\n\n```\n\n#### all_messages_json\n\n```python\nall_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n\n```\n\nReturn all messages from all_messages as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `bytes` | JSON bytes representing the messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n    \"\"\"Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResult.all_messages] as JSON bytes.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        JSON bytes representing the messages.\n    \"\"\"\n    return _messages.ModelMessagesTypeAdapter.dump_json(\n        self.all_messages(output_tool_return_content=output_tool_return_content)\n    )\n\n```\n\n#### new_messages\n\n```python\nnew_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n\n```\n\nReturn new messages associated with this run.\n\nMessages from older runs are excluded.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `list[ModelMessage]` | List of new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n    \"\"\"Return new messages associated with this run.\n\n    Messages from older runs are excluded.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        List of new messages.\n    \"\"\"\n    return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :]\n\n```\n\n#### new_messages_json\n\n```python\nnew_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n\n```\n\nReturn new messages from new_messages as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `bytes` | JSON bytes representing the new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n    \"\"\"Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResult.new_messages] as JSON bytes.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        JSON bytes representing the new messages.\n    \"\"\"\n    return _messages.ModelMessagesTypeAdapter.dump_json(\n        self.new_messages(output_tool_return_content=output_tool_return_content)\n    )\n\n```\n\n#### stream\n\n```python\nstream(\n    *, debounce_by: float | None = 0.1\n) -> AsyncIterator[OutputDataT]\n\n```\n\nDeprecated\n\n`StreamedRunResult.stream` is deprecated, use `stream_output` instead.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\n@deprecated('`StreamedRunResult.stream` is deprecated, use `stream_output` instead.')\nasync def stream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:\n    async for output in self.stream_output(debounce_by=debounce_by):\n        yield output\n\n```\n\n#### stream_output\n\n```python\nstream_output(\n    *, debounce_by: float | None = 0.1\n) -> AsyncIterator[OutputDataT]\n\n```\n\nStream the output as an async iterable.\n\nThe pydantic validator for structured data will be called in [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation) on each iteration.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `debounce_by` | `float | None` | by how much (if at all) to debounce/group the output chunks by. None means no debouncing. Debouncing is particularly important for long structured outputs to reduce the overhead of performing validation as each token is received. | `0.1` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[OutputDataT]` | An async iterable of the response data. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\nasync def stream_output(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:\n    \"\"\"Stream the output as an async iterable.\n\n    The pydantic validator for structured data will be called in\n    [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation)\n    on each iteration.\n\n    Args:\n        debounce_by: by how much (if at all) to debounce/group the output chunks by. `None` means no debouncing.\n            Debouncing is particularly important for long structured outputs to reduce the overhead of\n            performing validation as each token is received.\n\n    Returns:\n        An async iterable of the response data.\n    \"\"\"\n    if self._run_result is not None:\n        yield self._run_result.output\n        await self._marked_completed()\n    elif self._stream_response is not None:\n        async for output in self._stream_response.stream_output(debounce_by=debounce_by):\n            yield output\n        await self._marked_completed(self.response)\n    else:\n        raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n```\n\n#### stream_text\n\n```python\nstream_text(\n    *, delta: bool = False, debounce_by: float | None = 0.1\n) -> AsyncIterator[str]\n\n```\n\nStream the text result as an async iterable.\n\nNote\n\nResult validators will NOT be called on the text result if `delta=True`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `delta` | `bool` | if True, yield each chunk of text as it is received, if False (default), yield the full text up to the current point. | `False` | | `debounce_by` | `float | None` | by how much (if at all) to debounce/group the response chunks by. None means no debouncing. Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. | `0.1` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\nasync def stream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> AsyncIterator[str]:\n    \"\"\"Stream the text result as an async iterable.\n\n    !!! note\n        Result validators will NOT be called on the text result if `delta=True`.\n\n    Args:\n        delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\n            up to the current point.\n        debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n            Debouncing is particularly important for long structured responses to reduce the overhead of\n            performing validation as each token is received.\n    \"\"\"\n    if self._run_result is not None:  # pragma: no cover\n        # We can't really get here, as `_run_result` is only set in `run_stream` when `CallToolsNode` produces `DeferredToolRequests` output\n        # as a result of a tool function raising `CallDeferred` or `ApprovalRequired`.\n        # That'll change if we ever support something like `raise EndRun(output: OutputT)` where `OutputT` could be `str`.\n        if not isinstance(self._run_result.output, str):\n            raise exceptions.UserError('stream_text() can only be used with text responses')\n        yield self._run_result.output\n        await self._marked_completed()\n    elif self._stream_response is not None:\n        async for text in self._stream_response.stream_text(delta=delta, debounce_by=debounce_by):\n            yield text\n        await self._marked_completed(self.response)\n    else:\n        raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n```\n\n#### stream_structured\n\n```python\nstream_structured(\n    *, debounce_by: float | None = 0.1\n) -> AsyncIterator[tuple[ModelResponse, bool]]\n\n```\n\nDeprecated\n\n`StreamedRunResult.stream_structured` is deprecated, use `stream_responses` instead.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\n@deprecated('`StreamedRunResult.stream_structured` is deprecated, use `stream_responses` instead.')\nasync def stream_structured(\n    self, *, debounce_by: float | None = 0.1\n) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:\n    async for msg, last in self.stream_responses(debounce_by=debounce_by):\n        yield msg, last\n\n```\n\n#### stream_responses\n\n```python\nstream_responses(\n    *, debounce_by: float | None = 0.1\n) -> AsyncIterator[tuple[ModelResponse, bool]]\n\n```\n\nStream the response as an async iterable of Structured LLM Messages.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `debounce_by` | `float | None` | by how much (if at all) to debounce/group the response chunks by. None means no debouncing. Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. | `0.1` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[tuple[ModelResponse, bool]]` | An async iterable of the structured response message and whether that is the last message. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\nasync def stream_responses(\n    self, *, debounce_by: float | None = 0.1\n) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:\n    \"\"\"Stream the response as an async iterable of Structured LLM Messages.\n\n    Args:\n        debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n            Debouncing is particularly important for long structured responses to reduce the overhead of\n            performing validation as each token is received.\n\n    Returns:\n        An async iterable of the structured response message and whether that is the last message.\n    \"\"\"\n    if self._run_result is not None:\n        yield self.response, True\n        await self._marked_completed()\n    elif self._stream_response is not None:\n        # if the message currently has any parts with content, yield before streaming\n        async for msg in self._stream_response.stream_responses(debounce_by=debounce_by):\n            yield msg, False\n\n        msg = self.response\n        yield msg, True\n\n        await self._marked_completed(msg)\n    else:\n        raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n```\n\n#### get_output\n\n```python\nget_output() -> OutputDataT\n\n```\n\nStream the whole response, validate and return it.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\nasync def get_output(self) -> OutputDataT:\n    \"\"\"Stream the whole response, validate and return it.\"\"\"\n    if self._run_result is not None:\n        output = self._run_result.output\n        await self._marked_completed()\n        return output\n    elif self._stream_response is not None:\n        output = await self._stream_response.get_output()\n        await self._marked_completed(self.response)\n        return output\n    else:\n        raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n```\n\n#### response\n\n```python\nresponse: ModelResponse\n\n```\n\nReturn the current state of the response.\n\n#### usage\n\n```python\nusage() -> RunUsage\n\n```\n\nReturn the usage of the whole run.\n\nNote\n\nThis won't return the full usage until the stream is finished.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef usage(self) -> RunUsage:\n    \"\"\"Return the usage of the whole run.\n\n    !!! note\n        This won't return the full usage until the stream is finished.\n    \"\"\"\n    if self._run_result is not None:\n        return self._run_result.usage()\n    elif self._stream_response is not None:\n        return self._stream_response.usage()\n    else:\n        raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n```\n\n#### timestamp\n\n```python\ntimestamp() -> datetime\n\n```\n\nGet the timestamp of the response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef timestamp(self) -> datetime:\n    \"\"\"Get the timestamp of the response.\"\"\"\n    if self._run_result is not None:\n        return self._run_result.timestamp()\n    elif self._stream_response is not None:\n        return self._stream_response.timestamp()\n    else:\n        raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n```\n\n#### run_id\n\n```python\nrun_id: str\n\n```\n\nThe unique identifier for the agent run.\n\n#### validate_structured_output\n\n```python\nvalidate_structured_output(\n    message: ModelResponse, *, allow_partial: bool = False\n) -> OutputDataT\n\n```\n\nDeprecated\n\n`validate_structured_output` is deprecated, use `validate_response_output` instead.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\n@deprecated('`validate_structured_output` is deprecated, use `validate_response_output` instead.')\nasync def validate_structured_output(\n    self, message: _messages.ModelResponse, *, allow_partial: bool = False\n) -> OutputDataT:\n    return await self.validate_response_output(message, allow_partial=allow_partial)\n\n```\n\n#### validate_response_output\n\n```python\nvalidate_response_output(\n    message: ModelResponse, *, allow_partial: bool = False\n) -> OutputDataT\n\n```\n\nValidate a structured result message.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\nasync def validate_response_output(\n    self, message: _messages.ModelResponse, *, allow_partial: bool = False\n) -> OutputDataT:\n    \"\"\"Validate a structured result message.\"\"\"\n    if self._run_result is not None:\n        return self._run_result.output\n    elif self._stream_response is not None:\n        return await self._stream_response.validate_response_output(message, allow_partial=allow_partial)\n    else:\n        raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n```\n\n### StreamedRunResultSync\n\nBases: `Generic[AgentDepsT, OutputDataT]`\n\nSynchronous wrapper for StreamedRunResult that only exposes sync methods.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\n@dataclass(init=False)\nclass StreamedRunResultSync(Generic[AgentDepsT, OutputDataT]):\n    \"\"\"Synchronous wrapper for [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] that only exposes sync methods.\"\"\"\n\n    _streamed_run_result: StreamedRunResult[AgentDepsT, OutputDataT]\n\n    def __init__(self, streamed_run_result: StreamedRunResult[AgentDepsT, OutputDataT]) -> None:\n        self._streamed_run_result = streamed_run_result\n\n    def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return the history of messages.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of messages.\n        \"\"\"\n        return self._streamed_run_result.all_messages(output_tool_return_content=output_tool_return_content)\n\n    def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n        \"\"\"Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResultSync.all_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the messages.\n        \"\"\"\n        return self._streamed_run_result.all_messages_json(output_tool_return_content=output_tool_return_content)\n\n    def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return new messages associated with this run.\n\n        Messages from older runs are excluded.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of new messages.\n        \"\"\"\n        return self._streamed_run_result.new_messages(output_tool_return_content=output_tool_return_content)\n\n    def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n        \"\"\"Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResultSync.new_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the new messages.\n        \"\"\"\n        return self._streamed_run_result.new_messages_json(output_tool_return_content=output_tool_return_content)\n\n    def stream_output(self, *, debounce_by: float | None = 0.1) -> Iterator[OutputDataT]:\n        \"\"\"Stream the output as an iterable.\n\n        The pydantic validator for structured data will be called in\n        [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation)\n        on each iteration.\n\n        Args:\n            debounce_by: by how much (if at all) to debounce/group the output chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured outputs to reduce the overhead of\n                performing validation as each token is received.\n\n        Returns:\n            An iterable of the response data.\n        \"\"\"\n        return _utils.sync_async_iterator(self._streamed_run_result.stream_output(debounce_by=debounce_by))\n\n    def stream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> Iterator[str]:\n        \"\"\"Stream the text result as an iterable.\n\n        !!! note\n            Result validators will NOT be called on the text result if `delta=True`.\n\n        Args:\n            delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\n                up to the current point.\n            debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured responses to reduce the overhead of\n                performing validation as each token is received.\n        \"\"\"\n        return _utils.sync_async_iterator(self._streamed_run_result.stream_text(delta=delta, debounce_by=debounce_by))\n\n    def stream_responses(self, *, debounce_by: float | None = 0.1) -> Iterator[tuple[_messages.ModelResponse, bool]]:\n        \"\"\"Stream the response as an iterable of Structured LLM Messages.\n\n        Args:\n            debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured responses to reduce the overhead of\n                performing validation as each token is received.\n\n        Returns:\n            An iterable of the structured response message and whether that is the last message.\n        \"\"\"\n        return _utils.sync_async_iterator(self._streamed_run_result.stream_responses(debounce_by=debounce_by))\n\n    def get_output(self) -> OutputDataT:\n        \"\"\"Stream the whole response, validate and return it.\"\"\"\n        return _utils.get_event_loop().run_until_complete(self._streamed_run_result.get_output())\n\n    @property\n    def response(self) -> _messages.ModelResponse:\n        \"\"\"Return the current state of the response.\"\"\"\n        return self._streamed_run_result.response\n\n    def usage(self) -> RunUsage:\n        \"\"\"Return the usage of the whole run.\n\n        !!! note\n            This won't return the full usage until the stream is finished.\n        \"\"\"\n        return self._streamed_run_result.usage()\n\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._streamed_run_result.timestamp()\n\n    @property\n    def run_id(self) -> str:\n        \"\"\"The unique identifier for the agent run.\"\"\"\n        return self._streamed_run_result.run_id\n\n    def validate_response_output(self, message: _messages.ModelResponse, *, allow_partial: bool = False) -> OutputDataT:\n        \"\"\"Validate a structured result message.\"\"\"\n        return _utils.get_event_loop().run_until_complete(\n            self._streamed_run_result.validate_response_output(message, allow_partial=allow_partial)\n        )\n\n    @property\n    def is_complete(self) -> bool:\n        \"\"\"Whether the stream has all been received.\n\n        This is set to `True` when one of\n        [`stream_output`][pydantic_ai.result.StreamedRunResultSync.stream_output],\n        [`stream_text`][pydantic_ai.result.StreamedRunResultSync.stream_text],\n        [`stream_responses`][pydantic_ai.result.StreamedRunResultSync.stream_responses] or\n        [`get_output`][pydantic_ai.result.StreamedRunResultSync.get_output] completes.\n        \"\"\"\n        return self._streamed_run_result.is_complete\n\n```\n\n#### all_messages\n\n```python\nall_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n\n```\n\nReturn the history of messages.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `list[ModelMessage]` | List of messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n    \"\"\"Return the history of messages.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        List of messages.\n    \"\"\"\n    return self._streamed_run_result.all_messages(output_tool_return_content=output_tool_return_content)\n\n```\n\n#### all_messages_json\n\n```python\nall_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n\n```\n\nReturn all messages from all_messages as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `bytes` | JSON bytes representing the messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n    \"\"\"Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResultSync.all_messages] as JSON bytes.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        JSON bytes representing the messages.\n    \"\"\"\n    return self._streamed_run_result.all_messages_json(output_tool_return_content=output_tool_return_content)\n\n```\n\n#### new_messages\n\n```python\nnew_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n\n```\n\nReturn new messages associated with this run.\n\nMessages from older runs are excluded.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `list[ModelMessage]` | List of new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n    \"\"\"Return new messages associated with this run.\n\n    Messages from older runs are excluded.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        List of new messages.\n    \"\"\"\n    return self._streamed_run_result.new_messages(output_tool_return_content=output_tool_return_content)\n\n```\n\n#### new_messages_json\n\n```python\nnew_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n\n```\n\nReturn new messages from new_messages as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `bytes` | JSON bytes representing the new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n    \"\"\"Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResultSync.new_messages] as JSON bytes.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        JSON bytes representing the new messages.\n    \"\"\"\n    return self._streamed_run_result.new_messages_json(output_tool_return_content=output_tool_return_content)\n\n```\n\n#### stream_output\n\n```python\nstream_output(\n    *, debounce_by: float | None = 0.1\n) -> Iterator[OutputDataT]\n\n```\n\nStream the output as an iterable.\n\nThe pydantic validator for structured data will be called in [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation) on each iteration.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `debounce_by` | `float | None` | by how much (if at all) to debounce/group the output chunks by. None means no debouncing. Debouncing is particularly important for long structured outputs to reduce the overhead of performing validation as each token is received. | `0.1` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `Iterator[OutputDataT]` | An iterable of the response data. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef stream_output(self, *, debounce_by: float | None = 0.1) -> Iterator[OutputDataT]:\n    \"\"\"Stream the output as an iterable.\n\n    The pydantic validator for structured data will be called in\n    [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation)\n    on each iteration.\n\n    Args:\n        debounce_by: by how much (if at all) to debounce/group the output chunks by. `None` means no debouncing.\n            Debouncing is particularly important for long structured outputs to reduce the overhead of\n            performing validation as each token is received.\n\n    Returns:\n        An iterable of the response data.\n    \"\"\"\n    return _utils.sync_async_iterator(self._streamed_run_result.stream_output(debounce_by=debounce_by))\n\n```\n\n#### stream_text\n\n```python\nstream_text(\n    *, delta: bool = False, debounce_by: float | None = 0.1\n) -> Iterator[str]\n\n```\n\nStream the text result as an iterable.\n\nNote\n\nResult validators will NOT be called on the text result if `delta=True`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `delta` | `bool` | if True, yield each chunk of text as it is received, if False (default), yield the full text up to the current point. | `False` | | `debounce_by` | `float | None` | by how much (if at all) to debounce/group the response chunks by. None means no debouncing. Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. | `0.1` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef stream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> Iterator[str]:\n    \"\"\"Stream the text result as an iterable.\n\n    !!! note\n        Result validators will NOT be called on the text result if `delta=True`.\n\n    Args:\n        delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\n            up to the current point.\n        debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n            Debouncing is particularly important for long structured responses to reduce the overhead of\n            performing validation as each token is received.\n    \"\"\"\n    return _utils.sync_async_iterator(self._streamed_run_result.stream_text(delta=delta, debounce_by=debounce_by))\n\n```\n\n#### stream_responses\n\n```python\nstream_responses(\n    *, debounce_by: float | None = 0.1\n) -> Iterator[tuple[ModelResponse, bool]]\n\n```\n\nStream the response as an iterable of Structured LLM Messages.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `debounce_by` | `float | None` | by how much (if at all) to debounce/group the response chunks by. None means no debouncing. Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. | `0.1` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `Iterator[tuple[ModelResponse, bool]]` | An iterable of the structured response message and whether that is the last message. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef stream_responses(self, *, debounce_by: float | None = 0.1) -> Iterator[tuple[_messages.ModelResponse, bool]]:\n    \"\"\"Stream the response as an iterable of Structured LLM Messages.\n\n    Args:\n        debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n            Debouncing is particularly important for long structured responses to reduce the overhead of\n            performing validation as each token is received.\n\n    Returns:\n        An iterable of the structured response message and whether that is the last message.\n    \"\"\"\n    return _utils.sync_async_iterator(self._streamed_run_result.stream_responses(debounce_by=debounce_by))\n\n```\n\n#### get_output\n\n```python\nget_output() -> OutputDataT\n\n```\n\nStream the whole response, validate and return it.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef get_output(self) -> OutputDataT:\n    \"\"\"Stream the whole response, validate and return it.\"\"\"\n    return _utils.get_event_loop().run_until_complete(self._streamed_run_result.get_output())\n\n```\n\n#### response\n\n```python\nresponse: ModelResponse\n\n```\n\nReturn the current state of the response.\n\n#### usage\n\n```python\nusage() -> RunUsage\n\n```\n\nReturn the usage of the whole run.\n\nNote\n\nThis won't return the full usage until the stream is finished.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef usage(self) -> RunUsage:\n    \"\"\"Return the usage of the whole run.\n\n    !!! note\n        This won't return the full usage until the stream is finished.\n    \"\"\"\n    return self._streamed_run_result.usage()\n\n```\n\n#### timestamp\n\n```python\ntimestamp() -> datetime\n\n```\n\nGet the timestamp of the response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef timestamp(self) -> datetime:\n    \"\"\"Get the timestamp of the response.\"\"\"\n    return self._streamed_run_result.timestamp()\n\n```\n\n#### run_id\n\n```python\nrun_id: str\n\n```\n\nThe unique identifier for the agent run.\n\n#### validate_response_output\n\n```python\nvalidate_response_output(\n    message: ModelResponse, *, allow_partial: bool = False\n) -> OutputDataT\n\n```\n\nValidate a structured result message.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n```python\ndef validate_response_output(self, message: _messages.ModelResponse, *, allow_partial: bool = False) -> OutputDataT:\n    \"\"\"Validate a structured result message.\"\"\"\n    return _utils.get_event_loop().run_until_complete(\n        self._streamed_run_result.validate_response_output(message, allow_partial=allow_partial)\n    )\n\n```\n\n#### is_complete\n\n```python\nis_complete: bool\n\n```\n\nWhether the stream has all been received.\n\nThis is set to `True` when one of stream_output, stream_text, stream_responses or get_output completes.",
  "content_length": 52490
}