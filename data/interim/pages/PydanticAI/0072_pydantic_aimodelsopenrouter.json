{
  "title": "`pydantic_ai.models.openrouter`",
  "source_url": null,
  "content": "## Setup\n\nFor details on how to set up authentication with this model, see [model configuration for OpenRouter](../../../models/openrouter/).\n\n### KnownOpenRouterProviders\n\n```python\nKnownOpenRouterProviders = Literal[\n    \"z-ai\",\n    \"cerebras\",\n    \"venice\",\n    \"moonshotai\",\n    \"morph\",\n    \"stealth\",\n    \"wandb\",\n    \"klusterai\",\n    \"openai\",\n    \"sambanova\",\n    \"amazon-bedrock\",\n    \"mistral\",\n    \"nextbit\",\n    \"atoma\",\n    \"ai21\",\n    \"minimax\",\n    \"baseten\",\n    \"anthropic\",\n    \"featherless\",\n    \"groq\",\n    \"lambda\",\n    \"azure\",\n    \"ncompass\",\n    \"deepseek\",\n    \"hyperbolic\",\n    \"crusoe\",\n    \"cohere\",\n    \"mancer\",\n    \"avian\",\n    \"perplexity\",\n    \"novita\",\n    \"siliconflow\",\n    \"switchpoint\",\n    \"xai\",\n    \"inflection\",\n    \"fireworks\",\n    \"deepinfra\",\n    \"inference-net\",\n    \"inception\",\n    \"atlas-cloud\",\n    \"nvidia\",\n    \"alibaba\",\n    \"friendli\",\n    \"infermatic\",\n    \"targon\",\n    \"ubicloud\",\n    \"aion-labs\",\n    \"liquid\",\n    \"nineteen\",\n    \"cloudflare\",\n    \"nebius\",\n    \"chutes\",\n    \"enfer\",\n    \"crofai\",\n    \"open-inference\",\n    \"phala\",\n    \"gmicloud\",\n    \"meta\",\n    \"relace\",\n    \"parasail\",\n    \"together\",\n    \"google-ai-studio\",\n    \"google-vertex\",\n]\n\n```\n\nKnown providers in the OpenRouter marketplace\n\n### OpenRouterProviderName\n\n```python\nOpenRouterProviderName = str | KnownOpenRouterProviders\n\n```\n\nPossible OpenRouter provider names.\n\nSince OpenRouter is constantly updating their list of providers, we explicitly list some known providers but allow any name in the type hints. See [the OpenRouter API](https://openrouter.ai/docs/api-reference/list-available-providers) for a full list.\n\n### OpenRouterTransforms\n\n```python\nOpenRouterTransforms = Literal['middle-out']\n\n```\n\nAvailable messages transforms for OpenRouter models with limited token windows.\n\nCurrently only supports 'middle-out', but is expected to grow in the future.\n\n### OpenRouterProviderConfig\n\nBases: `TypedDict`\n\nRepresents the 'Provider' object from the OpenRouter API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openrouter.py`\n\n```python\nclass OpenRouterProviderConfig(TypedDict, total=False):\n    \"\"\"Represents the 'Provider' object from the OpenRouter API.\"\"\"\n\n    order: list[OpenRouterProviderName]\n    \"\"\"List of provider slugs to try in order (e.g. [\"anthropic\", \"openai\"]). [See details](https://openrouter.ai/docs/features/provider-routing#ordering-specific-providers)\"\"\"\n\n    allow_fallbacks: bool\n    \"\"\"Whether to allow backup providers when the primary is unavailable. [See details](https://openrouter.ai/docs/features/provider-routing#disabling-fallbacks)\"\"\"\n\n    require_parameters: bool\n    \"\"\"Only use providers that support all parameters in your request.\"\"\"\n\n    data_collection: Literal['allow', 'deny']\n    \"\"\"Control whether to use providers that may store data. [See details](https://openrouter.ai/docs/features/provider-routing#requiring-providers-to-comply-with-data-policies)\"\"\"\n\n    zdr: bool\n    \"\"\"Restrict routing to only ZDR (Zero Data Retention) endpoints. [See details](https://openrouter.ai/docs/features/provider-routing#zero-data-retention-enforcement)\"\"\"\n\n    only: list[OpenRouterProviderName]\n    \"\"\"List of provider slugs to allow for this request. [See details](https://openrouter.ai/docs/features/provider-routing#allowing-only-specific-providers)\"\"\"\n\n    ignore: list[str]\n    \"\"\"List of provider slugs to skip for this request. [See details](https://openrouter.ai/docs/features/provider-routing#ignoring-providers)\"\"\"\n\n    quantizations: list[Literal['int4', 'int8', 'fp4', 'fp6', 'fp8', 'fp16', 'bf16', 'fp32', 'unknown']]\n    \"\"\"List of quantization levels to filter by (e.g. [\"int4\", \"int8\"]). [See details](https://openrouter.ai/docs/features/provider-routing#quantization)\"\"\"\n\n    sort: Literal['price', 'throughput', 'latency']\n    \"\"\"Sort providers by price or throughput. (e.g. \"price\" or \"throughput\"). [See details](https://openrouter.ai/docs/features/provider-routing#provider-sorting)\"\"\"\n\n    max_price: _OpenRouterMaxPrice\n    \"\"\"The maximum pricing you want to pay for this request. [See details](https://openrouter.ai/docs/features/provider-routing#max-price)\"\"\"\n\n```\n\n#### order\n\n```python\norder: list[OpenRouterProviderName]\n\n```\n\nList of provider slugs to try in order (e.g. [\"anthropic\", \"openai\"]). [See details](https://openrouter.ai/docs/features/provider-routing#ordering-specific-providers)\n\n#### allow_fallbacks\n\n```python\nallow_fallbacks: bool\n\n```\n\nWhether to allow backup providers when the primary is unavailable. [See details](https://openrouter.ai/docs/features/provider-routing#disabling-fallbacks)\n\n#### require_parameters\n\n```python\nrequire_parameters: bool\n\n```\n\nOnly use providers that support all parameters in your request.\n\n#### data_collection\n\n```python\ndata_collection: Literal['allow', 'deny']\n\n```\n\nControl whether to use providers that may store data. [See details](https://openrouter.ai/docs/features/provider-routing#requiring-providers-to-comply-with-data-policies)\n\n#### zdr\n\n```python\nzdr: bool\n\n```\n\nRestrict routing to only ZDR (Zero Data Retention) endpoints. [See details](https://openrouter.ai/docs/features/provider-routing#zero-data-retention-enforcement)\n\n#### only\n\n```python\nonly: list[OpenRouterProviderName]\n\n```\n\nList of provider slugs to allow for this request. [See details](https://openrouter.ai/docs/features/provider-routing#allowing-only-specific-providers)\n\n#### ignore\n\n```python\nignore: list[str]\n\n```\n\nList of provider slugs to skip for this request. [See details](https://openrouter.ai/docs/features/provider-routing#ignoring-providers)\n\n#### quantizations\n\n```python\nquantizations: list[\n    Literal[\n        \"int4\",\n        \"int8\",\n        \"fp4\",\n        \"fp6\",\n        \"fp8\",\n        \"fp16\",\n        \"bf16\",\n        \"fp32\",\n        \"unknown\",\n    ]\n]\n\n```\n\nList of quantization levels to filter by (e.g. [\"int4\", \"int8\"]). [See details](https://openrouter.ai/docs/features/provider-routing#quantization)\n\n#### sort\n\n```python\nsort: Literal['price', 'throughput', 'latency']\n\n```\n\nSort providers by price or throughput. (e.g. \"price\" or \"throughput\"). [See details](https://openrouter.ai/docs/features/provider-routing#provider-sorting)\n\n#### max_price\n\n```python\nmax_price: _OpenRouterMaxPrice\n\n```\n\nThe maximum pricing you want to pay for this request. [See details](https://openrouter.ai/docs/features/provider-routing#max-price)\n\n### OpenRouterReasoning\n\nBases: `TypedDict`\n\nConfiguration for reasoning tokens in OpenRouter requests.\n\nReasoning tokens allow models to show their step-by-step thinking process. You can configure this using either OpenAI-style effort levels or Anthropic-style token limits, but not both simultaneously.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openrouter.py`\n\n```python\nclass OpenRouterReasoning(TypedDict, total=False):\n    \"\"\"Configuration for reasoning tokens in OpenRouter requests.\n\n    Reasoning tokens allow models to show their step-by-step thinking process.\n    You can configure this using either OpenAI-style effort levels or Anthropic-style\n    token limits, but not both simultaneously.\n    \"\"\"\n\n    effort: Literal['high', 'medium', 'low']\n    \"\"\"OpenAI-style reasoning effort level. Cannot be used with max_tokens.\"\"\"\n\n    max_tokens: int\n    \"\"\"Anthropic-style specific token limit for reasoning. Cannot be used with effort.\"\"\"\n\n    exclude: bool\n    \"\"\"Whether to exclude reasoning tokens from the response. Default is False. All models support this.\"\"\"\n\n    enabled: bool\n    \"\"\"Whether to enable reasoning with default parameters. Default is inferred from effort or max_tokens.\"\"\"\n\n```\n\n#### effort\n\n```python\neffort: Literal['high', 'medium', 'low']\n\n```\n\nOpenAI-style reasoning effort level. Cannot be used with max_tokens.\n\n#### max_tokens\n\n```python\nmax_tokens: int\n\n```\n\nAnthropic-style specific token limit for reasoning. Cannot be used with effort.\n\n#### exclude\n\n```python\nexclude: bool\n\n```\n\nWhether to exclude reasoning tokens from the response. Default is False. All models support this.\n\n#### enabled\n\n```python\nenabled: bool\n\n```\n\nWhether to enable reasoning with default parameters. Default is inferred from effort or max_tokens.\n\n### OpenRouterUsageConfig\n\nBases: `TypedDict`\n\nConfiguration for OpenRouter usage.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openrouter.py`\n\n```python\nclass OpenRouterUsageConfig(TypedDict, total=False):\n    \"\"\"Configuration for OpenRouter usage.\"\"\"\n\n    include: bool\n\n```\n\n### OpenRouterModelSettings\n\nBases: `ModelSettings`\n\nSettings used for an OpenRouter model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openrouter.py`\n\n```python\nclass OpenRouterModelSettings(ModelSettings, total=False):\n    \"\"\"Settings used for an OpenRouter model request.\"\"\"\n\n    # ALL FIELDS MUST BE `openrouter_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\n    openrouter_models: list[str]\n    \"\"\"A list of fallback models.\n\n    These models will be tried, in order, if the main model returns an error. [See details](https://openrouter.ai/docs/features/model-routing#the-models-parameter)\n    \"\"\"\n\n    openrouter_provider: OpenRouterProviderConfig\n    \"\"\"OpenRouter routes requests to the best available providers for your model. By default, requests are load balanced across the top providers to maximize uptime.\n\n    You can customize how your requests are routed using the provider object. [See more](https://openrouter.ai/docs/features/provider-routing)\"\"\"\n\n    openrouter_preset: str\n    \"\"\"Presets allow you to separate your LLM configuration from your code.\n\n    Create and manage presets through the OpenRouter web application to control provider routing, model selection, system prompts, and other parameters, then reference them in OpenRouter API requests. [See more](https://openrouter.ai/docs/features/presets)\"\"\"\n\n    openrouter_transforms: list[OpenRouterTransforms]\n    \"\"\"To help with prompts that exceed the maximum context size of a model.\n\n    Transforms work by removing or truncating messages from the middle of the prompt, until the prompt fits within the model's context window. [See more](https://openrouter.ai/docs/features/message-transforms)\n    \"\"\"\n\n    openrouter_reasoning: OpenRouterReasoning\n    \"\"\"To control the reasoning tokens in the request.\n\n    The reasoning config object consolidates settings for controlling reasoning strength across different models. [See more](https://openrouter.ai/docs/use-cases/reasoning-tokens)\n    \"\"\"\n\n    openrouter_usage: OpenRouterUsageConfig\n    \"\"\"To control the usage of the model.\n\n    The usage config object consolidates settings for enabling detailed usage information. [See more](https://openrouter.ai/docs/use-cases/usage-accounting)\n    \"\"\"\n\n```\n\n#### openrouter_models\n\n```python\nopenrouter_models: list[str]\n\n```\n\nA list of fallback models.\n\nThese models will be tried, in order, if the main model returns an error. [See details](https://openrouter.ai/docs/features/model-routing#the-models-parameter)\n\n#### openrouter_provider\n\n```python\nopenrouter_provider: OpenRouterProviderConfig\n\n```\n\nOpenRouter routes requests to the best available providers for your model. By default, requests are load balanced across the top providers to maximize uptime.\n\nYou can customize how your requests are routed using the provider object. [See more](https://openrouter.ai/docs/features/provider-routing)\n\n#### openrouter_preset\n\n```python\nopenrouter_preset: str\n\n```\n\nPresets allow you to separate your LLM configuration from your code.\n\nCreate and manage presets through the OpenRouter web application to control provider routing, model selection, system prompts, and other parameters, then reference them in OpenRouter API requests. [See more](https://openrouter.ai/docs/features/presets)\n\n#### openrouter_transforms\n\n```python\nopenrouter_transforms: list[OpenRouterTransforms]\n\n```\n\nTo help with prompts that exceed the maximum context size of a model.\n\nTransforms work by removing or truncating messages from the middle of the prompt, until the prompt fits within the model's context window. [See more](https://openrouter.ai/docs/features/message-transforms)\n\n#### openrouter_reasoning\n\n```python\nopenrouter_reasoning: OpenRouterReasoning\n\n```\n\nTo control the reasoning tokens in the request.\n\nThe reasoning config object consolidates settings for controlling reasoning strength across different models. [See more](https://openrouter.ai/docs/use-cases/reasoning-tokens)\n\n#### openrouter_usage\n\n```python\nopenrouter_usage: OpenRouterUsageConfig\n\n```\n\nTo control the usage of the model.\n\nThe usage config object consolidates settings for enabling detailed usage information. [See more](https://openrouter.ai/docs/use-cases/usage-accounting)\n\n### OpenRouterModel\n\nBases: `OpenAIChatModel`\n\nExtends OpenAIModel to capture extra metadata for Openrouter.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openrouter.py`\n\n```python\nclass OpenRouterModel(OpenAIChatModel):\n    \"\"\"Extends OpenAIModel to capture extra metadata for Openrouter.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        *,\n        provider: Literal['openrouter'] | Provider[AsyncOpenAI] = 'openrouter',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize an OpenRouter model.\n\n        Args:\n            model_name: The name of the model to use.\n            provider: The provider to use for authentication and API access. If not provided, a new provider will be created with the default settings.\n            profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n            settings: Model-specific settings that will be used as defaults for this model.\n        \"\"\"\n        super().__init__(model_name, provider=provider or OpenRouterProvider(), profile=profile, settings=settings)\n\n    @override\n    def prepare_request(\n        self,\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> tuple[ModelSettings | None, ModelRequestParameters]:\n        merged_settings, customized_parameters = super().prepare_request(model_settings, model_request_parameters)\n        new_settings = _openrouter_settings_to_openai_settings(cast(OpenRouterModelSettings, merged_settings or {}))\n        return new_settings, customized_parameters\n\n    @override\n    def _validate_completion(self, response: chat.ChatCompletion) -> _OpenRouterChatCompletion:\n        response = _OpenRouterChatCompletion.model_validate(response.model_dump())\n\n        if error := response.error:\n            raise ModelHTTPError(status_code=error.code, model_name=response.model, body=error.message)\n\n        return response\n\n    @override\n    def _process_thinking(self, message: chat.ChatCompletionMessage) -> list[ThinkingPart] | None:\n        assert isinstance(message, _OpenRouterCompletionMessage)\n\n        if reasoning_details := message.reasoning_details:\n            return [_from_reasoning_detail(detail) for detail in reasoning_details]\n        else:\n            return super()._process_thinking(message)\n\n    @override\n    def _process_provider_details(self, response: chat.ChatCompletion) -> dict[str, Any]:\n        assert isinstance(response, _OpenRouterChatCompletion)\n\n        provider_details = super()._process_provider_details(response)\n        provider_details.update(_map_openrouter_provider_details(response))\n        return provider_details\n\n    @dataclass\n    class _MapModelResponseContext(OpenAIChatModel._MapModelResponseContext):  # type: ignore[reportPrivateUsage]\n        reasoning_details: list[dict[str, Any]] = field(default_factory=list)\n\n        def _into_message_param(self) -> chat.ChatCompletionAssistantMessageParam:\n            message_param = super()._into_message_param()\n            if self.reasoning_details:\n                message_param['reasoning_details'] = self.reasoning_details  # type: ignore[reportGeneralTypeIssues]\n            return message_param\n\n        @override\n        def _map_response_thinking_part(self, item: ThinkingPart) -> None:\n            assert isinstance(self._model, OpenRouterModel)\n            if item.provider_name == self._model.system:\n                if reasoning_detail := _into_reasoning_detail(item):  # pragma: lax no cover\n                    self.reasoning_details.append(reasoning_detail.model_dump())\n            elif content := item.content:  # pragma: lax no cover\n                start_tag, end_tag = self._model.profile.thinking_tags\n                self.texts.append('\\n'.join([start_tag, content, end_tag]))\n            else:\n                pass\n\n    @property\n    @override\n    def _streamed_response_cls(self):\n        return OpenRouterStreamedResponse\n\n    @override\n    def _map_finish_reason(  # type: ignore[reportIncompatibleMethodOverride]\n        self, key: Literal['stop', 'length', 'tool_calls', 'content_filter', 'error']\n    ) -> FinishReason | None:\n        return _CHAT_FINISH_REASON_MAP.get(key)\n\n```\n\n#### __init__\n\n```python\n__init__(\n    model_name: str,\n    *,\n    provider: (\n        Literal[\"openrouter\"] | Provider[AsyncOpenAI]\n    ) = \"openrouter\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize an OpenRouter model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model_name` | `str` | The name of the model to use. | *required* | | `provider` | `Literal['openrouter'] | Provider[AsyncOpenAI]` | The provider to use for authentication and API access. If not provided, a new provider will be created with the default settings. | `'openrouter'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` | | `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openrouter.py`\n\n```python\ndef __init__(\n    self,\n    model_name: str,\n    *,\n    provider: Literal['openrouter'] | Provider[AsyncOpenAI] = 'openrouter',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize an OpenRouter model.\n\n    Args:\n        model_name: The name of the model to use.\n        provider: The provider to use for authentication and API access. If not provided, a new provider will be created with the default settings.\n        profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n        settings: Model-specific settings that will be used as defaults for this model.\n    \"\"\"\n    super().__init__(model_name, provider=provider or OpenRouterProvider(), profile=profile, settings=settings)\n\n```\n\n### OpenRouterStreamedResponse\n\nBases: `OpenAIStreamedResponse`\n\nImplementation of `StreamedResponse` for OpenRouter models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openrouter.py`\n\n```python\n@dataclass\nclass OpenRouterStreamedResponse(OpenAIStreamedResponse):\n    \"\"\"Implementation of `StreamedResponse` for OpenRouter models.\"\"\"\n\n    @override\n    async def _validate_response(self):\n        try:\n            async for chunk in self._response:\n                yield _OpenRouterChatCompletionChunk.model_validate(chunk.model_dump())\n        except APIError as e:\n            error = _OpenRouterError.model_validate(e.body)\n            raise ModelHTTPError(status_code=error.code, model_name=self._model_name, body=error.message)\n\n    @override\n    def _map_thinking_delta(self, choice: chat_completion_chunk.Choice) -> Iterable[ModelResponseStreamEvent]:\n        assert isinstance(choice, _OpenRouterChunkChoice)\n\n        if reasoning_details := choice.delta.reasoning_details:\n            for i, detail in enumerate(reasoning_details):\n                thinking_part = _from_reasoning_detail(detail)\n                # Use unique vendor_part_id for each reasoning detail type to prevent\n                # different detail types (e.g., reasoning.text, reasoning.encrypted)\n                # from being incorrectly merged into a single ThinkingPart.\n                # This is required for Gemini 3 Pro which returns multiple reasoning\n                # detail types that must be preserved separately for thought_signature handling.\n                vendor_id = f'reasoning_detail_{detail.type}_{i}'\n                yield self._parts_manager.handle_thinking_delta(\n                    vendor_part_id=vendor_id,\n                    id=thinking_part.id,\n                    content=thinking_part.content,\n                    signature=thinking_part.signature,\n                    provider_name=self._provider_name,\n                    provider_details=thinking_part.provider_details,\n                )\n        else:\n            return super()._map_thinking_delta(choice)\n\n    @override\n    def _map_provider_details(self, chunk: chat.ChatCompletionChunk) -> dict[str, Any] | None:\n        assert isinstance(chunk, _OpenRouterChatCompletionChunk)\n\n        if provider_details := super()._map_provider_details(chunk):\n            provider_details.update(_map_openrouter_provider_details(chunk))\n            return provider_details\n\n    @override\n    def _map_finish_reason(  # type: ignore[reportIncompatibleMethodOverride]\n        self, key: Literal['stop', 'length', 'tool_calls', 'content_filter', 'error']\n    ) -> FinishReason | None:\n        return _CHAT_FINISH_REASON_MAP.get(key)\n\n```",
  "content_length": 21498
}