{
  "title": "Durable Execution with DBOS",
  "source_url": null,
  "content": "[DBOS](https://www.dbos.dev/) is a lightweight [durable execution](https://docs.dbos.dev/architecture) library natively integrated with Pydantic AI.\n\n## Durable Execution\n\nDBOS workflows make your program **durable** by checkpointing its state in a database. If your program ever fails, when it restarts all your workflows will automatically resume from the last completed step.\n\n- **Workflows** must be deterministic and generally cannot include I/O.\n- **Steps** may perform I/O (network, disk, API calls). If a step fails, it restarts from the beginning.\n\nEvery workflow input and step output is durably stored in the system database. When workflow execution fails, whether from crashes, network issues, or server restarts, DBOS leverages these checkpoints to recover workflows from their last completed step.\n\nDBOS **queues** provide durable, database-backed alternatives to systems like Celery or BullMQ, supporting features such as concurrency limits, rate limits, timeouts, and prioritization. See the [DBOS docs](https://docs.dbos.dev/architecture) for details.\n\nThe diagram below shows the overall architecture of an agentic application in DBOS. DBOS runs fully in-process as a library. Functions remain normal Python functions but are checkpointed into a database (Postgres or SQLite).\n\n```text\n                    Clients\n            (HTTP, RPC, Kafka, etc.)\n                        |\n                        v\n+------------------------------------------------------+\n|               Application Servers                    |\n|                                                      |\n|   +----------------------------------------------+   |\n|   |        Pydantic AI + DBOS Libraries          |   |\n|   |                                              |   |\n|   |  [ Workflows (Agent Run Loop) ]              |   |\n|   |  [ Steps (Tool, MCP, Model) ]                |   |\n|   |  [ Queues ]   [ Cron Jobs ]   [ Messaging ]  |   |\n|   +----------------------------------------------+   |\n|                                                      |\n+------------------------------------------------------+\n                        |\n                        v\n+------------------------------------------------------+\n|                      Database                        |\n|   (Stores workflow and step state, schedules tasks)  |\n+------------------------------------------------------+\n\n```\n\nSee the [DBOS documentation](https://docs.dbos.dev/architecture) for more information.\n\n## Durable Agent\n\nAny agent can be wrapped in a DBOSAgent to get durable execution. `DBOSAgent` automatically:,\n\n- Wraps `Agent.run` and `Agent.run_sync` as DBOS workflows.\n- Wraps [model requests](../../models/overview/) and [MCP communication](../../mcp/client/) as DBOS steps.\n\nCustom tool functions and event stream handlers are **not automatically wrapped** by DBOS. If they involve non-deterministic behavior or perform I/O, you should explicitly decorate them with `@DBOS.step`.\n\nThe original agent, model, and MCP server can still be used as normal outside the DBOS workflow.\n\nHere is a simple but complete example of wrapping an agent for durable execution. All it requires is to install Pydantic AI with the DBOS [open-source library](https://github.com/dbos-inc/dbos-transact-py):\n\n```bash\npip install pydantic-ai[dbos]\n\n```\n\n```bash\nuv add pydantic-ai[dbos]\n\n```\n\nOr if you're using the slim package, you can install it with the `dbos` optional group:\n\n```bash\npip install pydantic-ai-slim[dbos]\n\n```\n\n```bash\nuv add pydantic-ai-slim[dbos]\n\n```\n\ndbos_agent.py\n\n```python\nfrom dbos import DBOS, DBOSConfig\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.dbos import DBOSAgent\n\ndbos_config: DBOSConfig = {\n    'name': 'pydantic_dbos_agent',\n    'system_database_url': 'sqlite:///dbostest.sqlite',  # (3)!\n}\nDBOS(config=dbos_config)\n\nagent = Agent(\n    'gpt-5',\n    instructions=\"You're an expert in geography.\",\n    name='geography',  # (4)!\n)\n\ndbos_agent = DBOSAgent(agent)  # (1)!\n\nasync def main():\n    DBOS.launch()\n    result = await dbos_agent.run('What is the capital of Mexico?')  # (2)!\n    print(result.output)\n    #> Mexico City (Ciudad de México, CDMX)\n\n```\n\n1. Workflows and `DBOSAgent` must be defined before `DBOS.launch()` so that recovery can correctly find all workflows.\n1. DBOSAgent.run() works like Agent.run(), but runs as a DBOS workflow and executes model requests, decorated tool calls, and MCP communication as DBOS steps.\n1. This example uses SQLite. Postgres is recommended for production.\n1. The agent's `name` is used to uniquely identify its workflows.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nBecause DBOS workflows need to be defined before calling `DBOS.launch()` and the `DBOSAgent` instance automatically registers `run` and `run_sync` as workflows, it needs to be defined before calling `DBOS.launch()` as well.\n\nFor more information on how to use DBOS in Python applications, see their [Python SDK guide](https://docs.dbos.dev/python/programming-guide).\n\n## DBOS Integration Considerations\n\nWhen using DBOS with Pydantic AI agents, there are a few important considerations to ensure workflows and toolsets behave correctly.\n\n### Agent and Toolset Requirements\n\nEach agent instance must have a unique `name` so DBOS can correctly resume workflows after a failure or restart.\n\nTools and event stream handlers are not automatically wrapped by DBOS. You can decide how to integrate them:\n\n- Decorate with `@DBOS.step` if the function involves non-determinism or I/O.\n- Skip the decorator if durability isn't needed, so you avoid the extra DB checkpoint write.\n- If the function needs to enqueue tasks or invoke other DBOS workflows, run it inside the agent's main workflow (not as a step).\n\nOther than that, any agent and toolset will just work!\n\n### Agent Run Context and Dependencies\n\nDBOS checkpoints workflow inputs/outputs and step outputs into a database using [`pickle`](https://docs.python.org/3/library/pickle.html). This means you need to make sure [dependencies](../../dependencies/) object provided to DBOSAgent.run() or DBOSAgent.run_sync(), and tool outputs can be serialized using pickle. You may also want to keep the inputs and outputs small (under ~2 MB). PostgreSQL and SQLite support up to 1 GB per field, but large objects may impact performance.\n\n### Streaming\n\nBecause DBOS cannot stream output directly to the workflow or step call site, Agent.run_stream() and Agent.run_stream_events() are not supported when running inside of a DBOS workflow.\n\nInstead, you can implement streaming by setting an event_stream_handler on the `Agent` or `DBOSAgent` instance and using DBOSAgent.run(). The event stream handler function will receive the agent run context and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/#streaming-all-events).\n\n## Step Configuration\n\nYou can customize DBOS step behavior, such as retries, by passing StepConfig objects to the `DBOSAgent` constructor:\n\n- `mcp_step_config`: The DBOS step config to use for MCP server communication. No retries if omitted.\n- `model_step_config`: The DBOS step config to use for model request steps. No retries if omitted.\n\nFor custom tools, you can annotate them directly with [`@DBOS.step`](https://docs.dbos.dev/python/reference/decorators#step) or [`@DBOS.workflow`](https://docs.dbos.dev/python/reference/decorators#workflow) decorators as needed. These decorators have no effect outside DBOS workflows, so tools remain usable in non-DBOS agents.\n\n## Step Retries\n\nOn top of the automatic retries for request failures that DBOS will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper `Retry-After` handling.\n\nWhen using DBOS, it's recommended to not use [HTTP Request Retries](../../retries/) and to turn off your provider API client's own retry logic, for example by setting `max_retries=0` on a [custom `OpenAIProvider` API client](../../models/openai/#custom-openai-client).\n\nYou can customize DBOS's retry policy using [step configuration](#step-configuration).\n\n## Observability with Logfire\n\nDBOS can be configured to generate OpenTelemetry spans for each workflow and step execution, and Pydantic AI emits spans for each agent run, model request, and tool invocation. You can send these spans to [Pydantic Logfire](../../logfire/) to get a full, end-to-end view of what's happening in your application.\n\nFor more information about DBOS logging and tracing, please see the [DBOS docs](https://docs.dbos.dev/python/tutorials/logging-and-tracing) for details.",
  "content_length": 8803
}