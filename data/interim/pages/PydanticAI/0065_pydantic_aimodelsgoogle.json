{
  "title": "`pydantic_ai.models.google`",
  "source_url": null,
  "content": "Interface that uses the [`google-genai`](https://pypi.org/project/google-genai/) package under the hood to access Google's Gemini models via both the Generative Language API and Vertex AI.\n\n## Setup\n\nFor details on how to set up authentication with this model, see [model configuration for Google](../../../models/google/).\n\n### LatestGoogleModelNames\n\n```python\nLatestGoogleModelNames = Literal[\n    \"gemini-flash-latest\",\n    \"gemini-flash-lite-latest\",\n    \"gemini-2.0-flash\",\n    \"gemini-2.0-flash-lite\",\n    \"gemini-2.5-flash\",\n    \"gemini-2.5-flash-preview-09-2025\",\n    \"gemini-2.5-flash-image\",\n    \"gemini-2.5-flash-lite\",\n    \"gemini-2.5-flash-lite-preview-09-2025\",\n    \"gemini-2.5-pro\",\n    \"gemini-3-pro-preview\",\n    \"gemini-3-pro-image-preview\",\n]\n\n```\n\nLatest Gemini models.\n\n### GoogleModelName\n\n```python\nGoogleModelName = str | LatestGoogleModelNames\n\n```\n\nPossible Gemini model names.\n\nSince Gemini supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Gemini API docs](https://ai.google.dev/gemini-api/docs/models/gemini#model-variations) for a full list.\n\n### GoogleModelSettings\n\nBases: `ModelSettings`\n\nSettings used for a Gemini model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/google.py`\n\n```python\nclass GoogleModelSettings(ModelSettings, total=False):\n    \"\"\"Settings used for a Gemini model request.\"\"\"\n\n    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\n    google_safety_settings: list[SafetySettingDict]\n    \"\"\"The safety settings to use for the model.\n\n    See <https://ai.google.dev/gemini-api/docs/safety-settings> for more information.\n    \"\"\"\n\n    google_thinking_config: ThinkingConfigDict\n    \"\"\"The thinking configuration to use for the model.\n\n    See <https://ai.google.dev/gemini-api/docs/thinking> for more information.\n    \"\"\"\n\n    google_labels: dict[str, str]\n    \"\"\"User-defined metadata to break down billed charges. Only supported by the Vertex AI API.\n\n    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.\n    \"\"\"\n\n    google_video_resolution: MediaResolution\n    \"\"\"The video resolution to use for the model.\n\n    See <https://ai.google.dev/api/generate-content#MediaResolution> for more information.\n    \"\"\"\n\n    google_cached_content: str\n    \"\"\"The name of the cached content to use for the model.\n\n    See <https://ai.google.dev/gemini-api/docs/caching> for more information.\n    \"\"\"\n\n```\n\n#### google_safety_settings\n\n```python\ngoogle_safety_settings: list[SafetySettingDict]\n\n```\n\nThe safety settings to use for the model.\n\nSee <https://ai.google.dev/gemini-api/docs/safety-settings> for more information.\n\n#### google_thinking_config\n\n```python\ngoogle_thinking_config: ThinkingConfigDict\n\n```\n\nThe thinking configuration to use for the model.\n\nSee <https://ai.google.dev/gemini-api/docs/thinking> for more information.\n\n#### google_labels\n\n```python\ngoogle_labels: dict[str, str]\n\n```\n\nUser-defined metadata to break down billed charges. Only supported by the Vertex AI API.\n\nSee the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.\n\n#### google_video_resolution\n\n```python\ngoogle_video_resolution: MediaResolution\n\n```\n\nThe video resolution to use for the model.\n\nSee <https://ai.google.dev/api/generate-content#MediaResolution> for more information.\n\n#### google_cached_content\n\n```python\ngoogle_cached_content: str\n\n```\n\nThe name of the cached content to use for the model.\n\nSee <https://ai.google.dev/gemini-api/docs/caching> for more information.\n\n### GoogleModel\n\nBases: `Model`\n\nA model that uses Gemini via `generativelanguage.googleapis.com` API.\n\nThis is implemented from scratch rather than using a dedicated SDK, good API documentation is available [here](https://ai.google.dev/api).\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/google.py`\n\n```python\n@dataclass(init=False)\nclass GoogleModel(Model):\n    \"\"\"A model that uses Gemini via `generativelanguage.googleapis.com` API.\n\n    This is implemented from scratch rather than using a dedicated SDK, good API documentation is\n    available [here](https://ai.google.dev/api).\n\n    Apart from `__init__`, all methods are private or match those of the base class.\n    \"\"\"\n\n    client: Client = field(repr=False)\n\n    _model_name: GoogleModelName = field(repr=False)\n    _provider: Provider[Client] = field(repr=False)\n\n    def __init__(\n        self,\n        model_name: GoogleModelName,\n        *,\n        provider: Literal['google-gla', 'google-vertex', 'gateway'] | Provider[Client] = 'google-gla',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize a Gemini model.\n\n        Args:\n            model_name: The name of the model to use.\n            provider: The provider to use for authentication and API access. Can be either the string\n                'google-gla' or 'google-vertex' or an instance of `Provider[google.genai.AsyncClient]`.\n                Defaults to 'google-gla'.\n            profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n            settings: The model settings to use. Defaults to None.\n        \"\"\"\n        self._model_name = model_name\n\n        if isinstance(provider, str):\n            provider = infer_provider('gateway/google-vertex' if provider == 'gateway' else provider)\n        self._provider = provider\n        self.client = provider.client\n\n        super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n    @property\n    def base_url(self) -> str:\n        return self._provider.base_url\n\n    @property\n    def model_name(self) -> GoogleModelName:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The model provider.\"\"\"\n        return self._provider.name\n\n    def prepare_request(\n        self, model_settings: ModelSettings | None, model_request_parameters: ModelRequestParameters\n    ) -> tuple[ModelSettings | None, ModelRequestParameters]:\n        supports_native_output_with_builtin_tools = GoogleModelProfile.from_profile(\n            self.profile\n        ).google_supports_native_output_with_builtin_tools\n        if model_request_parameters.builtin_tools and model_request_parameters.output_tools:\n            if model_request_parameters.output_mode == 'auto':\n                output_mode = 'native' if supports_native_output_with_builtin_tools else 'prompted'\n                model_request_parameters = replace(model_request_parameters, output_mode=output_mode)\n            else:\n                output_mode = 'NativeOutput' if supports_native_output_with_builtin_tools else 'PromptedOutput'\n                raise UserError(\n                    f'Google does not support output tools and built-in tools at the same time. Use `output_type={output_mode}(...)` instead.'\n                )\n        return super().prepare_request(model_settings, model_request_parameters)\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        model_settings = cast(GoogleModelSettings, model_settings or {})\n        response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n        return self._process_response(response)\n\n    async def count_tokens(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> usage.RequestUsage:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        model_settings = cast(GoogleModelSettings, model_settings or {})\n        contents, generation_config = await self._build_content_and_config(\n            messages, model_settings, model_request_parameters\n        )\n\n        # Annoyingly, the type of `GenerateContentConfigDict.get` is \"partially `Unknown`\" because `response_schema` includes `typing._UnionGenericAlias`,\n        # so without this we'd need `pyright: ignore[reportUnknownMemberType]` on every line and wouldn't get type checking anyway.\n        generation_config = cast(dict[str, Any], generation_config)\n\n        config = CountTokensConfigDict(\n            http_options=generation_config.get('http_options'),\n        )\n        if self._provider.name != 'google-gla':\n            # The fields are not supported by the Gemini API per https://github.com/googleapis/python-genai/blob/7e4ec284dc6e521949626f3ed54028163ef9121d/google/genai/models.py#L1195-L1214\n            config.update(  # pragma: lax no cover\n                system_instruction=generation_config.get('system_instruction'),\n                tools=cast(list[ToolDict], generation_config.get('tools')),\n                # Annoyingly, GenerationConfigDict has fewer fields than GenerateContentConfigDict, and no extra fields are allowed.\n                generation_config=GenerationConfigDict(\n                    temperature=generation_config.get('temperature'),\n                    top_p=generation_config.get('top_p'),\n                    max_output_tokens=generation_config.get('max_output_tokens'),\n                    stop_sequences=generation_config.get('stop_sequences'),\n                    presence_penalty=generation_config.get('presence_penalty'),\n                    frequency_penalty=generation_config.get('frequency_penalty'),\n                    seed=generation_config.get('seed'),\n                    thinking_config=generation_config.get('thinking_config'),\n                    media_resolution=generation_config.get('media_resolution'),\n                    response_mime_type=generation_config.get('response_mime_type'),\n                    response_json_schema=generation_config.get('response_json_schema'),\n                ),\n            )\n\n        response = await self.client.aio.models.count_tokens(\n            model=self._model_name,\n            contents=contents,\n            config=config,\n        )\n        if response.total_tokens is None:\n            raise UnexpectedModelBehavior(  # pragma: no cover\n                'Total tokens missing from Gemini response', str(response)\n            )\n        return usage.RequestUsage(\n            input_tokens=response.total_tokens,\n        )\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        model_settings = cast(GoogleModelSettings, model_settings or {})\n        response = await self._generate_content(messages, True, model_settings, model_request_parameters)\n        yield await self._process_streamed_response(response, model_request_parameters)  # type: ignore\n\n    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ToolDict] | None:\n        tools: list[ToolDict] = [\n            ToolDict(function_declarations=[_function_declaration_from_tool(t)])\n            for t in model_request_parameters.tool_defs.values()\n        ]\n\n        if model_request_parameters.builtin_tools:\n            if model_request_parameters.function_tools:\n                raise UserError('Google does not support function tools and built-in tools at the same time.')\n\n            for tool in model_request_parameters.builtin_tools:\n                if isinstance(tool, WebSearchTool):\n                    tools.append(ToolDict(google_search=GoogleSearchDict()))\n                elif isinstance(tool, WebFetchTool):\n                    tools.append(ToolDict(url_context=UrlContextDict()))\n                elif isinstance(tool, CodeExecutionTool):\n                    tools.append(ToolDict(code_execution=ToolCodeExecutionDict()))\n                elif isinstance(tool, ImageGenerationTool):  # pragma: no branch\n                    if not self.profile.supports_image_output:\n                        raise UserError(\n                            \"`ImageGenerationTool` is not supported by this model. Use a model with 'image' in the name instead.\"\n                        )\n                else:  # pragma: no cover\n                    raise UserError(\n                        f'`{tool.__class__.__name__}` is not supported by `GoogleModel`. If it should be, please file an issue.'\n                    )\n        return tools or None\n\n    def _get_tool_config(\n        self, model_request_parameters: ModelRequestParameters, tools: list[ToolDict] | None\n    ) -> ToolConfigDict | None:\n        if not model_request_parameters.allow_text_output and tools:\n            names: list[str] = []\n            for tool in tools:\n                for function_declaration in tool.get('function_declarations') or []:\n                    if name := function_declaration.get('name'):  # pragma: no branch\n                        names.append(name)\n            return _tool_config(names)\n        else:\n            return None\n\n    @overload\n    async def _generate_content(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[False],\n        model_settings: GoogleModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> GenerateContentResponse: ...\n\n    @overload\n    async def _generate_content(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[True],\n        model_settings: GoogleModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> Awaitable[AsyncIterator[GenerateContentResponse]]: ...\n\n    async def _generate_content(\n        self,\n        messages: list[ModelMessage],\n        stream: bool,\n        model_settings: GoogleModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> GenerateContentResponse | Awaitable[AsyncIterator[GenerateContentResponse]]:\n        contents, config = await self._build_content_and_config(messages, model_settings, model_request_parameters)\n        func = self.client.aio.models.generate_content_stream if stream else self.client.aio.models.generate_content\n        try:\n            return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n        except errors.APIError as e:\n            if (status_code := e.code) >= 400:\n                raise ModelHTTPError(\n                    status_code=status_code,\n                    model_name=self._model_name,\n                    body=cast(Any, e.details),  # pyright: ignore[reportUnknownMemberType]\n                ) from e\n            raise ModelAPIError(model_name=self._model_name, message=str(e)) from e\n\n    async def _build_content_and_config(\n        self,\n        messages: list[ModelMessage],\n        model_settings: GoogleModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> tuple[list[ContentUnionDict], GenerateContentConfigDict]:\n        tools = self._get_tools(model_request_parameters)\n        if model_request_parameters.function_tools and not self.profile.supports_tools:\n            raise UserError('Tools are not supported by this model.')\n\n        response_mime_type = None\n        response_schema = None\n        if model_request_parameters.output_mode == 'native':\n            if model_request_parameters.function_tools:\n                raise UserError(\n                    'Google does not support `NativeOutput` and function tools at the same time. Use `output_type=ToolOutput(...)` instead.'\n                )\n            response_mime_type = 'application/json'\n            output_object = model_request_parameters.output_object\n            assert output_object is not None\n            response_schema = self._map_response_schema(output_object)\n        elif model_request_parameters.output_mode == 'prompted' and not tools:\n            if not self.profile.supports_json_object_output:\n                raise UserError('JSON output is not supported by this model.')\n            response_mime_type = 'application/json'\n\n        tool_config = self._get_tool_config(model_request_parameters, tools)\n        system_instruction, contents = await self._map_messages(messages, model_request_parameters)\n\n        modalities = [Modality.TEXT.value]\n        if self.profile.supports_image_output:\n            modalities.append(Modality.IMAGE.value)\n\n        http_options: HttpOptionsDict = {\n            'headers': {'Content-Type': 'application/json', 'User-Agent': get_user_agent()}\n        }\n        if timeout := model_settings.get('timeout'):\n            if isinstance(timeout, int | float):\n                http_options['timeout'] = int(1000 * timeout)\n            else:\n                raise UserError('Google does not support setting ModelSettings.timeout to a httpx.Timeout')\n\n        config = GenerateContentConfigDict(\n            http_options=http_options,\n            system_instruction=system_instruction,\n            temperature=model_settings.get('temperature'),\n            top_p=model_settings.get('top_p'),\n            max_output_tokens=model_settings.get('max_tokens'),\n            stop_sequences=model_settings.get('stop_sequences'),\n            presence_penalty=model_settings.get('presence_penalty'),\n            frequency_penalty=model_settings.get('frequency_penalty'),\n            seed=model_settings.get('seed'),\n            safety_settings=model_settings.get('google_safety_settings'),\n            thinking_config=model_settings.get('google_thinking_config'),\n            labels=model_settings.get('google_labels'),\n            media_resolution=model_settings.get('google_video_resolution'),\n            cached_content=model_settings.get('google_cached_content'),\n            tools=cast(ToolListUnionDict, tools),\n            tool_config=tool_config,\n            response_mime_type=response_mime_type,\n            response_json_schema=response_schema,\n            response_modalities=modalities,\n        )\n        return contents, config\n\n    def _process_response(self, response: GenerateContentResponse) -> ModelResponse:\n        if not response.candidates:\n            raise UnexpectedModelBehavior('Expected at least one candidate in Gemini response')  # pragma: no cover\n\n        candidate = response.candidates[0]\n\n        vendor_id = response.response_id\n        vendor_details: dict[str, Any] | None = None\n        finish_reason: FinishReason | None = None\n        raw_finish_reason = candidate.finish_reason\n        if raw_finish_reason:  # pragma: no branch\n            vendor_details = {'finish_reason': raw_finish_reason.value}\n            finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n\n        if candidate.content is None or candidate.content.parts is None:\n            if finish_reason == 'content_filter' and raw_finish_reason:\n                raise UnexpectedModelBehavior(\n                    f'Content filter {raw_finish_reason.value!r} triggered', response.model_dump_json()\n                )\n            parts = []  # pragma: no cover\n        else:\n            parts = candidate.content.parts or []\n\n        usage = _metadata_as_usage(response, provider=self._provider.name, provider_url=self._provider.base_url)\n        return _process_response_from_parts(\n            parts,\n            candidate.grounding_metadata,\n            response.model_version or self._model_name,\n            self._provider.name,\n            usage,\n            vendor_id=vendor_id,\n            vendor_details=vendor_details,\n            finish_reason=finish_reason,\n            url_context_metadata=candidate.url_context_metadata,\n        )\n\n    async def _process_streamed_response(\n        self, response: AsyncIterator[GenerateContentResponse], model_request_parameters: ModelRequestParameters\n    ) -> StreamedResponse:\n        \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\n        peekable_response = _utils.PeekableAsyncStream(response)\n        first_chunk = await peekable_response.peek()\n        if isinstance(first_chunk, _utils.Unset):\n            raise UnexpectedModelBehavior('Streamed response ended without content or tool calls')  # pragma: no cover\n\n        return GeminiStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _model_name=first_chunk.model_version or self._model_name,\n            _response=peekable_response,\n            _timestamp=first_chunk.create_time or _utils.now_utc(),\n            _provider_name=self._provider.name,\n            _provider_url=self._provider.base_url,\n        )\n\n    async def _map_messages(\n        self, messages: list[ModelMessage], model_request_parameters: ModelRequestParameters\n    ) -> tuple[ContentDict | None, list[ContentUnionDict]]:\n        contents: list[ContentUnionDict] = []\n        system_parts: list[PartDict] = []\n\n        for m in messages:\n            if isinstance(m, ModelRequest):\n                message_parts: list[PartDict] = []\n\n                for part in m.parts:\n                    if isinstance(part, SystemPromptPart):\n                        system_parts.append({'text': part.content})\n                    elif isinstance(part, UserPromptPart):\n                        message_parts.extend(await self._map_user_prompt(part))\n                    elif isinstance(part, ToolReturnPart):\n                        message_parts.append(\n                            {\n                                'function_response': {\n                                    'name': part.tool_name,\n                                    'response': part.model_response_object(),\n                                    'id': part.tool_call_id,\n                                }\n                            }\n                        )\n                    elif isinstance(part, RetryPromptPart):\n                        if part.tool_name is None:\n                            message_parts.append({'text': part.model_response()})\n                        else:\n                            message_parts.append(\n                                {\n                                    'function_response': {\n                                        'name': part.tool_name,\n                                        'response': {'error': part.model_response()},\n                                        'id': part.tool_call_id,\n                                    }\n                                }\n                            )\n                    else:\n                        assert_never(part)\n\n                if message_parts:\n                    contents.append({'role': 'user', 'parts': message_parts})\n            elif isinstance(m, ModelResponse):\n                maybe_content = _content_model_response(m, self.system)\n                if maybe_content:\n                    contents.append(maybe_content)\n            else:\n                assert_never(m)\n\n        # Google GenAI requires at least one part in the message.\n        if not contents:\n            contents = [{'role': 'user', 'parts': [{'text': ''}]}]\n\n        if instructions := self._get_instructions(messages, model_request_parameters):\n            system_parts.insert(0, {'text': instructions})\n        system_instruction = ContentDict(role='user', parts=system_parts) if system_parts else None\n\n        return system_instruction, contents\n\n    async def _map_user_prompt(self, part: UserPromptPart) -> list[PartDict]:\n        if isinstance(part.content, str):\n            return [{'text': part.content}]\n        else:\n            content: list[PartDict] = []\n            for item in part.content:\n                if isinstance(item, str):\n                    content.append({'text': item})\n                elif isinstance(item, BinaryContent):\n                    inline_data_dict: BlobDict = {'data': item.data, 'mime_type': item.media_type}\n                    part_dict: PartDict = {'inline_data': inline_data_dict}\n                    if item.vendor_metadata:\n                        part_dict['video_metadata'] = cast(VideoMetadataDict, item.vendor_metadata)\n                    content.append(part_dict)\n                elif isinstance(item, VideoUrl) and item.is_youtube:\n                    file_data_dict: FileDataDict = {'file_uri': item.url, 'mime_type': item.media_type}\n                    part_dict: PartDict = {'file_data': file_data_dict}\n                    if item.vendor_metadata:  # pragma: no branch\n                        part_dict['video_metadata'] = cast(VideoMetadataDict, item.vendor_metadata)\n                    content.append(part_dict)\n                elif isinstance(item, FileUrl):\n                    if item.force_download or (\n                        # google-gla does not support passing file urls directly, except for youtube videos\n                        # (see above) and files uploaded to the file API (which cannot be downloaded anyway)\n                        self.system == 'google-gla'\n                        and not item.url.startswith(r'https://generativelanguage.googleapis.com/v1beta/files')\n                    ):\n                        downloaded_item = await download_item(item, data_format='bytes')\n                        inline_data: BlobDict = {\n                            'data': downloaded_item['data'],\n                            'mime_type': downloaded_item['data_type'],\n                        }\n                        content.append({'inline_data': inline_data})\n                    else:\n                        file_data_dict: FileDataDict = {'file_uri': item.url, 'mime_type': item.media_type}\n                        content.append({'file_data': file_data_dict})  # pragma: lax no cover\n                elif isinstance(item, CachePoint):\n                    # Google Gemini doesn't support prompt caching via CachePoint\n                    pass\n                else:\n                    assert_never(item)\n        return content\n\n    def _map_response_schema(self, o: OutputObjectDefinition) -> dict[str, Any]:\n        response_schema = o.json_schema.copy()\n        if o.name:\n            response_schema['title'] = o.name\n        if o.description:\n            response_schema['description'] = o.description\n\n        return response_schema\n\n```\n\n#### __init__\n\n```python\n__init__(\n    model_name: GoogleModelName,\n    *,\n    provider: (\n        Literal[\"google-gla\", \"google-vertex\", \"gateway\"]\n        | Provider[Client]\n    ) = \"google-gla\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize a Gemini model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model_name` | `GoogleModelName` | The name of the model to use. | *required* | | `provider` | `Literal['google-gla', 'google-vertex', 'gateway'] | Provider[Client]` | The provider to use for authentication and API access. Can be either the string 'google-gla' or 'google-vertex' or an instance of Provider[google.genai.AsyncClient]. Defaults to 'google-gla'. | `'google-gla'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` | | `settings` | `ModelSettings | None` | The model settings to use. Defaults to None. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/google.py`\n\n```python\ndef __init__(\n    self,\n    model_name: GoogleModelName,\n    *,\n    provider: Literal['google-gla', 'google-vertex', 'gateway'] | Provider[Client] = 'google-gla',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize a Gemini model.\n\n    Args:\n        model_name: The name of the model to use.\n        provider: The provider to use for authentication and API access. Can be either the string\n            'google-gla' or 'google-vertex' or an instance of `Provider[google.genai.AsyncClient]`.\n            Defaults to 'google-gla'.\n        profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n        settings: The model settings to use. Defaults to None.\n    \"\"\"\n    self._model_name = model_name\n\n    if isinstance(provider, str):\n        provider = infer_provider('gateway/google-vertex' if provider == 'gateway' else provider)\n    self._provider = provider\n    self.client = provider.client\n\n    super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n```\n\n#### model_name\n\n```python\nmodel_name: GoogleModelName\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe model provider.\n\n### GeminiStreamedResponse\n\nBases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for the Gemini model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/google.py`\n\n```python\n@dataclass\nclass GeminiStreamedResponse(StreamedResponse):\n    \"\"\"Implementation of `StreamedResponse` for the Gemini model.\"\"\"\n\n    _model_name: GoogleModelName\n    _response: AsyncIterator[GenerateContentResponse]\n    _timestamp: datetime\n    _provider_name: str\n    _provider_url: str\n\n    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:  # noqa: C901\n        code_execution_tool_call_id: str | None = None\n        async for chunk in self._response:\n            self._usage = _metadata_as_usage(chunk, self._provider_name, self._provider_url)\n\n            if not chunk.candidates:\n                continue  # pragma: no cover\n\n            candidate = chunk.candidates[0]\n\n            if chunk.response_id:  # pragma: no branch\n                self.provider_response_id = chunk.response_id\n\n            raw_finish_reason = candidate.finish_reason\n            if raw_finish_reason:\n                self.provider_details = {'finish_reason': raw_finish_reason.value}\n                self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n\n            # Google streams the grounding metadata (including the web search queries and results)\n            # _after_ the text that was generated using it, so it would show up out of order in the stream,\n            # and cause issues with the logic that doesn't consider text ahead of built-in tool calls as output.\n            # If that gets fixed (or we have a workaround), we can uncomment this:\n            # web_search_call, web_search_return = _map_grounding_metadata(\n            #     candidate.grounding_metadata, self.provider_name\n            # )\n            # if web_search_call and web_search_return:\n            #     yield self._parts_manager.handle_part(vendor_part_id=uuid4(), part=web_search_call)\n            #     yield self._parts_manager.handle_part(\n            #         vendor_part_id=uuid4(), part=web_search_return\n            #     )\n\n            # URL context metadata (for WebFetchTool) is streamed in the first chunk, before the text,\n            # so we can safely yield it here\n            web_fetch_call, web_fetch_return = _map_url_context_metadata(\n                candidate.url_context_metadata, self.provider_name\n            )\n            if web_fetch_call and web_fetch_return:\n                yield self._parts_manager.handle_part(vendor_part_id=uuid4(), part=web_fetch_call)\n                yield self._parts_manager.handle_part(vendor_part_id=uuid4(), part=web_fetch_return)\n\n            if candidate.content is None or candidate.content.parts is None:\n                if self.finish_reason == 'content_filter' and raw_finish_reason:  # pragma: no cover\n                    raise UnexpectedModelBehavior(\n                        f'Content filter {raw_finish_reason.value!r} triggered', chunk.model_dump_json()\n                    )\n                else:  # pragma: no cover\n                    continue\n\n            parts = candidate.content.parts\n            if not parts:\n                continue  # pragma: no cover\n\n            for part in parts:\n                provider_details: dict[str, Any] | None = None\n                if part.thought_signature:\n                    # Per https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#thought-signatures:\n                    # - Always send the thought_signature back to the model inside its original Part.\n                    # - Don't merge a Part containing a signature with one that does not. This breaks the positional context of the thought.\n                    # - Don't combine two Parts that both contain signatures, as the signature strings cannot be merged.\n                    thought_signature = base64.b64encode(part.thought_signature).decode('utf-8')\n                    provider_details = {'thought_signature': thought_signature}\n\n                if part.text is not None:\n                    if len(part.text) == 0 and not provider_details:\n                        continue\n                    if part.thought:\n                        yield self._parts_manager.handle_thinking_delta(\n                            vendor_part_id=None, content=part.text, provider_details=provider_details\n                        )\n                    else:\n                        maybe_event = self._parts_manager.handle_text_delta(\n                            vendor_part_id=None, content=part.text, provider_details=provider_details\n                        )\n                        if maybe_event is not None:  # pragma: no branch\n                            yield maybe_event\n                elif part.function_call:\n                    maybe_event = self._parts_manager.handle_tool_call_delta(\n                        vendor_part_id=uuid4(),\n                        tool_name=part.function_call.name,\n                        args=part.function_call.args,\n                        tool_call_id=part.function_call.id,\n                        provider_details=provider_details,\n                    )\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n                elif part.inline_data is not None:\n                    if part.thought:  # pragma: no cover\n                        # Per https://ai.google.dev/gemini-api/docs/image-generation#thinking-process:\n                        # > The model generates up to two interim images to test composition and logic. The last image within Thinking is also the final rendered image.\n                        # We currently don't expose these image thoughts as they can't be represented with `ThinkingPart`\n                        continue\n                    data = part.inline_data.data\n                    mime_type = part.inline_data.mime_type\n                    assert data and mime_type, 'Inline data must have data and mime type'\n                    content = BinaryContent(data=data, media_type=mime_type)\n                    yield self._parts_manager.handle_part(\n                        vendor_part_id=uuid4(),\n                        part=FilePart(content=BinaryContent.narrow_type(content), provider_details=provider_details),\n                    )\n                elif part.executable_code is not None:\n                    code_execution_tool_call_id = _utils.generate_tool_call_id()\n                    part = _map_executable_code(part.executable_code, self.provider_name, code_execution_tool_call_id)\n                    part.provider_details = provider_details\n                    yield self._parts_manager.handle_part(vendor_part_id=uuid4(), part=part)\n                elif part.code_execution_result is not None:\n                    assert code_execution_tool_call_id is not None\n                    part = _map_code_execution_result(\n                        part.code_execution_result, self.provider_name, code_execution_tool_call_id\n                    )\n                    part.provider_details = provider_details\n                    yield self._parts_manager.handle_part(vendor_part_id=uuid4(), part=part)\n                else:\n                    assert part.function_response is not None, f'Unexpected part: {part}'  # pragma: no cover\n\n    @property\n    def model_name(self) -> GoogleModelName:\n        \"\"\"Get the model name of the response.\"\"\"\n        return self._model_name\n\n    @property\n    def provider_name(self) -> str:\n        \"\"\"Get the provider name.\"\"\"\n        return self._provider_name\n\n    @property\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._timestamp\n\n```\n\n#### model_name\n\n```python\nmodel_name: GoogleModelName\n\n```\n\nGet the model name of the response.\n\n#### provider_name\n\n```python\nprovider_name: str\n\n```\n\nGet the provider name.\n\n#### timestamp\n\n```python\ntimestamp: datetime\n\n```\n\nGet the timestamp of the response.",
  "content_length": 37318
}