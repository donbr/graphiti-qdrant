{
  "title": "Question Graph",
  "source_url": null,
  "content": "Example of a graph for asking and evaluating questions.\n\nDemonstrates:\n\n- [`pydantic_graph`](../../graph/)\n\n## Running the Example\n\nWith [dependencies installed and environment variables set](../setup/#usage), run:\n\n```bash\npython -m pydantic_ai_examples.question_graph\n\n```\n\n```bash\nuv run -m pydantic_ai_examples.question_graph\n\n```\n\n## Example Code\n\n[Learn about Gateway](../../gateway) [question_graph.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/question_graph.py)\n\n```python\n\"\"\"Example of a graph for asking and evaluating questions.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.question_graph\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\n\nimport logfire\nfrom groq import BaseModel\n\nfrom pydantic_ai import Agent, ModelMessage, format_as_xml\nfrom pydantic_graph import (\n    BaseNode,\n    End,\n    Graph,\n    GraphRunContext,\n)\nfrom pydantic_graph.persistence.file import FileStatePersistence\n\n### 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\nask_agent = Agent('gateway/openai:gpt-5', output_type=str)\n\n\n@dataclass\nclass QuestionState:\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Ask(BaseNode[QuestionState]):\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.all_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n\n@dataclass\nclass Answer(BaseNode[QuestionState]):\n    question: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\n\nclass EvaluationOutput(BaseModel, use_attribute_docstrings=True):\n    correct: bool\n    \"\"\"Whether the answer is correct.\"\"\"\n    comment: str\n    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\n\nevaluate_agent = Agent(\n    'gateway/openai:gpt-5',\n    output_type=EvaluationOutput,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n\n@dataclass\nclass Evaluate(BaseNode[QuestionState, None, str]):\n    answer: str\n\n    async def run(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> End[str] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.all_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n\n@dataclass\nclass Reprimand(BaseNode[QuestionState]):\n    comment: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\n\nquestion_graph = Graph(\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n\n\nasync def run_as_continuous():\n    state = QuestionState()\n    node = Ask()\n    end = await question_graph.run(node, state=state)\n    print('END:', end.output)\n\n\nasync def run_as_cli(answer: str | None):\n    persistence = FileStatePersistence(Path('question_graph.json'))\n    persistence.set_graph_types(question_graph)\n\n    if snapshot := await persistence.load_next():\n        state = snapshot.state\n        assert answer is not None, (\n            'answer required, usage \"uv run -m pydantic_ai_examples.question_graph cli <answer>\"'\n        )\n        node = Evaluate(answer)\n    else:\n        state = QuestionState()\n        node = Ask()\n    # debug(state, node)\n\n    async with question_graph.iter(node, state=state, persistence=persistence) as run:\n        while True:\n            node = await run.next()\n            if isinstance(node, End):\n                print('END:', node.data)\n                history = await persistence.load_all()\n                print('history:', '\\n'.join(str(e.node) for e in history), sep='\\n')\n                print('Finished!')\n                break\n            elif isinstance(node, Answer):\n                print(node.question)\n                break\n            # otherwise just continue\n\n\nif __name__ == '__main__':\n    import asyncio\n    import sys\n\n    try:\n        sub_command = sys.argv[1]\n        assert sub_command in ('continuous', 'cli', 'mermaid')\n    except (IndexError, AssertionError):\n        print(\n            'Usage:\\n'\n            '  uv run -m pydantic_ai_examples.question_graph mermaid\\n'\n            'or:\\n'\n            '  uv run -m pydantic_ai_examples.question_graph continuous\\n'\n            'or:\\n'\n            '  uv run -m pydantic_ai_examples.question_graph cli [answer]',\n            file=sys.stderr,\n        )\n        sys.exit(1)\n\n    if sub_command == 'mermaid':\n        print(question_graph.mermaid_code(start_node=Ask))\n    elif sub_command == 'continuous':\n        asyncio.run(run_as_continuous())\n    else:\n        a = sys.argv[2] if len(sys.argv) > 2 else None\n        asyncio.run(run_as_cli(a))\n\n```\n\n[question_graph.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/question_graph.py)\n\n```python\n\"\"\"Example of a graph for asking and evaluating questions.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.question_graph\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\n\nimport logfire\nfrom groq import BaseModel\n\nfrom pydantic_ai import Agent, ModelMessage, format_as_xml\nfrom pydantic_graph import (\n    BaseNode,\n    End,\n    Graph,\n    GraphRunContext,\n)\nfrom pydantic_graph.persistence.file import FileStatePersistence\n\n### 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\nask_agent = Agent('openai:gpt-5', output_type=str)\n\n\n@dataclass\nclass QuestionState:\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Ask(BaseNode[QuestionState]):\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.all_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n\n@dataclass\nclass Answer(BaseNode[QuestionState]):\n    question: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\n\nclass EvaluationOutput(BaseModel, use_attribute_docstrings=True):\n    correct: bool\n    \"\"\"Whether the answer is correct.\"\"\"\n    comment: str\n    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\n\nevaluate_agent = Agent(\n    'openai:gpt-5',\n    output_type=EvaluationOutput,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n\n@dataclass\nclass Evaluate(BaseNode[QuestionState, None, str]):\n    answer: str\n\n    async def run(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> End[str] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.all_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n\n@dataclass\nclass Reprimand(BaseNode[QuestionState]):\n    comment: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\n\nquestion_graph = Graph(\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n\n\nasync def run_as_continuous():\n    state = QuestionState()\n    node = Ask()\n    end = await question_graph.run(node, state=state)\n    print('END:', end.output)\n\n\nasync def run_as_cli(answer: str | None):\n    persistence = FileStatePersistence(Path('question_graph.json'))\n    persistence.set_graph_types(question_graph)\n\n    if snapshot := await persistence.load_next():\n        state = snapshot.state\n        assert answer is not None, (\n            'answer required, usage \"uv run -m pydantic_ai_examples.question_graph cli <answer>\"'\n        )\n        node = Evaluate(answer)\n    else:\n        state = QuestionState()\n        node = Ask()\n    # debug(state, node)\n\n    async with question_graph.iter(node, state=state, persistence=persistence) as run:\n        while True:\n            node = await run.next()\n            if isinstance(node, End):\n                print('END:', node.data)\n                history = await persistence.load_all()\n                print('history:', '\\n'.join(str(e.node) for e in history), sep='\\n')\n                print('Finished!')\n                break\n            elif isinstance(node, Answer):\n                print(node.question)\n                break\n            # otherwise just continue\n\n\nif __name__ == '__main__':\n    import asyncio\n    import sys\n\n    try:\n        sub_command = sys.argv[1]\n        assert sub_command in ('continuous', 'cli', 'mermaid')\n    except (IndexError, AssertionError):\n        print(\n            'Usage:\\n'\n            '  uv run -m pydantic_ai_examples.question_graph mermaid\\n'\n            'or:\\n'\n            '  uv run -m pydantic_ai_examples.question_graph continuous\\n'\n            'or:\\n'\n            '  uv run -m pydantic_ai_examples.question_graph cli [answer]',\n            file=sys.stderr,\n        )\n        sys.exit(1)\n\n    if sub_command == 'mermaid':\n        print(question_graph.mermaid_code(start_node=Ask))\n    elif sub_command == 'continuous':\n        asyncio.run(run_as_continuous())\n    else:\n        a = sys.argv[2] if len(sys.argv) > 2 else None\n        asyncio.run(run_as_cli(a))\n\n```\n\nThe mermaid diagram generated in this example looks like this:\n\n```\n---\ntitle: question_graph\n---\nstateDiagram-v2\n  [*] --> Ask\n  Ask --> Answer: ask the question\n  Answer --> Evaluate: answer the question\n  Evaluate --> Congratulate\n  Evaluate --> Castigate\n  Congratulate --> [*]: success\n  Castigate --> Ask: try again\n```",
  "content_length": 11153
}