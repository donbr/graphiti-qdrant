{
  "title": "pydantic_ai.models.instrumented",
  "source_url": null,
  "content": "### instrument_model\n\n```python\ninstrument_model(\n    model: Model, instrument: InstrumentationSettings | bool\n) -> Model\n\n```\n\nInstrument a model with OpenTelemetry/logfire.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`\n\n```python\ndef instrument_model(model: Model, instrument: InstrumentationSettings | bool) -> Model:\n    \"\"\"Instrument a model with OpenTelemetry/logfire.\"\"\"\n    if instrument and not isinstance(model, InstrumentedModel):\n        if instrument is True:\n            instrument = InstrumentationSettings()\n\n        model = InstrumentedModel(model, instrument)\n\n    return model\n\n```\n\n### InstrumentationSettings\n\nOptions for instrumenting models and agents with OpenTelemetry.\n\nUsed in:\n\n- `Agent(instrument=...)`\n- Agent.instrument_all()\n- InstrumentedModel\n\nSee the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`\n\n```python\n@dataclass(init=False)\nclass InstrumentationSettings:\n    \"\"\"Options for instrumenting models and agents with OpenTelemetry.\n\n    Used in:\n\n    - `Agent(instrument=...)`\n    - [`Agent.instrument_all()`][pydantic_ai.agent.Agent.instrument_all]\n    - [`InstrumentedModel`][pydantic_ai.models.instrumented.InstrumentedModel]\n\n    See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.\n    \"\"\"\n\n    tracer: Tracer = field(repr=False)\n    event_logger: EventLogger = field(repr=False)\n    event_mode: Literal['attributes', 'logs'] = 'attributes'\n    include_binary_content: bool = True\n    include_content: bool = True\n    version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION\n\n    def __init__(\n        self,\n        *,\n        tracer_provider: TracerProvider | None = None,\n        meter_provider: MeterProvider | None = None,\n        include_binary_content: bool = True,\n        include_content: bool = True,\n        version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION,\n        event_mode: Literal['attributes', 'logs'] = 'attributes',\n        event_logger_provider: EventLoggerProvider | None = None,\n    ):\n        \"\"\"Create instrumentation options.\n\n        Args:\n            tracer_provider: The OpenTelemetry tracer provider to use.\n                If not provided, the global tracer provider is used.\n                Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.\n            meter_provider: The OpenTelemetry meter provider to use.\n                If not provided, the global meter provider is used.\n                Calling `logfire.configure()` sets the global meter provider, so most users don't need this.\n            include_binary_content: Whether to include binary content in the instrumentation events.\n            include_content: Whether to include prompts, completions, and tool call arguments and responses\n                in the instrumentation events.\n            version: Version of the data format. This is unrelated to the Pydantic AI package version.\n                Version 1 is based on the legacy event-based OpenTelemetry GenAI spec\n                    and will be removed in a future release.\n                    The parameters `event_mode` and `event_logger_provider` are only relevant for version 1.\n                Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes:\n                    - `gen_ai.system_instructions` for instructions passed to the agent.\n                    - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans.\n                    - `pydantic_ai.all_messages` on agent run spans.\n            event_mode: The mode for emitting events in version 1.\n                If `'attributes'`, events are attached to the span as attributes.\n                If `'logs'`, events are emitted as OpenTelemetry log-based events.\n            event_logger_provider: The OpenTelemetry event logger provider to use.\n                If not provided, the global event logger provider is used.\n                Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.\n                This is only used if `event_mode='logs'` and `version=1`.\n        \"\"\"\n        from pydantic_ai import __version__\n\n        tracer_provider = tracer_provider or get_tracer_provider()\n        meter_provider = meter_provider or get_meter_provider()\n        event_logger_provider = event_logger_provider or get_event_logger_provider()\n        scope_name = 'pydantic-ai'\n        self.tracer = tracer_provider.get_tracer(scope_name, __version__)\n        self.meter = meter_provider.get_meter(scope_name, __version__)\n        self.event_logger = event_logger_provider.get_event_logger(scope_name, __version__)\n        self.event_mode = event_mode\n        self.include_binary_content = include_binary_content\n        self.include_content = include_content\n\n        if event_mode == 'logs' and version != 1:\n            warnings.warn(\n                'event_mode is only relevant for version=1 which is deprecated and will be removed in a future release.',\n                stacklevel=2,\n            )\n            version = 1\n\n        self.version = version\n\n        # As specified in the OpenTelemetry GenAI metrics spec:\n        # https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage\n        tokens_histogram_kwargs = dict(\n            name='gen_ai.client.token.usage',\n            unit='{token}',\n            description='Measures number of input and output tokens used',\n        )\n        try:\n            self.tokens_histogram = self.meter.create_histogram(\n                **tokens_histogram_kwargs,\n                explicit_bucket_boundaries_advisory=TOKEN_HISTOGRAM_BOUNDARIES,\n            )\n        except TypeError:  # pragma: lax no cover\n            # Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory\n            self.tokens_histogram = self.meter.create_histogram(\n                **tokens_histogram_kwargs,  # pyright: ignore\n            )\n        self.cost_histogram = self.meter.create_histogram(\n            'operation.cost',\n            unit='{USD}',\n            description='Monetary cost',\n        )\n\n    def messages_to_otel_events(\n        self, messages: list[ModelMessage], parameters: ModelRequestParameters | None = None\n    ) -> list[Event]:\n        \"\"\"Convert a list of model messages to OpenTelemetry events.\n\n        Args:\n            messages: The messages to convert.\n            parameters: The model request parameters.\n\n        Returns:\n            A list of OpenTelemetry events.\n        \"\"\"\n        events: list[Event] = []\n        instructions = InstrumentedModel._get_instructions(messages, parameters)  # pyright: ignore [reportPrivateUsage]\n        if instructions is not None:\n            events.append(\n                Event(\n                    'gen_ai.system.message',\n                    body={**({'content': instructions} if self.include_content else {}), 'role': 'system'},\n                )\n            )\n\n        for message_index, message in enumerate(messages):\n            message_events: list[Event] = []\n            if isinstance(message, ModelRequest):\n                for part in message.parts:\n                    if hasattr(part, 'otel_event'):\n                        message_events.append(part.otel_event(self))\n            elif isinstance(message, ModelResponse):  # pragma: no branch\n                message_events = message.otel_events(self)\n            for event in message_events:\n                event.attributes = {\n                    'gen_ai.message.index': message_index,\n                    **(event.attributes or {}),\n                }\n            events.extend(message_events)\n\n        for event in events:\n            event.body = InstrumentedModel.serialize_any(event.body)\n        return events\n\n    def messages_to_otel_messages(self, messages: list[ModelMessage]) -> list[_otel_messages.ChatMessage]:\n        result: list[_otel_messages.ChatMessage] = []\n        for message in messages:\n            if isinstance(message, ModelRequest):\n                for is_system, group in itertools.groupby(message.parts, key=lambda p: isinstance(p, SystemPromptPart)):\n                    message_parts: list[_otel_messages.MessagePart] = []\n                    for part in group:\n                        if hasattr(part, 'otel_message_parts'):\n                            message_parts.extend(part.otel_message_parts(self))\n                    result.append(\n                        _otel_messages.ChatMessage(role='system' if is_system else 'user', parts=message_parts)\n                    )\n            elif isinstance(message, ModelResponse):  # pragma: no branch\n                otel_message = _otel_messages.OutputMessage(role='assistant', parts=message.otel_message_parts(self))\n                if message.finish_reason is not None:\n                    otel_message['finish_reason'] = message.finish_reason\n                result.append(otel_message)\n        return result\n\n    def handle_messages(\n        self,\n        input_messages: list[ModelMessage],\n        response: ModelResponse,\n        system: str,\n        span: Span,\n        parameters: ModelRequestParameters | None = None,\n    ):\n        if self.version == 1:\n            events = self.messages_to_otel_events(input_messages, parameters)\n            for event in self.messages_to_otel_events([response], parameters):\n                events.append(\n                    Event(\n                        'gen_ai.choice',\n                        body={\n                            'index': 0,\n                            'message': event.body,\n                        },\n                    )\n                )\n            for event in events:\n                event.attributes = {\n                    GEN_AI_SYSTEM_ATTRIBUTE: system,\n                    **(event.attributes or {}),\n                }\n            self._emit_events(span, events)\n        else:\n            output_messages = self.messages_to_otel_messages([response])\n            assert len(output_messages) == 1\n            output_message = output_messages[0]\n            instructions = InstrumentedModel._get_instructions(input_messages, parameters)  # pyright: ignore [reportPrivateUsage]\n            system_instructions_attributes = self.system_instructions_attributes(instructions)\n            attributes: dict[str, AttributeValue] = {\n                'gen_ai.input.messages': json.dumps(self.messages_to_otel_messages(input_messages)),\n                'gen_ai.output.messages': json.dumps([output_message]),\n                **system_instructions_attributes,\n                'logfire.json_schema': json.dumps(\n                    {\n                        'type': 'object',\n                        'properties': {\n                            'gen_ai.input.messages': {'type': 'array'},\n                            'gen_ai.output.messages': {'type': 'array'},\n                            **(\n                                {'gen_ai.system_instructions': {'type': 'array'}}\n                                if system_instructions_attributes\n                                else {}\n                            ),\n                            'model_request_parameters': {'type': 'object'},\n                        },\n                    }\n                ),\n            }\n            span.set_attributes(attributes)\n\n    def system_instructions_attributes(self, instructions: str | None) -> dict[str, str]:\n        if instructions and self.include_content:\n            return {\n                'gen_ai.system_instructions': json.dumps([_otel_messages.TextPart(type='text', content=instructions)]),\n            }\n        return {}\n\n    def _emit_events(self, span: Span, events: list[Event]) -> None:\n        if self.event_mode == 'logs':\n            for event in events:\n                self.event_logger.emit(event)\n        else:\n            attr_name = 'events'\n            span.set_attributes(\n                {\n                    attr_name: json.dumps([InstrumentedModel.event_to_dict(event) for event in events]),\n                    'logfire.json_schema': json.dumps(\n                        {\n                            'type': 'object',\n                            'properties': {\n                                attr_name: {'type': 'array'},\n                                'model_request_parameters': {'type': 'object'},\n                            },\n                        }\n                    ),\n                }\n            )\n\n    def record_metrics(\n        self,\n        response: ModelResponse,\n        price_calculation: PriceCalculation | None,\n        attributes: dict[str, AttributeValue],\n    ):\n        for typ in ['input', 'output']:\n            if not (tokens := getattr(response.usage, f'{typ}_tokens', 0)):  # pragma: no cover\n                continue\n            token_attributes = {**attributes, 'gen_ai.token.type': typ}\n            self.tokens_histogram.record(tokens, token_attributes)\n            if price_calculation:\n                cost = float(getattr(price_calculation, f'{typ}_price'))\n                self.cost_histogram.record(cost, token_attributes)\n\n```\n\n#### __init__\n\n```python\n__init__(\n    *,\n    tracer_provider: TracerProvider | None = None,\n    meter_provider: MeterProvider | None = None,\n    include_binary_content: bool = True,\n    include_content: bool = True,\n    version: Literal[\n        1, 2, 3\n    ] = DEFAULT_INSTRUMENTATION_VERSION,\n    event_mode: Literal[\n        \"attributes\", \"logs\"\n    ] = \"attributes\",\n    event_logger_provider: EventLoggerProvider | None = None\n)\n\n```\n\nCreate instrumentation options.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `tracer_provider` | `TracerProvider | None` | The OpenTelemetry tracer provider to use. If not provided, the global tracer provider is used. Calling logfire.configure() sets the global tracer provider, so most users don't need this. | `None` | | `meter_provider` | `MeterProvider | None` | The OpenTelemetry meter provider to use. If not provided, the global meter provider is used. Calling logfire.configure() sets the global meter provider, so most users don't need this. | `None` | | `include_binary_content` | `bool` | Whether to include binary content in the instrumentation events. | `True` | | `include_content` | `bool` | Whether to include prompts, completions, and tool call arguments and responses in the instrumentation events. | `True` | | `version` | `Literal[1, 2, 3]` | Version of the data format. This is unrelated to the Pydantic AI package version. Version 1 is based on the legacy event-based OpenTelemetry GenAI spec and will be removed in a future release. The parameters event_mode and event_logger_provider are only relevant for version 1. Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes: - gen_ai.system_instructions for instructions passed to the agent. - gen_ai.input.messages and gen_ai.output.messages on model request spans. - pydantic_ai.all_messages on agent run spans. | `DEFAULT_INSTRUMENTATION_VERSION` | | `event_mode` | `Literal['attributes', 'logs']` | The mode for emitting events in version 1. If 'attributes', events are attached to the span as attributes. If 'logs', events are emitted as OpenTelemetry log-based events. | `'attributes'` | | `event_logger_provider` | `EventLoggerProvider | None` | The OpenTelemetry event logger provider to use. If not provided, the global event logger provider is used. Calling logfire.configure() sets the global event logger provider, so most users don't need this. This is only used if event_mode='logs' and version=1. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    tracer_provider: TracerProvider | None = None,\n    meter_provider: MeterProvider | None = None,\n    include_binary_content: bool = True,\n    include_content: bool = True,\n    version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION,\n    event_mode: Literal['attributes', 'logs'] = 'attributes',\n    event_logger_provider: EventLoggerProvider | None = None,\n):\n    \"\"\"Create instrumentation options.\n\n    Args:\n        tracer_provider: The OpenTelemetry tracer provider to use.\n            If not provided, the global tracer provider is used.\n            Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.\n        meter_provider: The OpenTelemetry meter provider to use.\n            If not provided, the global meter provider is used.\n            Calling `logfire.configure()` sets the global meter provider, so most users don't need this.\n        include_binary_content: Whether to include binary content in the instrumentation events.\n        include_content: Whether to include prompts, completions, and tool call arguments and responses\n            in the instrumentation events.\n        version: Version of the data format. This is unrelated to the Pydantic AI package version.\n            Version 1 is based on the legacy event-based OpenTelemetry GenAI spec\n                and will be removed in a future release.\n                The parameters `event_mode` and `event_logger_provider` are only relevant for version 1.\n            Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes:\n                - `gen_ai.system_instructions` for instructions passed to the agent.\n                - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans.\n                - `pydantic_ai.all_messages` on agent run spans.\n        event_mode: The mode for emitting events in version 1.\n            If `'attributes'`, events are attached to the span as attributes.\n            If `'logs'`, events are emitted as OpenTelemetry log-based events.\n        event_logger_provider: The OpenTelemetry event logger provider to use.\n            If not provided, the global event logger provider is used.\n            Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.\n            This is only used if `event_mode='logs'` and `version=1`.\n    \"\"\"\n    from pydantic_ai import __version__\n\n    tracer_provider = tracer_provider or get_tracer_provider()\n    meter_provider = meter_provider or get_meter_provider()\n    event_logger_provider = event_logger_provider or get_event_logger_provider()\n    scope_name = 'pydantic-ai'\n    self.tracer = tracer_provider.get_tracer(scope_name, __version__)\n    self.meter = meter_provider.get_meter(scope_name, __version__)\n    self.event_logger = event_logger_provider.get_event_logger(scope_name, __version__)\n    self.event_mode = event_mode\n    self.include_binary_content = include_binary_content\n    self.include_content = include_content\n\n    if event_mode == 'logs' and version != 1:\n        warnings.warn(\n            'event_mode is only relevant for version=1 which is deprecated and will be removed in a future release.',\n            stacklevel=2,\n        )\n        version = 1\n\n    self.version = version\n\n    # As specified in the OpenTelemetry GenAI metrics spec:\n    # https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage\n    tokens_histogram_kwargs = dict(\n        name='gen_ai.client.token.usage',\n        unit='{token}',\n        description='Measures number of input and output tokens used',\n    )\n    try:\n        self.tokens_histogram = self.meter.create_histogram(\n            **tokens_histogram_kwargs,\n            explicit_bucket_boundaries_advisory=TOKEN_HISTOGRAM_BOUNDARIES,\n        )\n    except TypeError:  # pragma: lax no cover\n        # Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory\n        self.tokens_histogram = self.meter.create_histogram(\n            **tokens_histogram_kwargs,  # pyright: ignore\n        )\n    self.cost_histogram = self.meter.create_histogram(\n        'operation.cost',\n        unit='{USD}',\n        description='Monetary cost',\n    )\n\n```\n\n#### messages_to_otel_events\n\n```python\nmessages_to_otel_events(\n    messages: list[ModelMessage],\n    parameters: ModelRequestParameters | None = None,\n) -> list[Event]\n\n```\n\nConvert a list of model messages to OpenTelemetry events.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `messages` | `list[ModelMessage]` | The messages to convert. | *required* | | `parameters` | `ModelRequestParameters | None` | The model request parameters. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `list[Event]` | A list of OpenTelemetry events. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`\n\n```python\ndef messages_to_otel_events(\n    self, messages: list[ModelMessage], parameters: ModelRequestParameters | None = None\n) -> list[Event]:\n    \"\"\"Convert a list of model messages to OpenTelemetry events.\n\n    Args:\n        messages: The messages to convert.\n        parameters: The model request parameters.\n\n    Returns:\n        A list of OpenTelemetry events.\n    \"\"\"\n    events: list[Event] = []\n    instructions = InstrumentedModel._get_instructions(messages, parameters)  # pyright: ignore [reportPrivateUsage]\n    if instructions is not None:\n        events.append(\n            Event(\n                'gen_ai.system.message',\n                body={**({'content': instructions} if self.include_content else {}), 'role': 'system'},\n            )\n        )\n\n    for message_index, message in enumerate(messages):\n        message_events: list[Event] = []\n        if isinstance(message, ModelRequest):\n            for part in message.parts:\n                if hasattr(part, 'otel_event'):\n                    message_events.append(part.otel_event(self))\n        elif isinstance(message, ModelResponse):  # pragma: no branch\n            message_events = message.otel_events(self)\n        for event in message_events:\n            event.attributes = {\n                'gen_ai.message.index': message_index,\n                **(event.attributes or {}),\n            }\n        events.extend(message_events)\n\n    for event in events:\n        event.body = InstrumentedModel.serialize_any(event.body)\n    return events\n\n```\n\n### InstrumentedModel\n\nBases: `WrapperModel`\n\nModel which wraps another model so that requests are instrumented with OpenTelemetry.\n\nSee the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`\n\n```python\n@dataclass(init=False)\nclass InstrumentedModel(WrapperModel):\n    \"\"\"Model which wraps another model so that requests are instrumented with OpenTelemetry.\n\n    See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.\n    \"\"\"\n\n    instrumentation_settings: InstrumentationSettings\n    \"\"\"Instrumentation settings for this model.\"\"\"\n\n    def __init__(\n        self,\n        wrapped: Model | KnownModelName,\n        options: InstrumentationSettings | None = None,\n    ) -> None:\n        super().__init__(wrapped)\n        self.instrumentation_settings = options or InstrumentationSettings()\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        prepared_settings, prepared_parameters = self.wrapped.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        with self._instrument(messages, prepared_settings, prepared_parameters) as finish:\n            response = await self.wrapped.request(messages, model_settings, model_request_parameters)\n            finish(response, prepared_parameters)\n            return response\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        prepared_settings, prepared_parameters = self.wrapped.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        with self._instrument(messages, prepared_settings, prepared_parameters) as finish:\n            response_stream: StreamedResponse | None = None\n            try:\n                async with self.wrapped.request_stream(\n                    messages, model_settings, model_request_parameters, run_context\n                ) as response_stream:\n                    yield response_stream\n            finally:\n                if response_stream:  # pragma: no branch\n                    finish(response_stream.get(), prepared_parameters)\n\n    @contextmanager\n    def _instrument(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> Iterator[Callable[[ModelResponse, ModelRequestParameters], None]]:\n        operation = 'chat'\n        span_name = f'{operation} {self.model_name}'\n        # TODO Missing attributes:\n        #  - error.type: unclear if we should do something here or just always rely on span exceptions\n        #  - gen_ai.request.stop_sequences/top_k: model_settings doesn't include these\n        attributes: dict[str, AttributeValue] = {\n            'gen_ai.operation.name': operation,\n            **self.model_attributes(self.wrapped),\n            **self.model_request_parameters_attributes(model_request_parameters),\n            'logfire.json_schema': json.dumps(\n                {\n                    'type': 'object',\n                    'properties': {'model_request_parameters': {'type': 'object'}},\n                }\n            ),\n        }\n\n        if model_settings:\n            for key in MODEL_SETTING_ATTRIBUTES:\n                if isinstance(value := model_settings.get(key), float | int):\n                    attributes[f'gen_ai.request.{key}'] = value\n\n        record_metrics: Callable[[], None] | None = None\n        try:\n            with self.instrumentation_settings.tracer.start_as_current_span(span_name, attributes=attributes) as span:\n\n                def finish(response: ModelResponse, parameters: ModelRequestParameters):\n                    # FallbackModel updates these span attributes.\n                    attributes.update(getattr(span, 'attributes', {}))\n                    request_model = attributes[GEN_AI_REQUEST_MODEL_ATTRIBUTE]\n                    system = cast(str, attributes[GEN_AI_SYSTEM_ATTRIBUTE])\n\n                    response_model = response.model_name or request_model\n                    price_calculation = None\n\n                    def _record_metrics():\n                        metric_attributes = {\n                            GEN_AI_SYSTEM_ATTRIBUTE: system,\n                            'gen_ai.operation.name': operation,\n                            'gen_ai.request.model': request_model,\n                            'gen_ai.response.model': response_model,\n                        }\n                        self.instrumentation_settings.record_metrics(response, price_calculation, metric_attributes)\n\n                    nonlocal record_metrics\n                    record_metrics = _record_metrics\n\n                    if not span.is_recording():\n                        return\n\n                    self.instrumentation_settings.handle_messages(messages, response, system, span, parameters)\n\n                    attributes_to_set = {\n                        **response.usage.opentelemetry_attributes(),\n                        'gen_ai.response.model': response_model,\n                    }\n                    try:\n                        price_calculation = response.cost()\n                    except LookupError:\n                        # The cost of this provider/model is unknown, which is common.\n                        pass\n                    except Exception as e:\n                        warnings.warn(\n                            f'Failed to get cost from response: {type(e).__name__}: {e}', CostCalculationFailedWarning\n                        )\n                    else:\n                        attributes_to_set['operation.cost'] = float(price_calculation.total_price)\n\n                    if response.provider_response_id is not None:\n                        attributes_to_set['gen_ai.response.id'] = response.provider_response_id\n                    if response.finish_reason is not None:\n                        attributes_to_set['gen_ai.response.finish_reasons'] = [response.finish_reason]\n                    span.set_attributes(attributes_to_set)\n                    span.update_name(f'{operation} {request_model}')\n\n                yield finish\n        finally:\n            if record_metrics:\n                # We only want to record metrics after the span is finished,\n                # to prevent them from being redundantly recorded in the span itself by logfire.\n                record_metrics()\n\n    @staticmethod\n    def model_attributes(model: Model) -> dict[str, AttributeValue]:\n        attributes: dict[str, AttributeValue] = {\n            GEN_AI_SYSTEM_ATTRIBUTE: model.system,\n            GEN_AI_REQUEST_MODEL_ATTRIBUTE: model.model_name,\n        }\n        if base_url := model.base_url:\n            try:\n                parsed = urlparse(base_url)\n            except Exception:  # pragma: no cover\n                pass\n            else:\n                if parsed.hostname:  # pragma: no branch\n                    attributes['server.address'] = parsed.hostname\n                if parsed.port:  # pragma: no branch\n                    attributes['server.port'] = parsed.port\n\n        return attributes\n\n    @staticmethod\n    def model_request_parameters_attributes(\n        model_request_parameters: ModelRequestParameters,\n    ) -> dict[str, AttributeValue]:\n        return {'model_request_parameters': json.dumps(InstrumentedModel.serialize_any(model_request_parameters))}\n\n    @staticmethod\n    def event_to_dict(event: Event) -> dict[str, Any]:\n        if not event.body:\n            body = {}  # pragma: no cover\n        elif isinstance(event.body, Mapping):\n            body = event.body  # type: ignore\n        else:\n            body = {'body': event.body}\n        return {**body, **(event.attributes or {})}\n\n    @staticmethod\n    def serialize_any(value: Any) -> str:\n        try:\n            return ANY_ADAPTER.dump_python(value, mode='json')\n        except Exception:\n            try:\n                return str(value)\n            except Exception as e:\n                return f'Unable to serialize: {e}'\n\n```\n\n#### instrumentation_settings\n\n```python\ninstrumentation_settings: InstrumentationSettings = (\n    options or InstrumentationSettings()\n)\n\n```\n\nInstrumentation settings for this model.",
  "content_length": 30952
}