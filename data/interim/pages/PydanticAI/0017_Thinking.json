{
  "title": "Thinking",
  "source_url": null,
  "content": "Thinking (or reasoning) is the process by which a model works through a problem step-by-step before providing its final answer.\n\nThis capability is typically disabled by default and depends on the specific model being used. See the sections below for how to enable thinking for each provider.\n\n## OpenAI\n\nWhen using the OpenAIChatModel, text output inside `<think>` tags are converted to ThinkingPart objects. You can customize the tags using the thinking_tags field on the [model profile](../models/openai/#model-profile).\n\n### OpenAI Responses\n\nThe OpenAIResponsesModel can generate native thinking parts. To enable this functionality, you need to set the OpenAIResponsesModelSettings.openai_reasoning_effort and OpenAIResponsesModelSettings.openai_reasoning_summary [model settings](../agents/#model-run-settings).\n\nBy default, the unique IDs of reasoning, text, and function call parts from the message history are sent to the model, which can result in errors like `\"Item 'rs_123' of type 'reasoning' was provided without its required following item.\"` if the message history you're sending does not match exactly what was received from the Responses API in a previous response, for example if you're using a [history processor](../message-history/#processing-message-history). To disable this, you can disable the OpenAIResponsesModelSettings.openai_send_reasoning_ids [model setting](../agents/#model-run-settings).\n\nopenai_thinking_part.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('gpt-5')\nsettings = OpenAIResponsesModelSettings(\n    openai_reasoning_effort='low',\n    openai_reasoning_summary='detailed',\n)\nagent = Agent(model, model_settings=settings)\n...\n\n```\n\n## Anthropic\n\nTo enable thinking, use the AnthropicModelSettings.anthropic_thinking [model setting](../agents/#model-run-settings).\n\nanthropic_thinking_part.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel, AnthropicModelSettings\n\nmodel = AnthropicModel('claude-sonnet-4-0')\nsettings = AnthropicModelSettings(\n    anthropic_thinking={'type': 'enabled', 'budget_tokens': 1024},\n)\nagent = Agent(model, model_settings=settings)\n...\n\n```\n\n## Google\n\nTo enable thinking, use the GoogleModelSettings.google_thinking_config [model setting](../agents/#model-run-settings).\n\ngoogle_thinking_part.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel = GoogleModel('gemini-2.5-pro')\nsettings = GoogleModelSettings(google_thinking_config={'include_thoughts': True})\nagent = Agent(model, model_settings=settings)\n...\n\n```\n\n## Bedrock\n\nAlthough Bedrock Converse doesn't provide a unified API to enable thinking, you can still use BedrockModelSettings.bedrock_additional_model_requests_fields [model setting](../agents/#model-run-settings) to pass provider-specific configuration:\n\nbedrock_claude_thinking_part.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel, BedrockModelSettings\n\nmodel = BedrockConverseModel('us.anthropic.claude-sonnet-4-5-20250929-v1:0')\nmodel_settings = BedrockModelSettings(\n    bedrock_additional_model_requests_fields={\n        'thinking': {'type': 'enabled', 'budget_tokens': 1024}\n    }\n)\nagent = Agent(model=model, model_settings=model_settings)\n\n```\n\nbedrock_openai_thinking_part.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel, BedrockModelSettings\n\nmodel = BedrockConverseModel('openai.gpt-oss-120b-1:0')\nmodel_settings = BedrockModelSettings(\n    bedrock_additional_model_requests_fields={'reasoning_effort': 'low'}\n)\nagent = Agent(model=model, model_settings=model_settings)\n\n```\n\nbedrock_qwen_thinking_part.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel, BedrockModelSettings\n\nmodel = BedrockConverseModel('qwen.qwen3-32b-v1:0')\nmodel_settings = BedrockModelSettings(\n    bedrock_additional_model_requests_fields={'reasoning_config': 'high'}\n)\nagent = Agent(model=model, model_settings=model_settings)\n\n```\n\nReasoning is [always enabled](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-reasoning.html) for Deepseek model\n\nbedrock_deepseek_thinking_part.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\n\nmodel = BedrockConverseModel('us.deepseek.r1-v1:0')\nagent = Agent(model=model)\n\n```\n\n## Groq\n\nGroq supports different formats to receive thinking parts:\n\n- `\"raw\"`: The thinking part is included in the text content inside `<think>` tags, which are automatically converted to ThinkingPart objects.\n- `\"hidden\"`: The thinking part is not included in the text content.\n- `\"parsed\"`: The thinking part has its own structured part in the response which is converted into a ThinkingPart object.\n\nTo enable thinking, use the GroqModelSettings.groq_reasoning_format [model setting](../agents/#model-run-settings):\n\ngroq_thinking_part.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel, GroqModelSettings\n\nmodel = GroqModel('qwen-qwq-32b')\nsettings = GroqModelSettings(groq_reasoning_format='parsed')\nagent = Agent(model, model_settings=settings)\n...\n\n```\n\n## OpenRouter\n\nTo enable thinking, use the OpenRouterModelSettings.openrouter_reasoning [model setting](../agents/#model-run-settings).\n\nopenrouter_thinking_part.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openrouter import OpenRouterModel, OpenRouterModelSettings\n\nmodel = OpenRouterModel('openai/gpt-5')\nsettings = OpenRouterModelSettings(openrouter_reasoning={'effort': 'high'})\nagent = Agent(model, model_settings=settings)\n...\n\n```\n\n## Mistral\n\nThinking is supported by the `magistral` family of models. It does not need to be specifically enabled.\n\n## Cohere\n\nThinking is supported by the `command-a-reasoning-08-2025` model. It does not need to be specifically enabled.\n\n## Hugging Face\n\nText output inside `<think>` tags is automatically converted to ThinkingPart objects. You can customize the tags using the thinking_tags field on the [model profile](../models/openai/#model-profile).\n\n## Outlines\n\nSome local models run through Outlines include in their text output a thinking part delimited by tags. In that case, it will be handled by Pydantic AI that will separate the thinking part from the final answer without the need to specifically enable it. The thinking tags used by default are `\"<think>\"` and `\"</think>\"`. If your model uses different tags, you can specify them in the [model profile](../models/openai/#model-profile) using the thinking_tags field.\n\nOutlines currently does not support thinking along with structured output. If you provide an `output_type`, the model text output will not contain a thinking part with the associated tags, and you may experience degraded performance.",
  "content_length": 6992
}