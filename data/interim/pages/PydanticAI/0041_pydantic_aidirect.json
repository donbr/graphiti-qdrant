{
  "title": "`pydantic_ai.direct`",
  "source_url": null,
  "content": "Methods for making imperative requests to language models with minimal abstraction.\n\nThese methods allow you to make requests to LLMs where the only abstraction is input and output schema translation so you can use all models with the same API.\n\nThese methods are thin wrappers around Model implementations.\n\n### model_request\n\n```python\nmodel_request(\n    model: Model | KnownModelName | str,\n    messages: Sequence[ModelMessage],\n    *,\n    model_settings: ModelSettings | None = None,\n    model_request_parameters: (\n        ModelRequestParameters | None\n    ) = None,\n    instrument: InstrumentationSettings | bool | None = None\n) -> ModelResponse\n\n```\n\nMake a non-streamed request to a model.\n\nmodel_request_example.py\n\n```py\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request\n\n\nasync def main():\n    model_response = await model_request(\n        'anthropic:claude-haiku-4-5',\n        [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!\n    )\n    print(model_response)\n    '''\n    ModelResponse(\n        parts=[TextPart(content='The capital of France is Paris.')],\n        usage=RequestUsage(input_tokens=56, output_tokens=7),\n        model_name='claude-haiku-4-5',\n        timestamp=datetime.datetime(...),\n    )\n    '''\n\n```\n\n1. See ModelRequest.user_text_prompt for details.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model` | `Model | KnownModelName | str` | The model to make a request to. We allow str here since the actual list of allowed models changes frequently. | *required* | | `messages` | `Sequence[ModelMessage]` | Messages to send to the model | *required* | | `model_settings` | `ModelSettings | None` | optional model settings | `None` | | `model_request_parameters` | `ModelRequestParameters | None` | optional model request parameters | `None` | | `instrument` | `InstrumentationSettings | bool | None` | Whether to instrument the request with OpenTelemetry/Logfire, if None the value from logfire.instrument_pydantic_ai is used. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `ModelResponse` | The model response and token usage associated with the request. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n````python\nasync def model_request(\n    model: models.Model | models.KnownModelName | str,\n    messages: Sequence[messages.ModelMessage],\n    *,\n    model_settings: settings.ModelSettings | None = None,\n    model_request_parameters: models.ModelRequestParameters | None = None,\n    instrument: instrumented_models.InstrumentationSettings | bool | None = None,\n) -> messages.ModelResponse:\n    \"\"\"Make a non-streamed request to a model.\n\n    ```py title=\"model_request_example.py\"\n    from pydantic_ai import ModelRequest\n    from pydantic_ai.direct import model_request\n\n\n    async def main():\n        model_response = await model_request(\n            'anthropic:claude-haiku-4-5',\n            [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!\n        )\n        print(model_response)\n        '''\n        ModelResponse(\n            parts=[TextPart(content='The capital of France is Paris.')],\n            usage=RequestUsage(input_tokens=56, output_tokens=7),\n            model_name='claude-haiku-4-5',\n            timestamp=datetime.datetime(...),\n        )\n        '''\n    ```\n\n    1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.\n\n    Args:\n        model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.\n        messages: Messages to send to the model\n        model_settings: optional model settings\n        model_request_parameters: optional model request parameters\n        instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from\n            [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.\n\n    Returns:\n        The model response and token usage associated with the request.\n    \"\"\"\n    model_instance = _prepare_model(model, instrument)\n    return await model_instance.request(\n        list(messages),\n        model_settings,\n        model_request_parameters or models.ModelRequestParameters(),\n    )\n\n````\n\n### model_request_sync\n\n```python\nmodel_request_sync(\n    model: Model | KnownModelName | str,\n    messages: Sequence[ModelMessage],\n    *,\n    model_settings: ModelSettings | None = None,\n    model_request_parameters: (\n        ModelRequestParameters | None\n    ) = None,\n    instrument: InstrumentationSettings | bool | None = None\n) -> ModelResponse\n\n```\n\nMake a Synchronous, non-streamed request to a model.\n\nThis is a convenience method that wraps model_request with `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.\n\nmodel_request_sync_example.py\n\n```py\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_sync\n\nmodel_response = model_request_sync(\n    'anthropic:claude-haiku-4-5',\n    [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!\n)\nprint(model_response)\n'''\nModelResponse(\n    parts=[TextPart(content='The capital of France is Paris.')],\n    usage=RequestUsage(input_tokens=56, output_tokens=7),\n    model_name='claude-haiku-4-5',\n    timestamp=datetime.datetime(...),\n)\n'''\n\n```\n\n1. See ModelRequest.user_text_prompt for details.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model` | `Model | KnownModelName | str` | The model to make a request to. We allow str here since the actual list of allowed models changes frequently. | *required* | | `messages` | `Sequence[ModelMessage]` | Messages to send to the model | *required* | | `model_settings` | `ModelSettings | None` | optional model settings | `None` | | `model_request_parameters` | `ModelRequestParameters | None` | optional model request parameters | `None` | | `instrument` | `InstrumentationSettings | bool | None` | Whether to instrument the request with OpenTelemetry/Logfire, if None the value from logfire.instrument_pydantic_ai is used. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `ModelResponse` | The model response and token usage associated with the request. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n````python\ndef model_request_sync(\n    model: models.Model | models.KnownModelName | str,\n    messages: Sequence[messages.ModelMessage],\n    *,\n    model_settings: settings.ModelSettings | None = None,\n    model_request_parameters: models.ModelRequestParameters | None = None,\n    instrument: instrumented_models.InstrumentationSettings | bool | None = None,\n) -> messages.ModelResponse:\n    \"\"\"Make a Synchronous, non-streamed request to a model.\n\n    This is a convenience method that wraps [`model_request`][pydantic_ai.direct.model_request] with\n    `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.\n\n    ```py title=\"model_request_sync_example.py\"\n    from pydantic_ai import ModelRequest\n    from pydantic_ai.direct import model_request_sync\n\n    model_response = model_request_sync(\n        'anthropic:claude-haiku-4-5',\n        [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!\n    )\n    print(model_response)\n    '''\n    ModelResponse(\n        parts=[TextPart(content='The capital of France is Paris.')],\n        usage=RequestUsage(input_tokens=56, output_tokens=7),\n        model_name='claude-haiku-4-5',\n        timestamp=datetime.datetime(...),\n    )\n    '''\n    ```\n\n    1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.\n\n    Args:\n        model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.\n        messages: Messages to send to the model\n        model_settings: optional model settings\n        model_request_parameters: optional model request parameters\n        instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from\n            [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.\n\n    Returns:\n        The model response and token usage associated with the request.\n    \"\"\"\n    return _get_event_loop().run_until_complete(\n        model_request(\n            model,\n            list(messages),\n            model_settings=model_settings,\n            model_request_parameters=model_request_parameters,\n            instrument=instrument,\n        )\n    )\n\n````\n\n### model_request_stream\n\n```python\nmodel_request_stream(\n    model: Model | KnownModelName | str,\n    messages: Sequence[ModelMessage],\n    *,\n    model_settings: ModelSettings | None = None,\n    model_request_parameters: (\n        ModelRequestParameters | None\n    ) = None,\n    instrument: InstrumentationSettings | bool | None = None\n) -> AbstractAsyncContextManager[StreamedResponse]\n\n```\n\nMake a streamed async request to a model.\n\nmodel_request_stream_example.py\n\n```py\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_stream\n\n\nasync def main():\n    messages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]  # (1)!\n    async with model_request_stream('openai:gpt-4.1-mini', messages) as stream:\n        chunks = []\n        async for chunk in stream:\n            chunks.append(chunk)\n        print(chunks)\n        '''\n        [\n            PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),\n            FinalResultEvent(tool_name=None, tool_call_id=None),\n            PartDeltaEvent(\n                index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')\n            ),\n            PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),\n            PartEndEvent(\n                index=0,\n                part=TextPart(\n                    content='Albert Einstein was a German-born theoretical physicist.'\n                ),\n            ),\n        ]\n        '''\n\n```\n\n1. See ModelRequest.user_text_prompt for details.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model` | `Model | KnownModelName | str` | The model to make a request to. We allow str here since the actual list of allowed models changes frequently. | *required* | | `messages` | `Sequence[ModelMessage]` | Messages to send to the model | *required* | | `model_settings` | `ModelSettings | None` | optional model settings | `None` | | `model_request_parameters` | `ModelRequestParameters | None` | optional model request parameters | `None` | | `instrument` | `InstrumentationSettings | bool | None` | Whether to instrument the request with OpenTelemetry/Logfire, if None the value from logfire.instrument_pydantic_ai is used. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AbstractAsyncContextManager[StreamedResponse]` | A stream response async context manager. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n````python\ndef model_request_stream(\n    model: models.Model | models.KnownModelName | str,\n    messages: Sequence[messages.ModelMessage],\n    *,\n    model_settings: settings.ModelSettings | None = None,\n    model_request_parameters: models.ModelRequestParameters | None = None,\n    instrument: instrumented_models.InstrumentationSettings | bool | None = None,\n) -> AbstractAsyncContextManager[models.StreamedResponse]:\n    \"\"\"Make a streamed async request to a model.\n\n    ```py {title=\"model_request_stream_example.py\"}\n\n    from pydantic_ai import ModelRequest\n    from pydantic_ai.direct import model_request_stream\n\n\n    async def main():\n        messages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]  # (1)!\n        async with model_request_stream('openai:gpt-4.1-mini', messages) as stream:\n            chunks = []\n            async for chunk in stream:\n                chunks.append(chunk)\n            print(chunks)\n            '''\n            [\n                PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),\n                FinalResultEvent(tool_name=None, tool_call_id=None),\n                PartDeltaEvent(\n                    index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')\n                ),\n                PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),\n                PartEndEvent(\n                    index=0,\n                    part=TextPart(\n                        content='Albert Einstein was a German-born theoretical physicist.'\n                    ),\n                ),\n            ]\n            '''\n    ```\n\n    1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.\n\n    Args:\n        model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.\n        messages: Messages to send to the model\n        model_settings: optional model settings\n        model_request_parameters: optional model request parameters\n        instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from\n            [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.\n\n    Returns:\n        A [stream response][pydantic_ai.models.StreamedResponse] async context manager.\n    \"\"\"\n    model_instance = _prepare_model(model, instrument)\n    return model_instance.request_stream(\n        list(messages),\n        model_settings,\n        model_request_parameters or models.ModelRequestParameters(),\n    )\n\n````\n\n### model_request_stream_sync\n\n```python\nmodel_request_stream_sync(\n    model: Model | KnownModelName | str,\n    messages: Sequence[ModelMessage],\n    *,\n    model_settings: ModelSettings | None = None,\n    model_request_parameters: (\n        ModelRequestParameters | None\n    ) = None,\n    instrument: InstrumentationSettings | bool | None = None\n) -> StreamedResponseSync\n\n```\n\nMake a streamed synchronous request to a model.\n\nThis is the synchronous version of model_request_stream. It uses threading to run the asynchronous stream in the background while providing a synchronous iterator interface.\n\nmodel_request_stream_sync_example.py\n\n```py\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_stream_sync\n\nmessages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]\nwith model_request_stream_sync('openai:gpt-4.1-mini', messages) as stream:\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk)\n    print(chunks)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(\n            index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')\n        ),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),\n        PartEndEvent(\n            index=0,\n            part=TextPart(\n                content='Albert Einstein was a German-born theoretical physicist.'\n            ),\n        ),\n    ]\n    '''\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model` | `Model | KnownModelName | str` | The model to make a request to. We allow str here since the actual list of allowed models changes frequently. | *required* | | `messages` | `Sequence[ModelMessage]` | Messages to send to the model | *required* | | `model_settings` | `ModelSettings | None` | optional model settings | `None` | | `model_request_parameters` | `ModelRequestParameters | None` | optional model request parameters | `None` | | `instrument` | `InstrumentationSettings | bool | None` | Whether to instrument the request with OpenTelemetry/Logfire, if None the value from logfire.instrument_pydantic_ai is used. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `StreamedResponseSync` | A sync stream response context manager. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n````python\ndef model_request_stream_sync(\n    model: models.Model | models.KnownModelName | str,\n    messages: Sequence[messages.ModelMessage],\n    *,\n    model_settings: settings.ModelSettings | None = None,\n    model_request_parameters: models.ModelRequestParameters | None = None,\n    instrument: instrumented_models.InstrumentationSettings | bool | None = None,\n) -> StreamedResponseSync:\n    \"\"\"Make a streamed synchronous request to a model.\n\n    This is the synchronous version of [`model_request_stream`][pydantic_ai.direct.model_request_stream].\n    It uses threading to run the asynchronous stream in the background while providing a synchronous iterator interface.\n\n    ```py {title=\"model_request_stream_sync_example.py\"}\n\n    from pydantic_ai import ModelRequest\n    from pydantic_ai.direct import model_request_stream_sync\n\n    messages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]\n    with model_request_stream_sync('openai:gpt-4.1-mini', messages) as stream:\n        chunks = []\n        for chunk in stream:\n            chunks.append(chunk)\n        print(chunks)\n        '''\n        [\n            PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),\n            FinalResultEvent(tool_name=None, tool_call_id=None),\n            PartDeltaEvent(\n                index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')\n            ),\n            PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),\n            PartEndEvent(\n                index=0,\n                part=TextPart(\n                    content='Albert Einstein was a German-born theoretical physicist.'\n                ),\n            ),\n        ]\n        '''\n    ```\n\n    Args:\n        model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.\n        messages: Messages to send to the model\n        model_settings: optional model settings\n        model_request_parameters: optional model request parameters\n        instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from\n            [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.\n\n    Returns:\n        A [sync stream response][pydantic_ai.direct.StreamedResponseSync] context manager.\n    \"\"\"\n    async_stream_cm = model_request_stream(\n        model=model,\n        messages=list(messages),\n        model_settings=model_settings,\n        model_request_parameters=model_request_parameters,\n        instrument=instrument,\n    )\n\n    return StreamedResponseSync(async_stream_cm)\n\n````\n\n### StreamedResponseSync\n\nSynchronous wrapper to async streaming responses by running the async producer in a background thread and providing a synchronous iterator.\n\nThis class must be used as a context manager with the `with` statement.\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n```python\n@dataclass\nclass StreamedResponseSync:\n    \"\"\"Synchronous wrapper to async streaming responses by running the async producer in a background thread and providing a synchronous iterator.\n\n    This class must be used as a context manager with the `with` statement.\n    \"\"\"\n\n    _async_stream_cm: AbstractAsyncContextManager[StreamedResponse]\n    _queue: queue.Queue[messages.ModelResponseStreamEvent | Exception | None] = field(\n        default_factory=queue.Queue, init=False\n    )\n    _thread: threading.Thread | None = field(default=None, init=False)\n    _stream_response: StreamedResponse | None = field(default=None, init=False)\n    _exception: Exception | None = field(default=None, init=False)\n    _context_entered: bool = field(default=False, init=False)\n    _stream_ready: threading.Event = field(default_factory=threading.Event, init=False)\n\n    def __enter__(self) -> StreamedResponseSync:\n        self._context_entered = True\n        self._start_producer()\n        return self\n\n    def __exit__(\n        self,\n        _exc_type: type[BaseException] | None,\n        _exc_val: BaseException | None,\n        _exc_tb: TracebackType | None,\n    ) -> None:\n        self._cleanup()\n\n    def __iter__(self) -> Iterator[messages.ModelResponseStreamEvent]:\n        \"\"\"Stream the response as an iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\"\"\"\n        self._check_context_manager_usage()\n\n        while True:\n            item = self._queue.get()\n            if item is None:  # End of stream\n                break\n            elif isinstance(item, Exception):\n                raise item\n            else:\n                yield item\n\n    def __repr__(self) -> str:\n        if self._stream_response:\n            return repr(self._stream_response)\n        else:\n            return f'{self.__class__.__name__}(context_entered={self._context_entered})'\n\n    __str__ = __repr__\n\n    def _check_context_manager_usage(self) -> None:\n        if not self._context_entered:\n            raise RuntimeError(\n                'StreamedResponseSync must be used as a context manager. '\n                'Use: `with model_request_stream_sync(...) as stream:`'\n            )\n\n    def _ensure_stream_ready(self) -> StreamedResponse:\n        self._check_context_manager_usage()\n\n        if self._stream_response is None:\n            # Wait for the background thread to signal that the stream is ready\n            if not self._stream_ready.wait(timeout=STREAM_INITIALIZATION_TIMEOUT):\n                raise RuntimeError('Stream failed to initialize within timeout')\n\n            if self._stream_response is None:  # pragma: no cover\n                raise RuntimeError('Stream failed to initialize')\n\n        return self._stream_response\n\n    def _start_producer(self):\n        self._thread = threading.Thread(target=self._async_producer, daemon=True)\n        self._thread.start()\n\n    def _async_producer(self):\n        async def _consume_async_stream():\n            try:\n                async with self._async_stream_cm as stream:\n                    self._stream_response = stream\n                    # Signal that the stream is ready\n                    self._stream_ready.set()\n                    async for event in stream:\n                        self._queue.put(event)\n            except Exception as e:\n                # Signal ready even on error so waiting threads don't hang\n                self._stream_ready.set()\n                self._queue.put(e)\n            finally:\n                self._queue.put(None)  # Signal end\n\n        _get_event_loop().run_until_complete(_consume_async_stream())\n\n    def _cleanup(self):\n        if self._thread and self._thread.is_alive():\n            self._thread.join()\n\n    # TODO (v2): Drop in favor of `response` property\n    def get(self) -> messages.ModelResponse:\n        \"\"\"Build a ModelResponse from the data received from the stream so far.\"\"\"\n        return self._ensure_stream_ready().get()\n\n    @property\n    def response(self) -> messages.ModelResponse:\n        \"\"\"Get the current state of the response.\"\"\"\n        return self.get()\n\n    # TODO (v2): Make this a property\n    def usage(self) -> RequestUsage:\n        \"\"\"Get the usage of the response so far.\"\"\"\n        return self._ensure_stream_ready().usage()\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"Get the model name of the response.\"\"\"\n        return self._ensure_stream_ready().model_name\n\n    @property\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._ensure_stream_ready().timestamp\n\n```\n\n#### __iter__\n\n```python\n__iter__() -> Iterator[ModelResponseStreamEvent]\n\n```\n\nStream the response as an iterable of ModelResponseStreamEvents.\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n```python\ndef __iter__(self) -> Iterator[messages.ModelResponseStreamEvent]:\n    \"\"\"Stream the response as an iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\"\"\"\n    self._check_context_manager_usage()\n\n    while True:\n        item = self._queue.get()\n        if item is None:  # End of stream\n            break\n        elif isinstance(item, Exception):\n            raise item\n        else:\n            yield item\n\n```\n\n#### get\n\n```python\nget() -> ModelResponse\n\n```\n\nBuild a ModelResponse from the data received from the stream so far.\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n```python\ndef get(self) -> messages.ModelResponse:\n    \"\"\"Build a ModelResponse from the data received from the stream so far.\"\"\"\n    return self._ensure_stream_ready().get()\n\n```\n\n#### response\n\n```python\nresponse: ModelResponse\n\n```\n\nGet the current state of the response.\n\n#### usage\n\n```python\nusage() -> RequestUsage\n\n```\n\nGet the usage of the response so far.\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n```python\ndef usage(self) -> RequestUsage:\n    \"\"\"Get the usage of the response so far.\"\"\"\n    return self._ensure_stream_ready().usage()\n\n```\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nGet the model name of the response.\n\n#### timestamp\n\n```python\ntimestamp: datetime\n\n```\n\nGet the timestamp of the response.",
  "content_length": 25221
}