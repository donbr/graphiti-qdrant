{
  "title": "`pydantic_ai.durable_exec`",
  "source_url": null,
  "content": "### TemporalAgent\n\nBases: `WrapperAgent[AgentDepsT, OutputDataT]`\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`\n\n````python\nclass TemporalAgent(WrapperAgent[AgentDepsT, OutputDataT]):\n    def __init__(\n        self,\n        wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n        *,\n        name: str | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        activity_config: ActivityConfig | None = None,\n        model_activity_config: ActivityConfig | None = None,\n        toolset_activity_config: dict[str, ActivityConfig] | None = None,\n        tool_activity_config: dict[str, dict[str, ActivityConfig | Literal[False]]] | None = None,\n        run_context_type: type[TemporalRunContext[AgentDepsT]] = TemporalRunContext[AgentDepsT],\n        temporalize_toolset_func: Callable[\n            [\n                AbstractToolset[AgentDepsT],\n                str,\n                ActivityConfig,\n                dict[str, ActivityConfig | Literal[False]],\n                type[AgentDepsT],\n                type[TemporalRunContext[AgentDepsT]],\n            ],\n            AbstractToolset[AgentDepsT],\n        ] = temporalize_toolset,\n    ):\n        \"\"\"Wrap an agent to enable it to be used inside a Temporal workflow, by automatically offloading model requests, tool calls, and MCP server communication to Temporal activities.\n\n        After wrapping, the original agent can still be used as normal outside of the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.\n\n        Args:\n            wrapped: The agent to wrap.\n            name: Optional unique agent name to use in the Temporal activities' names. If not provided, the agent's `name` will be used.\n            event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.\n            activity_config: The base Temporal activity config to use for all activities. If no config is provided, a `start_to_close_timeout` of 60 seconds is used.\n            model_activity_config: The Temporal activity config to use for model request activities. This is merged with the base activity config.\n            toolset_activity_config: The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config.\n            tool_activity_config: The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name.\n                This is merged with the base and toolset-specific activity configs.\n                If a tool does not use IO, you can specify `False` to disable using an activity.\n                Note that the tool is required to be defined as an `async` function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities.\n            run_context_type: The `TemporalRunContext` subclass to use to serialize and deserialize the run context for use inside a Temporal activity.\n                By default, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `retry` and `run_step` attributes will be available.\n                To make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute.\n            temporalize_toolset_func: Optional function to use to prepare \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) for Temporal by wrapping them in a `TemporalWrapperToolset` that moves methods that require IO to Temporal activities.\n                If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Temporal.\n                The function takes the toolset, the activity name prefix, the toolset-specific activity config, the tool-specific activity configs and the run context type.\n        \"\"\"\n        super().__init__(wrapped)\n\n        self._name = name\n        self._event_stream_handler = event_stream_handler\n        self.run_context_type = run_context_type\n\n        # start_to_close_timeout is required\n        activity_config = activity_config or ActivityConfig(start_to_close_timeout=timedelta(seconds=60))\n\n        # `pydantic_ai.exceptions.UserError` and `pydantic.errors.PydanticUserError` are not retryable\n        retry_policy = activity_config.get('retry_policy') or RetryPolicy()\n        retry_policy.non_retryable_error_types = [\n            *(retry_policy.non_retryable_error_types or []),\n            UserError.__name__,\n            PydanticUserError.__name__,\n        ]\n        activity_config['retry_policy'] = retry_policy\n        self.activity_config = activity_config\n\n        model_activity_config = model_activity_config or {}\n        toolset_activity_config = toolset_activity_config or {}\n        tool_activity_config = tool_activity_config or {}\n\n        if self.name is None:\n            raise UserError(\n                \"An agent needs to have a unique `name` in order to be used with Temporal. The name will be used to identify the agent's activities within the workflow.\"\n            )\n\n        activity_name_prefix = f'agent__{self.name}'\n\n        activities: list[Callable[..., Any]] = []\n        if not isinstance(wrapped.model, Model):\n            raise UserError(\n                'An agent needs to have a `model` in order to be used with Temporal, it cannot be set at agent run time.'\n            )\n\n        async def event_stream_handler_activity(params: _EventStreamHandlerParams, deps: AgentDepsT) -> None:\n            # We can never get here without an `event_stream_handler`, as `TemporalAgent.run_stream` and `TemporalAgent.iter` raise an error saying to use `TemporalAgent.run` instead,\n            # and that only ends up calling `event_stream_handler` if it is set.\n            assert self.event_stream_handler is not None\n\n            run_context = self.run_context_type.deserialize_run_context(params.serialized_run_context, deps=deps)\n\n            async def streamed_response():\n                yield params.event\n\n            await self.event_stream_handler(run_context, streamed_response())\n\n        # Set type hint explicitly so that Temporal can take care of serialization and deserialization\n        event_stream_handler_activity.__annotations__['deps'] = self.deps_type\n\n        self.event_stream_handler_activity = activity.defn(name=f'{activity_name_prefix}__event_stream_handler')(\n            event_stream_handler_activity\n        )\n        activities.append(self.event_stream_handler_activity)\n\n        temporal_model = TemporalModel(\n            wrapped.model,\n            activity_name_prefix=activity_name_prefix,\n            activity_config=activity_config | model_activity_config,\n            deps_type=self.deps_type,\n            run_context_type=self.run_context_type,\n            event_stream_handler=self.event_stream_handler,\n        )\n        activities.extend(temporal_model.temporal_activities)\n\n        def temporalize_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:\n            id = toolset.id\n            if id is None:\n                raise UserError(\n                    \"Toolsets that are 'leaves' (i.e. those that implement their own tool listing and calling) need to have a unique `id` in order to be used with Temporal. The ID will be used to identify the toolset's activities within the workflow.\"\n                )\n\n            toolset = temporalize_toolset_func(\n                toolset,\n                activity_name_prefix,\n                activity_config | toolset_activity_config.get(id, {}),\n                tool_activity_config.get(id, {}),\n                self.deps_type,\n                self.run_context_type,\n            )\n            if isinstance(toolset, TemporalWrapperToolset):\n                activities.extend(toolset.temporal_activities)\n            return toolset\n\n        temporal_toolsets = [toolset.visit_and_replace(temporalize_toolset) for toolset in wrapped.toolsets]\n\n        self._model = temporal_model\n        self._toolsets = temporal_toolsets\n        self._temporal_activities = activities\n\n        self._temporal_overrides_active: ContextVar[bool] = ContextVar('_temporal_overrides_active', default=False)\n\n    @property\n    def name(self) -> str | None:\n        return self._name or super().name\n\n    @name.setter\n    def name(self, value: str | None) -> None:  # pragma: no cover\n        raise UserError(\n            'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'\n        )\n\n    @property\n    def model(self) -> Model:\n        return self._model\n\n    @property\n    def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:\n        handler = self._event_stream_handler or super().event_stream_handler\n        if handler is None:\n            return None\n        elif workflow.in_workflow():\n            return self._call_event_stream_handler_activity\n        else:\n            return handler\n\n    async def _call_event_stream_handler_activity(\n        self, ctx: RunContext[AgentDepsT], stream: AsyncIterable[_messages.AgentStreamEvent]\n    ) -> None:\n        serialized_run_context = self.run_context_type.serialize_run_context(ctx)\n        async for event in stream:\n            await workflow.execute_activity(\n                activity=self.event_stream_handler_activity,\n                args=[\n                    _EventStreamHandlerParams(\n                        event=event,\n                        serialized_run_context=serialized_run_context,\n                    ),\n                    ctx.deps,\n                ],\n                **self.activity_config,\n            )\n\n    @property\n    def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:\n        with self._temporal_overrides():\n            return super().toolsets\n\n    @property\n    def temporal_activities(self) -> list[Callable[..., Any]]:\n        return self._temporal_activities\n\n    @contextmanager\n    def _temporal_overrides(self) -> Iterator[None]:\n        # We reset tools here as the temporalized function toolset is already in self._toolsets.\n        with super().override(model=self._model, toolsets=self._toolsets, tools=[]):\n            token = self._temporal_overrides_active.set(True)\n            try:\n                yield\n            except PydanticSerializationError as e:\n                raise UserError(\n                    \"The `deps` object failed to be serialized. Temporal requires all objects that are passed to activities to be serializable using Pydantic's `TypeAdapter`.\"\n                ) from e\n            finally:\n                self._temporal_overrides_active.reset(token)\n\n    @overload\n    async def run(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[OutputDataT]: ...\n\n    @overload\n    async def run(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[RunOutputDataT]: ...\n\n    async def run(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AgentRunResult[Any]:\n        \"\"\"Run the agent with a user prompt in async mode.\n\n        This method builds an internal agent graph (using system prompts, tools and result schemas) and then\n        runs the graph to completion. The result of the run is returned.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            agent_run = await agent.run('What is the capital of France?')\n            print(agent_run.output)\n            #> The capital of France is Paris.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            event_stream_handler: Optional event stream handler to use for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if workflow.in_workflow() and event_stream_handler is not None:\n            raise UserError(\n                'Event stream handler cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'\n            )\n\n        with self._temporal_overrides():\n            return await super().run(\n                user_prompt,\n                output_type=output_type,\n                message_history=message_history,\n                deferred_tool_results=deferred_tool_results,\n                model=model,\n                instructions=instructions,\n                deps=deps,\n                model_settings=model_settings,\n                usage_limits=usage_limits,\n                usage=usage,\n                infer_name=infer_name,\n                toolsets=toolsets,\n                builtin_tools=builtin_tools,\n                event_stream_handler=event_stream_handler or self.event_stream_handler,\n                **_deprecated_kwargs,\n            )\n\n    @overload\n    def run_sync(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[OutputDataT]: ...\n\n    @overload\n    def run_sync(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[RunOutputDataT]: ...\n\n    def run_sync(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AgentRunResult[Any]:\n        \"\"\"Synchronously run the agent with a user prompt.\n\n        This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\n        You therefore can't use this method inside async code or if there's an active event loop.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        result_sync = agent.run_sync('What is the capital of Italy?')\n        print(result_sync.output)\n        #> The capital of Italy is Rome.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            event_stream_handler: Optional event stream handler to use for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if workflow.in_workflow():\n            raise UserError(\n                '`agent.run_sync()` cannot be used inside a Temporal workflow. Use `await agent.run()` instead.'\n            )\n\n        return super().run_sync(\n            user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n            event_stream_handler=event_stream_handler,\n            **_deprecated_kwargs,\n        )\n\n    @overload\n    def run_stream(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, OutputDataT]]: ...\n\n    @overload\n    def run_stream(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, RunOutputDataT]]: ...\n\n    @asynccontextmanager\n    async def run_stream(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:\n        \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            async with agent.run_stream('What is the capital of the UK?') as response:\n                print(await response.get_output())\n                #> The capital of the UK is London.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n            event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if workflow.in_workflow():\n            raise UserError(\n                '`agent.run_stream()` cannot be used inside a Temporal workflow. '\n                'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n            )\n\n        async with super().run_stream(\n            user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            event_stream_handler=event_stream_handler,\n            builtin_tools=builtin_tools,\n            **_deprecated_kwargs,\n        ) as result:\n            yield result\n\n    @overload\n    def run_stream_events(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[OutputDataT]]: ...\n\n    @overload\n    def run_stream_events(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]]: ...\n\n    def run_stream_events(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:\n        \"\"\"Run the agent with a user prompt in async mode and stream events from the run.\n\n        This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\n        uses the `event_stream_handler` kwarg to get a stream of events from the run.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            events: list[AgentStreamEvent | AgentRunResultEvent] = []\n            async for event in agent.run_stream_events('What is the capital of France?'):\n                events.append(event)\n            print(events)\n            '''\n            [\n                PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n                FinalResultEvent(tool_name=None, tool_call_id=None),\n                PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n                PartEndEvent(\n                    index=0, part=TextPart(content='The capital of France is Paris. ')\n                ),\n                AgentRunResultEvent(\n                    result=AgentRunResult(output='The capital of France is Paris. ')\n                ),\n            ]\n            '''\n        ```\n\n        Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\n        except that `event_stream_handler` is now allowed.\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n            run result.\n        \"\"\"\n        if workflow.in_workflow():\n            raise UserError(\n                '`agent.run_stream_events()` cannot be used inside a Temporal workflow. '\n                'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n            )\n\n        return super().run_stream_events(\n            user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n        )\n\n    @overload\n    def iter(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, OutputDataT]]: ...\n\n    @overload\n    def iter(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, RunOutputDataT]]: ...\n\n    @asynccontextmanager\n    async def iter(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:\n        \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\n        This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n        `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\n        executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\n        stream of events coming from the execution of tools.\n\n        The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\n        and the final result of the run once it has completed.\n\n        For more details, see the documentation of `AgentRun`.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            nodes = []\n            async with agent.iter('What is the capital of France?') as agent_run:\n                async for node in agent_run:\n                    nodes.append(node)\n            print(nodes)\n            '''\n            [\n                UserPromptNode(\n                    user_prompt='What is the capital of France?',\n                    instructions_functions=[],\n                    system_prompts=(),\n                    system_prompt_functions=[],\n                    system_prompt_dynamic_functions={},\n                ),\n                ModelRequestNode(\n                    request=ModelRequest(\n                        parts=[\n                            UserPromptPart(\n                                content='What is the capital of France?',\n                                timestamp=datetime.datetime(...),\n                            )\n                        ],\n                        run_id='...',\n                    )\n                ),\n                CallToolsNode(\n                    model_response=ModelResponse(\n                        parts=[TextPart(content='The capital of France is Paris.')],\n                        usage=RequestUsage(input_tokens=56, output_tokens=7),\n                        model_name='gpt-4o',\n                        timestamp=datetime.datetime(...),\n                        run_id='...',\n                    )\n                ),\n                End(data=FinalResult(output='The capital of France is Paris.')),\n            ]\n            '''\n            print(agent_run.result.output)\n            #> The capital of France is Paris.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if workflow.in_workflow():\n            if not self._temporal_overrides_active.get():\n                raise UserError(\n                    '`agent.iter()` cannot be used inside a Temporal workflow. '\n                    'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n                )\n\n            if model is not None:\n                raise UserError(\n                    'Model cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'\n                )\n            if toolsets is not None:\n                raise UserError(\n                    'Toolsets cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'\n                )\n\n        async with super().iter(\n            user_prompt=user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n            **_deprecated_kwargs,\n        ) as run:\n            yield run\n\n    @contextmanager\n    def override(\n        self,\n        *,\n        name: str | _utils.Unset = _utils.UNSET,\n        deps: AgentDepsT | _utils.Unset = _utils.UNSET,\n        model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,\n        tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,\n        instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,\n    ) -> Iterator[None]:\n        \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\n        This is particularly useful when testing.\n        You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\n        Args:\n            name: The name to use instead of the name passed to the agent constructor and agent run.\n            deps: The dependencies to use instead of the dependencies passed to the agent run.\n            model: The model to use instead of the model passed to the agent run.\n            toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n            tools: The tools to use instead of the tools registered with the agent.\n            instructions: The instructions to use instead of the instructions registered with the agent.\n        \"\"\"\n        if workflow.in_workflow():\n            if _utils.is_set(model):\n                raise UserError(\n                    'Model cannot be contextually overridden inside a Temporal workflow, it must be set at agent creation time.'\n                )\n            if _utils.is_set(toolsets):\n                raise UserError(\n                    'Toolsets cannot be contextually overridden inside a Temporal workflow, they must be set at agent creation time.'\n                )\n            if _utils.is_set(tools):\n                raise UserError(\n                    'Tools cannot be contextually overridden inside a Temporal workflow, they must be set at agent creation time.'\n                )\n\n        with super().override(\n            name=name,\n            deps=deps,\n            model=model,\n            toolsets=toolsets,\n            tools=tools,\n            instructions=instructions,\n        ):\n            yield\n\n````\n\n#### __init__\n\n```python\n__init__(\n    wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    name: str | None = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    activity_config: ActivityConfig | None = None,\n    model_activity_config: ActivityConfig | None = None,\n    toolset_activity_config: (\n        dict[str, ActivityConfig] | None\n    ) = None,\n    tool_activity_config: (\n        dict[\n            str, dict[str, ActivityConfig | Literal[False]]\n        ]\n        | None\n    ) = None,\n    run_context_type: type[\n        TemporalRunContext[AgentDepsT]\n    ] = TemporalRunContext[AgentDepsT],\n    temporalize_toolset_func: Callable[\n        [\n            AbstractToolset[AgentDepsT],\n            str,\n            ActivityConfig,\n            dict[str, ActivityConfig | Literal[False]],\n            type[AgentDepsT],\n            type[TemporalRunContext[AgentDepsT]],\n        ],\n        AbstractToolset[AgentDepsT],\n    ] = temporalize_toolset\n)\n\n```\n\nWrap an agent to enable it to be used inside a Temporal workflow, by automatically offloading model requests, tool calls, and MCP server communication to Temporal activities.\n\nAfter wrapping, the original agent can still be used as normal outside of the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `wrapped` | `AbstractAgent[AgentDepsT, OutputDataT]` | The agent to wrap. | *required* | | `name` | `str | None` | Optional unique agent name to use in the Temporal activities' names. If not provided, the agent's name will be used. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use instead of the one set on the wrapped agent. | `None` | | `activity_config` | `ActivityConfig | None` | The base Temporal activity config to use for all activities. If no config is provided, a start_to_close_timeout of 60 seconds is used. | `None` | | `model_activity_config` | `ActivityConfig | None` | The Temporal activity config to use for model request activities. This is merged with the base activity config. | `None` | | `toolset_activity_config` | `dict[str, ActivityConfig] | None` | The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config. | `None` | | `tool_activity_config` | `dict[str, dict[str, ActivityConfig | Literal[False]]] | None` | The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name. This is merged with the base and toolset-specific activity configs. If a tool does not use IO, you can specify False to disable using an activity. Note that the tool is required to be defined as an async function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities. | `None` | | `run_context_type` | `type[TemporalRunContext[AgentDepsT]]` | The TemporalRunContext subclass to use to serialize and deserialize the run context for use inside a Temporal activity. By default, only the deps, retries, tool_call_id, tool_name, retry and run_step attributes will be available. To make another attribute available, create a TemporalRunContext subclass with a custom serialize_run_context class method that returns a dictionary that includes the attribute. | `TemporalRunContext[AgentDepsT]` | | `temporalize_toolset_func` | `Callable[[AbstractToolset[AgentDepsT], str, ActivityConfig, dict[str, ActivityConfig | Literal[False]], type[AgentDepsT], type[TemporalRunContext[AgentDepsT]]], AbstractToolset[AgentDepsT]]` | Optional function to use to prepare \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) for Temporal by wrapping them in a TemporalWrapperToolset that moves methods that require IO to Temporal activities. If not provided, only FunctionToolset and MCPServer will be prepared for Temporal. The function takes the toolset, the activity name prefix, the toolset-specific activity config, the tool-specific activity configs and the run context type. | `temporalize_toolset` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`\n\n```python\ndef __init__(\n    self,\n    wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    name: str | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    activity_config: ActivityConfig | None = None,\n    model_activity_config: ActivityConfig | None = None,\n    toolset_activity_config: dict[str, ActivityConfig] | None = None,\n    tool_activity_config: dict[str, dict[str, ActivityConfig | Literal[False]]] | None = None,\n    run_context_type: type[TemporalRunContext[AgentDepsT]] = TemporalRunContext[AgentDepsT],\n    temporalize_toolset_func: Callable[\n        [\n            AbstractToolset[AgentDepsT],\n            str,\n            ActivityConfig,\n            dict[str, ActivityConfig | Literal[False]],\n            type[AgentDepsT],\n            type[TemporalRunContext[AgentDepsT]],\n        ],\n        AbstractToolset[AgentDepsT],\n    ] = temporalize_toolset,\n):\n    \"\"\"Wrap an agent to enable it to be used inside a Temporal workflow, by automatically offloading model requests, tool calls, and MCP server communication to Temporal activities.\n\n    After wrapping, the original agent can still be used as normal outside of the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.\n\n    Args:\n        wrapped: The agent to wrap.\n        name: Optional unique agent name to use in the Temporal activities' names. If not provided, the agent's `name` will be used.\n        event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.\n        activity_config: The base Temporal activity config to use for all activities. If no config is provided, a `start_to_close_timeout` of 60 seconds is used.\n        model_activity_config: The Temporal activity config to use for model request activities. This is merged with the base activity config.\n        toolset_activity_config: The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config.\n        tool_activity_config: The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name.\n            This is merged with the base and toolset-specific activity configs.\n            If a tool does not use IO, you can specify `False` to disable using an activity.\n            Note that the tool is required to be defined as an `async` function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities.\n        run_context_type: The `TemporalRunContext` subclass to use to serialize and deserialize the run context for use inside a Temporal activity.\n            By default, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `retry` and `run_step` attributes will be available.\n            To make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute.\n        temporalize_toolset_func: Optional function to use to prepare \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) for Temporal by wrapping them in a `TemporalWrapperToolset` that moves methods that require IO to Temporal activities.\n            If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Temporal.\n            The function takes the toolset, the activity name prefix, the toolset-specific activity config, the tool-specific activity configs and the run context type.\n    \"\"\"\n    super().__init__(wrapped)\n\n    self._name = name\n    self._event_stream_handler = event_stream_handler\n    self.run_context_type = run_context_type\n\n    # start_to_close_timeout is required\n    activity_config = activity_config or ActivityConfig(start_to_close_timeout=timedelta(seconds=60))\n\n    # `pydantic_ai.exceptions.UserError` and `pydantic.errors.PydanticUserError` are not retryable\n    retry_policy = activity_config.get('retry_policy') or RetryPolicy()\n    retry_policy.non_retryable_error_types = [\n        *(retry_policy.non_retryable_error_types or []),\n        UserError.__name__,\n        PydanticUserError.__name__,\n    ]\n    activity_config['retry_policy'] = retry_policy\n    self.activity_config = activity_config\n\n    model_activity_config = model_activity_config or {}\n    toolset_activity_config = toolset_activity_config or {}\n    tool_activity_config = tool_activity_config or {}\n\n    if self.name is None:\n        raise UserError(\n            \"An agent needs to have a unique `name` in order to be used with Temporal. The name will be used to identify the agent's activities within the workflow.\"\n        )\n\n    activity_name_prefix = f'agent__{self.name}'\n\n    activities: list[Callable[..., Any]] = []\n    if not isinstance(wrapped.model, Model):\n        raise UserError(\n            'An agent needs to have a `model` in order to be used with Temporal, it cannot be set at agent run time.'\n        )\n\n    async def event_stream_handler_activity(params: _EventStreamHandlerParams, deps: AgentDepsT) -> None:\n        # We can never get here without an `event_stream_handler`, as `TemporalAgent.run_stream` and `TemporalAgent.iter` raise an error saying to use `TemporalAgent.run` instead,\n        # and that only ends up calling `event_stream_handler` if it is set.\n        assert self.event_stream_handler is not None\n\n        run_context = self.run_context_type.deserialize_run_context(params.serialized_run_context, deps=deps)\n\n        async def streamed_response():\n            yield params.event\n\n        await self.event_stream_handler(run_context, streamed_response())\n\n    # Set type hint explicitly so that Temporal can take care of serialization and deserialization\n    event_stream_handler_activity.__annotations__['deps'] = self.deps_type\n\n    self.event_stream_handler_activity = activity.defn(name=f'{activity_name_prefix}__event_stream_handler')(\n        event_stream_handler_activity\n    )\n    activities.append(self.event_stream_handler_activity)\n\n    temporal_model = TemporalModel(\n        wrapped.model,\n        activity_name_prefix=activity_name_prefix,\n        activity_config=activity_config | model_activity_config,\n        deps_type=self.deps_type,\n        run_context_type=self.run_context_type,\n        event_stream_handler=self.event_stream_handler,\n    )\n    activities.extend(temporal_model.temporal_activities)\n\n    def temporalize_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:\n        id = toolset.id\n        if id is None:\n            raise UserError(\n                \"Toolsets that are 'leaves' (i.e. those that implement their own tool listing and calling) need to have a unique `id` in order to be used with Temporal. The ID will be used to identify the toolset's activities within the workflow.\"\n            )\n\n        toolset = temporalize_toolset_func(\n            toolset,\n            activity_name_prefix,\n            activity_config | toolset_activity_config.get(id, {}),\n            tool_activity_config.get(id, {}),\n            self.deps_type,\n            self.run_context_type,\n        )\n        if isinstance(toolset, TemporalWrapperToolset):\n            activities.extend(toolset.temporal_activities)\n        return toolset\n\n    temporal_toolsets = [toolset.visit_and_replace(temporalize_toolset) for toolset in wrapped.toolsets]\n\n    self._model = temporal_model\n    self._toolsets = temporal_toolsets\n    self._temporal_activities = activities\n\n    self._temporal_overrides_active: ContextVar[bool] = ContextVar('_temporal_overrides_active', default=False)\n\n```\n\n#### run\n\n```python\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\n```\n\n```python\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\n```\n\n```python\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n\n```\n\nRun the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`\n\n````python\nasync def run(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AgentRunResult[Any]:\n    \"\"\"Run the agent with a user prompt in async mode.\n\n    This method builds an internal agent graph (using system prompts, tools and result schemas) and then\n    runs the graph to completion. The result of the run is returned.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        agent_run = await agent.run('What is the capital of France?')\n        print(agent_run.output)\n        #> The capital of France is Paris.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        event_stream_handler: Optional event stream handler to use for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if workflow.in_workflow() and event_stream_handler is not None:\n        raise UserError(\n            'Event stream handler cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'\n        )\n\n    with self._temporal_overrides():\n        return await super().run(\n            user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n            event_stream_handler=event_stream_handler or self.event_stream_handler,\n            **_deprecated_kwargs,\n        )\n\n````\n\n#### run_sync\n\n```python\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\n```\n\n```python\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\n```\n\n```python\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n\n```\n\nSynchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps self.run with `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`\n\n````python\ndef run_sync(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AgentRunResult[Any]:\n    \"\"\"Synchronously run the agent with a user prompt.\n\n    This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\n    You therefore can't use this method inside async code or if there's an active event loop.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    result_sync = agent.run_sync('What is the capital of Italy?')\n    print(result_sync.output)\n    #> The capital of Italy is Rome.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        event_stream_handler: Optional event stream handler to use for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if workflow.in_workflow():\n        raise UserError(\n            '`agent.run_sync()` cannot be used inside a Temporal workflow. Use `await agent.run()` instead.'\n        )\n\n    return super().run_sync(\n        user_prompt,\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        instructions=instructions,\n        deps=deps,\n        model_settings=model_settings,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        builtin_tools=builtin_tools,\n        event_stream_handler=event_stream_handler,\n        **_deprecated_kwargs,\n    )\n\n````\n\n#### run_stream\n\n```python\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, OutputDataT]\n]\n\n```\n\n```python\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, RunOutputDataT]\n]\n\n```\n\n```python\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]\n\n```\n\nRun the agent with a user prompt in async mode, returning a streamed response.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[StreamedRunResult[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`\n\n````python\n@asynccontextmanager\nasync def run_stream(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:\n    \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        async with agent.run_stream('What is the capital of the UK?') as response:\n            print(await response.get_output())\n            #> The capital of the UK is London.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n        event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if workflow.in_workflow():\n        raise UserError(\n            '`agent.run_stream()` cannot be used inside a Temporal workflow. '\n            'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n        )\n\n    async with super().run_stream(\n        user_prompt,\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        instructions=instructions,\n        deps=deps,\n        model_settings=model_settings,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        event_stream_handler=event_stream_handler,\n        builtin_tools=builtin_tools,\n        **_deprecated_kwargs,\n    ) as result:\n        yield result\n\n````\n\n#### run_stream_events\n\n```python\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[OutputDataT]\n]\n\n```\n\n```python\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]\n]\n\n```\n\n```python\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[Any]\n]\n\n```\n\nRun the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps self.run and uses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of France is Paris. ')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n\n```\n\nArguments are the same as for self.run, except that `event_stream_handler` is now allowed.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | An async iterable of stream events AgentStreamEvent and finally a AgentRunResultEvent with the final | | `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | run result. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`\n\n````python\ndef run_stream_events(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:\n    \"\"\"Run the agent with a user prompt in async mode and stream events from the run.\n\n    This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\n    uses the `event_stream_handler` kwarg to get a stream of events from the run.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        events: list[AgentStreamEvent | AgentRunResultEvent] = []\n        async for event in agent.run_stream_events('What is the capital of France?'):\n            events.append(event)\n        print(events)\n        '''\n        [\n            PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n            FinalResultEvent(tool_name=None, tool_call_id=None),\n            PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n            PartEndEvent(\n                index=0, part=TextPart(content='The capital of France is Paris. ')\n            ),\n            AgentRunResultEvent(\n                result=AgentRunResult(output='The capital of France is Paris. ')\n            ),\n        ]\n        '''\n    ```\n\n    Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\n    except that `event_stream_handler` is now allowed.\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n        run result.\n    \"\"\"\n    if workflow.in_workflow():\n        raise UserError(\n            '`agent.run_stream_events()` cannot be used inside a Temporal workflow. '\n            'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n        )\n\n    return super().run_stream_events(\n        user_prompt,\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        instructions=instructions,\n        deps=deps,\n        model_settings=model_settings,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        builtin_tools=builtin_tools,\n    )\n\n````\n\n#### iter\n\n```python\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, OutputDataT]\n]\n\n```\n\n```python\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, RunOutputDataT]\n]\n\n```\n\n```python\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]\n\n```\n\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[AgentRun[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`\n\n````python\n@asynccontextmanager\nasync def iter(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:\n    \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\n    This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n    `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\n    executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\n    stream of events coming from the execution of tools.\n\n    The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\n    and the final result of the run once it has completed.\n\n    For more details, see the documentation of `AgentRun`.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        nodes = []\n        async with agent.iter('What is the capital of France?') as agent_run:\n            async for node in agent_run:\n                nodes.append(node)\n        print(nodes)\n        '''\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ],\n                    run_id='...',\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                    run_id='...',\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        '''\n        print(agent_run.result.output)\n        #> The capital of France is Paris.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if workflow.in_workflow():\n        if not self._temporal_overrides_active.get():\n            raise UserError(\n                '`agent.iter()` cannot be used inside a Temporal workflow. '\n                'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n            )\n\n        if model is not None:\n            raise UserError(\n                'Model cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'\n            )\n        if toolsets is not None:\n            raise UserError(\n                'Toolsets cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'\n            )\n\n    async with super().iter(\n        user_prompt=user_prompt,\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        instructions=instructions,\n        deps=deps,\n        model_settings=model_settings,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        builtin_tools=builtin_tools,\n        **_deprecated_kwargs,\n    ) as run:\n        yield run\n\n````\n\n#### override\n\n```python\noverride(\n    *,\n    name: str | Unset = UNSET,\n    deps: AgentDepsT | Unset = UNSET,\n    model: Model | KnownModelName | str | Unset = UNSET,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | Unset\n    ) = UNSET,\n    tools: (\n        Sequence[\n            Tool[AgentDepsT]\n            | ToolFuncEither[AgentDepsT, ...]\n        ]\n        | Unset\n    ) = UNSET,\n    instructions: Instructions[AgentDepsT] | Unset = UNSET\n) -> Iterator[None]\n\n```\n\nContext manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing. You can find an example of this [here](../../testing/#overriding-model-via-pytest-fixtures).\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str | Unset` | The name to use instead of the name passed to the agent constructor and agent run. | `UNSET` | | `deps` | `AgentDepsT | Unset` | The dependencies to use instead of the dependencies passed to the agent run. | `UNSET` | | `model` | `Model | KnownModelName | str | Unset` | The model to use instead of the model passed to the agent run. | `UNSET` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | Unset` | The toolsets to use instead of the toolsets passed to the agent constructor and agent run. | `UNSET` | | `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | Unset` | The tools to use instead of the tools registered with the agent. | `UNSET` | | `instructions` | `Instructions[AgentDepsT] | Unset` | The instructions to use instead of the instructions registered with the agent. | `UNSET` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`\n\n```python\n@contextmanager\ndef override(\n    self,\n    *,\n    name: str | _utils.Unset = _utils.UNSET,\n    deps: AgentDepsT | _utils.Unset = _utils.UNSET,\n    model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,\n    tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,\n    instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,\n) -> Iterator[None]:\n    \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\n    This is particularly useful when testing.\n    You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\n    Args:\n        name: The name to use instead of the name passed to the agent constructor and agent run.\n        deps: The dependencies to use instead of the dependencies passed to the agent run.\n        model: The model to use instead of the model passed to the agent run.\n        toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n        tools: The tools to use instead of the tools registered with the agent.\n        instructions: The instructions to use instead of the instructions registered with the agent.\n    \"\"\"\n    if workflow.in_workflow():\n        if _utils.is_set(model):\n            raise UserError(\n                'Model cannot be contextually overridden inside a Temporal workflow, it must be set at agent creation time.'\n            )\n        if _utils.is_set(toolsets):\n            raise UserError(\n                'Toolsets cannot be contextually overridden inside a Temporal workflow, they must be set at agent creation time.'\n            )\n        if _utils.is_set(tools):\n            raise UserError(\n                'Tools cannot be contextually overridden inside a Temporal workflow, they must be set at agent creation time.'\n            )\n\n    with super().override(\n        name=name,\n        deps=deps,\n        model=model,\n        toolsets=toolsets,\n        tools=tools,\n        instructions=instructions,\n    ):\n        yield\n\n```\n\n### LogfirePlugin\n\nBases: `SimplePlugin`\n\nTemporal client plugin for Logfire.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_logfire.py`\n\n```python\nclass LogfirePlugin(SimplePlugin):\n    \"\"\"Temporal client plugin for Logfire.\"\"\"\n\n    def __init__(self, setup_logfire: Callable[[], Logfire] = _default_setup_logfire, *, metrics: bool = True):\n        try:\n            import logfire  # noqa: F401 # pyright: ignore[reportUnusedImport]\n            from opentelemetry.trace import get_tracer\n            from temporalio.contrib.opentelemetry import TracingInterceptor\n        except ImportError as _import_error:\n            raise ImportError(\n                'Please install the `logfire` package to use the Logfire plugin, '\n                'you can use the `logfire` optional group  `pip install \"pydantic-ai-slim[logfire]\"`'\n            ) from _import_error\n\n        self.setup_logfire = setup_logfire\n        self.metrics = metrics\n\n        super().__init__(  # type: ignore[reportUnknownMemberType]\n            name='LogfirePlugin',\n            client_interceptors=[TracingInterceptor(get_tracer('temporalio'))],\n        )\n\n    async def connect_service_client(\n        self, config: ConnectConfig, next: Callable[[ConnectConfig], Awaitable[ServiceClient]]\n    ) -> ServiceClient:\n        logfire = self.setup_logfire()\n\n        if self.metrics:\n            logfire_config = logfire.config\n            token = logfire_config.token\n            if logfire_config.send_to_logfire and token is not None and logfire_config.metrics is not False:\n                base_url = logfire_config.advanced.generate_base_url(token)\n                metrics_url = base_url + '/v1/metrics'\n                headers = {'Authorization': f'Bearer {token}'}\n\n                config.runtime = Runtime(\n                    telemetry=TelemetryConfig(metrics=OpenTelemetryConfig(url=metrics_url, headers=headers))\n                )\n\n        return await next(config)\n\n```\n\n### TemporalRunContext\n\nBases: `RunContext[AgentDepsT]`\n\nThe RunContext subclass to use to serialize and deserialize the run context for use inside a Temporal activity.\n\nBy default, only the `deps`, `run_id`, `retries`, `tool_call_id`, `tool_name`, `tool_call_approved`, `retry`, `max_retries`, `run_step`, `usage`, and `partial_output` attributes will be available. To make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute and pass it to TemporalAgent.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py`\n\n```python\nclass TemporalRunContext(RunContext[AgentDepsT]):\n    \"\"\"The [`RunContext`][pydantic_ai.tools.RunContext] subclass to use to serialize and deserialize the run context for use inside a Temporal activity.\n\n    By default, only the `deps`, `run_id`, `retries`, `tool_call_id`, `tool_name`, `tool_call_approved`, `retry`, `max_retries`, `run_step`, `usage`, and `partial_output` attributes will be available.\n    To make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute and pass it to [`TemporalAgent`][pydantic_ai.durable_exec.temporal.TemporalAgent].\n    \"\"\"\n\n    def __init__(self, deps: AgentDepsT, **kwargs: Any):\n        self.__dict__ = {**kwargs, 'deps': deps}\n        setattr(\n            self,\n            '__dataclass_fields__',\n            {name: field for name, field in RunContext.__dataclass_fields__.items() if name in self.__dict__},\n        )\n\n    def __getattribute__(self, name: str) -> Any:\n        try:\n            return super().__getattribute__(name)\n        except AttributeError as e:  # pragma: no cover\n            if name in RunContext.__dataclass_fields__:\n                raise UserError(\n                    f'{self.__class__.__name__!r} object has no attribute {name!r}. '\n                    'To make the attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute and pass it to `TemporalAgent`.'\n                )\n            else:\n                raise e\n\n    @classmethod\n    def serialize_run_context(cls, ctx: RunContext[Any]) -> dict[str, Any]:\n        \"\"\"Serialize the run context to a `dict[str, Any]`.\"\"\"\n        return {\n            'run_id': ctx.run_id,\n            'retries': ctx.retries,\n            'tool_call_id': ctx.tool_call_id,\n            'tool_name': ctx.tool_name,\n            'tool_call_approved': ctx.tool_call_approved,\n            'retry': ctx.retry,\n            'max_retries': ctx.max_retries,\n            'run_step': ctx.run_step,\n            'partial_output': ctx.partial_output,\n            'usage': ctx.usage,\n        }\n\n    @classmethod\n    def deserialize_run_context(cls, ctx: dict[str, Any], deps: Any) -> TemporalRunContext[Any]:\n        \"\"\"Deserialize the run context from a `dict[str, Any]`.\"\"\"\n        return cls(**ctx, deps=deps)\n\n```\n\n#### serialize_run_context\n\n```python\nserialize_run_context(\n    ctx: RunContext[Any],\n) -> dict[str, Any]\n\n```\n\nSerialize the run context to a `dict[str, Any]`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py`\n\n```python\n@classmethod\ndef serialize_run_context(cls, ctx: RunContext[Any]) -> dict[str, Any]:\n    \"\"\"Serialize the run context to a `dict[str, Any]`.\"\"\"\n    return {\n        'run_id': ctx.run_id,\n        'retries': ctx.retries,\n        'tool_call_id': ctx.tool_call_id,\n        'tool_name': ctx.tool_name,\n        'tool_call_approved': ctx.tool_call_approved,\n        'retry': ctx.retry,\n        'max_retries': ctx.max_retries,\n        'run_step': ctx.run_step,\n        'partial_output': ctx.partial_output,\n        'usage': ctx.usage,\n    }\n\n```\n\n#### deserialize_run_context\n\n```python\ndeserialize_run_context(\n    ctx: dict[str, Any], deps: Any\n) -> TemporalRunContext[Any]\n\n```\n\nDeserialize the run context from a `dict[str, Any]`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py`\n\n```python\n@classmethod\ndef deserialize_run_context(cls, ctx: dict[str, Any], deps: Any) -> TemporalRunContext[Any]:\n    \"\"\"Deserialize the run context from a `dict[str, Any]`.\"\"\"\n    return cls(**ctx, deps=deps)\n\n```\n\n### PydanticAIPlugin\n\nBases: `SimplePlugin`\n\nTemporal client and worker plugin for Pydantic AI.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py`\n\n```python\nclass PydanticAIPlugin(SimplePlugin):\n    \"\"\"Temporal client and worker plugin for Pydantic AI.\"\"\"\n\n    def __init__(self):\n        super().__init__(  # type: ignore[reportUnknownMemberType]\n            name='PydanticAIPlugin',\n            data_converter=_data_converter,\n            workflow_runner=_workflow_runner,\n            workflow_failure_exception_types=[UserError, PydanticUserError],\n        )\n\n```\n\n### AgentPlugin\n\nBases: `SimplePlugin`\n\nTemporal worker plugin for a specific Pydantic AI agent.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py`\n\n```python\nclass AgentPlugin(SimplePlugin):\n    \"\"\"Temporal worker plugin for a specific Pydantic AI agent.\"\"\"\n\n    def __init__(self, agent: TemporalAgent[Any, Any]):\n        super().__init__(  # type: ignore[reportUnknownMemberType]\n            name='AgentPlugin',\n            activities=agent.temporal_activities,\n        )\n\n```\n\n### DBOSAgent\n\nBases: `WrapperAgent[AgentDepsT, OutputDataT]`, `DBOSConfiguredInstance`\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`\n\n````python\n@DBOS.dbos_class()\nclass DBOSAgent(WrapperAgent[AgentDepsT, OutputDataT], DBOSConfiguredInstance):\n    def __init__(\n        self,\n        wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n        *,\n        name: str | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        mcp_step_config: StepConfig | None = None,\n        model_step_config: StepConfig | None = None,\n    ):\n        \"\"\"Wrap an agent to enable it with DBOS durable workflows, by automatically offloading model requests, tool calls, and MCP server communication to DBOS steps.\n\n        After wrapping, the original agent can still be used as normal outside of the DBOS workflow.\n\n        Args:\n            wrapped: The agent to wrap.\n            name: Optional unique agent name to use as the DBOS configured instance name. If not provided, the agent's `name` will be used.\n            event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.\n            mcp_step_config: The base DBOS step config to use for MCP server steps. If no config is provided, use the default settings of DBOS.\n            model_step_config: The DBOS step config to use for model request steps. If no config is provided, use the default settings of DBOS.\n        \"\"\"\n        super().__init__(wrapped)\n\n        self._name = name or wrapped.name\n        self._event_stream_handler = event_stream_handler\n        if self._name is None:\n            raise UserError(\n                \"An agent needs to have a unique `name` in order to be used with DBOS. The name will be used to identify the agent's workflows and steps.\"\n            )\n\n        # Merge the config with the default DBOS config\n        self._mcp_step_config = mcp_step_config or {}\n        self._model_step_config = model_step_config or {}\n\n        if not isinstance(wrapped.model, Model):\n            raise UserError(\n                'An agent needs to have a `model` in order to be used with DBOS, it cannot be set at agent run time.'\n            )\n\n        dbos_model = DBOSModel(\n            wrapped.model,\n            step_name_prefix=self._name,\n            step_config=self._model_step_config,\n            event_stream_handler=self.event_stream_handler,\n        )\n        self._model = dbos_model\n\n        dbosagent_name = self._name\n\n        def dbosify_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:\n            # Replace MCPServer with DBOSMCPServer\n            try:\n                from pydantic_ai.mcp import MCPServer\n\n                from ._mcp_server import DBOSMCPServer\n            except ImportError:\n                pass\n            else:\n                if isinstance(toolset, MCPServer):\n                    return DBOSMCPServer(\n                        wrapped=toolset,\n                        step_name_prefix=dbosagent_name,\n                        step_config=self._mcp_step_config,\n                    )\n\n            # Replace FastMCPToolset with DBOSFastMCPToolset\n            try:\n                from pydantic_ai.toolsets.fastmcp import FastMCPToolset\n\n                from ._fastmcp_toolset import DBOSFastMCPToolset\n            except ImportError:\n                pass\n            else:\n                if isinstance(toolset, FastMCPToolset):\n                    return DBOSFastMCPToolset(\n                        wrapped=toolset,\n                        step_name_prefix=dbosagent_name,\n                        step_config=self._mcp_step_config,\n                    )\n\n            return toolset\n\n        dbos_toolsets = [toolset.visit_and_replace(dbosify_toolset) for toolset in wrapped.toolsets]\n        self._toolsets = dbos_toolsets\n        DBOSConfiguredInstance.__init__(self, self._name)\n\n        # Wrap the `run` method in a DBOS workflow\n        @DBOS.workflow(name=f'{self._name}.run')\n        async def wrapped_run_workflow(\n            user_prompt: str | Sequence[_messages.UserContent] | None = None,\n            *,\n            output_type: OutputSpec[RunOutputDataT] | None = None,\n            message_history: Sequence[_messages.ModelMessage] | None = None,\n            deferred_tool_results: DeferredToolResults | None = None,\n            model: models.Model | models.KnownModelName | str | None = None,\n            instructions: Instructions[AgentDepsT] = None,\n            deps: AgentDepsT,\n            model_settings: ModelSettings | None = None,\n            usage_limits: _usage.UsageLimits | None = None,\n            usage: _usage.RunUsage | None = None,\n            infer_name: bool = True,\n            toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n            builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n            event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n            **_deprecated_kwargs: Never,\n        ) -> AgentRunResult[Any]:\n            with self._dbos_overrides():\n                return await super(WrapperAgent, self).run(\n                    user_prompt,\n                    output_type=output_type,\n                    message_history=message_history,\n                    deferred_tool_results=deferred_tool_results,\n                    model=model,\n                    instructions=instructions,\n                    deps=deps,\n                    model_settings=model_settings,\n                    usage_limits=usage_limits,\n                    usage=usage,\n                    infer_name=infer_name,\n                    toolsets=toolsets,\n                    builtin_tools=builtin_tools,\n                    event_stream_handler=event_stream_handler,\n                    **_deprecated_kwargs,\n                )\n\n        self.dbos_wrapped_run_workflow = wrapped_run_workflow\n\n        # Wrap the `run_sync` method in a DBOS workflow\n        @DBOS.workflow(name=f'{self._name}.run_sync')\n        def wrapped_run_sync_workflow(\n            user_prompt: str | Sequence[_messages.UserContent] | None = None,\n            *,\n            output_type: OutputSpec[RunOutputDataT] | None = None,\n            message_history: Sequence[_messages.ModelMessage] | None = None,\n            deferred_tool_results: DeferredToolResults | None = None,\n            model: models.Model | models.KnownModelName | str | None = None,\n            deps: AgentDepsT,\n            model_settings: ModelSettings | None = None,\n            instructions: Instructions[AgentDepsT] = None,\n            usage_limits: _usage.UsageLimits | None = None,\n            usage: _usage.RunUsage | None = None,\n            infer_name: bool = True,\n            toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n            builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n            event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n            **_deprecated_kwargs: Never,\n        ) -> AgentRunResult[Any]:\n            with self._dbos_overrides():\n                return super(DBOSAgent, self).run_sync(\n                    user_prompt,\n                    output_type=output_type,\n                    message_history=message_history,\n                    deferred_tool_results=deferred_tool_results,\n                    model=model,\n                    instructions=instructions,\n                    deps=deps,\n                    model_settings=model_settings,\n                    usage_limits=usage_limits,\n                    usage=usage,\n                    infer_name=infer_name,\n                    toolsets=toolsets,\n                    builtin_tools=builtin_tools,\n                    event_stream_handler=event_stream_handler,\n                    **_deprecated_kwargs,\n                )\n\n        self.dbos_wrapped_run_sync_workflow = wrapped_run_sync_workflow\n\n    @property\n    def name(self) -> str | None:\n        return self._name\n\n    @name.setter\n    def name(self, value: str | None) -> None:  # pragma: no cover\n        raise UserError(\n            'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'\n        )\n\n    @property\n    def model(self) -> Model:\n        return self._model\n\n    @property\n    def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:\n        handler = self._event_stream_handler or super().event_stream_handler\n        if handler is None:\n            return None\n        elif DBOS.workflow_id is not None and DBOS.step_id is None:\n            # Special case if it's in a DBOS workflow but not a step, we need to iterate through all events and call the handler.\n            return self._call_event_stream_handler_in_workflow\n        else:\n            return handler\n\n    async def _call_event_stream_handler_in_workflow(\n        self, ctx: RunContext[AgentDepsT], stream: AsyncIterable[_messages.AgentStreamEvent]\n    ) -> None:\n        handler = self._event_stream_handler or super().event_stream_handler\n        assert handler is not None\n\n        async def streamed_response(event: _messages.AgentStreamEvent):\n            yield event\n\n        async for event in stream:\n            await handler(ctx, streamed_response(event))\n\n    @property\n    def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:\n        with self._dbos_overrides():\n            return super().toolsets\n\n    @contextmanager\n    def _dbos_overrides(self) -> Iterator[None]:\n        # Override with DBOSModel and DBOSMCPServer in the toolsets.\n        with (\n            super().override(model=self._model, toolsets=self._toolsets, tools=[]),\n            self.sequential_tool_calls(),\n        ):\n            yield\n\n    @overload\n    async def run(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[OutputDataT]: ...\n\n    @overload\n    async def run(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[RunOutputDataT]: ...\n\n    async def run(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AgentRunResult[Any]:\n        \"\"\"Run the agent with a user prompt in async mode.\n\n        This method builds an internal agent graph (using system prompts, tools and result schemas) and then\n        runs the graph to completion. The result of the run is returned.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            agent_run = await agent.run('What is the capital of France?')\n            print(agent_run.output)\n            #> The capital of France is Paris.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n            event_stream_handler: Optional event stream handler to use for this run.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if model is not None and not isinstance(model, DBOSModel):\n            raise UserError(\n                'Non-DBOS model cannot be set at agent run time inside a DBOS workflow, it must be set at agent creation time.'\n            )\n        return await self.dbos_wrapped_run_workflow(\n            user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n            event_stream_handler=event_stream_handler,\n            **_deprecated_kwargs,\n        )\n\n    @overload\n    def run_sync(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[OutputDataT]: ...\n\n    @overload\n    def run_sync(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[RunOutputDataT]: ...\n\n    def run_sync(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AgentRunResult[Any]:\n        \"\"\"Synchronously run the agent with a user prompt.\n\n        This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\n        You therefore can't use this method inside async code or if there's an active event loop.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        result_sync = agent.run_sync('What is the capital of Italy?')\n        print(result_sync.output)\n        #> The capital of Italy is Rome.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n            event_stream_handler: Optional event stream handler to use for this run.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if model is not None and not isinstance(model, DBOSModel):  # pragma: lax no cover\n            raise UserError(\n                'Non-DBOS model cannot be set at agent run time inside a DBOS workflow, it must be set at agent creation time.'\n            )\n        return self.dbos_wrapped_run_sync_workflow(\n            user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n            event_stream_handler=event_stream_handler,\n            **_deprecated_kwargs,\n        )\n\n    @overload\n    def run_stream(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, OutputDataT]]: ...\n\n    @overload\n    def run_stream(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        deps: AgentDepsT = None,\n        instructions: Instructions[AgentDepsT] = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, RunOutputDataT]]: ...\n\n    @asynccontextmanager\n    async def run_stream(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:\n        \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            async with agent.run_stream('What is the capital of the UK?') as response:\n                print(await response.get_output())\n                #> The capital of the UK is London.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n            event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if DBOS.workflow_id is not None and DBOS.step_id is None:\n            raise UserError(\n                '`agent.run_stream()` cannot be used inside a DBOS workflow. '\n                'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n            )\n\n        async with super().run_stream(\n            user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n            event_stream_handler=event_stream_handler,\n            **_deprecated_kwargs,\n        ) as result:\n            yield result\n\n    @overload\n    def run_stream_events(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[OutputDataT]]: ...\n\n    @overload\n    def run_stream_events(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]]: ...\n\n    def run_stream_events(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:\n        \"\"\"Run the agent with a user prompt in async mode and stream events from the run.\n\n        This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\n        uses the `event_stream_handler` kwarg to get a stream of events from the run.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            events: list[AgentStreamEvent | AgentRunResultEvent] = []\n            async for event in agent.run_stream_events('What is the capital of France?'):\n                events.append(event)\n            print(events)\n            '''\n            [\n                PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n                FinalResultEvent(tool_name=None, tool_call_id=None),\n                PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n                PartEndEvent(\n                    index=0, part=TextPart(content='The capital of France is Paris. ')\n                ),\n                AgentRunResultEvent(\n                    result=AgentRunResult(output='The capital of France is Paris. ')\n                ),\n            ]\n            '''\n        ```\n\n        Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\n        except that `event_stream_handler` is now allowed.\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n            run result.\n        \"\"\"\n        raise UserError(\n            '`agent.run_stream_events()` cannot be used with DBOS. '\n            'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n        )\n\n    @overload\n    def iter(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, OutputDataT]]: ...\n\n    @overload\n    def iter(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, RunOutputDataT]]: ...\n\n    @asynccontextmanager\n    async def iter(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:\n        \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\n        This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n        `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\n        executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\n        stream of events coming from the execution of tools.\n\n        The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\n        and the final result of the run once it has completed.\n\n        For more details, see the documentation of `AgentRun`.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            nodes = []\n            async with agent.iter('What is the capital of France?') as agent_run:\n                async for node in agent_run:\n                    nodes.append(node)\n            print(nodes)\n            '''\n            [\n                UserPromptNode(\n                    user_prompt='What is the capital of France?',\n                    instructions_functions=[],\n                    system_prompts=(),\n                    system_prompt_functions=[],\n                    system_prompt_dynamic_functions={},\n                ),\n                ModelRequestNode(\n                    request=ModelRequest(\n                        parts=[\n                            UserPromptPart(\n                                content='What is the capital of France?',\n                                timestamp=datetime.datetime(...),\n                            )\n                        ],\n                        run_id='...',\n                    )\n                ),\n                CallToolsNode(\n                    model_response=ModelResponse(\n                        parts=[TextPart(content='The capital of France is Paris.')],\n                        usage=RequestUsage(input_tokens=56, output_tokens=7),\n                        model_name='gpt-4o',\n                        timestamp=datetime.datetime(...),\n                        run_id='...',\n                    )\n                ),\n                End(data=FinalResult(output='The capital of France is Paris.')),\n            ]\n            '''\n            print(agent_run.result.output)\n            #> The capital of France is Paris.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if model is not None and not isinstance(model, DBOSModel):  # pragma: lax no cover\n            raise UserError(\n                'Non-DBOS model cannot be set at agent run time inside a DBOS workflow, it must be set at agent creation time.'\n            )\n\n        with self._dbos_overrides():\n            async with super().iter(\n                user_prompt=user_prompt,\n                output_type=output_type,\n                message_history=message_history,\n                deferred_tool_results=deferred_tool_results,\n                model=model,\n                instructions=instructions,\n                deps=deps,\n                model_settings=model_settings,\n                usage_limits=usage_limits,\n                usage=usage,\n                infer_name=infer_name,\n                toolsets=toolsets,\n                builtin_tools=builtin_tools,\n                **_deprecated_kwargs,\n            ) as run:\n                yield run\n\n    @contextmanager\n    def override(\n        self,\n        *,\n        name: str | _utils.Unset = _utils.UNSET,\n        deps: AgentDepsT | _utils.Unset = _utils.UNSET,\n        model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,\n        tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,\n        instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,\n    ) -> Iterator[None]:\n        \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\n        This is particularly useful when testing.\n        You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\n        Args:\n            name: The name to use instead of the name passed to the agent constructor and agent run.\n            deps: The dependencies to use instead of the dependencies passed to the agent run.\n            model: The model to use instead of the model passed to the agent run.\n            toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n            tools: The tools to use instead of the tools registered with the agent.\n            instructions: The instructions to use instead of the instructions registered with the agent.\n        \"\"\"\n        if _utils.is_set(model) and not isinstance(model, (DBOSModel)):\n            raise UserError(\n                'Non-DBOS model cannot be contextually overridden inside a DBOS workflow, it must be set at agent creation time.'\n            )\n\n        with super().override(\n            name=name,\n            deps=deps,\n            model=model,\n            toolsets=toolsets,\n            tools=tools,\n            instructions=instructions,\n        ):\n            yield\n\n````\n\n#### __init__\n\n```python\n__init__(\n    wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    name: str | None = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    mcp_step_config: StepConfig | None = None,\n    model_step_config: StepConfig | None = None\n)\n\n```\n\nWrap an agent to enable it with DBOS durable workflows, by automatically offloading model requests, tool calls, and MCP server communication to DBOS steps.\n\nAfter wrapping, the original agent can still be used as normal outside of the DBOS workflow.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `wrapped` | `AbstractAgent[AgentDepsT, OutputDataT]` | The agent to wrap. | *required* | | `name` | `str | None` | Optional unique agent name to use as the DBOS configured instance name. If not provided, the agent's name will be used. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use instead of the one set on the wrapped agent. | `None` | | `mcp_step_config` | `StepConfig | None` | The base DBOS step config to use for MCP server steps. If no config is provided, use the default settings of DBOS. | `None` | | `model_step_config` | `StepConfig | None` | The DBOS step config to use for model request steps. If no config is provided, use the default settings of DBOS. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`\n\n```python\ndef __init__(\n    self,\n    wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    name: str | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    mcp_step_config: StepConfig | None = None,\n    model_step_config: StepConfig | None = None,\n):\n    \"\"\"Wrap an agent to enable it with DBOS durable workflows, by automatically offloading model requests, tool calls, and MCP server communication to DBOS steps.\n\n    After wrapping, the original agent can still be used as normal outside of the DBOS workflow.\n\n    Args:\n        wrapped: The agent to wrap.\n        name: Optional unique agent name to use as the DBOS configured instance name. If not provided, the agent's `name` will be used.\n        event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.\n        mcp_step_config: The base DBOS step config to use for MCP server steps. If no config is provided, use the default settings of DBOS.\n        model_step_config: The DBOS step config to use for model request steps. If no config is provided, use the default settings of DBOS.\n    \"\"\"\n    super().__init__(wrapped)\n\n    self._name = name or wrapped.name\n    self._event_stream_handler = event_stream_handler\n    if self._name is None:\n        raise UserError(\n            \"An agent needs to have a unique `name` in order to be used with DBOS. The name will be used to identify the agent's workflows and steps.\"\n        )\n\n    # Merge the config with the default DBOS config\n    self._mcp_step_config = mcp_step_config or {}\n    self._model_step_config = model_step_config or {}\n\n    if not isinstance(wrapped.model, Model):\n        raise UserError(\n            'An agent needs to have a `model` in order to be used with DBOS, it cannot be set at agent run time.'\n        )\n\n    dbos_model = DBOSModel(\n        wrapped.model,\n        step_name_prefix=self._name,\n        step_config=self._model_step_config,\n        event_stream_handler=self.event_stream_handler,\n    )\n    self._model = dbos_model\n\n    dbosagent_name = self._name\n\n    def dbosify_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:\n        # Replace MCPServer with DBOSMCPServer\n        try:\n            from pydantic_ai.mcp import MCPServer\n\n            from ._mcp_server import DBOSMCPServer\n        except ImportError:\n            pass\n        else:\n            if isinstance(toolset, MCPServer):\n                return DBOSMCPServer(\n                    wrapped=toolset,\n                    step_name_prefix=dbosagent_name,\n                    step_config=self._mcp_step_config,\n                )\n\n        # Replace FastMCPToolset with DBOSFastMCPToolset\n        try:\n            from pydantic_ai.toolsets.fastmcp import FastMCPToolset\n\n            from ._fastmcp_toolset import DBOSFastMCPToolset\n        except ImportError:\n            pass\n        else:\n            if isinstance(toolset, FastMCPToolset):\n                return DBOSFastMCPToolset(\n                    wrapped=toolset,\n                    step_name_prefix=dbosagent_name,\n                    step_config=self._mcp_step_config,\n                )\n\n        return toolset\n\n    dbos_toolsets = [toolset.visit_and_replace(dbosify_toolset) for toolset in wrapped.toolsets]\n    self._toolsets = dbos_toolsets\n    DBOSConfiguredInstance.__init__(self, self._name)\n\n    # Wrap the `run` method in a DBOS workflow\n    @DBOS.workflow(name=f'{self._name}.run')\n    async def wrapped_run_workflow(\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AgentRunResult[Any]:\n        with self._dbos_overrides():\n            return await super(WrapperAgent, self).run(\n                user_prompt,\n                output_type=output_type,\n                message_history=message_history,\n                deferred_tool_results=deferred_tool_results,\n                model=model,\n                instructions=instructions,\n                deps=deps,\n                model_settings=model_settings,\n                usage_limits=usage_limits,\n                usage=usage,\n                infer_name=infer_name,\n                toolsets=toolsets,\n                builtin_tools=builtin_tools,\n                event_stream_handler=event_stream_handler,\n                **_deprecated_kwargs,\n            )\n\n    self.dbos_wrapped_run_workflow = wrapped_run_workflow\n\n    # Wrap the `run_sync` method in a DBOS workflow\n    @DBOS.workflow(name=f'{self._name}.run_sync')\n    def wrapped_run_sync_workflow(\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        deps: AgentDepsT,\n        model_settings: ModelSettings | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AgentRunResult[Any]:\n        with self._dbos_overrides():\n            return super(DBOSAgent, self).run_sync(\n                user_prompt,\n                output_type=output_type,\n                message_history=message_history,\n                deferred_tool_results=deferred_tool_results,\n                model=model,\n                instructions=instructions,\n                deps=deps,\n                model_settings=model_settings,\n                usage_limits=usage_limits,\n                usage=usage,\n                infer_name=infer_name,\n                toolsets=toolsets,\n                builtin_tools=builtin_tools,\n                event_stream_handler=event_stream_handler,\n                **_deprecated_kwargs,\n            )\n\n    self.dbos_wrapped_run_sync_workflow = wrapped_run_sync_workflow\n\n```\n\n#### run\n\n```python\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\n```\n\n```python\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\n```\n\n```python\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n\n```\n\nRun the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`\n\n````python\nasync def run(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AgentRunResult[Any]:\n    \"\"\"Run the agent with a user prompt in async mode.\n\n    This method builds an internal agent graph (using system prompts, tools and result schemas) and then\n    runs the graph to completion. The result of the run is returned.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        agent_run = await agent.run('What is the capital of France?')\n        print(agent_run.output)\n        #> The capital of France is Paris.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n        event_stream_handler: Optional event stream handler to use for this run.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if model is not None and not isinstance(model, DBOSModel):\n        raise UserError(\n            'Non-DBOS model cannot be set at agent run time inside a DBOS workflow, it must be set at agent creation time.'\n        )\n    return await self.dbos_wrapped_run_workflow(\n        user_prompt,\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        instructions=instructions,\n        deps=deps,\n        model_settings=model_settings,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        builtin_tools=builtin_tools,\n        event_stream_handler=event_stream_handler,\n        **_deprecated_kwargs,\n    )\n\n````\n\n#### run_sync\n\n```python\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\n```\n\n```python\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\n```\n\n```python\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n\n```\n\nSynchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps self.run with `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`\n\n````python\ndef run_sync(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AgentRunResult[Any]:\n    \"\"\"Synchronously run the agent with a user prompt.\n\n    This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\n    You therefore can't use this method inside async code or if there's an active event loop.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    result_sync = agent.run_sync('What is the capital of Italy?')\n    print(result_sync.output)\n    #> The capital of Italy is Rome.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n        event_stream_handler: Optional event stream handler to use for this run.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if model is not None and not isinstance(model, DBOSModel):  # pragma: lax no cover\n        raise UserError(\n            'Non-DBOS model cannot be set at agent run time inside a DBOS workflow, it must be set at agent creation time.'\n        )\n    return self.dbos_wrapped_run_sync_workflow(\n        user_prompt,\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        instructions=instructions,\n        deps=deps,\n        model_settings=model_settings,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        builtin_tools=builtin_tools,\n        event_stream_handler=event_stream_handler,\n        **_deprecated_kwargs,\n    )\n\n````\n\n#### run_stream\n\n```python\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, OutputDataT]\n]\n\n```\n\n```python\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    instructions: Instructions[AgentDepsT] = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, RunOutputDataT]\n]\n\n```\n\n```python\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]\n\n```\n\nRun the agent with a user prompt in async mode, returning a streamed response.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[StreamedRunResult[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`\n\n````python\n@asynccontextmanager\nasync def run_stream(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:\n    \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        async with agent.run_stream('What is the capital of the UK?') as response:\n            print(await response.get_output())\n            #> The capital of the UK is London.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n        event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if DBOS.workflow_id is not None and DBOS.step_id is None:\n        raise UserError(\n            '`agent.run_stream()` cannot be used inside a DBOS workflow. '\n            'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n        )\n\n    async with super().run_stream(\n        user_prompt,\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        instructions=instructions,\n        deps=deps,\n        model_settings=model_settings,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        builtin_tools=builtin_tools,\n        event_stream_handler=event_stream_handler,\n        **_deprecated_kwargs,\n    ) as result:\n        yield result\n\n````\n\n#### run_stream_events\n\n```python\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[OutputDataT]\n]\n\n```\n\n```python\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]\n]\n\n```\n\n```python\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[Any]\n]\n\n```\n\nRun the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps self.run and uses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of France is Paris. ')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n\n```\n\nArguments are the same as for self.run, except that `event_stream_handler` is now allowed.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | An async iterable of stream events AgentStreamEvent and finally a AgentRunResultEvent with the final | | `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | run result. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`\n\n````python\ndef run_stream_events(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:\n    \"\"\"Run the agent with a user prompt in async mode and stream events from the run.\n\n    This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\n    uses the `event_stream_handler` kwarg to get a stream of events from the run.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        events: list[AgentStreamEvent | AgentRunResultEvent] = []\n        async for event in agent.run_stream_events('What is the capital of France?'):\n            events.append(event)\n        print(events)\n        '''\n        [\n            PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n            FinalResultEvent(tool_name=None, tool_call_id=None),\n            PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n            PartEndEvent(\n                index=0, part=TextPart(content='The capital of France is Paris. ')\n            ),\n            AgentRunResultEvent(\n                result=AgentRunResult(output='The capital of France is Paris. ')\n            ),\n        ]\n        '''\n    ```\n\n    Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\n    except that `event_stream_handler` is now allowed.\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n        run result.\n    \"\"\"\n    raise UserError(\n        '`agent.run_stream_events()` cannot be used with DBOS. '\n        'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n    )\n\n````\n\n#### iter\n\n```python\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, OutputDataT]\n]\n\n```\n\n```python\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, RunOutputDataT]\n]\n\n```\n\n```python\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]\n\n```\n\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[AgentRun[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`\n\n````python\n@asynccontextmanager\nasync def iter(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:\n    \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\n    This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n    `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\n    executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\n    stream of events coming from the execution of tools.\n\n    The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\n    and the final result of the run once it has completed.\n\n    For more details, see the documentation of `AgentRun`.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        nodes = []\n        async with agent.iter('What is the capital of France?') as agent_run:\n            async for node in agent_run:\n                nodes.append(node)\n        print(nodes)\n        '''\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ],\n                    run_id='...',\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                    run_id='...',\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        '''\n        print(agent_run.result.output)\n        #> The capital of France is Paris.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if model is not None and not isinstance(model, DBOSModel):  # pragma: lax no cover\n        raise UserError(\n            'Non-DBOS model cannot be set at agent run time inside a DBOS workflow, it must be set at agent creation time.'\n        )\n\n    with self._dbos_overrides():\n        async with super().iter(\n            user_prompt=user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n            **_deprecated_kwargs,\n        ) as run:\n            yield run\n\n````\n\n#### override\n\n```python\noverride(\n    *,\n    name: str | Unset = UNSET,\n    deps: AgentDepsT | Unset = UNSET,\n    model: Model | KnownModelName | str | Unset = UNSET,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | Unset\n    ) = UNSET,\n    tools: (\n        Sequence[\n            Tool[AgentDepsT]\n            | ToolFuncEither[AgentDepsT, ...]\n        ]\n        | Unset\n    ) = UNSET,\n    instructions: Instructions[AgentDepsT] | Unset = UNSET\n) -> Iterator[None]\n\n```\n\nContext manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing. You can find an example of this [here](../../testing/#overriding-model-via-pytest-fixtures).\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str | Unset` | The name to use instead of the name passed to the agent constructor and agent run. | `UNSET` | | `deps` | `AgentDepsT | Unset` | The dependencies to use instead of the dependencies passed to the agent run. | `UNSET` | | `model` | `Model | KnownModelName | str | Unset` | The model to use instead of the model passed to the agent run. | `UNSET` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | Unset` | The toolsets to use instead of the toolsets passed to the agent constructor and agent run. | `UNSET` | | `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | Unset` | The tools to use instead of the tools registered with the agent. | `UNSET` | | `instructions` | `Instructions[AgentDepsT] | Unset` | The instructions to use instead of the instructions registered with the agent. | `UNSET` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`\n\n```python\n@contextmanager\ndef override(\n    self,\n    *,\n    name: str | _utils.Unset = _utils.UNSET,\n    deps: AgentDepsT | _utils.Unset = _utils.UNSET,\n    model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,\n    tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,\n    instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,\n) -> Iterator[None]:\n    \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\n    This is particularly useful when testing.\n    You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\n    Args:\n        name: The name to use instead of the name passed to the agent constructor and agent run.\n        deps: The dependencies to use instead of the dependencies passed to the agent run.\n        model: The model to use instead of the model passed to the agent run.\n        toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n        tools: The tools to use instead of the tools registered with the agent.\n        instructions: The instructions to use instead of the instructions registered with the agent.\n    \"\"\"\n    if _utils.is_set(model) and not isinstance(model, (DBOSModel)):\n        raise UserError(\n            'Non-DBOS model cannot be contextually overridden inside a DBOS workflow, it must be set at agent creation time.'\n        )\n\n    with super().override(\n        name=name,\n        deps=deps,\n        model=model,\n        toolsets=toolsets,\n        tools=tools,\n        instructions=instructions,\n    ):\n        yield\n\n```\n\n### DBOSMCPServer\n\nBases: `DBOSMCPToolset[AgentDepsT]`\n\nA wrapper for MCPServer that integrates with DBOS, turning call_tool and get_tools to DBOS steps.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp_server.py`\n\n```python\nclass DBOSMCPServer(DBOSMCPToolset[AgentDepsT]):\n    \"\"\"A wrapper for MCPServer that integrates with DBOS, turning call_tool and get_tools to DBOS steps.\"\"\"\n\n    def __init__(\n        self,\n        wrapped: MCPServer,\n        *,\n        step_name_prefix: str,\n        step_config: StepConfig,\n    ):\n        super().__init__(\n            wrapped,\n            step_name_prefix=step_name_prefix,\n            step_config=step_config,\n        )\n\n    def tool_for_tool_def(self, tool_def: ToolDefinition) -> ToolsetTool[AgentDepsT]:\n        assert isinstance(self.wrapped, MCPServer)\n        return self.wrapped.tool_for_tool_def(tool_def)\n\n```\n\n### DBOSModel\n\nBases: `WrapperModel`\n\nA wrapper for Model that integrates with DBOS, turning request and request_stream to DBOS steps.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py`\n\n```python\nclass DBOSModel(WrapperModel):\n    \"\"\"A wrapper for Model that integrates with DBOS, turning request and request_stream to DBOS steps.\"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        *,\n        step_name_prefix: str,\n        step_config: StepConfig,\n        event_stream_handler: EventStreamHandler[Any] | None = None,\n    ):\n        super().__init__(model)\n        self.step_config = step_config\n        self.event_stream_handler = event_stream_handler\n        self._step_name_prefix = step_name_prefix\n\n        # Wrap the request in a DBOS step.\n        @DBOS.step(\n            name=f'{self._step_name_prefix}__model.request',\n            **self.step_config,\n        )\n        async def wrapped_request_step(\n            messages: list[ModelMessage],\n            model_settings: ModelSettings | None,\n            model_request_parameters: ModelRequestParameters,\n        ) -> ModelResponse:\n            return await super(DBOSModel, self).request(messages, model_settings, model_request_parameters)\n\n        self._dbos_wrapped_request_step = wrapped_request_step\n\n        # Wrap the request_stream in a DBOS step.\n        @DBOS.step(\n            name=f'{self._step_name_prefix}__model.request_stream',\n            **self.step_config,\n        )\n        async def wrapped_request_stream_step(\n            messages: list[ModelMessage],\n            model_settings: ModelSettings | None,\n            model_request_parameters: ModelRequestParameters,\n            run_context: RunContext[Any] | None = None,\n        ) -> ModelResponse:\n            async with super(DBOSModel, self).request_stream(\n                messages, model_settings, model_request_parameters, run_context\n            ) as streamed_response:\n                if self.event_stream_handler is not None:\n                    assert run_context is not None, (\n                        'A DBOS model cannot be used with `pydantic_ai.direct.model_request_stream()` as it requires a `run_context`. Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n                    )\n                    await self.event_stream_handler(run_context, streamed_response)\n\n                async for _ in streamed_response:\n                    pass\n            return streamed_response.get()\n\n        self._dbos_wrapped_request_stream_step = wrapped_request_stream_step\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        return await self._dbos_wrapped_request_step(messages, model_settings, model_request_parameters)\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        # If not in a workflow (could be in a step), just call the wrapped request_stream method.\n        if DBOS.workflow_id is None or DBOS.step_id is not None:\n            async with super().request_stream(\n                messages, model_settings, model_request_parameters, run_context\n            ) as streamed_response:\n                yield streamed_response\n                return\n\n        response = await self._dbos_wrapped_request_stream_step(\n            messages, model_settings, model_request_parameters, run_context\n        )\n        yield DBOSStreamedResponse(model_request_parameters, response)\n\n```\n\n### StepConfig\n\nBases: `TypedDict`\n\nConfiguration for a step in the DBOS workflow.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_utils.py`\n\n```python\nclass StepConfig(TypedDict, total=False):\n    \"\"\"Configuration for a step in the DBOS workflow.\"\"\"\n\n    retries_allowed: bool\n    interval_seconds: float\n    max_attempts: int\n    backoff_rate: float\n\n```\n\n### PrefectAgent\n\nBases: `WrapperAgent[AgentDepsT, OutputDataT]`\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`\n\n````python\nclass PrefectAgent(WrapperAgent[AgentDepsT, OutputDataT]):\n    def __init__(\n        self,\n        wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n        *,\n        name: str | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        mcp_task_config: TaskConfig | None = None,\n        model_task_config: TaskConfig | None = None,\n        tool_task_config: TaskConfig | None = None,\n        tool_task_config_by_name: dict[str, TaskConfig | None] | None = None,\n        event_stream_handler_task_config: TaskConfig | None = None,\n        prefectify_toolset_func: Callable[\n            [AbstractToolset[AgentDepsT], TaskConfig, TaskConfig, dict[str, TaskConfig | None]],\n            AbstractToolset[AgentDepsT],\n        ] = prefectify_toolset,\n    ):\n        \"\"\"Wrap an agent to enable it with Prefect durable flows, by automatically offloading model requests, tool calls, and MCP server communication to Prefect tasks.\n\n        After wrapping, the original agent can still be used as normal outside of the Prefect flow.\n\n        Args:\n            wrapped: The agent to wrap.\n            name: Optional unique agent name to use as the Prefect flow name prefix. If not provided, the agent's `name` will be used.\n            event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.\n            mcp_task_config: The base Prefect task config to use for MCP server tasks. If no config is provided, use the default settings of Prefect.\n            model_task_config: The Prefect task config to use for model request tasks. If no config is provided, use the default settings of Prefect.\n            tool_task_config: The default Prefect task config to use for tool calls. If no config is provided, use the default settings of Prefect.\n            tool_task_config_by_name: Per-tool task configuration. Keys are tool names, values are TaskConfig or None (None disables task wrapping for that tool).\n            event_stream_handler_task_config: The Prefect task config to use for the event stream handler task. If no config is provided, use the default settings of Prefect.\n            prefectify_toolset_func: Optional function to use to prepare toolsets for Prefect by wrapping them in a `PrefectWrapperToolset` that moves methods that require IO to Prefect tasks.\n                If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Prefect.\n                The function takes the toolset, the task config, the tool-specific task config, and the tool-specific task config by name.\n        \"\"\"\n        super().__init__(wrapped)\n\n        self._name = name or wrapped.name\n        self._event_stream_handler = event_stream_handler\n        if self._name is None:\n            raise UserError(\n                \"An agent needs to have a unique `name` in order to be used with Prefect. The name will be used to identify the agent's flows and tasks.\"\n            )\n\n        # Merge the config with the default Prefect config\n        self._mcp_task_config = default_task_config | (mcp_task_config or {})\n        self._model_task_config = default_task_config | (model_task_config or {})\n        self._tool_task_config = default_task_config | (tool_task_config or {})\n        self._tool_task_config_by_name = tool_task_config_by_name or {}\n        self._event_stream_handler_task_config = default_task_config | (event_stream_handler_task_config or {})\n\n        if not isinstance(wrapped.model, Model):\n            raise UserError(\n                'An agent needs to have a `model` in order to be used with Prefect, it cannot be set at agent run time.'\n            )\n\n        prefect_model = PrefectModel(\n            wrapped.model,\n            task_config=self._model_task_config,\n            event_stream_handler=self.event_stream_handler,\n        )\n        self._model = prefect_model\n\n        def _prefectify_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:\n            \"\"\"Convert a toolset to its Prefect equivalent.\"\"\"\n            return prefectify_toolset_func(\n                toolset,\n                self._mcp_task_config,\n                self._tool_task_config,\n                self._tool_task_config_by_name,\n            )\n\n        prefect_toolsets = [toolset.visit_and_replace(_prefectify_toolset) for toolset in wrapped.toolsets]\n        self._toolsets = prefect_toolsets\n\n        # Context variable to track when we're inside this agent's Prefect flow\n        self._in_prefect_agent_flow: ContextVar[bool] = ContextVar(\n            f'_in_prefect_agent_flow_{self._name}', default=False\n        )\n\n    @property\n    def name(self) -> str | None:\n        return self._name\n\n    @name.setter\n    def name(self, value: str | None) -> None:  # pragma: no cover\n        raise UserError(\n            'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'\n        )\n\n    @property\n    def model(self) -> Model:\n        return self._model\n\n    @property\n    def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:\n        handler = self._event_stream_handler or super().event_stream_handler\n        if handler is None:\n            return None\n        elif FlowRunContext.get() is not None:\n            # Special case if it's in a Prefect flow, we need to iterate through all events and call the handler.\n            return self._call_event_stream_handler_in_flow\n        else:\n            return handler\n\n    async def _call_event_stream_handler_in_flow(\n        self, ctx: RunContext[AgentDepsT], stream: AsyncIterable[_messages.AgentStreamEvent]\n    ) -> None:\n        handler = self._event_stream_handler or super().event_stream_handler\n        assert handler is not None\n\n        # Create a task to handle each event\n        @task(name='Handle Stream Event', **self._event_stream_handler_task_config)\n        async def event_stream_handler_task(event: _messages.AgentStreamEvent) -> None:\n            async def streamed_response():\n                yield event\n\n            await handler(ctx, streamed_response())\n\n        async for event in stream:\n            await event_stream_handler_task(event)\n\n    @property\n    def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:\n        with self._prefect_overrides():\n            return super().toolsets\n\n    @contextmanager\n    def _prefect_overrides(self) -> Iterator[None]:\n        # Override with PrefectModel and PrefectMCPServer in the toolsets.\n        with super().override(model=self._model, toolsets=self._toolsets, tools=[]):\n            yield\n\n    @overload\n    async def run(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[OutputDataT]: ...\n\n    @overload\n    async def run(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[RunOutputDataT]: ...\n\n    async def run(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AgentRunResult[Any]:\n        \"\"\"Run the agent with a user prompt in async mode.\n\n        This method builds an internal agent graph (using system prompts, tools and result schemas) and then\n        runs the graph to completion. The result of the run is returned.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            agent_run = await agent.run('What is the capital of France?')\n            print(agent_run.output)\n            #> The capital of France is Paris.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            event_stream_handler: Optional event stream handler to use for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n\n        @flow(name=f'{self._name} Run')\n        async def wrapped_run_flow() -> AgentRunResult[Any]:\n            # Mark that we're inside a PrefectAgent flow\n            token = self._in_prefect_agent_flow.set(True)\n            try:\n                with self._prefect_overrides():\n                    result = await super(WrapperAgent, self).run(\n                        user_prompt,\n                        output_type=output_type,\n                        message_history=message_history,\n                        deferred_tool_results=deferred_tool_results,\n                        model=model,\n                        instructions=instructions,\n                        deps=deps,\n                        model_settings=model_settings,\n                        usage_limits=usage_limits,\n                        usage=usage,\n                        infer_name=infer_name,\n                        toolsets=toolsets,\n                        event_stream_handler=event_stream_handler,\n                    )\n                    return result\n            finally:\n                self._in_prefect_agent_flow.reset(token)\n\n        return await wrapped_run_flow()\n\n    @overload\n    def run_sync(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[OutputDataT]: ...\n\n    @overload\n    def run_sync(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AgentRunResult[RunOutputDataT]: ...\n\n    def run_sync(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AgentRunResult[Any]:\n        \"\"\"Synchronously run the agent with a user prompt.\n\n        This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\n        You therefore can't use this method inside async code or if there's an active event loop.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        result_sync = agent.run_sync('What is the capital of Italy?')\n        print(result_sync.output)\n        #> The capital of Italy is Rome.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            event_stream_handler: Optional event stream handler to use for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n\n        @flow(name=f'{self._name} Sync Run')\n        def wrapped_run_sync_flow() -> AgentRunResult[Any]:\n            # Mark that we're inside a PrefectAgent flow\n            token = self._in_prefect_agent_flow.set(True)\n            try:\n                with self._prefect_overrides():\n                    # Using `run_coro_as_sync` from Prefect with async `run` to avoid event loop conflicts.\n                    result = run_coro_as_sync(\n                        super(PrefectAgent, self).run(\n                            user_prompt,\n                            output_type=output_type,\n                            message_history=message_history,\n                            deferred_tool_results=deferred_tool_results,\n                            model=model,\n                            instructions=instructions,\n                            deps=deps,\n                            model_settings=model_settings,\n                            usage_limits=usage_limits,\n                            usage=usage,\n                            infer_name=infer_name,\n                            toolsets=toolsets,\n                            event_stream_handler=event_stream_handler,\n                        )\n                    )\n                    return result\n            finally:\n                self._in_prefect_agent_flow.reset(token)\n\n        return wrapped_run_sync_flow()\n\n    @overload\n    def run_stream(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, OutputDataT]]: ...\n\n    @overload\n    def run_stream(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, RunOutputDataT]]: ...\n\n    @asynccontextmanager\n    async def run_stream(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n        **_deprecated_kwargs: Never,\n    ) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:\n        \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            async with agent.run_stream('What is the capital of the UK?') as response:\n                print(await response.get_output())\n                #> The capital of the UK is London.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n            event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if FlowRunContext.get() is not None:\n            raise UserError(\n                '`agent.run_stream()` cannot be used inside a Prefect flow. '\n                'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n            )\n\n        async with super().run_stream(\n            user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            event_stream_handler=event_stream_handler,\n            builtin_tools=builtin_tools,\n            **_deprecated_kwargs,\n        ) as result:\n            yield result\n\n    @overload\n    def run_stream_events(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[OutputDataT]]: ...\n\n    @overload\n    def run_stream_events(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]]: ...\n\n    def run_stream_events(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:\n        \"\"\"Run the agent with a user prompt in async mode and stream events from the run.\n\n        This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\n        uses the `event_stream_handler` kwarg to get a stream of events from the run.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            events: list[AgentStreamEvent | AgentRunResultEvent] = []\n            async for event in agent.run_stream_events('What is the capital of France?'):\n                events.append(event)\n            print(events)\n            '''\n            [\n                PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n                FinalResultEvent(tool_name=None, tool_call_id=None),\n                PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n                PartEndEvent(\n                    index=0, part=TextPart(content='The capital of France is Paris. ')\n                ),\n                AgentRunResultEvent(\n                    result=AgentRunResult(output='The capital of France is Paris. ')\n                ),\n            ]\n            '''\n        ```\n\n        Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\n        except that `event_stream_handler` is now allowed.\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n            run result.\n        \"\"\"\n        if FlowRunContext.get() is not None:\n            raise UserError(\n                '`agent.run_stream_events()` cannot be used inside a Prefect flow. '\n                'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n            )\n\n        return super().run_stream_events(\n            user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n        )\n\n    @overload\n    def iter(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, OutputDataT]]: ...\n\n    @overload\n    def iter(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT],\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, RunOutputDataT]]: ...\n\n    @asynccontextmanager\n    async def iter(\n        self,\n        user_prompt: str | Sequence[_messages.UserContent] | None = None,\n        *,\n        output_type: OutputSpec[RunOutputDataT] | None = None,\n        message_history: Sequence[_messages.ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: models.Model | models.KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: _usage.UsageLimits | None = None,\n        usage: _usage.RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:\n        \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\n        This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n        `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\n        executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\n        stream of events coming from the execution of tools.\n\n        The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\n        and the final result of the run once it has completed.\n\n        For more details, see the documentation of `AgentRun`.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            nodes = []\n            async with agent.iter('What is the capital of France?') as agent_run:\n                async for node in agent_run:\n                    nodes.append(node)\n            print(nodes)\n            '''\n            [\n                UserPromptNode(\n                    user_prompt='What is the capital of France?',\n                    instructions_functions=[],\n                    system_prompts=(),\n                    system_prompt_functions=[],\n                    system_prompt_dynamic_functions={},\n                ),\n                ModelRequestNode(\n                    request=ModelRequest(\n                        parts=[\n                            UserPromptPart(\n                                content='What is the capital of France?',\n                                timestamp=datetime.datetime(...),\n                            )\n                        ],\n                        run_id='...',\n                    )\n                ),\n                CallToolsNode(\n                    model_response=ModelResponse(\n                        parts=[TextPart(content='The capital of France is Paris.')],\n                        usage=RequestUsage(input_tokens=56, output_tokens=7),\n                        model_name='gpt-4o',\n                        timestamp=datetime.datetime(...),\n                        run_id='...',\n                    )\n                ),\n                End(data=FinalResult(output='The capital of France is Paris.')),\n            ]\n            '''\n            print(agent_run.result.output)\n            #> The capital of France is Paris.\n        ```\n\n        Args:\n            user_prompt: User input to start/continue the conversation.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            deps: Optional dependencies to use for this run.\n            instructions: Optional additional instructions to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools for this run.\n\n        Returns:\n            The result of the run.\n        \"\"\"\n        if model is not None and not isinstance(model, PrefectModel):\n            raise UserError(\n                'Non-Prefect model cannot be set at agent run time inside a Prefect flow, it must be set at agent creation time.'\n            )\n\n        with self._prefect_overrides():\n            async with super().iter(\n                user_prompt=user_prompt,\n                output_type=output_type,\n                message_history=message_history,\n                deferred_tool_results=deferred_tool_results,\n                model=model,\n                instructions=instructions,\n                deps=deps,\n                model_settings=model_settings,\n                usage_limits=usage_limits,\n                usage=usage,\n                infer_name=infer_name,\n                toolsets=toolsets,\n            ) as run:\n                yield run\n\n    @contextmanager\n    def override(\n        self,\n        *,\n        name: str | _utils.Unset = _utils.UNSET,\n        deps: AgentDepsT | _utils.Unset = _utils.UNSET,\n        model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,\n        tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,\n        instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,\n    ) -> Iterator[None]:\n        \"\"\"Context manager to temporarily override agent dependencies, model, toolsets, tools, or instructions.\n\n        This is particularly useful when testing.\n        You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\n        Args:\n            name: The name to use instead of the name passed to the agent constructor and agent run.\n            deps: The dependencies to use instead of the dependencies passed to the agent run.\n            model: The model to use instead of the model passed to the agent run.\n            toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n            tools: The tools to use instead of the tools registered with the agent.\n            instructions: The instructions to use instead of the instructions registered with the agent.\n        \"\"\"\n        if _utils.is_set(model) and not isinstance(model, PrefectModel):\n            raise UserError(\n                'Non-Prefect model cannot be contextually overridden inside a Prefect flow, it must be set at agent creation time.'\n            )\n\n        with super().override(\n            name=name, deps=deps, model=model, toolsets=toolsets, tools=tools, instructions=instructions\n        ):\n            yield\n\n````\n\n#### __init__\n\n```python\n__init__(\n    wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    name: str | None = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    mcp_task_config: TaskConfig | None = None,\n    model_task_config: TaskConfig | None = None,\n    tool_task_config: TaskConfig | None = None,\n    tool_task_config_by_name: (\n        dict[str, TaskConfig | None] | None\n    ) = None,\n    event_stream_handler_task_config: (\n        TaskConfig | None\n    ) = None,\n    prefectify_toolset_func: Callable[\n        [\n            AbstractToolset[AgentDepsT],\n            TaskConfig,\n            TaskConfig,\n            dict[str, TaskConfig | None],\n        ],\n        AbstractToolset[AgentDepsT],\n    ] = prefectify_toolset\n)\n\n```\n\nWrap an agent to enable it with Prefect durable flows, by automatically offloading model requests, tool calls, and MCP server communication to Prefect tasks.\n\nAfter wrapping, the original agent can still be used as normal outside of the Prefect flow.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `wrapped` | `AbstractAgent[AgentDepsT, OutputDataT]` | The agent to wrap. | *required* | | `name` | `str | None` | Optional unique agent name to use as the Prefect flow name prefix. If not provided, the agent's name will be used. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use instead of the one set on the wrapped agent. | `None` | | `mcp_task_config` | `TaskConfig | None` | The base Prefect task config to use for MCP server tasks. If no config is provided, use the default settings of Prefect. | `None` | | `model_task_config` | `TaskConfig | None` | The Prefect task config to use for model request tasks. If no config is provided, use the default settings of Prefect. | `None` | | `tool_task_config` | `TaskConfig | None` | The default Prefect task config to use for tool calls. If no config is provided, use the default settings of Prefect. | `None` | | `tool_task_config_by_name` | `dict[str, TaskConfig | None] | None` | Per-tool task configuration. Keys are tool names, values are TaskConfig or None (None disables task wrapping for that tool). | `None` | | `event_stream_handler_task_config` | `TaskConfig | None` | The Prefect task config to use for the event stream handler task. If no config is provided, use the default settings of Prefect. | `None` | | `prefectify_toolset_func` | `Callable[[AbstractToolset[AgentDepsT], TaskConfig, TaskConfig, dict[str, TaskConfig | None]], AbstractToolset[AgentDepsT]]` | Optional function to use to prepare toolsets for Prefect by wrapping them in a PrefectWrapperToolset that moves methods that require IO to Prefect tasks. If not provided, only FunctionToolset and MCPServer will be prepared for Prefect. The function takes the toolset, the task config, the tool-specific task config, and the tool-specific task config by name. | `prefectify_toolset` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`\n\n```python\ndef __init__(\n    self,\n    wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    name: str | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    mcp_task_config: TaskConfig | None = None,\n    model_task_config: TaskConfig | None = None,\n    tool_task_config: TaskConfig | None = None,\n    tool_task_config_by_name: dict[str, TaskConfig | None] | None = None,\n    event_stream_handler_task_config: TaskConfig | None = None,\n    prefectify_toolset_func: Callable[\n        [AbstractToolset[AgentDepsT], TaskConfig, TaskConfig, dict[str, TaskConfig | None]],\n        AbstractToolset[AgentDepsT],\n    ] = prefectify_toolset,\n):\n    \"\"\"Wrap an agent to enable it with Prefect durable flows, by automatically offloading model requests, tool calls, and MCP server communication to Prefect tasks.\n\n    After wrapping, the original agent can still be used as normal outside of the Prefect flow.\n\n    Args:\n        wrapped: The agent to wrap.\n        name: Optional unique agent name to use as the Prefect flow name prefix. If not provided, the agent's `name` will be used.\n        event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.\n        mcp_task_config: The base Prefect task config to use for MCP server tasks. If no config is provided, use the default settings of Prefect.\n        model_task_config: The Prefect task config to use for model request tasks. If no config is provided, use the default settings of Prefect.\n        tool_task_config: The default Prefect task config to use for tool calls. If no config is provided, use the default settings of Prefect.\n        tool_task_config_by_name: Per-tool task configuration. Keys are tool names, values are TaskConfig or None (None disables task wrapping for that tool).\n        event_stream_handler_task_config: The Prefect task config to use for the event stream handler task. If no config is provided, use the default settings of Prefect.\n        prefectify_toolset_func: Optional function to use to prepare toolsets for Prefect by wrapping them in a `PrefectWrapperToolset` that moves methods that require IO to Prefect tasks.\n            If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Prefect.\n            The function takes the toolset, the task config, the tool-specific task config, and the tool-specific task config by name.\n    \"\"\"\n    super().__init__(wrapped)\n\n    self._name = name or wrapped.name\n    self._event_stream_handler = event_stream_handler\n    if self._name is None:\n        raise UserError(\n            \"An agent needs to have a unique `name` in order to be used with Prefect. The name will be used to identify the agent's flows and tasks.\"\n        )\n\n    # Merge the config with the default Prefect config\n    self._mcp_task_config = default_task_config | (mcp_task_config or {})\n    self._model_task_config = default_task_config | (model_task_config or {})\n    self._tool_task_config = default_task_config | (tool_task_config or {})\n    self._tool_task_config_by_name = tool_task_config_by_name or {}\n    self._event_stream_handler_task_config = default_task_config | (event_stream_handler_task_config or {})\n\n    if not isinstance(wrapped.model, Model):\n        raise UserError(\n            'An agent needs to have a `model` in order to be used with Prefect, it cannot be set at agent run time.'\n        )\n\n    prefect_model = PrefectModel(\n        wrapped.model,\n        task_config=self._model_task_config,\n        event_stream_handler=self.event_stream_handler,\n    )\n    self._model = prefect_model\n\n    def _prefectify_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:\n        \"\"\"Convert a toolset to its Prefect equivalent.\"\"\"\n        return prefectify_toolset_func(\n            toolset,\n            self._mcp_task_config,\n            self._tool_task_config,\n            self._tool_task_config_by_name,\n        )\n\n    prefect_toolsets = [toolset.visit_and_replace(_prefectify_toolset) for toolset in wrapped.toolsets]\n    self._toolsets = prefect_toolsets\n\n    # Context variable to track when we're inside this agent's Prefect flow\n    self._in_prefect_agent_flow: ContextVar[bool] = ContextVar(\n        f'_in_prefect_agent_flow_{self._name}', default=False\n    )\n\n```\n\n#### run\n\n```python\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\n```\n\n```python\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\n```\n\n```python\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n\n```\n\nRun the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`\n\n````python\nasync def run(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AgentRunResult[Any]:\n    \"\"\"Run the agent with a user prompt in async mode.\n\n    This method builds an internal agent graph (using system prompts, tools and result schemas) and then\n    runs the graph to completion. The result of the run is returned.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        agent_run = await agent.run('What is the capital of France?')\n        print(agent_run.output)\n        #> The capital of France is Paris.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        event_stream_handler: Optional event stream handler to use for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n\n    @flow(name=f'{self._name} Run')\n    async def wrapped_run_flow() -> AgentRunResult[Any]:\n        # Mark that we're inside a PrefectAgent flow\n        token = self._in_prefect_agent_flow.set(True)\n        try:\n            with self._prefect_overrides():\n                result = await super(WrapperAgent, self).run(\n                    user_prompt,\n                    output_type=output_type,\n                    message_history=message_history,\n                    deferred_tool_results=deferred_tool_results,\n                    model=model,\n                    instructions=instructions,\n                    deps=deps,\n                    model_settings=model_settings,\n                    usage_limits=usage_limits,\n                    usage=usage,\n                    infer_name=infer_name,\n                    toolsets=toolsets,\n                    event_stream_handler=event_stream_handler,\n                )\n                return result\n        finally:\n            self._in_prefect_agent_flow.reset(token)\n\n    return await wrapped_run_flow()\n\n````\n\n#### run_sync\n\n```python\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\n```\n\n```python\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\n```\n\n```python\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n\n```\n\nSynchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps self.run with `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`\n\n````python\ndef run_sync(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AgentRunResult[Any]:\n    \"\"\"Synchronously run the agent with a user prompt.\n\n    This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\n    You therefore can't use this method inside async code or if there's an active event loop.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    result_sync = agent.run_sync('What is the capital of Italy?')\n    print(result_sync.output)\n    #> The capital of Italy is Rome.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        event_stream_handler: Optional event stream handler to use for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n\n    @flow(name=f'{self._name} Sync Run')\n    def wrapped_run_sync_flow() -> AgentRunResult[Any]:\n        # Mark that we're inside a PrefectAgent flow\n        token = self._in_prefect_agent_flow.set(True)\n        try:\n            with self._prefect_overrides():\n                # Using `run_coro_as_sync` from Prefect with async `run` to avoid event loop conflicts.\n                result = run_coro_as_sync(\n                    super(PrefectAgent, self).run(\n                        user_prompt,\n                        output_type=output_type,\n                        message_history=message_history,\n                        deferred_tool_results=deferred_tool_results,\n                        model=model,\n                        instructions=instructions,\n                        deps=deps,\n                        model_settings=model_settings,\n                        usage_limits=usage_limits,\n                        usage=usage,\n                        infer_name=infer_name,\n                        toolsets=toolsets,\n                        event_stream_handler=event_stream_handler,\n                    )\n                )\n                return result\n        finally:\n            self._in_prefect_agent_flow.reset(token)\n\n    return wrapped_run_sync_flow()\n\n````\n\n#### run_stream\n\n```python\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, OutputDataT]\n]\n\n```\n\n```python\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, RunOutputDataT]\n]\n\n```\n\n```python\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]\n\n```\n\nRun the agent with a user prompt in async mode, returning a streamed response.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` | | `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[StreamedRunResult[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`\n\n````python\n@asynccontextmanager\nasync def run_stream(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,\n    **_deprecated_kwargs: Never,\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:\n    \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        async with agent.run_stream('What is the capital of the UK?') as response:\n            print(await response.get_output())\n            #> The capital of the UK is London.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n        event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if FlowRunContext.get() is not None:\n        raise UserError(\n            '`agent.run_stream()` cannot be used inside a Prefect flow. '\n            'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n        )\n\n    async with super().run_stream(\n        user_prompt,\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        instructions=instructions,\n        deps=deps,\n        model_settings=model_settings,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        event_stream_handler=event_stream_handler,\n        builtin_tools=builtin_tools,\n        **_deprecated_kwargs,\n    ) as result:\n        yield result\n\n````\n\n#### run_stream_events\n\n```python\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[OutputDataT]\n]\n\n```\n\n```python\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]\n]\n\n```\n\n```python\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[Any]\n]\n\n```\n\nRun the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps self.run and uses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of France is Paris. ')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n\n```\n\nArguments are the same as for self.run, except that `event_stream_handler` is now allowed.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | An async iterable of stream events AgentStreamEvent and finally a AgentRunResultEvent with the final | | `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | run result. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`\n\n````python\ndef run_stream_events(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:\n    \"\"\"Run the agent with a user prompt in async mode and stream events from the run.\n\n    This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\n    uses the `event_stream_handler` kwarg to get a stream of events from the run.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        events: list[AgentStreamEvent | AgentRunResultEvent] = []\n        async for event in agent.run_stream_events('What is the capital of France?'):\n            events.append(event)\n        print(events)\n        '''\n        [\n            PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n            FinalResultEvent(tool_name=None, tool_call_id=None),\n            PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n            PartEndEvent(\n                index=0, part=TextPart(content='The capital of France is Paris. ')\n            ),\n            AgentRunResultEvent(\n                result=AgentRunResult(output='The capital of France is Paris. ')\n            ),\n        ]\n        '''\n    ```\n\n    Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\n    except that `event_stream_handler` is now allowed.\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n        run result.\n    \"\"\"\n    if FlowRunContext.get() is not None:\n        raise UserError(\n            '`agent.run_stream_events()` cannot be used inside a Prefect flow. '\n            'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n        )\n\n    return super().run_stream_events(\n        user_prompt,\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        instructions=instructions,\n        deps=deps,\n        model_settings=model_settings,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        builtin_tools=builtin_tools,\n    )\n\n````\n\n#### iter\n\n```python\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, OutputDataT]\n]\n\n```\n\n```python\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, RunOutputDataT]\n]\n\n```\n\n```python\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]\n\n```\n\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` | | `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AsyncIterator[AgentRun[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`\n\n````python\n@asynccontextmanager\nasync def iter(\n    self,\n    user_prompt: str | Sequence[_messages.UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[_messages.ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: models.Model | models.KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: _usage.UsageLimits | None = None,\n    usage: _usage.RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:\n    \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\n    This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n    `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\n    executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\n    stream of events coming from the execution of tools.\n\n    The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\n    and the final result of the run once it has completed.\n\n    For more details, see the documentation of `AgentRun`.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        nodes = []\n        async with agent.iter('What is the capital of France?') as agent_run:\n            async for node in agent_run:\n                nodes.append(node)\n        print(nodes)\n        '''\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ],\n                    run_id='...',\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                    run_id='...',\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        '''\n        print(agent_run.result.output)\n        #> The capital of France is Paris.\n    ```\n\n    Args:\n        user_prompt: User input to start/continue the conversation.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        deps: Optional dependencies to use for this run.\n        instructions: Optional additional instructions to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools for this run.\n\n    Returns:\n        The result of the run.\n    \"\"\"\n    if model is not None and not isinstance(model, PrefectModel):\n        raise UserError(\n            'Non-Prefect model cannot be set at agent run time inside a Prefect flow, it must be set at agent creation time.'\n        )\n\n    with self._prefect_overrides():\n        async with super().iter(\n            user_prompt=user_prompt,\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n        ) as run:\n            yield run\n\n````\n\n#### override\n\n```python\noverride(\n    *,\n    name: str | Unset = UNSET,\n    deps: AgentDepsT | Unset = UNSET,\n    model: Model | KnownModelName | str | Unset = UNSET,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | Unset\n    ) = UNSET,\n    tools: (\n        Sequence[\n            Tool[AgentDepsT]\n            | ToolFuncEither[AgentDepsT, ...]\n        ]\n        | Unset\n    ) = UNSET,\n    instructions: Instructions[AgentDepsT] | Unset = UNSET\n) -> Iterator[None]\n\n```\n\nContext manager to temporarily override agent dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing. You can find an example of this [here](../../testing/#overriding-model-via-pytest-fixtures).\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str | Unset` | The name to use instead of the name passed to the agent constructor and agent run. | `UNSET` | | `deps` | `AgentDepsT | Unset` | The dependencies to use instead of the dependencies passed to the agent run. | `UNSET` | | `model` | `Model | KnownModelName | str | Unset` | The model to use instead of the model passed to the agent run. | `UNSET` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | Unset` | The toolsets to use instead of the toolsets passed to the agent constructor and agent run. | `UNSET` | | `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | Unset` | The tools to use instead of the tools registered with the agent. | `UNSET` | | `instructions` | `Instructions[AgentDepsT] | Unset` | The instructions to use instead of the instructions registered with the agent. | `UNSET` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`\n\n```python\n@contextmanager\ndef override(\n    self,\n    *,\n    name: str | _utils.Unset = _utils.UNSET,\n    deps: AgentDepsT | _utils.Unset = _utils.UNSET,\n    model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,\n    tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,\n    instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,\n) -> Iterator[None]:\n    \"\"\"Context manager to temporarily override agent dependencies, model, toolsets, tools, or instructions.\n\n    This is particularly useful when testing.\n    You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\n    Args:\n        name: The name to use instead of the name passed to the agent constructor and agent run.\n        deps: The dependencies to use instead of the dependencies passed to the agent run.\n        model: The model to use instead of the model passed to the agent run.\n        toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n        tools: The tools to use instead of the tools registered with the agent.\n        instructions: The instructions to use instead of the instructions registered with the agent.\n    \"\"\"\n    if _utils.is_set(model) and not isinstance(model, PrefectModel):\n        raise UserError(\n            'Non-Prefect model cannot be contextually overridden inside a Prefect flow, it must be set at agent creation time.'\n        )\n\n    with super().override(\n        name=name, deps=deps, model=model, toolsets=toolsets, tools=tools, instructions=instructions\n    ):\n        yield\n\n```\n\n### PrefectFunctionToolset\n\nBases: `PrefectWrapperToolset[AgentDepsT]`\n\nA wrapper for FunctionToolset that integrates with Prefect, turning tool calls into Prefect tasks.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py`\n\n```python\nclass PrefectFunctionToolset(PrefectWrapperToolset[AgentDepsT]):\n    \"\"\"A wrapper for FunctionToolset that integrates with Prefect, turning tool calls into Prefect tasks.\"\"\"\n\n    def __init__(\n        self,\n        wrapped: FunctionToolset[AgentDepsT],\n        *,\n        task_config: TaskConfig,\n        tool_task_config: dict[str, TaskConfig | None],\n    ):\n        super().__init__(wrapped)\n        self._task_config = default_task_config | (task_config or {})\n        self._tool_task_config = tool_task_config or {}\n\n        @task\n        async def _call_tool_task(\n            tool_name: str,\n            tool_args: dict[str, Any],\n            ctx: RunContext[AgentDepsT],\n            tool: ToolsetTool[AgentDepsT],\n        ) -> Any:\n            return await super(PrefectFunctionToolset, self).call_tool(tool_name, tool_args, ctx, tool)\n\n        self._call_tool_task = _call_tool_task\n\n    async def call_tool(\n        self,\n        name: str,\n        tool_args: dict[str, Any],\n        ctx: RunContext[AgentDepsT],\n        tool: ToolsetTool[AgentDepsT],\n    ) -> Any:\n        \"\"\"Call a tool, wrapped as a Prefect task with a descriptive name.\"\"\"\n        # Check if this specific tool has custom config or is disabled\n        tool_specific_config = self._tool_task_config.get(name, default_task_config)\n        if tool_specific_config is None:\n            # None means this tool should not be wrapped as a task\n            return await super().call_tool(name, tool_args, ctx, tool)\n\n        # Merge tool-specific config with default config\n        merged_config = self._task_config | tool_specific_config\n\n        return await self._call_tool_task.with_options(name=f'Call Tool: {name}', **merged_config)(\n            name, tool_args, ctx, tool\n        )\n\n```\n\n#### call_tool\n\n```python\ncall_tool(\n    name: str,\n    tool_args: dict[str, Any],\n    ctx: RunContext[AgentDepsT],\n    tool: ToolsetTool[AgentDepsT],\n) -> Any\n\n```\n\nCall a tool, wrapped as a Prefect task with a descriptive name.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py`\n\n```python\nasync def call_tool(\n    self,\n    name: str,\n    tool_args: dict[str, Any],\n    ctx: RunContext[AgentDepsT],\n    tool: ToolsetTool[AgentDepsT],\n) -> Any:\n    \"\"\"Call a tool, wrapped as a Prefect task with a descriptive name.\"\"\"\n    # Check if this specific tool has custom config or is disabled\n    tool_specific_config = self._tool_task_config.get(name, default_task_config)\n    if tool_specific_config is None:\n        # None means this tool should not be wrapped as a task\n        return await super().call_tool(name, tool_args, ctx, tool)\n\n    # Merge tool-specific config with default config\n    merged_config = self._task_config | tool_specific_config\n\n    return await self._call_tool_task.with_options(name=f'Call Tool: {name}', **merged_config)(\n        name, tool_args, ctx, tool\n    )\n\n```\n\n### PrefectMCPServer\n\nBases: `PrefectWrapperToolset[AgentDepsT]`, `ABC`\n\nA wrapper for MCPServer that integrates with Prefect, turning call_tool and get_tools into Prefect tasks.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py`\n\n```python\nclass PrefectMCPServer(PrefectWrapperToolset[AgentDepsT], ABC):\n    \"\"\"A wrapper for MCPServer that integrates with Prefect, turning call_tool and get_tools into Prefect tasks.\"\"\"\n\n    def __init__(\n        self,\n        wrapped: MCPServer,\n        *,\n        task_config: TaskConfig,\n    ):\n        super().__init__(wrapped)\n        self._task_config = default_task_config | (task_config or {})\n        self._mcp_id = wrapped.id\n\n        @task\n        async def _call_tool_task(\n            tool_name: str,\n            tool_args: dict[str, Any],\n            ctx: RunContext[AgentDepsT],\n            tool: ToolsetTool[AgentDepsT],\n        ) -> ToolResult:\n            return await super(PrefectMCPServer, self).call_tool(tool_name, tool_args, ctx, tool)\n\n        self._call_tool_task = _call_tool_task\n\n    async def __aenter__(self) -> Self:\n        await self.wrapped.__aenter__()\n        return self\n\n    async def __aexit__(self, *args: Any) -> bool | None:\n        return await self.wrapped.__aexit__(*args)\n\n    async def call_tool(\n        self,\n        name: str,\n        tool_args: dict[str, Any],\n        ctx: RunContext[AgentDepsT],\n        tool: ToolsetTool[AgentDepsT],\n    ) -> ToolResult:\n        \"\"\"Call an MCP tool, wrapped as a Prefect task with a descriptive name.\"\"\"\n        return await self._call_tool_task.with_options(name=f'Call MCP Tool: {name}', **self._task_config)(\n            name, tool_args, ctx, tool\n        )\n\n```\n\n#### call_tool\n\n```python\ncall_tool(\n    name: str,\n    tool_args: dict[str, Any],\n    ctx: RunContext[AgentDepsT],\n    tool: ToolsetTool[AgentDepsT],\n) -> ToolResult\n\n```\n\nCall an MCP tool, wrapped as a Prefect task with a descriptive name.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py`\n\n```python\nasync def call_tool(\n    self,\n    name: str,\n    tool_args: dict[str, Any],\n    ctx: RunContext[AgentDepsT],\n    tool: ToolsetTool[AgentDepsT],\n) -> ToolResult:\n    \"\"\"Call an MCP tool, wrapped as a Prefect task with a descriptive name.\"\"\"\n    return await self._call_tool_task.with_options(name=f'Call MCP Tool: {name}', **self._task_config)(\n        name, tool_args, ctx, tool\n    )\n\n```\n\n### PrefectModel\n\nBases: `WrapperModel`\n\nA wrapper for Model that integrates with Prefect, turning request and request_stream into Prefect tasks.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py`\n\n```python\nclass PrefectModel(WrapperModel):\n    \"\"\"A wrapper for Model that integrates with Prefect, turning request and request_stream into Prefect tasks.\"\"\"\n\n    def __init__(\n        self,\n        model: Any,\n        *,\n        task_config: TaskConfig,\n        event_stream_handler: EventStreamHandler[Any] | None = None,\n    ):\n        super().__init__(model)\n        self.task_config = default_task_config | (task_config or {})\n        self.event_stream_handler = event_stream_handler\n\n        @task\n        async def wrapped_request(\n            messages: list[ModelMessage],\n            model_settings: ModelSettings | None,\n            model_request_parameters: ModelRequestParameters,\n        ) -> ModelResponse:\n            response = await super(PrefectModel, self).request(messages, model_settings, model_request_parameters)\n            return response\n\n        self._wrapped_request = wrapped_request\n\n        @task\n        async def request_stream_task(\n            messages: list[ModelMessage],\n            model_settings: ModelSettings | None,\n            model_request_parameters: ModelRequestParameters,\n            ctx: RunContext[Any] | None,\n        ) -> ModelResponse:\n            async with super(PrefectModel, self).request_stream(\n                messages, model_settings, model_request_parameters, ctx\n            ) as streamed_response:\n                if self.event_stream_handler is not None:\n                    assert ctx is not None, (\n                        'A Prefect model cannot be used with `pydantic_ai.direct.model_request_stream()` as it requires a `run_context`. '\n                        'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'\n                    )\n                    await self.event_stream_handler(ctx, streamed_response)\n\n                # Consume the entire stream\n                async for _ in streamed_response:\n                    pass\n            response = streamed_response.get()\n            return response\n\n        self._wrapped_request_stream = request_stream_task\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        \"\"\"Make a model request, wrapped as a Prefect task when in a flow.\"\"\"\n        return await self._wrapped_request.with_options(\n            name=f'Model Request: {self.wrapped.model_name}', **self.task_config\n        )(messages, model_settings, model_request_parameters)\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        \"\"\"Make a streaming model request.\n\n        When inside a Prefect flow, the stream is consumed within a task and\n        a non-streaming response is returned. When not in a flow, behaves normally.\n        \"\"\"\n        # Check if we're in a flow context\n        flow_run_context = FlowRunContext.get()\n\n        # If not in a flow, just call the wrapped request_stream method\n        if flow_run_context is None:\n            async with super().request_stream(\n                messages, model_settings, model_request_parameters, run_context\n            ) as streamed_response:\n                yield streamed_response\n                return\n\n        # If in a flow, consume the stream in a task and return the final response\n        response = await self._wrapped_request_stream.with_options(\n            name=f'Model Request (Streaming): {self.wrapped.model_name}', **self.task_config\n        )(messages, model_settings, model_request_parameters, run_context)\n        yield PrefectStreamedResponse(model_request_parameters, response)\n\n```\n\n#### request\n\n```python\nrequest(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse\n\n```\n\nMake a model request, wrapped as a Prefect task when in a flow.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py`\n\n```python\nasync def request(\n    self,\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse:\n    \"\"\"Make a model request, wrapped as a Prefect task when in a flow.\"\"\"\n    return await self._wrapped_request.with_options(\n        name=f'Model Request: {self.wrapped.model_name}', **self.task_config\n    )(messages, model_settings, model_request_parameters)\n\n```\n\n#### request_stream\n\n```python\nrequest_stream(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]\n\n```\n\nMake a streaming model request.\n\nWhen inside a Prefect flow, the stream is consumed within a task and a non-streaming response is returned. When not in a flow, behaves normally.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py`\n\n```python\n@asynccontextmanager\nasync def request_stream(\n    self,\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]:\n    \"\"\"Make a streaming model request.\n\n    When inside a Prefect flow, the stream is consumed within a task and\n    a non-streaming response is returned. When not in a flow, behaves normally.\n    \"\"\"\n    # Check if we're in a flow context\n    flow_run_context = FlowRunContext.get()\n\n    # If not in a flow, just call the wrapped request_stream method\n    if flow_run_context is None:\n        async with super().request_stream(\n            messages, model_settings, model_request_parameters, run_context\n        ) as streamed_response:\n            yield streamed_response\n            return\n\n    # If in a flow, consume the stream in a task and return the final response\n    response = await self._wrapped_request_stream.with_options(\n        name=f'Model Request (Streaming): {self.wrapped.model_name}', **self.task_config\n    )(messages, model_settings, model_request_parameters, run_context)\n    yield PrefectStreamedResponse(model_request_parameters, response)\n\n```\n\n### TaskConfig\n\nBases: `TypedDict`\n\nConfiguration for a task in Prefect.\n\nThese options are passed to the `@task` decorator.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_types.py`\n\n```python\nclass TaskConfig(TypedDict, total=False):\n    \"\"\"Configuration for a task in Prefect.\n\n    These options are passed to the `@task` decorator.\n    \"\"\"\n\n    retries: int\n    \"\"\"Maximum number of retries for the task.\"\"\"\n\n    retry_delay_seconds: float | list[float]\n    \"\"\"Delay between retries in seconds. Can be a single value or a list for custom backoff.\"\"\"\n\n    timeout_seconds: float\n    \"\"\"Maximum time in seconds for the task to complete.\"\"\"\n\n    cache_policy: CachePolicy\n    \"\"\"Prefect cache policy for the task.\"\"\"\n\n    persist_result: bool\n    \"\"\"Whether to persist the task result.\"\"\"\n\n    result_storage: ResultStorage\n    \"\"\"Prefect result storage for the task. Should be a storage block or a block slug like `s3-bucket/my-storage`.\"\"\"\n\n    log_prints: bool\n    \"\"\"Whether to log print statements from the task.\"\"\"\n\n```\n\n#### retries\n\n```python\nretries: int\n\n```\n\nMaximum number of retries for the task.\n\n#### retry_delay_seconds\n\n```python\nretry_delay_seconds: float | list[float]\n\n```\n\nDelay between retries in seconds. Can be a single value or a list for custom backoff.\n\n#### timeout_seconds\n\n```python\ntimeout_seconds: float\n\n```\n\nMaximum time in seconds for the task to complete.\n\n#### cache_policy\n\n```python\ncache_policy: CachePolicy\n\n```\n\nPrefect cache policy for the task.\n\n#### persist_result\n\n```python\npersist_result: bool\n\n```\n\nWhether to persist the task result.\n\n#### result_storage\n\n```python\nresult_storage: ResultStorage\n\n```\n\nPrefect result storage for the task. Should be a storage block or a block slug like `s3-bucket/my-storage`.\n\n#### log_prints\n\n```python\nlog_prints: bool\n\n```\n\nWhether to log print statements from the task.",
  "content_length": 336035
}