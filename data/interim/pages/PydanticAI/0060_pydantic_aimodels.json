{
  "title": "`pydantic_ai.models`",
  "source_url": null,
  "content": "Logic related to making requests to an LLM.\n\nThe aim here is to make a common interface for different LLMs, so that the rest of the code can be agnostic to the specific LLM being used.\n\n### KnownModelName\n\n```python\nKnownModelName = TypeAliasType(\n    \"KnownModelName\",\n    Literal[\n        \"anthropic:claude-3-5-haiku-20241022\",\n        \"anthropic:claude-3-5-haiku-latest\",\n        \"anthropic:claude-3-7-sonnet-20250219\",\n        \"anthropic:claude-3-7-sonnet-latest\",\n        \"anthropic:claude-3-haiku-20240307\",\n        \"anthropic:claude-3-opus-20240229\",\n        \"anthropic:claude-3-opus-latest\",\n        \"anthropic:claude-4-opus-20250514\",\n        \"anthropic:claude-4-sonnet-20250514\",\n        \"anthropic:claude-haiku-4-5\",\n        \"anthropic:claude-haiku-4-5-20251001\",\n        \"anthropic:claude-opus-4-0\",\n        \"anthropic:claude-opus-4-1-20250805\",\n        \"anthropic:claude-opus-4-20250514\",\n        \"anthropic:claude-opus-4-5\",\n        \"anthropic:claude-opus-4-5-20251101\",\n        \"anthropic:claude-sonnet-4-0\",\n        \"anthropic:claude-sonnet-4-20250514\",\n        \"anthropic:claude-sonnet-4-5\",\n        \"anthropic:claude-sonnet-4-5-20250929\",\n        \"bedrock:amazon.titan-text-express-v1\",\n        \"bedrock:amazon.titan-text-lite-v1\",\n        \"bedrock:amazon.titan-tg1-large\",\n        \"bedrock:anthropic.claude-3-5-haiku-20241022-v1:0\",\n        \"bedrock:anthropic.claude-3-5-sonnet-20240620-v1:0\",\n        \"bedrock:anthropic.claude-3-5-sonnet-20241022-v2:0\",\n        \"bedrock:anthropic.claude-3-7-sonnet-20250219-v1:0\",\n        \"bedrock:anthropic.claude-3-haiku-20240307-v1:0\",\n        \"bedrock:anthropic.claude-3-opus-20240229-v1:0\",\n        \"bedrock:anthropic.claude-3-sonnet-20240229-v1:0\",\n        \"bedrock:anthropic.claude-haiku-4-5-20251001-v1:0\",\n        \"bedrock:anthropic.claude-instant-v1\",\n        \"bedrock:anthropic.claude-opus-4-20250514-v1:0\",\n        \"bedrock:anthropic.claude-sonnet-4-20250514-v1:0\",\n        \"bedrock:anthropic.claude-sonnet-4-5-20250929-v1:0\",\n        \"bedrock:anthropic.claude-v2\",\n        \"bedrock:anthropic.claude-v2:1\",\n        \"bedrock:cohere.command-light-text-v14\",\n        \"bedrock:cohere.command-r-plus-v1:0\",\n        \"bedrock:cohere.command-r-v1:0\",\n        \"bedrock:cohere.command-text-v14\",\n        \"bedrock:eu.anthropic.claude-haiku-4-5-20251001-v1:0\",\n        \"bedrock:eu.anthropic.claude-sonnet-4-20250514-v1:0\",\n        \"bedrock:eu.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n        \"bedrock:global.anthropic.claude-opus-4-5-20251101-v1:0\",\n        \"bedrock:meta.llama3-1-405b-instruct-v1:0\",\n        \"bedrock:meta.llama3-1-70b-instruct-v1:0\",\n        \"bedrock:meta.llama3-1-8b-instruct-v1:0\",\n        \"bedrock:meta.llama3-70b-instruct-v1:0\",\n        \"bedrock:meta.llama3-8b-instruct-v1:0\",\n        \"bedrock:mistral.mistral-7b-instruct-v0:2\",\n        \"bedrock:mistral.mistral-large-2402-v1:0\",\n        \"bedrock:mistral.mistral-large-2407-v1:0\",\n        \"bedrock:mistral.mixtral-8x7b-instruct-v0:1\",\n        \"bedrock:us.amazon.nova-lite-v1:0\",\n        \"bedrock:us.amazon.nova-micro-v1:0\",\n        \"bedrock:us.amazon.nova-pro-v1:0\",\n        \"bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n        \"bedrock:us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n        \"bedrock:us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n        \"bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n        \"bedrock:us.anthropic.claude-3-haiku-20240307-v1:0\",\n        \"bedrock:us.anthropic.claude-3-opus-20240229-v1:0\",\n        \"bedrock:us.anthropic.claude-3-sonnet-20240229-v1:0\",\n        \"bedrock:us.anthropic.claude-haiku-4-5-20251001-v1:0\",\n        \"bedrock:us.anthropic.claude-opus-4-20250514-v1:0\",\n        \"bedrock:us.anthropic.claude-sonnet-4-20250514-v1:0\",\n        \"bedrock:us.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n        \"bedrock:us.meta.llama3-1-70b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-1-8b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-2-11b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-2-1b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-2-3b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-2-90b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-3-70b-instruct-v1:0\",\n        \"cerebras:gpt-oss-120b\",\n        \"cerebras:llama-3.3-70b\",\n        \"cerebras:llama3.1-8b\",\n        \"cerebras:qwen-3-235b-a22b-instruct-2507\",\n        \"cerebras:qwen-3-235b-a22b-thinking-2507\",\n        \"cerebras:qwen-3-32b\",\n        \"cerebras:zai-glm-4.6\",\n        \"cohere:c4ai-aya-expanse-32b\",\n        \"cohere:c4ai-aya-expanse-8b\",\n        \"cohere:command-nightly\",\n        \"cohere:command-r-08-2024\",\n        \"cohere:command-r-plus-08-2024\",\n        \"cohere:command-r7b-12-2024\",\n        \"deepseek:deepseek-chat\",\n        \"deepseek:deepseek-reasoner\",\n        \"google-gla:gemini-flash-latest\",\n        \"google-gla:gemini-flash-lite-latest\",\n        \"google-gla:gemini-2.0-flash\",\n        \"google-gla:gemini-2.0-flash-lite\",\n        \"google-gla:gemini-2.5-flash\",\n        \"google-gla:gemini-2.5-flash-preview-09-2025\",\n        \"google-gla:gemini-2.5-flash-image\",\n        \"google-gla:gemini-2.5-flash-lite\",\n        \"google-gla:gemini-2.5-flash-lite-preview-09-2025\",\n        \"google-gla:gemini-2.5-pro\",\n        \"google-gla:gemini-3-pro-preview\",\n        \"google-gla:gemini-3-pro-image-preview\",\n        \"google-vertex:gemini-flash-latest\",\n        \"google-vertex:gemini-flash-lite-latest\",\n        \"google-vertex:gemini-2.0-flash\",\n        \"google-vertex:gemini-2.0-flash-lite\",\n        \"google-vertex:gemini-2.5-flash\",\n        \"google-vertex:gemini-2.5-flash-preview-09-2025\",\n        \"google-vertex:gemini-2.5-flash-image\",\n        \"google-vertex:gemini-2.5-flash-lite\",\n        \"google-vertex:gemini-2.5-flash-lite-preview-09-2025\",\n        \"google-vertex:gemini-2.5-pro\",\n        \"google-vertex:gemini-3-pro-preview\",\n        \"google-vertex:gemini-3-pro-image-preview\",\n        \"grok:grok-2-image-1212\",\n        \"grok:grok-2-vision-1212\",\n        \"grok:grok-3\",\n        \"grok:grok-3-fast\",\n        \"grok:grok-3-mini\",\n        \"grok:grok-3-mini-fast\",\n        \"grok:grok-4\",\n        \"grok:grok-4-0709\",\n        \"groq:deepseek-r1-distill-llama-70b\",\n        \"groq:deepseek-r1-distill-qwen-32b\",\n        \"groq:distil-whisper-large-v3-en\",\n        \"groq:gemma2-9b-it\",\n        \"groq:llama-3.1-8b-instant\",\n        \"groq:llama-3.2-11b-vision-preview\",\n        \"groq:llama-3.2-1b-preview\",\n        \"groq:llama-3.2-3b-preview\",\n        \"groq:llama-3.2-90b-vision-preview\",\n        \"groq:llama-3.3-70b-specdec\",\n        \"groq:llama-3.3-70b-versatile\",\n        \"groq:llama-guard-3-8b\",\n        \"groq:llama3-70b-8192\",\n        \"groq:llama3-8b-8192\",\n        \"groq:mistral-saba-24b\",\n        \"groq:moonshotai/kimi-k2-instruct\",\n        \"groq:playai-tts\",\n        \"groq:playai-tts-arabic\",\n        \"groq:qwen-2.5-32b\",\n        \"groq:qwen-2.5-coder-32b\",\n        \"groq:qwen-qwq-32b\",\n        \"groq:whisper-large-v3\",\n        \"groq:whisper-large-v3-turbo\",\n        \"heroku:amazon-rerank-1-0\",\n        \"heroku:claude-3-5-haiku\",\n        \"heroku:claude-3-5-sonnet-latest\",\n        \"heroku:claude-3-7-sonnet\",\n        \"heroku:claude-3-haiku\",\n        \"heroku:claude-4-5-haiku\",\n        \"heroku:claude-4-5-sonnet\",\n        \"heroku:claude-4-sonnet\",\n        \"heroku:cohere-rerank-3-5\",\n        \"heroku:gpt-oss-120b\",\n        \"heroku:nova-lite\",\n        \"heroku:nova-pro\",\n        \"huggingface:Qwen/QwQ-32B\",\n        \"huggingface:Qwen/Qwen2.5-72B-Instruct\",\n        \"huggingface:Qwen/Qwen3-235B-A22B\",\n        \"huggingface:Qwen/Qwen3-32B\",\n        \"huggingface:deepseek-ai/DeepSeek-R1\",\n        \"huggingface:meta-llama/Llama-3.3-70B-Instruct\",\n        \"huggingface:meta-llama/Llama-4-Maverick-17B-128E-Instruct\",\n        \"huggingface:meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n        \"mistral:codestral-latest\",\n        \"mistral:mistral-large-latest\",\n        \"mistral:mistral-moderation-latest\",\n        \"mistral:mistral-small-latest\",\n        \"moonshotai:kimi-k2-0711-preview\",\n        \"moonshotai:kimi-latest\",\n        \"moonshotai:kimi-thinking-preview\",\n        \"moonshotai:moonshot-v1-128k\",\n        \"moonshotai:moonshot-v1-128k-vision-preview\",\n        \"moonshotai:moonshot-v1-32k\",\n        \"moonshotai:moonshot-v1-32k-vision-preview\",\n        \"moonshotai:moonshot-v1-8k\",\n        \"moonshotai:moonshot-v1-8k-vision-preview\",\n        \"openai:chatgpt-4o-latest\",\n        \"openai:codex-mini-latest\",\n        \"openai:computer-use-preview\",\n        \"openai:computer-use-preview-2025-03-11\",\n        \"openai:gpt-3.5-turbo\",\n        \"openai:gpt-3.5-turbo-0125\",\n        \"openai:gpt-3.5-turbo-0301\",\n        \"openai:gpt-3.5-turbo-0613\",\n        \"openai:gpt-3.5-turbo-1106\",\n        \"openai:gpt-3.5-turbo-16k\",\n        \"openai:gpt-3.5-turbo-16k-0613\",\n        \"openai:gpt-4\",\n        \"openai:gpt-4-0125-preview\",\n        \"openai:gpt-4-0314\",\n        \"openai:gpt-4-0613\",\n        \"openai:gpt-4-1106-preview\",\n        \"openai:gpt-4-32k\",\n        \"openai:gpt-4-32k-0314\",\n        \"openai:gpt-4-32k-0613\",\n        \"openai:gpt-4-turbo\",\n        \"openai:gpt-4-turbo-2024-04-09\",\n        \"openai:gpt-4-turbo-preview\",\n        \"openai:gpt-4-vision-preview\",\n        \"openai:gpt-4.1\",\n        \"openai:gpt-4.1-2025-04-14\",\n        \"openai:gpt-4.1-mini\",\n        \"openai:gpt-4.1-mini-2025-04-14\",\n        \"openai:gpt-4.1-nano\",\n        \"openai:gpt-4.1-nano-2025-04-14\",\n        \"openai:gpt-4o\",\n        \"openai:gpt-4o-2024-05-13\",\n        \"openai:gpt-4o-2024-08-06\",\n        \"openai:gpt-4o-2024-11-20\",\n        \"openai:gpt-4o-audio-preview\",\n        \"openai:gpt-4o-audio-preview-2024-10-01\",\n        \"openai:gpt-4o-audio-preview-2024-12-17\",\n        \"openai:gpt-4o-audio-preview-2025-06-03\",\n        \"openai:gpt-4o-mini\",\n        \"openai:gpt-4o-mini-2024-07-18\",\n        \"openai:gpt-4o-mini-audio-preview\",\n        \"openai:gpt-4o-mini-audio-preview-2024-12-17\",\n        \"openai:gpt-4o-mini-search-preview\",\n        \"openai:gpt-4o-mini-search-preview-2025-03-11\",\n        \"openai:gpt-4o-search-preview\",\n        \"openai:gpt-4o-search-preview-2025-03-11\",\n        \"openai:gpt-5\",\n        \"openai:gpt-5-2025-08-07\",\n        \"openai:gpt-5-chat-latest\",\n        \"openai:gpt-5-codex\",\n        \"openai:gpt-5-mini\",\n        \"openai:gpt-5-mini-2025-08-07\",\n        \"openai:gpt-5-nano\",\n        \"openai:gpt-5-nano-2025-08-07\",\n        \"openai:gpt-5-pro\",\n        \"openai:gpt-5-pro-2025-10-06\",\n        \"openai:gpt-5.1\",\n        \"openai:gpt-5.1-2025-11-13\",\n        \"openai:gpt-5.1-chat-latest\",\n        \"openai:gpt-5.1-codex\",\n        \"openai:gpt-5.1-mini\",\n        \"openai:o1\",\n        \"openai:o1-2024-12-17\",\n        \"openai:o1-mini\",\n        \"openai:o1-mini-2024-09-12\",\n        \"openai:o1-preview\",\n        \"openai:o1-preview-2024-09-12\",\n        \"openai:o1-pro\",\n        \"openai:o1-pro-2025-03-19\",\n        \"openai:o3\",\n        \"openai:o3-2025-04-16\",\n        \"openai:o3-deep-research\",\n        \"openai:o3-deep-research-2025-06-26\",\n        \"openai:o3-mini\",\n        \"openai:o3-mini-2025-01-31\",\n        \"openai:o3-pro\",\n        \"openai:o3-pro-2025-06-10\",\n        \"openai:o4-mini\",\n        \"openai:o4-mini-2025-04-16\",\n        \"openai:o4-mini-deep-research\",\n        \"openai:o4-mini-deep-research-2025-06-26\",\n        \"test\",\n    ],\n)\n\n```\n\nKnown model names that can be used with the `model` parameter of Agent.\n\n`KnownModelName` is provided as a concise way to specify a model.\n\n### ModelRequestParameters\n\nConfiguration for an agent's request to a model, specifically related to tools and output handling.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\n@dataclass(repr=False, kw_only=True)\nclass ModelRequestParameters:\n    \"\"\"Configuration for an agent's request to a model, specifically related to tools and output handling.\"\"\"\n\n    function_tools: list[ToolDefinition] = field(default_factory=list)\n    builtin_tools: list[AbstractBuiltinTool] = field(default_factory=list)\n\n    output_mode: OutputMode = 'text'\n    output_object: OutputObjectDefinition | None = None\n    output_tools: list[ToolDefinition] = field(default_factory=list)\n    prompted_output_template: str | None = None\n    allow_text_output: bool = True\n    allow_image_output: bool = False\n\n    @cached_property\n    def tool_defs(self) -> dict[str, ToolDefinition]:\n        return {tool_def.name: tool_def for tool_def in [*self.function_tools, *self.output_tools]}\n\n    @cached_property\n    def prompted_output_instructions(self) -> str | None:\n        if self.output_mode == 'prompted' and self.prompted_output_template and self.output_object:\n            return PromptedOutputSchema.build_instructions(self.prompted_output_template, self.output_object)\n        return None\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n```\n\n### Model\n\nBases: `ABC`\n\nAbstract class for a model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\nclass Model(ABC):\n    \"\"\"Abstract class for a model.\"\"\"\n\n    _profile: ModelProfileSpec | None = None\n    _settings: ModelSettings | None = None\n\n    def __init__(\n        self,\n        *,\n        settings: ModelSettings | None = None,\n        profile: ModelProfileSpec | None = None,\n    ) -> None:\n        \"\"\"Initialize the model with optional settings and profile.\n\n        Args:\n            settings: Model-specific settings that will be used as defaults for this model.\n            profile: The model profile to use.\n        \"\"\"\n        self._settings = settings\n        self._profile = profile\n\n    @property\n    def settings(self) -> ModelSettings | None:\n        \"\"\"Get the model settings.\"\"\"\n        return self._settings\n\n    @abstractmethod\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        \"\"\"Make a request to the model.\n\n        This is ultimately called by `pydantic_ai._agent_graph.ModelRequestNode._make_request(...)`.\n        \"\"\"\n        raise NotImplementedError()\n\n    async def count_tokens(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> RequestUsage:\n        \"\"\"Make a request to the model for counting tokens.\"\"\"\n        # This method is not required, but you need to implement it if you want to support `UsageLimits.count_tokens_before_request`.\n        raise NotImplementedError(f'Token counting ahead of the request is not supported by {self.__class__.__name__}')\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        \"\"\"Make a request to the model and return a streaming response.\"\"\"\n        # This method is not required, but you need to implement it if you want to support streamed responses\n        raise NotImplementedError(f'Streamed requests not supported by this {self.__class__.__name__}')\n        # yield is required to make this a generator for type checking\n        # noinspection PyUnreachableCode\n        yield  # pragma: no cover\n\n    def customize_request_parameters(self, model_request_parameters: ModelRequestParameters) -> ModelRequestParameters:\n        \"\"\"Customize the request parameters for the model.\n\n        This method can be overridden by subclasses to modify the request parameters before sending them to the model.\n        In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary\n        for vendor/model-specific reasons.\n        \"\"\"\n        if transformer := self.profile.json_schema_transformer:\n            model_request_parameters = replace(\n                model_request_parameters,\n                function_tools=[_customize_tool_def(transformer, t) for t in model_request_parameters.function_tools],\n                output_tools=[_customize_tool_def(transformer, t) for t in model_request_parameters.output_tools],\n            )\n            if output_object := model_request_parameters.output_object:\n                model_request_parameters = replace(\n                    model_request_parameters,\n                    output_object=_customize_output_object(transformer, output_object),\n                )\n\n        return model_request_parameters\n\n    def prepare_request(\n        self,\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> tuple[ModelSettings | None, ModelRequestParameters]:\n        \"\"\"Prepare request inputs before they are passed to the provider.\n\n        This merges the given `model_settings` with the model's own `settings` attribute and ensures\n        `customize_request_parameters` is applied to the resolved\n        [`ModelRequestParameters`][pydantic_ai.models.ModelRequestParameters]. Subclasses can override this method if\n        they need to customize the preparation flow further, but most implementations should simply call\n        `self.prepare_request(...)` at the start of their `request` (and related) methods.\n        \"\"\"\n        model_settings = merge_model_settings(self.settings, model_settings)\n\n        params = self.customize_request_parameters(model_request_parameters)\n\n        if builtin_tools := params.builtin_tools:\n            # Deduplicate builtin tools\n            params = replace(\n                params,\n                builtin_tools=list({tool.unique_id: tool for tool in builtin_tools}.values()),\n            )\n\n        if params.output_mode == 'auto':\n            output_mode = self.profile.default_structured_output_mode\n            params = replace(\n                params,\n                output_mode=output_mode,\n                allow_text_output=output_mode in ('native', 'prompted'),\n            )\n\n        # Reset irrelevant fields\n        if params.output_tools and params.output_mode != 'tool':\n            params = replace(params, output_tools=[])\n        if params.output_object and params.output_mode not in ('native', 'prompted'):\n            params = replace(params, output_object=None)\n        if params.prompted_output_template and params.output_mode != 'prompted':\n            params = replace(params, prompted_output_template=None)  # pragma: no cover\n\n        # Set default prompted output template\n        if params.output_mode == 'prompted' and not params.prompted_output_template:\n            params = replace(params, prompted_output_template=self.profile.prompted_output_template)\n\n        # Check if output mode is supported\n        if params.output_mode == 'native' and not self.profile.supports_json_schema_output:\n            raise UserError('Native structured output is not supported by this model.')\n        if params.output_mode == 'tool' and not self.profile.supports_tools:\n            raise UserError('Tool output is not supported by this model.')\n        if params.allow_image_output and not self.profile.supports_image_output:\n            raise UserError('Image output is not supported by this model.')\n\n        return model_settings, params\n\n    @property\n    @abstractmethod\n    def model_name(self) -> str:\n        \"\"\"The model name.\"\"\"\n        raise NotImplementedError()\n\n    @cached_property\n    def profile(self) -> ModelProfile:\n        \"\"\"The model profile.\"\"\"\n        _profile = self._profile\n        if callable(_profile):\n            _profile = _profile(self.model_name)\n\n        if _profile is None:\n            return DEFAULT_PROFILE\n\n        return _profile\n\n    @property\n    @abstractmethod\n    def system(self) -> str:\n        \"\"\"The model provider, ex: openai.\n\n        Use to populate the `gen_ai.system` OpenTelemetry semantic convention attribute,\n        so should use well-known values listed in\n        https://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/#gen-ai-system\n        when applicable.\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def base_url(self) -> str | None:\n        \"\"\"The base URL for the provider API, if available.\"\"\"\n        return None\n\n    @staticmethod\n    def _get_instructions(\n        messages: list[ModelMessage], model_request_parameters: ModelRequestParameters | None = None\n    ) -> str | None:\n        \"\"\"Get instructions from the first ModelRequest found when iterating messages in reverse.\n\n        In the case that a \"mock\" request was generated to include a tool-return part for a result tool,\n        we want to use the instructions from the second-to-most-recent request (which should correspond to the\n        original request that generated the response that resulted in the tool-return part).\n        \"\"\"\n        instructions = None\n\n        last_two_requests: list[ModelRequest] = []\n        for message in reversed(messages):\n            if isinstance(message, ModelRequest):\n                last_two_requests.append(message)\n                if len(last_two_requests) == 2:\n                    break\n                if message.instructions is not None:\n                    instructions = message.instructions\n                    break\n\n        # If we don't have two requests, and we didn't already return instructions, there are definitely not any:\n        if instructions is None and len(last_two_requests) == 2:\n            most_recent_request = last_two_requests[0]\n            second_most_recent_request = last_two_requests[1]\n\n            # If we've gotten this far and the most recent request consists of only tool-return parts or retry-prompt parts,\n            # we use the instructions from the second-to-most-recent request. This is necessary because when handling\n            # result tools, we generate a \"mock\" ModelRequest with a tool-return part for it, and that ModelRequest will not\n            # have the relevant instructions from the agent.\n\n            # While it's possible that you could have a message history where the most recent request has only tool returns,\n            # I believe there is no way to achieve that would _change_ the instructions without manually crafting the most\n            # recent message. That might make sense in principle for some usage pattern, but it's enough of an edge case\n            # that I think it's not worth worrying about, since you can work around this by inserting another ModelRequest\n            # with no parts at all immediately before the request that has the tool calls (that works because we only look\n            # at the two most recent ModelRequests here).\n\n            # If you have a use case where this causes pain, please open a GitHub issue and we can discuss alternatives.\n\n            if all(p.part_kind == 'tool-return' or p.part_kind == 'retry-prompt' for p in most_recent_request.parts):\n                instructions = second_most_recent_request.instructions\n\n        if model_request_parameters and (output_instructions := model_request_parameters.prompted_output_instructions):\n            if instructions:\n                instructions = '\\n\\n'.join([instructions, output_instructions])\n            else:\n                instructions = output_instructions\n\n        return instructions\n\n```\n\n#### __init__\n\n```python\n__init__(\n    *,\n    settings: ModelSettings | None = None,\n    profile: ModelProfileSpec | None = None\n) -> None\n\n```\n\nInitialize the model with optional settings and profile.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` | | `profile` | `ModelProfileSpec | None` | The model profile to use. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    settings: ModelSettings | None = None,\n    profile: ModelProfileSpec | None = None,\n) -> None:\n    \"\"\"Initialize the model with optional settings and profile.\n\n    Args:\n        settings: Model-specific settings that will be used as defaults for this model.\n        profile: The model profile to use.\n    \"\"\"\n    self._settings = settings\n    self._profile = profile\n\n```\n\n#### settings\n\n```python\nsettings: ModelSettings | None\n\n```\n\nGet the model settings.\n\n#### request\n\n```python\nrequest(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse\n\n```\n\nMake a request to the model.\n\nThis is ultimately called by `pydantic_ai._agent_graph.ModelRequestNode._make_request(...)`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\n@abstractmethod\nasync def request(\n    self,\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse:\n    \"\"\"Make a request to the model.\n\n    This is ultimately called by `pydantic_ai._agent_graph.ModelRequestNode._make_request(...)`.\n    \"\"\"\n    raise NotImplementedError()\n\n```\n\n#### count_tokens\n\n```python\ncount_tokens(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> RequestUsage\n\n```\n\nMake a request to the model for counting tokens.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\nasync def count_tokens(\n    self,\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> RequestUsage:\n    \"\"\"Make a request to the model for counting tokens.\"\"\"\n    # This method is not required, but you need to implement it if you want to support `UsageLimits.count_tokens_before_request`.\n    raise NotImplementedError(f'Token counting ahead of the request is not supported by {self.__class__.__name__}')\n\n```\n\n#### request_stream\n\n```python\nrequest_stream(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]\n\n```\n\nMake a request to the model and return a streaming response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\n@asynccontextmanager\nasync def request_stream(\n    self,\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]:\n    \"\"\"Make a request to the model and return a streaming response.\"\"\"\n    # This method is not required, but you need to implement it if you want to support streamed responses\n    raise NotImplementedError(f'Streamed requests not supported by this {self.__class__.__name__}')\n    # yield is required to make this a generator for type checking\n    # noinspection PyUnreachableCode\n    yield  # pragma: no cover\n\n```\n\n#### customize_request_parameters\n\n```python\ncustomize_request_parameters(\n    model_request_parameters: ModelRequestParameters,\n) -> ModelRequestParameters\n\n```\n\nCustomize the request parameters for the model.\n\nThis method can be overridden by subclasses to modify the request parameters before sending them to the model. In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary for vendor/model-specific reasons.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\ndef customize_request_parameters(self, model_request_parameters: ModelRequestParameters) -> ModelRequestParameters:\n    \"\"\"Customize the request parameters for the model.\n\n    This method can be overridden by subclasses to modify the request parameters before sending them to the model.\n    In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary\n    for vendor/model-specific reasons.\n    \"\"\"\n    if transformer := self.profile.json_schema_transformer:\n        model_request_parameters = replace(\n            model_request_parameters,\n            function_tools=[_customize_tool_def(transformer, t) for t in model_request_parameters.function_tools],\n            output_tools=[_customize_tool_def(transformer, t) for t in model_request_parameters.output_tools],\n        )\n        if output_object := model_request_parameters.output_object:\n            model_request_parameters = replace(\n                model_request_parameters,\n                output_object=_customize_output_object(transformer, output_object),\n            )\n\n    return model_request_parameters\n\n```\n\n#### prepare_request\n\n```python\nprepare_request(\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> tuple[ModelSettings | None, ModelRequestParameters]\n\n```\n\nPrepare request inputs before they are passed to the provider.\n\nThis merges the given `model_settings` with the model's own `settings` attribute and ensures `customize_request_parameters` is applied to the resolved ModelRequestParameters. Subclasses can override this method if they need to customize the preparation flow further, but most implementations should simply call `self.prepare_request(...)` at the start of their `request` (and related) methods.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\ndef prepare_request(\n    self,\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> tuple[ModelSettings | None, ModelRequestParameters]:\n    \"\"\"Prepare request inputs before they are passed to the provider.\n\n    This merges the given `model_settings` with the model's own `settings` attribute and ensures\n    `customize_request_parameters` is applied to the resolved\n    [`ModelRequestParameters`][pydantic_ai.models.ModelRequestParameters]. Subclasses can override this method if\n    they need to customize the preparation flow further, but most implementations should simply call\n    `self.prepare_request(...)` at the start of their `request` (and related) methods.\n    \"\"\"\n    model_settings = merge_model_settings(self.settings, model_settings)\n\n    params = self.customize_request_parameters(model_request_parameters)\n\n    if builtin_tools := params.builtin_tools:\n        # Deduplicate builtin tools\n        params = replace(\n            params,\n            builtin_tools=list({tool.unique_id: tool for tool in builtin_tools}.values()),\n        )\n\n    if params.output_mode == 'auto':\n        output_mode = self.profile.default_structured_output_mode\n        params = replace(\n            params,\n            output_mode=output_mode,\n            allow_text_output=output_mode in ('native', 'prompted'),\n        )\n\n    # Reset irrelevant fields\n    if params.output_tools and params.output_mode != 'tool':\n        params = replace(params, output_tools=[])\n    if params.output_object and params.output_mode not in ('native', 'prompted'):\n        params = replace(params, output_object=None)\n    if params.prompted_output_template and params.output_mode != 'prompted':\n        params = replace(params, prompted_output_template=None)  # pragma: no cover\n\n    # Set default prompted output template\n    if params.output_mode == 'prompted' and not params.prompted_output_template:\n        params = replace(params, prompted_output_template=self.profile.prompted_output_template)\n\n    # Check if output mode is supported\n    if params.output_mode == 'native' and not self.profile.supports_json_schema_output:\n        raise UserError('Native structured output is not supported by this model.')\n    if params.output_mode == 'tool' and not self.profile.supports_tools:\n        raise UserError('Tool output is not supported by this model.')\n    if params.allow_image_output and not self.profile.supports_image_output:\n        raise UserError('Image output is not supported by this model.')\n\n    return model_settings, params\n\n```\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nThe model name.\n\n#### profile\n\n```python\nprofile: ModelProfile\n\n```\n\nThe model profile.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe model provider, ex: openai.\n\nUse to populate the `gen_ai.system` OpenTelemetry semantic convention attribute, so should use well-known values listed in https://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/#gen-ai-system when applicable.\n\n#### base_url\n\n```python\nbase_url: str | None\n\n```\n\nThe base URL for the provider API, if available.\n\n### StreamedResponse\n\nBases: `ABC`\n\nStreamed response from an LLM when calling a tool.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\n@dataclass\nclass StreamedResponse(ABC):\n    \"\"\"Streamed response from an LLM when calling a tool.\"\"\"\n\n    model_request_parameters: ModelRequestParameters\n\n    final_result_event: FinalResultEvent | None = field(default=None, init=False)\n\n    provider_response_id: str | None = field(default=None, init=False)\n    provider_details: dict[str, Any] | None = field(default=None, init=False)\n    finish_reason: FinishReason | None = field(default=None, init=False)\n\n    _parts_manager: ModelResponsePartsManager = field(default_factory=ModelResponsePartsManager, init=False)\n    _event_iterator: AsyncIterator[ModelResponseStreamEvent] | None = field(default=None, init=False)\n    _usage: RequestUsage = field(default_factory=RequestUsage, init=False)\n\n    def __aiter__(self) -> AsyncIterator[ModelResponseStreamEvent]:\n        \"\"\"Stream the response as an async iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\n\n        This proxies the `_event_iterator()` and emits all events, while also checking for matches\n        on the result schema and emitting a [`FinalResultEvent`][pydantic_ai.messages.FinalResultEvent] if/when the\n        first match is found.\n        \"\"\"\n        if self._event_iterator is None:\n\n            async def iterator_with_final_event(\n                iterator: AsyncIterator[ModelResponseStreamEvent],\n            ) -> AsyncIterator[ModelResponseStreamEvent]:\n                async for event in iterator:\n                    yield event\n                    if (\n                        final_result_event := _get_final_result_event(event, self.model_request_parameters)\n                    ) is not None:\n                        self.final_result_event = final_result_event\n                        yield final_result_event\n                        break\n\n                # If we broke out of the above loop, we need to yield the rest of the events\n                # If we didn't, this will just be a no-op\n                async for event in iterator:\n                    yield event\n\n            async def iterator_with_part_end(\n                iterator: AsyncIterator[ModelResponseStreamEvent],\n            ) -> AsyncIterator[ModelResponseStreamEvent]:\n                last_start_event: PartStartEvent | None = None\n\n                def part_end_event(next_part: ModelResponsePart | None = None) -> PartEndEvent | None:\n                    if not last_start_event:\n                        return None\n\n                    index = last_start_event.index\n                    part = self._parts_manager.get_parts()[index]\n                    if not isinstance(part, TextPart | ThinkingPart | BaseToolCallPart):\n                        # Parts other than these 3 don't have deltas, so don't need an end part.\n                        return None\n\n                    return PartEndEvent(\n                        index=index,\n                        part=part,\n                        next_part_kind=next_part.part_kind if next_part else None,\n                    )\n\n                async for event in iterator:\n                    if isinstance(event, PartStartEvent):\n                        if last_start_event:\n                            end_event = part_end_event(event.part)\n                            if end_event:\n                                yield end_event\n\n                            event.previous_part_kind = last_start_event.part.part_kind\n                        last_start_event = event\n\n                    yield event\n\n                end_event = part_end_event()\n                if end_event:\n                    yield end_event\n\n            self._event_iterator = iterator_with_part_end(iterator_with_final_event(self._get_event_iterator()))\n        return self._event_iterator\n\n    @abstractmethod\n    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\n        \"\"\"Return an async iterator of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\n\n        This method should be implemented by subclasses to translate the vendor-specific stream of events into\n        pydantic_ai-format events.\n\n        It should use the `_parts_manager` to handle deltas, and should update the `_usage` attributes as it goes.\n        \"\"\"\n        raise NotImplementedError()\n        # noinspection PyUnreachableCode\n        yield\n\n    def get(self) -> ModelResponse:\n        \"\"\"Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far.\"\"\"\n        return ModelResponse(\n            parts=self._parts_manager.get_parts(),\n            model_name=self.model_name,\n            timestamp=self.timestamp,\n            usage=self.usage(),\n            provider_name=self.provider_name,\n            provider_response_id=self.provider_response_id,\n            provider_details=self.provider_details,\n            finish_reason=self.finish_reason,\n        )\n\n    # TODO (v2): Make this a property\n    def usage(self) -> RequestUsage:\n        \"\"\"Get the usage of the response so far. This will not be the final usage until the stream is exhausted.\"\"\"\n        return self._usage\n\n    @property\n    @abstractmethod\n    def model_name(self) -> str:\n        \"\"\"Get the model name of the response.\"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def provider_name(self) -> str | None:\n        \"\"\"Get the provider name.\"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        raise NotImplementedError()\n\n```\n\n#### __aiter__\n\n```python\n__aiter__() -> AsyncIterator[ModelResponseStreamEvent]\n\n```\n\nStream the response as an async iterable of ModelResponseStreamEvents.\n\nThis proxies the `_event_iterator()` and emits all events, while also checking for matches on the result schema and emitting a FinalResultEvent if/when the first match is found.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\ndef __aiter__(self) -> AsyncIterator[ModelResponseStreamEvent]:\n    \"\"\"Stream the response as an async iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\n\n    This proxies the `_event_iterator()` and emits all events, while also checking for matches\n    on the result schema and emitting a [`FinalResultEvent`][pydantic_ai.messages.FinalResultEvent] if/when the\n    first match is found.\n    \"\"\"\n    if self._event_iterator is None:\n\n        async def iterator_with_final_event(\n            iterator: AsyncIterator[ModelResponseStreamEvent],\n        ) -> AsyncIterator[ModelResponseStreamEvent]:\n            async for event in iterator:\n                yield event\n                if (\n                    final_result_event := _get_final_result_event(event, self.model_request_parameters)\n                ) is not None:\n                    self.final_result_event = final_result_event\n                    yield final_result_event\n                    break\n\n            # If we broke out of the above loop, we need to yield the rest of the events\n            # If we didn't, this will just be a no-op\n            async for event in iterator:\n                yield event\n\n        async def iterator_with_part_end(\n            iterator: AsyncIterator[ModelResponseStreamEvent],\n        ) -> AsyncIterator[ModelResponseStreamEvent]:\n            last_start_event: PartStartEvent | None = None\n\n            def part_end_event(next_part: ModelResponsePart | None = None) -> PartEndEvent | None:\n                if not last_start_event:\n                    return None\n\n                index = last_start_event.index\n                part = self._parts_manager.get_parts()[index]\n                if not isinstance(part, TextPart | ThinkingPart | BaseToolCallPart):\n                    # Parts other than these 3 don't have deltas, so don't need an end part.\n                    return None\n\n                return PartEndEvent(\n                    index=index,\n                    part=part,\n                    next_part_kind=next_part.part_kind if next_part else None,\n                )\n\n            async for event in iterator:\n                if isinstance(event, PartStartEvent):\n                    if last_start_event:\n                        end_event = part_end_event(event.part)\n                        if end_event:\n                            yield end_event\n\n                        event.previous_part_kind = last_start_event.part.part_kind\n                    last_start_event = event\n\n                yield event\n\n            end_event = part_end_event()\n            if end_event:\n                yield end_event\n\n        self._event_iterator = iterator_with_part_end(iterator_with_final_event(self._get_event_iterator()))\n    return self._event_iterator\n\n```\n\n#### get\n\n```python\nget() -> ModelResponse\n\n```\n\nBuild a ModelResponse from the data received from the stream so far.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\ndef get(self) -> ModelResponse:\n    \"\"\"Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far.\"\"\"\n    return ModelResponse(\n        parts=self._parts_manager.get_parts(),\n        model_name=self.model_name,\n        timestamp=self.timestamp,\n        usage=self.usage(),\n        provider_name=self.provider_name,\n        provider_response_id=self.provider_response_id,\n        provider_details=self.provider_details,\n        finish_reason=self.finish_reason,\n    )\n\n```\n\n#### usage\n\n```python\nusage() -> RequestUsage\n\n```\n\nGet the usage of the response so far. This will not be the final usage until the stream is exhausted.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\ndef usage(self) -> RequestUsage:\n    \"\"\"Get the usage of the response so far. This will not be the final usage until the stream is exhausted.\"\"\"\n    return self._usage\n\n```\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nGet the model name of the response.\n\n#### provider_name\n\n```python\nprovider_name: str | None\n\n```\n\nGet the provider name.\n\n#### timestamp\n\n```python\ntimestamp: datetime\n\n```\n\nGet the timestamp of the response.\n\n### ALLOW_MODEL_REQUESTS\n\n```python\nALLOW_MODEL_REQUESTS = True\n\n```\n\nWhether to allow requests to models.\n\nThis global setting allows you to disable request to most models, e.g. to make sure you don't accidentally make costly requests to a model during tests.\n\nThe testing models TestModel and FunctionModel are no affected by this setting.\n\n### check_allow_model_requests\n\n```python\ncheck_allow_model_requests() -> None\n\n```\n\nCheck if model requests are allowed.\n\nIf you're defining your own models that have costs or latency associated with their use, you should call this in Model.request and Model.request_stream.\n\nRaises:\n\n| Type | Description | | --- | --- | | `RuntimeError` | If model requests are not allowed. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\ndef check_allow_model_requests() -> None:\n    \"\"\"Check if model requests are allowed.\n\n    If you're defining your own models that have costs or latency associated with their use, you should call this in\n    [`Model.request`][pydantic_ai.models.Model.request] and [`Model.request_stream`][pydantic_ai.models.Model.request_stream].\n\n    Raises:\n        RuntimeError: If model requests are not allowed.\n    \"\"\"\n    if not ALLOW_MODEL_REQUESTS:\n        raise RuntimeError('Model requests are not allowed, since ALLOW_MODEL_REQUESTS is False')\n\n```\n\n### override_allow_model_requests\n\n```python\noverride_allow_model_requests(\n    allow_model_requests: bool,\n) -> Iterator[None]\n\n```\n\nContext manager to temporarily override ALLOW_MODEL_REQUESTS.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `allow_model_requests` | `bool` | Whether to allow model requests within the context. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n```python\n@contextmanager\ndef override_allow_model_requests(allow_model_requests: bool) -> Iterator[None]:\n    \"\"\"Context manager to temporarily override [`ALLOW_MODEL_REQUESTS`][pydantic_ai.models.ALLOW_MODEL_REQUESTS].\n\n    Args:\n        allow_model_requests: Whether to allow model requests within the context.\n    \"\"\"\n    global ALLOW_MODEL_REQUESTS\n    old_value = ALLOW_MODEL_REQUESTS\n    ALLOW_MODEL_REQUESTS = allow_model_requests  # pyright: ignore[reportConstantRedefinition]\n    try:\n        yield\n    finally:\n        ALLOW_MODEL_REQUESTS = old_value  # pyright: ignore[reportConstantRedefinition]\n\n```",
  "content_length": 45239
}