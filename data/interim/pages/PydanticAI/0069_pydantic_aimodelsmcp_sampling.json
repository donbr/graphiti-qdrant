{
  "title": "pydantic_ai.models.mcp_sampling",
  "source_url": null,
  "content": "### MCPSamplingModelSettings\n\nBases: `ModelSettings`\n\nSettings used for an MCP Sampling model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py`\n\n```python\nclass MCPSamplingModelSettings(ModelSettings, total=False):\n    \"\"\"Settings used for an MCP Sampling model request.\"\"\"\n\n    # ALL FIELDS MUST BE `mcp_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\n    mcp_model_preferences: ModelPreferences\n    \"\"\"Model preferences to use for MCP Sampling.\"\"\"\n\n```\n\n#### mcp_model_preferences\n\n```python\nmcp_model_preferences: ModelPreferences\n\n```\n\nModel preferences to use for MCP Sampling.\n\n### MCPSamplingModel\n\nBases: `Model`\n\nA model that uses MCP Sampling.\n\n[MCP Sampling](https://modelcontextprotocol.io/docs/concepts/sampling) allows an MCP server to make requests to a model by calling back to the MCP client that connected to it.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py`\n\n```python\n@dataclass\nclass MCPSamplingModel(Model):\n    \"\"\"A model that uses MCP Sampling.\n\n    [MCP Sampling](https://modelcontextprotocol.io/docs/concepts/sampling)\n    allows an MCP server to make requests to a model by calling back to the MCP client that connected to it.\n    \"\"\"\n\n    session: ServerSession\n    \"\"\"The MCP server session to use for sampling.\"\"\"\n\n    _: KW_ONLY\n\n    default_max_tokens: int = 16_384\n    \"\"\"Default max tokens to use if not set in [`ModelSettings`][pydantic_ai.settings.ModelSettings.max_tokens].\n\n    Max tokens is a required parameter for MCP Sampling, but optional on\n    [`ModelSettings`][pydantic_ai.settings.ModelSettings], so this value is used as fallback.\n    \"\"\"\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        system_prompt, sampling_messages = _mcp.map_from_pai_messages(messages)\n\n        model_settings, _ = self.prepare_request(model_settings, model_request_parameters)\n        model_settings = cast(MCPSamplingModelSettings, model_settings or {})\n\n        result = await self.session.create_message(\n            sampling_messages,\n            max_tokens=model_settings.get('max_tokens', self.default_max_tokens),\n            system_prompt=system_prompt,\n            temperature=model_settings.get('temperature'),\n            model_preferences=model_settings.get('mcp_model_preferences'),\n            stop_sequences=model_settings.get('stop_sequences'),\n        )\n        if result.role == 'assistant':\n            return ModelResponse(\n                parts=[_mcp.map_from_sampling_content(result.content)],\n                model_name=result.model,\n            )\n        else:\n            raise exceptions.UnexpectedModelBehavior(\n                f'Unexpected result from MCP sampling, expected \"assistant\" role, got {result.role}.'\n            )\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        raise NotImplementedError('MCP Sampling does not support streaming')\n        yield\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"The model name.\n\n        Since the model name isn't known until the request is made, this property always returns `'mcp-sampling'`.\n        \"\"\"\n        return 'mcp-sampling'\n\n    @property\n    def system(self) -> str:\n        \"\"\"The system / model provider, returns `'MCP'`.\"\"\"\n        return 'MCP'\n\n```\n\n#### session\n\n```python\nsession: ServerSession\n\n```\n\nThe MCP server session to use for sampling.\n\n#### default_max_tokens\n\n```python\ndefault_max_tokens: int = 16384\n\n```\n\nDefault max tokens to use if not set in ModelSettings.\n\nMax tokens is a required parameter for MCP Sampling, but optional on ModelSettings, so this value is used as fallback.\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nThe model name.\n\nSince the model name isn't known until the request is made, this property always returns `'mcp-sampling'`.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe system / model provider, returns `'MCP'`.",
  "content_length": 4267
}