{
  "title": "Advanced Tool Features",
  "source_url": null,
  "content": "This page covers advanced features for function tools in Pydantic AI. For basic tool usage, see the [Function Tools](../tools/) documentation.\n\n## Tool Output\n\nTools can return anything that Pydantic can serialize to JSON, as well as audio, video, image or document content depending on the types of [multi-modal input](../input/) the model supports:\n\nfunction_tool_output.py\n\n```python\nfrom datetime import datetime\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, DocumentUrl, ImageUrl\nfrom pydantic_ai.models.openai import OpenAIResponsesModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nagent = Agent(model=OpenAIResponsesModel('gpt-5'))\n\n\n@agent.tool_plain\ndef get_current_time() -> datetime:\n    return datetime.now()\n\n\n@agent.tool_plain\ndef get_user() -> User:\n    return User(name='John', age=30)\n\n\n@agent.tool_plain\ndef get_company_logo() -> ImageUrl:\n    return ImageUrl(url='https://iili.io/3Hs4FMg.png')\n\n\n@agent.tool_plain\ndef get_document() -> DocumentUrl:\n    return DocumentUrl(url='https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf')\n\n\nresult = agent.run_sync('What time is it?')\nprint(result.output)\n#> The current time is 10:45 PM on April 17, 2025.\n\nresult = agent.run_sync('What is the user name?')\nprint(result.output)\n#> The user's name is John.\n\nresult = agent.run_sync('What is the company name in the logo?')\nprint(result.output)\n#> The company name in the logo is \"Pydantic.\"\n\nresult = agent.run_sync('What is the main content of the document?')\nprint(result.output)\n#> The document contains just the text \"Dummy PDF file.\"\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nSome models (e.g. Gemini) natively support semi-structured return values, while some expect text (OpenAI) but seem to be just as good at extracting meaning from the data. If a Python object is returned and the model expects a string, the value will be serialized to JSON.\n\n### Advanced Tool Returns\n\nFor scenarios where you need more control over both the tool's return value and the content sent to the model, you can use ToolReturn. This is particularly useful when you want to:\n\n- Provide rich multi-modal content (images, documents, etc.) to the model as context\n- Separate the programmatic return value from the model's context\n- Include additional metadata that shouldn't be sent to the LLM\n\nHere's an example of a computer automation tool that captures screenshots and provides visual feedback:\n\n[Learn about Gateway](../gateway) advanced_tool_return.py\n\n```python\nimport time\nfrom pydantic_ai import Agent\nfrom pydantic_ai import ToolReturn, BinaryContent\n\nagent = Agent('gateway/openai:gpt-5')\n\n@agent.tool_plain\ndef click_and_capture(x: int, y: int) -> ToolReturn:\n    \"\"\"Click at coordinates and show before/after screenshots.\"\"\"\n    # Take screenshot before action\n    before_screenshot = capture_screen()\n\n    # Perform click operation\n    perform_click(x, y)\n    time.sleep(0.5)  # Wait for UI to update\n\n    # Take screenshot after action\n    after_screenshot = capture_screen()\n\n    return ToolReturn(\n        return_value=f\"Successfully clicked at ({x}, {y})\",\n        content=[\n            f\"Clicked at coordinates ({x}, {y}). Here's the comparison:\",\n            \"Before:\",\n            BinaryContent(data=before_screenshot, media_type=\"image/png\"),\n            \"After:\",\n            BinaryContent(data=after_screenshot, media_type=\"image/png\"),\n            \"Please analyze the changes and suggest next steps.\"\n        ],\n        metadata={\n            \"coordinates\": {\"x\": x, \"y\": y},\n            \"action_type\": \"click_and_capture\",\n            \"timestamp\": time.time()\n        }\n    )\n\n### The model receives the rich visual content for analysis\n### while your application can access the structured return_value and metadata\nresult = agent.run_sync(\"Click on the submit button and tell me what happened\")\nprint(result.output)\n### The model can analyze the screenshots and provide detailed feedback\n\n```\n\nadvanced_tool_return.py\n\n```python\nimport time\nfrom pydantic_ai import Agent\nfrom pydantic_ai import ToolReturn, BinaryContent\n\nagent = Agent('openai:gpt-5')\n\n@agent.tool_plain\ndef click_and_capture(x: int, y: int) -> ToolReturn:\n    \"\"\"Click at coordinates and show before/after screenshots.\"\"\"\n    # Take screenshot before action\n    before_screenshot = capture_screen()\n\n    # Perform click operation\n    perform_click(x, y)\n    time.sleep(0.5)  # Wait for UI to update\n\n    # Take screenshot after action\n    after_screenshot = capture_screen()\n\n    return ToolReturn(\n        return_value=f\"Successfully clicked at ({x}, {y})\",\n        content=[\n            f\"Clicked at coordinates ({x}, {y}). Here's the comparison:\",\n            \"Before:\",\n            BinaryContent(data=before_screenshot, media_type=\"image/png\"),\n            \"After:\",\n            BinaryContent(data=after_screenshot, media_type=\"image/png\"),\n            \"Please analyze the changes and suggest next steps.\"\n        ],\n        metadata={\n            \"coordinates\": {\"x\": x, \"y\": y},\n            \"action_type\": \"click_and_capture\",\n            \"timestamp\": time.time()\n        }\n    )\n\n### The model receives the rich visual content for analysis\n### while your application can access the structured return_value and metadata\nresult = agent.run_sync(\"Click on the submit button and tell me what happened\")\nprint(result.output)\n### The model can analyze the screenshots and provide detailed feedback\n\n```\n\n- **`return_value`**: The actual return value used in the tool response. This is what gets serialized and sent back to the model as the tool's result.\n- **`content`**: A sequence of content (text, images, documents, etc.) that provides additional context to the model. This appears as a separate user message.\n- **`metadata`**: Optional metadata that your application can access but is not sent to the LLM. Useful for logging, debugging, or additional processing. Some other AI frameworks call this feature \"artifacts\".\n\nThis separation allows you to provide rich context to the model while maintaining clean, structured return values for your application logic.\n\n## Custom Tool Schema\n\nIf you have a function that lacks appropriate documentation (i.e. poorly named, no type information, poor docstring, use of \\*args or \\*\\*kwargs and suchlike) then you can still turn it into a tool that can be effectively used by the agent with the Tool.from_schema function. With this you provide the name, description, JSON schema, and whether the function takes a `RunContext` for the function directly:\n\n```python\nfrom pydantic_ai import Agent, Tool\nfrom pydantic_ai.models.test import TestModel\n\n\ndef foobar(**kwargs) -> str:\n    return kwargs['a'] + kwargs['b']\n\ntool = Tool.from_schema(\n    function=foobar,\n    name='sum',\n    description='Sum two numbers.',\n    json_schema={\n        'additionalProperties': False,\n        'properties': {\n            'a': {'description': 'the first number', 'type': 'integer'},\n            'b': {'description': 'the second number', 'type': 'integer'},\n        },\n        'required': ['a', 'b'],\n        'type': 'object',\n    },\n    takes_ctx=False,\n)\n\ntest_model = TestModel()\nagent = Agent(test_model, tools=[tool])\n\nresult = agent.run_sync('testing...')\nprint(result.output)\n#> {\"sum\":0}\n\n```\n\nPlease note that validation of the tool arguments will not be performed, and this will pass all arguments as keyword arguments.\n\n## Dynamic Tools\n\nTools can optionally be defined with another function: `prepare`, which is called at each step of a run to customize the definition of the tool passed to the model, or omit the tool completely from that step.\n\nA `prepare` method can be registered via the `prepare` kwarg to any of the tool registration mechanisms:\n\n- @agent.tool decorator\n- @agent.tool_plain decorator\n- Tool dataclass\n\nThe `prepare` method, should be of type ToolPrepareFunc, a function which takes RunContext and a pre-built ToolDefinition, and should either return that `ToolDefinition` with or without modifying it, return a new `ToolDefinition`, or return `None` to indicate this tools should not be registered for that step.\n\nHere's a simple `prepare` method that only includes the tool if the value of the dependency is `42`.\n\nAs with the previous example, we use TestModel to demonstrate the behavior without calling a real model.\n\ntool_only_if_42.py\n\n```python\nfrom pydantic_ai import Agent, RunContext, ToolDefinition\n\nagent = Agent('test')\n\n\nasync def only_if_42(\n    ctx: RunContext[int], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    if ctx.deps == 42:\n        return tool_def\n\n\n@agent.tool(prepare=only_if_42)\ndef hitchhiker(ctx: RunContext[int], answer: str) -> str:\n    return f'{ctx.deps} {answer}'\n\n\nresult = agent.run_sync('testing...', deps=41)\nprint(result.output)\n#> success (no tool calls)\nresult = agent.run_sync('testing...', deps=42)\nprint(result.output)\n#> {\"hitchhiker\":\"42 a\"}\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nHere's a more complex example where we change the description of the `name` parameter to based on the value of `deps`\n\nFor the sake of variation, we create this tool using the Tool dataclass.\n\ncustomize_name.py\n\n```python\nfrom __future__ import annotations\n\nfrom typing import Literal\n\nfrom pydantic_ai import Agent, RunContext, Tool, ToolDefinition\nfrom pydantic_ai.models.test import TestModel\n\n\ndef greet(name: str) -> str:\n    return f'hello {name}'\n\n\nasync def prepare_greet(\n    ctx: RunContext[Literal['human', 'machine']], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    d = f'Name of the {ctx.deps} to greet.'\n    tool_def.parameters_json_schema['properties']['name']['description'] = d\n    return tool_def\n\n\ngreet_tool = Tool(greet, prepare=prepare_greet)\ntest_model = TestModel()\nagent = Agent(test_model, tools=[greet_tool], deps_type=Literal['human', 'machine'])\n\nresult = agent.run_sync('testing...', deps='human')\nprint(result.output)\n#> {\"greet\":\"hello a\"}\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='greet',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {\n                'name': {'type': 'string', 'description': 'Name of the human to greet.'}\n            },\n            'required': ['name'],\n            'type': 'object',\n        },\n    )\n]\n\"\"\"\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\n### Agent-wide Dynamic Tools\n\nIn addition to per-tool `prepare` methods, you can also define an agent-wide `prepare_tools` function. This function is called at each step of a run and allows you to filter or modify the list of all tool definitions available to the agent for that step. This is especially useful if you want to enable or disable multiple tools at once, or apply global logic based on the current context.\n\nThe `prepare_tools` function should be of type ToolsPrepareFunc, which takes the RunContext and a list of ToolDefinition, and returns a new list of tool definitions (or `None` to disable all tools for that step).\n\nNote\n\nThe list of tool definitions passed to `prepare_tools` includes both regular function tools and tools from any [toolsets](../toolsets/) registered on the agent, but not [output tools](../output/#tool-output).\n\nTo modify output tools, you can set a `prepare_output_tools` function instead.\n\nHere's an example that makes all tools strict if the model is an OpenAI model:\n\nagent_prepare_tools_customize.py\n\n```python\nfrom dataclasses import replace\n\nfrom pydantic_ai import Agent, RunContext, ToolDefinition\nfrom pydantic_ai.models.test import TestModel\n\n\nasync def turn_on_strict_if_openai(\n    ctx: RunContext[None], tool_defs: list[ToolDefinition]\n) -> list[ToolDefinition] | None:\n    if ctx.model.system == 'openai':\n        return [replace(tool_def, strict=True) for tool_def in tool_defs]\n    return tool_defs\n\n\ntest_model = TestModel()\nagent = Agent(test_model, prepare_tools=turn_on_strict_if_openai)\n\n\n@agent.tool_plain\ndef echo(message: str) -> str:\n    return message\n\n\nagent.run_sync('testing...')\nassert test_model.last_model_request_parameters.function_tools[0].strict is None\n\n### Set the system attribute of the test_model to 'openai'\ntest_model._system = 'openai'\n\nagent.run_sync('testing with openai...')\nassert test_model.last_model_request_parameters.function_tools[0].strict\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nHere's another example that conditionally filters out the tools by name if the dependency (`ctx.deps`) is `True`:\n\nagent_prepare_tools_filter_out.py\n\n```python\nfrom pydantic_ai import Agent, RunContext, Tool, ToolDefinition\n\n\ndef launch_potato(target: str) -> str:\n    return f'Potato launched at {target}!'\n\n\nasync def filter_out_tools_by_name(\n    ctx: RunContext[bool], tool_defs: list[ToolDefinition]\n) -> list[ToolDefinition] | None:\n    if ctx.deps:\n        return [tool_def for tool_def in tool_defs if tool_def.name != 'launch_potato']\n    return tool_defs\n\n\nagent = Agent(\n    'test',\n    tools=[Tool(launch_potato)],\n    prepare_tools=filter_out_tools_by_name,\n    deps_type=bool,\n)\n\nresult = agent.run_sync('testing...', deps=False)\nprint(result.output)\n#> {\"launch_potato\":\"Potato launched at a!\"}\nresult = agent.run_sync('testing...', deps=True)\nprint(result.output)\n#> success (no tool calls)\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nYou can use `prepare_tools` to:\n\n- Dynamically enable or disable tools based on the current model, dependencies, or other context\n- Modify tool definitions globally (e.g., set all tools to strict mode, change descriptions, etc.)\n\nIf both per-tool `prepare` and agent-wide `prepare_tools` are used, the per-tool `prepare` is applied first to each tool, and then `prepare_tools` is called with the resulting list of tool definitions.\n\n## Tool Execution and Retries\n\nWhen a tool is executed, its arguments (provided by the LLM) are first validated against the function's signature using Pydantic (with optional [validation context](../output/#validation-context)). If validation fails (e.g., due to incorrect types or missing required arguments), a `ValidationError` is raised, and the framework automatically generates a RetryPromptPart containing the validation details. This prompt is sent back to the LLM, informing it of the error and allowing it to correct the parameters and retry the tool call.\n\nBeyond automatic validation errors, the tool's own internal logic can also explicitly request a retry by raising the ModelRetry exception. This is useful for situations where the parameters were technically valid, but an issue occurred during execution (like a transient network error, or the tool determining the initial attempt needs modification).\n\n```python\nfrom pydantic_ai import ModelRetry\n\n\ndef my_flaky_tool(query: str) -> str:\n    if query == 'bad':\n        # Tell the LLM the query was bad and it should try again\n        raise ModelRetry(\"The query 'bad' is not allowed. Please provide a different query.\")\n    # ... process query ...\n    return 'Success!'\n\n```\n\nRaising `ModelRetry` also generates a `RetryPromptPart` containing the exception message, which is sent back to the LLM to guide its next attempt. Both `ValidationError` and `ModelRetry` respect the `retries` setting configured on the `Tool` or `Agent`.\n\n### Parallel tool calls & concurrency\n\nWhen a model returns multiple tool calls in one response, Pydantic AI schedules them concurrently using `asyncio.create_task`. If a tool requires sequential/serial execution, you can pass the sequential flag when registering the tool, or wrap the agent run in the with agent.sequential_tool_calls() context manager.\n\nAsync functions are run on the event loop, while sync functions are offloaded to threads. To get the best performance, *always* use an async function *unless* you're doing blocking I/O (and there's no way to use a non-blocking library instead) or CPU-bound work (like `numpy` or `scikit-learn` operations), so that simple functions are not offloaded to threads unnecessarily.\n\nLimiting tool executions\n\nYou can cap tool executions within a run using [`UsageLimits(tool_calls_limit=...)`](../agents/#usage-limits). The counter increments only after a successful tool invocation. Output tools (used for [structured output](../output/)) are not counted in the `tool_calls` metric.\n\n## See Also\n\n- [Function Tools](../tools/) - Basic tool concepts and registration\n- [Toolsets](../toolsets/) - Managing collections of tools\n- [Deferred Tools](../deferred-tools/) - Tools requiring approval or external execution\n- [Third-Party Tools](../third-party-tools/) - Integrations with external tool libraries",
  "content_length": 16667
}