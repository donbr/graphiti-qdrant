{
  "title": "`pydantic_ai.settings`",
  "source_url": null,
  "content": "### ModelSettings\n\nBases: `TypedDict`\n\nSettings to configure an LLM.\n\nHere we include only settings which apply to multiple models / model providers, though not all of these settings are supported by all models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/settings.py`\n\n```python\nclass ModelSettings(TypedDict, total=False):\n    \"\"\"Settings to configure an LLM.\n\n    Here we include only settings which apply to multiple models / model providers,\n    though not all of these settings are supported by all models.\n    \"\"\"\n\n    max_tokens: int\n    \"\"\"The maximum number of tokens to generate before stopping.\n\n    Supported by:\n\n    * Gemini\n    * Anthropic\n    * OpenAI\n    * Groq\n    * Cohere\n    * Mistral\n    * Bedrock\n    * MCP Sampling\n    * Outlines (all providers)\n    \"\"\"\n\n    temperature: float\n    \"\"\"Amount of randomness injected into the response.\n\n    Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's\n    maximum `temperature` for creative and generative tasks.\n\n    Note that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\n    Supported by:\n\n    * Gemini\n    * Anthropic\n    * OpenAI\n    * Groq\n    * Cohere\n    * Mistral\n    * Bedrock\n    * Outlines (Transformers, LlamaCpp, SgLang, VLLMOffline)\n    \"\"\"\n\n    top_p: float\n    \"\"\"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.\n\n    So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\n    You should either alter `temperature` or `top_p`, but not both.\n\n    Supported by:\n\n    * Gemini\n    * Anthropic\n    * OpenAI\n    * Groq\n    * Cohere\n    * Mistral\n    * Bedrock\n    * Outlines (Transformers, LlamaCpp, SgLang, VLLMOffline)\n    \"\"\"\n\n    timeout: float | Timeout\n    \"\"\"Override the client-level default timeout for a request, in seconds.\n\n    Supported by:\n\n    * Gemini\n    * Anthropic\n    * OpenAI\n    * Groq\n    * Mistral\n    \"\"\"\n\n    parallel_tool_calls: bool\n    \"\"\"Whether to allow parallel tool calls.\n\n    Supported by:\n\n    * OpenAI (some models, not o1)\n    * Groq\n    * Anthropic\n    \"\"\"\n\n    seed: int\n    \"\"\"The random seed to use for the model, theoretically allowing for deterministic results.\n\n    Supported by:\n\n    * OpenAI\n    * Groq\n    * Cohere\n    * Mistral\n    * Gemini\n    * Outlines (LlamaCpp, VLLMOffline)\n    \"\"\"\n\n    presence_penalty: float\n    \"\"\"Penalize new tokens based on whether they have appeared in the text so far.\n\n    Supported by:\n\n    * OpenAI\n    * Groq\n    * Cohere\n    * Gemini\n    * Mistral\n    * Outlines (LlamaCpp, SgLang, VLLMOffline)\n    \"\"\"\n\n    frequency_penalty: float\n    \"\"\"Penalize new tokens based on their existing frequency in the text so far.\n\n    Supported by:\n\n    * OpenAI\n    * Groq\n    * Cohere\n    * Gemini\n    * Mistral\n    * Outlines (LlamaCpp, SgLang, VLLMOffline)\n    \"\"\"\n\n    logit_bias: dict[str, int]\n    \"\"\"Modify the likelihood of specified tokens appearing in the completion.\n\n    Supported by:\n\n    * OpenAI\n    * Groq\n    * Outlines (Transformers, LlamaCpp, VLLMOffline)\n    \"\"\"\n\n    stop_sequences: list[str]\n    \"\"\"Sequences that will cause the model to stop generating.\n\n    Supported by:\n\n    * OpenAI\n    * Anthropic\n    * Bedrock\n    * Mistral\n    * Groq\n    * Cohere\n    * Google\n    \"\"\"\n\n    extra_headers: dict[str, str]\n    \"\"\"Extra headers to send to the model.\n\n    Supported by:\n\n    * OpenAI\n    * Anthropic\n    * Groq\n    \"\"\"\n\n    extra_body: object\n    \"\"\"Extra body to send to the model.\n\n    Supported by:\n\n    * OpenAI\n    * Anthropic\n    * Groq\n    * Outlines (all providers)\n    \"\"\"\n\n```\n\n#### max_tokens\n\n```python\nmax_tokens: int\n\n```\n\nThe maximum number of tokens to generate before stopping.\n\nSupported by:\n\n- Gemini\n- Anthropic\n- OpenAI\n- Groq\n- Cohere\n- Mistral\n- Bedrock\n- MCP Sampling\n- Outlines (all providers)\n\n#### temperature\n\n```python\ntemperature: float\n\n```\n\nAmount of randomness injected into the response.\n\nUse `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's maximum `temperature` for creative and generative tasks.\n\nNote that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\nSupported by:\n\n- Gemini\n- Anthropic\n- OpenAI\n- Groq\n- Cohere\n- Mistral\n- Bedrock\n- Outlines (Transformers, LlamaCpp, SgLang, VLLMOffline)\n\n#### top_p\n\n```python\ntop_p: float\n\n```\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.\n\nSo 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nYou should either alter `temperature` or `top_p`, but not both.\n\nSupported by:\n\n- Gemini\n- Anthropic\n- OpenAI\n- Groq\n- Cohere\n- Mistral\n- Bedrock\n- Outlines (Transformers, LlamaCpp, SgLang, VLLMOffline)\n\n#### timeout\n\n```python\ntimeout: float | Timeout\n\n```\n\nOverride the client-level default timeout for a request, in seconds.\n\nSupported by:\n\n- Gemini\n- Anthropic\n- OpenAI\n- Groq\n- Mistral\n\n#### parallel_tool_calls\n\n```python\nparallel_tool_calls: bool\n\n```\n\nWhether to allow parallel tool calls.\n\nSupported by:\n\n- OpenAI (some models, not o1)\n- Groq\n- Anthropic\n\n#### seed\n\n```python\nseed: int\n\n```\n\nThe random seed to use for the model, theoretically allowing for deterministic results.\n\nSupported by:\n\n- OpenAI\n- Groq\n- Cohere\n- Mistral\n- Gemini\n- Outlines (LlamaCpp, VLLMOffline)\n\n#### presence_penalty\n\n```python\npresence_penalty: float\n\n```\n\nPenalize new tokens based on whether they have appeared in the text so far.\n\nSupported by:\n\n- OpenAI\n- Groq\n- Cohere\n- Gemini\n- Mistral\n- Outlines (LlamaCpp, SgLang, VLLMOffline)\n\n#### frequency_penalty\n\n```python\nfrequency_penalty: float\n\n```\n\nPenalize new tokens based on their existing frequency in the text so far.\n\nSupported by:\n\n- OpenAI\n- Groq\n- Cohere\n- Gemini\n- Mistral\n- Outlines (LlamaCpp, SgLang, VLLMOffline)\n\n#### logit_bias\n\n```python\nlogit_bias: dict[str, int]\n\n```\n\nModify the likelihood of specified tokens appearing in the completion.\n\nSupported by:\n\n- OpenAI\n- Groq\n- Outlines (Transformers, LlamaCpp, VLLMOffline)\n\n#### stop_sequences\n\n```python\nstop_sequences: list[str]\n\n```\n\nSequences that will cause the model to stop generating.\n\nSupported by:\n\n- OpenAI\n- Anthropic\n- Bedrock\n- Mistral\n- Groq\n- Cohere\n- Google\n\n#### extra_headers\n\n```python\nextra_headers: dict[str, str]\n\n```\n\nExtra headers to send to the model.\n\nSupported by:\n\n- OpenAI\n- Anthropic\n- Groq\n\n#### extra_body\n\n```python\nextra_body: object\n\n```\n\nExtra body to send to the model.\n\nSupported by:\n\n- OpenAI\n- Anthropic\n- Groq\n- Outlines (all providers)",
  "content_length": 6651
}