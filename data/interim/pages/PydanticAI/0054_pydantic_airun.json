{
  "title": "`pydantic_ai.run`",
  "source_url": null,
  "content": "### AgentRun\n\nBases: `Generic[AgentDepsT, OutputDataT]`\n\nA stateful, async-iterable run of an Agent.\n\nYou generally obtain an `AgentRun` instance by calling `async with my_agent.iter(...) as agent_run:`.\n\nOnce you have an instance, you can use it to iterate through the run's nodes as they execute. When an End is reached, the run finishes and result becomes available.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    # Iterate through the run, recording each node along the way:\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n\n```\n\nYou can also manually drive the iteration using the next method for more granular control.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n````python\n@dataclasses.dataclass(repr=False)\nclass AgentRun(Generic[AgentDepsT, OutputDataT]):\n    \"\"\"A stateful, async-iterable run of an [`Agent`][pydantic_ai.agent.Agent].\n\n    You generally obtain an `AgentRun` instance by calling `async with my_agent.iter(...) as agent_run:`.\n\n    Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an\n    [`End`][pydantic_graph.nodes.End] is reached, the run finishes and [`result`][pydantic_ai.agent.AgentRun.result]\n    becomes available.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        nodes = []\n        # Iterate through the run, recording each node along the way:\n        async with agent.iter('What is the capital of France?') as agent_run:\n            async for node in agent_run:\n                nodes.append(node)\n        print(nodes)\n        '''\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ],\n                    run_id='...',\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                    run_id='...',\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        '''\n        print(agent_run.result.output)\n        #> The capital of France is Paris.\n    ```\n\n    You can also manually drive the iteration using the [`next`][pydantic_ai.agent.AgentRun.next] method for\n    more granular control.\n    \"\"\"\n\n    _graph_run: GraphRun[\n        _agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any], FinalResult[OutputDataT]\n    ]\n\n    @overload\n    def _traceparent(self, *, required: Literal[False]) -> str | None: ...\n    @overload\n    def _traceparent(self) -> str: ...\n    def _traceparent(self, *, required: bool = True) -> str | None:\n        traceparent = self._graph_run._traceparent(required=False)  # type: ignore[reportPrivateUsage]\n        if traceparent is None and required:  # pragma: no cover\n            raise AttributeError('No span was created for this agent run')\n        return traceparent\n\n    @property\n    def ctx(self) -> GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]]:\n        \"\"\"The current context of the agent run.\"\"\"\n        return GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]](\n            state=self._graph_run.state, deps=self._graph_run.deps\n        )\n\n    @property\n    def next_node(\n        self,\n    ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n        \"\"\"The next node that will be run in the agent graph.\n\n        This is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\n        \"\"\"\n        task = self._graph_run.next_task\n        return self._task_to_node(task)\n\n    @property\n    def result(self) -> AgentRunResult[OutputDataT] | None:\n        \"\"\"The final result of the run if it has ended, otherwise `None`.\n\n        Once the run returns an [`End`][pydantic_graph.nodes.End] node, `result` is populated\n        with an [`AgentRunResult`][pydantic_ai.agent.AgentRunResult].\n        \"\"\"\n        graph_run_output = self._graph_run.output\n        if graph_run_output is None:\n            return None\n        return AgentRunResult(\n            graph_run_output.output,\n            graph_run_output.tool_name,\n            self._graph_run.state,\n            self._graph_run.deps.new_message_index,\n            self._traceparent(required=False),\n        )\n\n    def all_messages(self) -> list[_messages.ModelMessage]:\n        \"\"\"Return all messages for the run so far.\n\n        Messages from older runs are included.\n        \"\"\"\n        return self.ctx.state.message_history\n\n    def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:\n        \"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRun.all_messages] as JSON bytes.\n\n        Returns:\n            JSON bytes representing the messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(self.all_messages())\n\n    def new_messages(self) -> list[_messages.ModelMessage]:\n        \"\"\"Return new messages for the run so far.\n\n        Messages from older runs are excluded.\n        \"\"\"\n        return self.all_messages()[self.ctx.deps.new_message_index :]\n\n    def new_messages_json(self) -> bytes:\n        \"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRun.new_messages] as JSON bytes.\n\n        Returns:\n            JSON bytes representing the new messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(self.new_messages())\n\n    def __aiter__(\n        self,\n    ) -> AsyncIterator[_agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]]:\n        \"\"\"Provide async-iteration over the nodes in the agent run.\"\"\"\n        return self\n\n    async def __anext__(\n        self,\n    ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n        \"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\n        task = await anext(self._graph_run)\n        return self._task_to_node(task)\n\n    def _task_to_node(\n        self, task: EndMarker[FinalResult[OutputDataT]] | JoinItem | Sequence[GraphTaskRequest]\n    ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n        if isinstance(task, Sequence) and len(task) == 1:\n            first_task = task[0]\n            if isinstance(first_task.inputs, BaseNode):  # pragma: no branch\n                base_node: BaseNode[\n                    _agent_graph.GraphAgentState,\n                    _agent_graph.GraphAgentDeps[AgentDepsT, OutputDataT],\n                    FinalResult[OutputDataT],\n                ] = first_task.inputs  # type: ignore[reportUnknownMemberType]\n                if _agent_graph.is_agent_node(node=base_node):  # pragma: no branch\n                    return base_node\n        if isinstance(task, EndMarker):\n            return End(task.value)\n        raise exceptions.AgentRunError(f'Unexpected node: {task}')  # pragma: no cover\n\n    def _node_to_task(self, node: _agent_graph.AgentNode[AgentDepsT, OutputDataT]) -> GraphTaskRequest:\n        return GraphTaskRequest(NodeStep(type(node)).id, inputs=node, fork_stack=())\n\n    async def next(\n        self,\n        node: _agent_graph.AgentNode[AgentDepsT, OutputDataT],\n    ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n        \"\"\"Manually drive the agent run by passing in the node you want to run next.\n\n        This lets you inspect or mutate the node before continuing execution, or skip certain nodes\n        under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]\n        node.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n        from pydantic_graph import End\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            async with agent.iter('What is the capital of France?') as agent_run:\n                next_node = agent_run.next_node  # start with the first node\n                nodes = [next_node]\n                while not isinstance(next_node, End):\n                    next_node = await agent_run.next(next_node)\n                    nodes.append(next_node)\n                # Once `next_node` is an End, we've finished:\n                print(nodes)\n                '''\n                [\n                    UserPromptNode(\n                        user_prompt='What is the capital of France?',\n                        instructions_functions=[],\n                        system_prompts=(),\n                        system_prompt_functions=[],\n                        system_prompt_dynamic_functions={},\n                    ),\n                    ModelRequestNode(\n                        request=ModelRequest(\n                            parts=[\n                                UserPromptPart(\n                                    content='What is the capital of France?',\n                                    timestamp=datetime.datetime(...),\n                                )\n                            ],\n                            run_id='...',\n                        )\n                    ),\n                    CallToolsNode(\n                        model_response=ModelResponse(\n                            parts=[TextPart(content='The capital of France is Paris.')],\n                            usage=RequestUsage(input_tokens=56, output_tokens=7),\n                            model_name='gpt-4o',\n                            timestamp=datetime.datetime(...),\n                            run_id='...',\n                        )\n                    ),\n                    End(data=FinalResult(output='The capital of France is Paris.')),\n                ]\n                '''\n                print('Final result:', agent_run.result.output)\n                #> Final result: The capital of France is Paris.\n        ```\n\n        Args:\n            node: The node to run next in the graph.\n\n        Returns:\n            The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if\n            the run has completed.\n        \"\"\"\n        # Note: It might be nice to expose a synchronous interface for iteration, but we shouldn't do it\n        # on this class, or else IDEs won't warn you if you accidentally use `for` instead of `async for` to iterate.\n        task = [self._node_to_task(node)]\n        try:\n            task = await self._graph_run.next(task)\n        except StopAsyncIteration:\n            pass\n        return self._task_to_node(task)\n\n    # TODO (v2): Make this a property\n    def usage(self) -> _usage.RunUsage:\n        \"\"\"Get usage statistics for the run so far, including token usage, model requests, and so on.\"\"\"\n        return self._graph_run.state.usage\n\n    @property\n    def run_id(self) -> str:\n        \"\"\"The unique identifier for the agent run.\"\"\"\n        return self._graph_run.state.run_id\n\n    def __repr__(self) -> str:  # pragma: no cover\n        result = self._graph_run.output\n        result_repr = '<run not finished>' if result is None else repr(result.output)\n        return f'<{type(self).__name__} result={result_repr} usage={self.usage()}>'\n\n````\n\n#### ctx\n\n```python\nctx: GraphRunContext[\n    GraphAgentState, GraphAgentDeps[AgentDepsT, Any]\n]\n\n```\n\nThe current context of the agent run.\n\n#### next_node\n\n```python\nnext_node: (\n    AgentNode[AgentDepsT, OutputDataT]\n    | End[FinalResult[OutputDataT]]\n)\n\n```\n\nThe next node that will be run in the agent graph.\n\nThis is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\n\n#### result\n\n```python\nresult: AgentRunResult[OutputDataT] | None\n\n```\n\nThe final result of the run if it has ended, otherwise `None`.\n\nOnce the run returns an End node, `result` is populated with an AgentRunResult.\n\n#### all_messages\n\n```python\nall_messages() -> list[ModelMessage]\n\n```\n\nReturn all messages for the run so far.\n\nMessages from older runs are included.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef all_messages(self) -> list[_messages.ModelMessage]:\n    \"\"\"Return all messages for the run so far.\n\n    Messages from older runs are included.\n    \"\"\"\n    return self.ctx.state.message_history\n\n```\n\n#### all_messages_json\n\n```python\nall_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n\n```\n\nReturn all messages from all_messages as JSON bytes.\n\nReturns:\n\n| Type | Description | | --- | --- | | `bytes` | JSON bytes representing the messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:\n    \"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRun.all_messages] as JSON bytes.\n\n    Returns:\n        JSON bytes representing the messages.\n    \"\"\"\n    return _messages.ModelMessagesTypeAdapter.dump_json(self.all_messages())\n\n```\n\n#### new_messages\n\n```python\nnew_messages() -> list[ModelMessage]\n\n```\n\nReturn new messages for the run so far.\n\nMessages from older runs are excluded.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef new_messages(self) -> list[_messages.ModelMessage]:\n    \"\"\"Return new messages for the run so far.\n\n    Messages from older runs are excluded.\n    \"\"\"\n    return self.all_messages()[self.ctx.deps.new_message_index :]\n\n```\n\n#### new_messages_json\n\n```python\nnew_messages_json() -> bytes\n\n```\n\nReturn new messages from new_messages as JSON bytes.\n\nReturns:\n\n| Type | Description | | --- | --- | | `bytes` | JSON bytes representing the new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef new_messages_json(self) -> bytes:\n    \"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRun.new_messages] as JSON bytes.\n\n    Returns:\n        JSON bytes representing the new messages.\n    \"\"\"\n    return _messages.ModelMessagesTypeAdapter.dump_json(self.new_messages())\n\n```\n\n#### __aiter__\n\n```python\n__aiter__() -> (\n    AsyncIterator[\n        AgentNode[AgentDepsT, OutputDataT]\n        | End[FinalResult[OutputDataT]]\n    ]\n)\n\n```\n\nProvide async-iteration over the nodes in the agent run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef __aiter__(\n    self,\n) -> AsyncIterator[_agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]]:\n    \"\"\"Provide async-iteration over the nodes in the agent run.\"\"\"\n    return self\n\n```\n\n#### __anext__\n\n```python\n__anext__() -> (\n    AgentNode[AgentDepsT, OutputDataT]\n    | End[FinalResult[OutputDataT]]\n)\n\n```\n\nAdvance to the next node automatically based on the last returned node.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\nasync def __anext__(\n    self,\n) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n    \"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\n    task = await anext(self._graph_run)\n    return self._task_to_node(task)\n\n```\n\n#### next\n\n```python\nnext(\n    node: AgentNode[AgentDepsT, OutputDataT],\n) -> (\n    AgentNode[AgentDepsT, OutputDataT]\n    | End[FinalResult[OutputDataT]]\n)\n\n```\n\nManually drive the agent run by passing in the node you want to run next.\n\nThis lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an End node.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_graph import End\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.iter('What is the capital of France?') as agent_run:\n        next_node = agent_run.next_node  # start with the first node\n        nodes = [next_node]\n        while not isinstance(next_node, End):\n            next_node = await agent_run.next(next_node)\n            nodes.append(next_node)\n        # Once `next_node` is an End, we've finished:\n        print(nodes)\n        '''\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ],\n                    run_id='...',\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                    run_id='...',\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        '''\n        print('Final result:', agent_run.result.output)\n        #> Final result: The capital of France is Paris.\n\n```\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `node` | `AgentNode[AgentDepsT, OutputDataT]` | The node to run next in the graph. | *required* |\n\nReturns:\n\n| Type | Description | | --- | --- | | `AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]` | The next node returned by the graph logic, or an End node if | | `AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]` | the run has completed. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n````python\nasync def next(\n    self,\n    node: _agent_graph.AgentNode[AgentDepsT, OutputDataT],\n) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n    \"\"\"Manually drive the agent run by passing in the node you want to run next.\n\n    This lets you inspect or mutate the node before continuing execution, or skip certain nodes\n    under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]\n    node.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n    from pydantic_graph import End\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        async with agent.iter('What is the capital of France?') as agent_run:\n            next_node = agent_run.next_node  # start with the first node\n            nodes = [next_node]\n            while not isinstance(next_node, End):\n                next_node = await agent_run.next(next_node)\n                nodes.append(next_node)\n            # Once `next_node` is an End, we've finished:\n            print(nodes)\n            '''\n            [\n                UserPromptNode(\n                    user_prompt='What is the capital of France?',\n                    instructions_functions=[],\n                    system_prompts=(),\n                    system_prompt_functions=[],\n                    system_prompt_dynamic_functions={},\n                ),\n                ModelRequestNode(\n                    request=ModelRequest(\n                        parts=[\n                            UserPromptPart(\n                                content='What is the capital of France?',\n                                timestamp=datetime.datetime(...),\n                            )\n                        ],\n                        run_id='...',\n                    )\n                ),\n                CallToolsNode(\n                    model_response=ModelResponse(\n                        parts=[TextPart(content='The capital of France is Paris.')],\n                        usage=RequestUsage(input_tokens=56, output_tokens=7),\n                        model_name='gpt-4o',\n                        timestamp=datetime.datetime(...),\n                        run_id='...',\n                    )\n                ),\n                End(data=FinalResult(output='The capital of France is Paris.')),\n            ]\n            '''\n            print('Final result:', agent_run.result.output)\n            #> Final result: The capital of France is Paris.\n    ```\n\n    Args:\n        node: The node to run next in the graph.\n\n    Returns:\n        The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if\n        the run has completed.\n    \"\"\"\n    # Note: It might be nice to expose a synchronous interface for iteration, but we shouldn't do it\n    # on this class, or else IDEs won't warn you if you accidentally use `for` instead of `async for` to iterate.\n    task = [self._node_to_task(node)]\n    try:\n        task = await self._graph_run.next(task)\n    except StopAsyncIteration:\n        pass\n    return self._task_to_node(task)\n\n````\n\n#### usage\n\n```python\nusage() -> RunUsage\n\n```\n\nGet usage statistics for the run so far, including token usage, model requests, and so on.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef usage(self) -> _usage.RunUsage:\n    \"\"\"Get usage statistics for the run so far, including token usage, model requests, and so on.\"\"\"\n    return self._graph_run.state.usage\n\n```\n\n#### run_id\n\n```python\nrun_id: str\n\n```\n\nThe unique identifier for the agent run.\n\n### AgentRunResult\n\nBases: `Generic[OutputDataT]`\n\nThe final result of an agent run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\n@dataclasses.dataclass\nclass AgentRunResult(Generic[OutputDataT]):\n    \"\"\"The final result of an agent run.\"\"\"\n\n    output: OutputDataT\n    \"\"\"The output data from the agent run.\"\"\"\n\n    _output_tool_name: str | None = dataclasses.field(repr=False, compare=False, default=None)\n    _state: _agent_graph.GraphAgentState = dataclasses.field(\n        repr=False, compare=False, default_factory=_agent_graph.GraphAgentState\n    )\n    _new_message_index: int = dataclasses.field(repr=False, compare=False, default=0)\n    _traceparent_value: str | None = dataclasses.field(repr=False, compare=False, default=None)\n\n    @overload\n    def _traceparent(self, *, required: Literal[False]) -> str | None: ...\n    @overload\n    def _traceparent(self) -> str: ...\n    def _traceparent(self, *, required: bool = True) -> str | None:\n        if self._traceparent_value is None and required:  # pragma: no cover\n            raise AttributeError('No span was created for this agent run')\n        return self._traceparent_value\n\n    def _set_output_tool_return(self, return_content: str) -> list[_messages.ModelMessage]:\n        \"\"\"Set return content for the output tool.\n\n        Useful if you want to continue the conversation and want to set the response to the output tool call.\n        \"\"\"\n        if not self._output_tool_name:\n            raise ValueError('Cannot set output tool return content when the return type is `str`.')\n\n        messages = self._state.message_history\n        last_message = messages[-1]\n        for idx, part in enumerate(last_message.parts):\n            if isinstance(part, _messages.ToolReturnPart) and part.tool_name == self._output_tool_name:\n                # Only do deepcopy when we have to modify\n                copied_messages = list(messages)\n                copied_last = deepcopy(last_message)\n                copied_last.parts[idx].content = return_content  # type: ignore[misc]\n                copied_messages[-1] = copied_last\n                return copied_messages\n\n        raise LookupError(f'No tool call found with tool name {self._output_tool_name!r}.')\n\n    def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return the history of _messages.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of messages.\n        \"\"\"\n        if output_tool_return_content is not None:\n            return self._set_output_tool_return(output_tool_return_content)\n        else:\n            return self._state.message_history\n\n    def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:\n        \"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(\n            self.all_messages(output_tool_return_content=output_tool_return_content)\n        )\n\n    def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return new messages associated with this run.\n\n        Messages from older runs are excluded.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of new messages.\n        \"\"\"\n        return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :]\n\n    def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:\n        \"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the new messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(\n            self.new_messages(output_tool_return_content=output_tool_return_content)\n        )\n\n    @property\n    def response(self) -> _messages.ModelResponse:\n        \"\"\"Return the last response from the message history.\"\"\"\n        # The response may not be the very last item if it contained an output tool call. See `CallToolsNode._handle_final_result`.\n        for message in reversed(self.all_messages()):\n            if isinstance(message, _messages.ModelResponse):\n                return message\n        raise ValueError('No response found in the message history')  # pragma: no cover\n\n    # TODO (v2): Make this a property\n    def usage(self) -> _usage.RunUsage:\n        \"\"\"Return the usage of the whole run.\"\"\"\n        return self._state.usage\n\n    # TODO (v2): Make this a property\n    def timestamp(self) -> datetime:\n        \"\"\"Return the timestamp of last response.\"\"\"\n        return self.response.timestamp\n\n    @property\n    def run_id(self) -> str:\n        \"\"\"The unique identifier for the agent run.\"\"\"\n        return self._state.run_id\n\n```\n\n#### output\n\n```python\noutput: OutputDataT\n\n```\n\nThe output data from the agent run.\n\n#### all_messages\n\n```python\nall_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n\n```\n\nReturn the history of \\_messages.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `list[ModelMessage]` | List of messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n    \"\"\"Return the history of _messages.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        List of messages.\n    \"\"\"\n    if output_tool_return_content is not None:\n        return self._set_output_tool_return(output_tool_return_content)\n    else:\n        return self._state.message_history\n\n```\n\n#### all_messages_json\n\n```python\nall_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n\n```\n\nReturn all messages from all_messages as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `bytes` | JSON bytes representing the messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:\n    \"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        JSON bytes representing the messages.\n    \"\"\"\n    return _messages.ModelMessagesTypeAdapter.dump_json(\n        self.all_messages(output_tool_return_content=output_tool_return_content)\n    )\n\n```\n\n#### new_messages\n\n```python\nnew_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n\n```\n\nReturn new messages associated with this run.\n\nMessages from older runs are excluded.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `list[ModelMessage]` | List of new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n    \"\"\"Return new messages associated with this run.\n\n    Messages from older runs are excluded.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        List of new messages.\n    \"\"\"\n    return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :]\n\n```\n\n#### new_messages_json\n\n```python\nnew_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n\n```\n\nReturn new messages from new_messages as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If None, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `bytes` | JSON bytes representing the new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:\n    \"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.\n\n    Args:\n        output_tool_return_content: The return content of the tool call to set in the last message.\n            This provides a convenient way to modify the content of the output tool call if you want to continue\n            the conversation and want to set the response to the output tool call. If `None`, the last message will\n            not be modified.\n\n    Returns:\n        JSON bytes representing the new messages.\n    \"\"\"\n    return _messages.ModelMessagesTypeAdapter.dump_json(\n        self.new_messages(output_tool_return_content=output_tool_return_content)\n    )\n\n```\n\n#### response\n\n```python\nresponse: ModelResponse\n\n```\n\nReturn the last response from the message history.\n\n#### usage\n\n```python\nusage() -> RunUsage\n\n```\n\nReturn the usage of the whole run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef usage(self) -> _usage.RunUsage:\n    \"\"\"Return the usage of the whole run.\"\"\"\n    return self._state.usage\n\n```\n\n#### timestamp\n\n```python\ntimestamp() -> datetime\n\n```\n\nReturn the timestamp of last response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\ndef timestamp(self) -> datetime:\n    \"\"\"Return the timestamp of last response.\"\"\"\n    return self.response.timestamp\n\n```\n\n#### run_id\n\n```python\nrun_id: str\n\n```\n\nThe unique identifier for the agent run.\n\n### AgentRunResultEvent\n\nBases: `Generic[OutputDataT]`\n\nAn event indicating the agent run ended and containing the final result of the agent run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n```python\n@dataclasses.dataclass(repr=False)\nclass AgentRunResultEvent(Generic[OutputDataT]):\n    \"\"\"An event indicating the agent run ended and containing the final result of the agent run.\"\"\"\n\n    result: AgentRunResult[OutputDataT]\n    \"\"\"The result of the run.\"\"\"\n\n    _: dataclasses.KW_ONLY\n\n    event_kind: Literal['agent_run_result'] = 'agent_run_result'\n    \"\"\"Event type identifier, used as a discriminator.\"\"\"\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n```\n\n#### result\n\n```python\nresult: AgentRunResult[OutputDataT]\n\n```\n\nThe result of the run.\n\n#### event_kind\n\n```python\nevent_kind: Literal[\"agent_run_result\"] = \"agent_run_result\"\n\n```\n\nEvent type identifier, used as a discriminator.",
  "content_length": 37654
}