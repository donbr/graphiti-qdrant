{
  "title": "SQL Generation",
  "source_url": null,
  "content": "Example demonstrating how to use Pydantic AI to generate SQL queries based on user input.\n\nDemonstrates:\n\n- [dynamic system prompt](../../agents/#system-prompts)\n- [structured `output_type`](../../output/#structured-output)\n- [output validation](../../output/#output-validator-functions)\n- [agent dependencies](../../dependencies/)\n\n## Running the Example\n\nThe resulting SQL is validated by running it as an `EXPLAIN` query on PostgreSQL. To run the example, you first need to run PostgreSQL, e.g. via Docker:\n\n```bash\ndocker run --rm -e POSTGRES_PASSWORD=postgres -p 54320:5432 postgres\n\n```\n\n*(we run postgres on port `54320` to avoid conflicts with any other postgres instances you may have running)*\n\nWith [dependencies installed and environment variables set](../setup/#usage), run:\n\n```bash\npython -m pydantic_ai_examples.sql_gen\n\n```\n\n```bash\nuv run -m pydantic_ai_examples.sql_gen\n\n```\n\nor to use a custom prompt:\n\n```bash\npython -m pydantic_ai_examples.sql_gen \"find me errors\"\n\n```\n\n```bash\nuv run -m pydantic_ai_examples.sql_gen \"find me errors\"\n\n```\n\nThis model uses `gemini-2.5-flash` by default since Gemini is good at single shot queries of this kind.\n\n## Example Code\n\n[sql_gen.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/sql_gen.py)\n\n```python\n\"\"\"Example demonstrating how to use Pydantic AI to generate SQL queries based on user input.\n\nRun postgres with:\n\n    mkdir postgres-data\n    docker run --rm -e POSTGRES_PASSWORD=postgres -p 54320:5432 postgres\n\nRun with:\n\n    uv run -m pydantic_ai_examples.sql_gen \"show me logs from yesterday, with level 'error'\"\n\"\"\"\n\nimport asyncio\nimport sys\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\nfrom datetime import date\nfrom typing import Annotated, Any, TypeAlias\n\nimport asyncpg\nimport logfire\nfrom annotated_types import MinLen\nfrom devtools import debug\nfrom pydantic import BaseModel, Field\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext, format_as_xml\n\n### 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_asyncpg()\nlogfire.instrument_pydantic_ai()\n\nDB_SCHEMA = \"\"\"\nCREATE TABLE records (\n    created_at timestamptz,\n    start_timestamp timestamptz,\n    end_timestamp timestamptz,\n    trace_id text,\n    span_id text,\n    parent_span_id text,\n    level log_level,\n    span_name text,\n    message text,\n    attributes_json_schema text,\n    attributes jsonb,\n    tags text[],\n    is_exception boolean,\n    otel_status_message text,\n    service_name text\n);\n\"\"\"\nSQL_EXAMPLES = [\n    {\n        'request': 'show me records where foobar is false',\n        'response': \"SELECT * FROM records WHERE attributes->>'foobar' = false\",\n    },\n    {\n        'request': 'show me records where attributes include the key \"foobar\"',\n        'response': \"SELECT * FROM records WHERE attributes ? 'foobar'\",\n    },\n    {\n        'request': 'show me records from yesterday',\n        'response': \"SELECT * FROM records WHERE start_timestamp::date > CURRENT_TIMESTAMP - INTERVAL '1 day'\",\n    },\n    {\n        'request': 'show me error records with the tag \"foobar\"',\n        'response': \"SELECT * FROM records WHERE level = 'error' and 'foobar' = ANY(tags)\",\n    },\n]\n\n\n@dataclass\nclass Deps:\n    conn: asyncpg.Connection\n\n\nclass Success(BaseModel):\n    \"\"\"Response when SQL could be successfully generated.\"\"\"\n\n    sql_query: Annotated[str, MinLen(1)]\n    explanation: str = Field(\n        '', description='Explanation of the SQL query, as markdown'\n    )\n\n\nclass InvalidRequest(BaseModel):\n    \"\"\"Response the user input didn't include enough information to generate SQL.\"\"\"\n\n    error_message: str\n\n\nResponse: TypeAlias = Success | InvalidRequest\nagent = Agent[Deps, Response](\n    'google-gla:gemini-2.5-flash',\n    # Type ignore while we wait for PEP-0747, nonetheless unions will work fine everywhere else\n    output_type=Response,  # type: ignore\n    deps_type=Deps,\n)\n\n\n@agent.system_prompt\nasync def system_prompt() -> str:\n    return f\"\"\"\\\nGiven the following PostgreSQL table of records, your job is to\nwrite a SQL query that suits the user's request.\n\nDatabase schema:\n\n{DB_SCHEMA}\n\ntoday's date = {date.today()}\n\n{format_as_xml(SQL_EXAMPLES)}\n\"\"\"\n\n\n@agent.output_validator\nasync def validate_output(ctx: RunContext[Deps], output: Response) -> Response:\n    if isinstance(output, InvalidRequest):\n        return output\n\n    # gemini often adds extraneous backslashes to SQL\n    output.sql_query = output.sql_query.replace('\\\\', '')\n    if not output.sql_query.upper().startswith('SELECT'):\n        raise ModelRetry('Please create a SELECT query')\n\n    try:\n        await ctx.deps.conn.execute(f'EXPLAIN {output.sql_query}')\n    except asyncpg.exceptions.PostgresError as e:\n        raise ModelRetry(f'Invalid query: {e}') from e\n    else:\n        return output\n\n\nasync def main():\n    if len(sys.argv) == 1:\n        prompt = 'show me logs from yesterday, with level \"error\"'\n    else:\n        prompt = sys.argv[1]\n\n    async with database_connect(\n        'postgresql://postgres:postgres@localhost:54320', 'pydantic_ai_sql_gen'\n    ) as conn:\n        deps = Deps(conn)\n        result = await agent.run(prompt, deps=deps)\n    debug(result.output)\n\n\n### pyright: reportUnknownMemberType=false\n### pyright: reportUnknownVariableType=false\n@asynccontextmanager\nasync def database_connect(server_dsn: str, database: str) -> AsyncGenerator[Any, None]:\n    with logfire.span('check and create DB'):\n        conn = await asyncpg.connect(server_dsn)\n        try:\n            db_exists = await conn.fetchval(\n                'SELECT 1 FROM pg_database WHERE datname = $1', database\n            )\n            if not db_exists:\n                await conn.execute(f'CREATE DATABASE {database}')\n        finally:\n            await conn.close()\n\n    conn = await asyncpg.connect(f'{server_dsn}/{database}')\n    try:\n        with logfire.span('create schema'):\n            async with conn.transaction():\n                if not db_exists:\n                    await conn.execute(\n                        \"CREATE TYPE log_level AS ENUM ('debug', 'info', 'warning', 'error', 'critical')\"\n                    )\n                    await conn.execute(DB_SCHEMA)\n        yield conn\n    finally:\n        await conn.close()\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\nThis example shows how to stream markdown from an agent, using the [`rich`](https://github.com/Textualize/rich) library to highlight the output in the terminal.\n\nIt'll run the example with both OpenAI and Google Gemini models if the required environment variables are set.\n\nDemonstrates:\n\n- [streaming text responses](../../output/#streaming-text)\n\n## Running the Example\n\nWith [dependencies installed and environment variables set](../setup/#usage), run:\n\n```bash\npython -m pydantic_ai_examples.stream_markdown\n\n```\n\n```bash\nuv run -m pydantic_ai_examples.stream_markdown\n\n```\n\n## Example Code\n\n[stream_markdown.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_markdown.py)\n\n```python\n\"\"\"This example shows how to stream markdown from an agent, using the `rich` library to display the markdown.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.stream_markdown\n\"\"\"\n\nimport asyncio\nimport os\n\nimport logfire\nfrom rich.console import Console, ConsoleOptions, RenderResult\nfrom rich.live import Live\nfrom rich.markdown import CodeBlock, Markdown\nfrom rich.syntax import Syntax\nfrom rich.text import Text\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models import KnownModelName\n\n### 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\nagent = Agent()\n\n### models to try, and the appropriate env var\nmodels: list[tuple[KnownModelName, str]] = [\n    ('google-gla:gemini-2.5-flash', 'GEMINI_API_KEY'),\n    ('openai:gpt-5-mini', 'OPENAI_API_KEY'),\n    ('groq:llama-3.3-70b-versatile', 'GROQ_API_KEY'),\n]\n\n\nasync def main():\n    prettier_code_blocks()\n    console = Console()\n    prompt = 'Show me a short example of using Pydantic.'\n    console.log(f'Asking: {prompt}...', style='cyan')\n    for model, env_var in models:\n        if env_var in os.environ:\n            console.log(f'Using model: {model}')\n            with Live('', console=console, vertical_overflow='visible') as live:\n                async with agent.run_stream(prompt, model=model) as result:\n                    async for message in result.stream_output():\n                        live.update(Markdown(message))\n            console.log(result.usage())\n        else:\n            console.log(f'{model} requires {env_var} to be set.')\n\n\ndef prettier_code_blocks():\n    \"\"\"Make rich code blocks prettier and easier to copy.\n\n    From https://github.com/samuelcolvin/aicli/blob/v0.8.0/samuelcolvin_aicli.py#L22\n    \"\"\"\n\n    class SimpleCodeBlock(CodeBlock):\n        def __rich_console__(\n            self, console: Console, options: ConsoleOptions\n        ) -> RenderResult:\n            code = str(self.text).rstrip()\n            yield Text(self.lexer_name, style='dim')\n            yield Syntax(\n                code,\n                self.lexer_name,\n                theme=self.theme,\n                background_color='default',\n                word_wrap=True,\n            )\n            yield Text(f'/{self.lexer_name}', style='dim')\n\n    Markdown.elements['fence'] = SimpleCodeBlock\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\nInformation about whales â€” an example of streamed structured response validation.\n\nDemonstrates:\n\n- [streaming structured output](../../output/#streaming-structured-output)\n\nThis script streams structured responses from GPT-4 about whales, validates the data and displays it as a dynamic table using [`rich`](https://github.com/Textualize/rich) as the data is received.\n\n## Running the Example\n\nWith [dependencies installed and environment variables set](../setup/#usage), run:\n\n```bash\npython -m pydantic_ai_examples.stream_whales\n\n```\n\n```bash\nuv run -m pydantic_ai_examples.stream_whales\n\n```\n\nShould give an output like this:\n\n## Example Code\n\n[Learn about Gateway](../../gateway) [stream_whales.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_whales.py)\n\n```python\n\"\"\"Information about whales â€” an example of streamed structured response validation.\n\nThis script streams structured responses from GPT-4 about whales, validates the data\nand displays it as a dynamic table using Rich as the data is received.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.stream_whales\n\"\"\"\n\nfrom typing import Annotated\n\nimport logfire\nfrom pydantic import Field\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.table import Table\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom pydantic_ai import Agent\n\n### 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\nclass Whale(TypedDict):\n    name: str\n    length: Annotated[\n        float, Field(description='Average length of an adult whale in meters.')\n    ]\n    weight: NotRequired[\n        Annotated[\n            float,\n            Field(description='Average weight of an adult whale in kilograms.', ge=50),\n        ]\n    ]\n    ocean: NotRequired[str]\n    description: NotRequired[Annotated[str, Field(description='Short Description')]]\n\n\nagent = Agent('gateway/openai:gpt-4', output_type=list[Whale])\n\n\nasync def main():\n    console = Console()\n    with Live('\\n' * 36, console=console) as live:\n        console.print('Requesting data...', style='cyan')\n        async with agent.run_stream(\n            'Generate me details of 5 species of Whale.'\n        ) as result:\n            console.print('Response:', style='green')\n\n            async for whales in result.stream_output(debounce_by=0.01):\n                table = Table(\n                    title='Species of Whale',\n                    caption='Streaming Structured responses from GPT-4',\n                    width=120,\n                )\n                table.add_column('ID', justify='right')\n                table.add_column('Name')\n                table.add_column('Avg. Length (m)', justify='right')\n                table.add_column('Avg. Weight (kg)', justify='right')\n                table.add_column('Ocean')\n                table.add_column('Description', justify='right')\n\n                for wid, whale in enumerate(whales, start=1):\n                    table.add_row(\n                        str(wid),\n                        whale['name'],\n                        f'{whale[\"length\"]:0.0f}',\n                        f'{w:0.0f}' if (w := whale.get('weight')) else 'â€¦',\n                        whale.get('ocean') or 'â€¦',\n                        whale.get('description') or 'â€¦',\n                    )\n                live.update(table)\n\n\nif __name__ == '__main__':\n    import asyncio\n\n    asyncio.run(main())\n\n```\n\n[stream_whales.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_whales.py)\n\n```python\n\"\"\"Information about whales â€” an example of streamed structured response validation.\n\nThis script streams structured responses from GPT-4 about whales, validates the data\nand displays it as a dynamic table using Rich as the data is received.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.stream_whales\n\"\"\"\n\nfrom typing import Annotated\n\nimport logfire\nfrom pydantic import Field\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.table import Table\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom pydantic_ai import Agent\n\n### 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\nclass Whale(TypedDict):\n    name: str\n    length: Annotated[\n        float, Field(description='Average length of an adult whale in meters.')\n    ]\n    weight: NotRequired[\n        Annotated[\n            float,\n            Field(description='Average weight of an adult whale in kilograms.', ge=50),\n        ]\n    ]\n    ocean: NotRequired[str]\n    description: NotRequired[Annotated[str, Field(description='Short Description')]]\n\n\nagent = Agent('openai:gpt-4', output_type=list[Whale])\n\n\nasync def main():\n    console = Console()\n    with Live('\\n' * 36, console=console) as live:\n        console.print('Requesting data...', style='cyan')\n        async with agent.run_stream(\n            'Generate me details of 5 species of Whale.'\n        ) as result:\n            console.print('Response:', style='green')\n\n            async for whales in result.stream_output(debounce_by=0.01):\n                table = Table(\n                    title='Species of Whale',\n                    caption='Streaming Structured responses from GPT-4',\n                    width=120,\n                )\n                table.add_column('ID', justify='right')\n                table.add_column('Name')\n                table.add_column('Avg. Length (m)', justify='right')\n                table.add_column('Avg. Weight (kg)', justify='right')\n                table.add_column('Ocean')\n                table.add_column('Description', justify='right')\n\n                for wid, whale in enumerate(whales, start=1):\n                    table.add_row(\n                        str(wid),\n                        whale['name'],\n                        f'{whale[\"length\"]:0.0f}',\n                        f'{w:0.0f}' if (w := whale.get('weight')) else 'â€¦',\n                        whale.get('ocean') or 'â€¦',\n                        whale.get('description') or 'â€¦',\n                    )\n                live.update(table)\n\n\nif __name__ == '__main__':\n    import asyncio\n\n    asyncio.run(main())\n\n```\n\nExample of Pydantic AI with multiple tools which the LLM needs to call in turn to answer a question.\n\nDemonstrates:\n\n- [tools](../../tools/)\n- [agent dependencies](../../dependencies/)\n- [streaming text responses](../../output/#streaming-text)\n- Building a [Gradio](https://www.gradio.app/) UI for the agent\n\nIn this case the idea is a \"weather\" agent â€” the user can ask for the weather in multiple locations, the agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use the `get_weather` tool to get the weather for those locations.\n\n## Running the Example\n\nTo run this example properly, you might want to add two extra API keys **(Note if either key is missing, the code will fall back to dummy data, so they're not required)**:\n\n- A weather API key from [tomorrow.io](https://www.tomorrow.io/weather-api/) set via `WEATHER_API_KEY`\n- A geocoding API key from [geocode.maps.co](https://geocode.maps.co/) set via `GEO_API_KEY`\n\nWith [dependencies installed and environment variables set](../setup/#usage), run:\n\n```bash\npython -m pydantic_ai_examples.weather_agent\n\n```\n\n```bash\nuv run -m pydantic_ai_examples.weather_agent\n\n```\n\n## Example Code\n\n[Learn about Gateway](../../gateway) [weather_agent.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent.py)\n\n```python\n\"\"\"Example of Pydantic AI with multiple tools which the LLM needs to call in turn to answer a question.\n\nIn this case the idea is a \"weather\" agent â€” the user can ask for the weather in multiple cities,\nthe agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use\nthe `get_weather` tool to get the weather.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.weather_agent\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport logfire\nfrom httpx import AsyncClient\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext\n\n### 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n\n\nweather_agent = Agent(\n    'gateway/openai:gpt-5-mini',\n    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    instructions='Be concise, reply with one sentence.',\n    deps_type=Deps,\n    retries=2,\n)\n\n\nclass LatLng(BaseModel):\n    lat: float\n    lng: float\n\n\n@weather_agent.tool\nasync def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> LatLng:\n    \"\"\"Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    \"\"\"\n    # NOTE: the response here will be random, and is not related to the location description.\n    r = await ctx.deps.client.get(\n        'https://demo-endpoints.pydantic.workers.dev/latlng',\n        params={'location': location_description},\n    )\n    r.raise_for_status()\n    return LatLng.model_validate_json(r.content)\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    \"\"\"Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    \"\"\"\n    # NOTE: the responses here will be random, and are not related to the lat and lng.\n    temp_response, descr_response = await asyncio.gather(\n        ctx.deps.client.get(\n            'https://demo-endpoints.pydantic.workers.dev/number',\n            params={'min': 10, 'max': 30},\n        ),\n        ctx.deps.client.get(\n            'https://demo-endpoints.pydantic.workers.dev/weather',\n            params={'lat': lat, 'lng': lng},\n        ),\n    )\n    temp_response.raise_for_status()\n    descr_response.raise_for_status()\n    return {\n        'temperature': f'{temp_response.text} Â°C',\n        'description': descr_response.text,\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        logfire.instrument_httpx(client, capture_all=True)\n        deps = Deps(client=client)\n        result = await weather_agent.run(\n            'What is the weather like in London and in Wiltshire?', deps=deps\n        )\n        print('Response:', result.output)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\n[weather_agent.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent.py)\n\n```python\n\"\"\"Example of Pydantic AI with multiple tools which the LLM needs to call in turn to answer a question.\n\nIn this case the idea is a \"weather\" agent â€” the user can ask for the weather in multiple cities,\nthe agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use\nthe `get_weather` tool to get the weather.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.weather_agent\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport logfire\nfrom httpx import AsyncClient\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext\n\n### 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n\n\nweather_agent = Agent(\n    'openai:gpt-5-mini',\n    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    instructions='Be concise, reply with one sentence.',\n    deps_type=Deps,\n    retries=2,\n)\n\n\nclass LatLng(BaseModel):\n    lat: float\n    lng: float\n\n\n@weather_agent.tool\nasync def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> LatLng:\n    \"\"\"Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    \"\"\"\n    # NOTE: the response here will be random, and is not related to the location description.\n    r = await ctx.deps.client.get(\n        'https://demo-endpoints.pydantic.workers.dev/latlng',\n        params={'location': location_description},\n    )\n    r.raise_for_status()\n    return LatLng.model_validate_json(r.content)\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    \"\"\"Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    \"\"\"\n    # NOTE: the responses here will be random, and are not related to the lat and lng.\n    temp_response, descr_response = await asyncio.gather(\n        ctx.deps.client.get(\n            'https://demo-endpoints.pydantic.workers.dev/number',\n            params={'min': 10, 'max': 30},\n        ),\n        ctx.deps.client.get(\n            'https://demo-endpoints.pydantic.workers.dev/weather',\n            params={'lat': lat, 'lng': lng},\n        ),\n    )\n    temp_response.raise_for_status()\n    descr_response.raise_for_status()\n    return {\n        'temperature': f'{temp_response.text} Â°C',\n        'description': descr_response.text,\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        logfire.instrument_httpx(client, capture_all=True)\n        deps = Deps(client=client)\n        result = await weather_agent.run(\n            'What is the weather like in London and in Wiltshire?', deps=deps\n        )\n        print('Response:', result.output)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\n## Running the UI\n\nYou can build multi-turn chat applications for your agent with [Gradio](https://www.gradio.app/), a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file!\n\nHere's what the UI looks like for the weather agent:\n\n```bash\npip install gradio>=5.9.0\npython/uv-run -m pydantic_ai_examples.weather_agent_gradio\n\n```\n\n## UI Code\n\n[weather_agent_gradio.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent_gradio.py)\n\n```python\nfrom __future__ import annotations as _annotations\n\nimport json\n\nfrom httpx import AsyncClient\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import ToolCallPart, ToolReturnPart\nfrom pydantic_ai_examples.weather_agent import Deps, weather_agent\n\ntry:\n    import gradio as gr\nexcept ImportError as e:\n    raise ImportError(\n        'Please install gradio with `pip install gradio`. You must use python>=3.10.'\n    ) from e\n\nTOOL_TO_DISPLAY_NAME = {'get_lat_lng': 'Geocoding API', 'get_weather': 'Weather API'}\n\nclient = AsyncClient()\ndeps = Deps(client=client)\n\n\nasync def stream_from_agent(prompt: str, chatbot: list[dict], past_messages: list):\n    chatbot.append({'role': 'user', 'content': prompt})\n    yield gr.Textbox(interactive=False, value=''), chatbot, gr.skip()\n    async with weather_agent.run_stream(\n        prompt, deps=deps, message_history=past_messages\n    ) as result:\n        for message in result.new_messages():\n            for call in message.parts:\n                if isinstance(call, ToolCallPart):\n                    call_args = call.args_as_json_str()\n                    metadata = {\n                        'title': f'ðŸ› ï¸ Using {TOOL_TO_DISPLAY_NAME[call.tool_name]}',\n                    }\n                    if call.tool_call_id is not None:\n                        metadata['id'] = call.tool_call_id\n\n                    gr_message = {\n                        'role': 'assistant',\n                        'content': 'Parameters: ' + call_args,\n                        'metadata': metadata,\n                    }\n                    chatbot.append(gr_message)\n                if isinstance(call, ToolReturnPart):\n                    for gr_message in chatbot:\n                        if (\n                            gr_message.get('metadata', {}).get('id', '')\n                            == call.tool_call_id\n                        ):\n                            if isinstance(call.content, BaseModel):\n                                json_content = call.content.model_dump_json()\n                            else:\n                                json_content = json.dumps(call.content)\n                            gr_message['content'] += f'\\nOutput: {json_content}'\n                yield gr.skip(), chatbot, gr.skip()\n        chatbot.append({'role': 'assistant', 'content': ''})\n        async for message in result.stream_text():\n            chatbot[-1]['content'] = message\n            yield gr.skip(), chatbot, gr.skip()\n        past_messages = result.all_messages()\n\n        yield gr.Textbox(interactive=True), gr.skip(), past_messages\n\n\nasync def handle_retry(chatbot, past_messages: list, retry_data: gr.RetryData):\n    new_history = chatbot[: retry_data.index]\n    previous_prompt = chatbot[retry_data.index]['content']\n    past_messages = past_messages[: retry_data.index]\n    async for update in stream_from_agent(previous_prompt, new_history, past_messages):\n        yield update\n\n\ndef undo(chatbot, past_messages: list, undo_data: gr.UndoData):\n    new_history = chatbot[: undo_data.index]\n    past_messages = past_messages[: undo_data.index]\n    return chatbot[undo_data.index]['content'], new_history, past_messages\n\n\ndef select_data(message: gr.SelectData) -> str:\n    return message.value['text']\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\n        \"\"\"\n<div style=\"display: flex; justify-content: center; align-items: center; gap: 2rem; padding: 1rem; width: 100%\">\n    <img src=\"https://ai.pydantic.dev/img/logo-white.svg\" style=\"max-width: 200px; height: auto\">\n    <div>\n        <h1 style=\"margin: 0 0 1rem 0\">Weather Assistant</h1>\n        <h3 style=\"margin: 0 0 0.5rem 0\">\n            This assistant answer your weather questions.\n        </h3>\n    </div>\n</div>\n\"\"\"\n    )\n    past_messages = gr.State([])\n    chatbot = gr.Chatbot(\n        label='Packing Assistant',\n        type='messages',\n        avatar_images=(None, 'https://ai.pydantic.dev/img/logo-white.svg'),\n        examples=[\n            {'text': 'What is the weather like in Miami?'},\n            {'text': 'What is the weather like in London?'},\n        ],\n    )\n    with gr.Row():\n        prompt = gr.Textbox(\n            lines=1,\n            show_label=False,\n            placeholder='What is the weather like in New York City?',\n        )\n    generation = prompt.submit(\n        stream_from_agent,\n        inputs=[prompt, chatbot, past_messages],\n        outputs=[prompt, chatbot, past_messages],\n    )\n    chatbot.example_select(select_data, None, [prompt])\n    chatbot.retry(\n        handle_retry, [chatbot, past_messages], [prompt, chatbot, past_messages]\n    )\n    chatbot.undo(undo, [chatbot, past_messages], [prompt, chatbot, past_messages])\n\n\nif __name__ == '__main__':\n    demo.launch()\n\n```",
  "content_length": 29256
}