{
  "title": "`pydantic_ai.providers`",
  "source_url": null,
  "content": "Bases: `ABC`, `Generic[InterfaceClient]`\n\nAbstract class for a provider.\n\nThe provider is in charge of providing an authenticated client to the API.\n\nEach provider only supports a specific interface. A interface can be supported by multiple providers.\n\nFor example, the `OpenAIChatModel` interface can be supported by the `OpenAIProvider` and the `DeepSeekProvider`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/__init__.py`\n\n```python\nclass Provider(ABC, Generic[InterfaceClient]):\n    \"\"\"Abstract class for a provider.\n\n    The provider is in charge of providing an authenticated client to the API.\n\n    Each provider only supports a specific interface. A interface can be supported by multiple providers.\n\n    For example, the `OpenAIChatModel` interface can be supported by the `OpenAIProvider` and the `DeepSeekProvider`.\n    \"\"\"\n\n    _client: InterfaceClient\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"The provider name.\"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def base_url(self) -> str:\n        \"\"\"The base URL for the provider API.\"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def client(self) -> InterfaceClient:\n        \"\"\"The client for the provider.\"\"\"\n        raise NotImplementedError()\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        \"\"\"The model profile for the named model, if available.\"\"\"\n        return None  # pragma: no cover\n\n    def __repr__(self) -> str:\n        return f'{self.__class__.__name__}(name={self.name}, base_url={self.base_url})'  # pragma: lax no cover\n\n```\n\n### name\n\n```python\nname: str\n\n```\n\nThe provider name.\n\n### base_url\n\n```python\nbase_url: str\n\n```\n\nThe base URL for the provider API.\n\n### client\n\n```python\nclient: InterfaceClient\n\n```\n\nThe client for the provider.\n\n### model_profile\n\n```python\nmodel_profile(model_name: str) -> ModelProfile | None\n\n```\n\nThe model profile for the named model, if available.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/__init__.py`\n\n```python\ndef model_profile(self, model_name: str) -> ModelProfile | None:\n    \"\"\"The model profile for the named model, if available.\"\"\"\n    return None  # pragma: no cover\n\n```\n\nCreate a new Gateway provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `upstream_provider` | `UpstreamProvider | str` | The upstream provider to use. | *required* | | `route` | `str | None` | The name of the provider or routing group to use to handle the request. If not provided, the default routing group for the API format will be used. | `None` | | `api_key` | `str | None` | The API key to use for authentication. If not provided, the PYDANTIC_AI_GATEWAY_API_KEY environment variable will be used if available. | `None` | | `base_url` | `str | None` | The base URL to use for the Gateway. If not provided, the PYDANTIC_AI_GATEWAY_BASE_URL environment variable will be used if available. Otherwise, defaults to https://gateway.pydantic.dev/proxy. | `None` | | `http_client` | `AsyncClient | None` | The HTTP client to use for the Gateway. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/gateway.py`\n\n```python\ndef gateway_provider(\n    upstream_provider: UpstreamProvider | str,\n    /,\n    *,\n    # Every provider\n    route: str | None = None,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    # OpenAI, Groq, Anthropic & Gemini - Only Bedrock doesn't have an HTTPX client.\n    http_client: httpx.AsyncClient | None = None,\n) -> Provider[Any]:\n    \"\"\"Create a new Gateway provider.\n\n    Args:\n        upstream_provider: The upstream provider to use.\n        route: The name of the provider or routing group to use to handle the request. If not provided, the default\n            routing group for the API format will be used.\n        api_key: The API key to use for authentication. If not provided, the `PYDANTIC_AI_GATEWAY_API_KEY`\n            environment variable will be used if available.\n        base_url: The base URL to use for the Gateway. If not provided, the `PYDANTIC_AI_GATEWAY_BASE_URL`\n            environment variable will be used if available. Otherwise, defaults to `https://gateway.pydantic.dev/proxy`.\n        http_client: The HTTP client to use for the Gateway.\n    \"\"\"\n    api_key = api_key or os.getenv('PYDANTIC_AI_GATEWAY_API_KEY', os.getenv('PAIG_API_KEY'))\n    if not api_key:\n        raise UserError(\n            'Set the `PYDANTIC_AI_GATEWAY_API_KEY` environment variable or pass it via `gateway_provider(..., api_key=...)`'\n            ' to use the Pydantic AI Gateway provider.'\n        )\n\n    base_url = base_url or os.getenv('PYDANTIC_AI_GATEWAY_BASE_URL', os.getenv('PAIG_BASE_URL', GATEWAY_BASE_URL))\n    http_client = http_client or cached_async_http_client(provider=f'gateway/{upstream_provider}')\n    http_client.event_hooks = {'request': [_request_hook(api_key)]}\n\n    if route is None:\n        # Use the implied providerId as the default route.\n        route = normalize_gateway_provider(upstream_provider)\n\n    base_url = _merge_url_path(base_url, route)\n\n    if upstream_provider in ('openai', 'openai-chat', 'openai-responses', 'chat', 'responses'):\n        from .openai import OpenAIProvider\n\n        return OpenAIProvider(api_key=api_key, base_url=base_url, http_client=http_client)\n    elif upstream_provider == 'groq':\n        from .groq import GroqProvider\n\n        return GroqProvider(api_key=api_key, base_url=base_url, http_client=http_client)\n    elif upstream_provider == 'anthropic':\n        from anthropic import AsyncAnthropic\n\n        from .anthropic import AnthropicProvider\n\n        return AnthropicProvider(\n            anthropic_client=AsyncAnthropic(auth_token=api_key, base_url=base_url, http_client=http_client)\n        )\n    elif upstream_provider in ('bedrock', 'converse'):\n        from .bedrock import BedrockProvider\n\n        return BedrockProvider(\n            api_key=api_key,\n            base_url=base_url,\n            region_name='pydantic-ai-gateway',  # Fake region name to avoid NoRegionError\n        )\n    elif upstream_provider in ('google-vertex', 'gemini'):\n        from .google import GoogleProvider\n\n        return GoogleProvider(vertexai=True, api_key=api_key, base_url=base_url, http_client=http_client)\n    else:\n        raise UserError(f'Unknown upstream provider: {upstream_provider}')\n\n```\n\n### GoogleProvider\n\nBases: `Provider[Client]`\n\nProvider for Google.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/google.py`\n\n```python\nclass GoogleProvider(Provider[Client]):\n    \"\"\"Provider for Google.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'google-vertex' if self._client._api_client.vertexai else 'google-gla'  # type: ignore[reportPrivateUsage]\n\n    @property\n    def base_url(self) -> str:\n        return str(self._client._api_client._http_options.base_url)  # type: ignore[reportPrivateUsage]\n\n    @property\n    def client(self) -> Client:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        return google_model_profile(model_name)\n\n    @overload\n    def __init__(\n        self, *, api_key: str, http_client: httpx.AsyncClient | None = None, base_url: str | None = None\n    ) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        *,\n        credentials: Credentials | None = None,\n        project: str | None = None,\n        location: VertexAILocation | Literal['global'] | str | None = None,\n        http_client: httpx.AsyncClient | None = None,\n        base_url: str | None = None,\n    ) -> None: ...\n\n    @overload\n    def __init__(self, *, client: Client) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        *,\n        vertexai: bool = False,\n        api_key: str | None = None,\n        http_client: httpx.AsyncClient | None = None,\n        base_url: str | None = None,\n    ) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        credentials: Credentials | None = None,\n        project: str | None = None,\n        location: VertexAILocation | Literal['global'] | str | None = None,\n        vertexai: bool | None = None,\n        client: Client | None = None,\n        http_client: httpx.AsyncClient | None = None,\n        base_url: str | None = None,\n    ) -> None:\n        \"\"\"Create a new Google provider.\n\n        Args:\n            api_key: The `API key <https://ai.google.dev/gemini-api/docs/api-key>`_ to\n                use for authentication. It can also be set via the `GOOGLE_API_KEY` environment variable.\n            credentials: The credentials to use for authentication when calling the Vertex AI APIs. Credentials can be\n                obtained from environment variables and default credentials. For more information, see Set up\n                Application Default Credentials. Applies to the Vertex AI API only.\n            project: The Google Cloud project ID to use for quota. Can be obtained from environment variables\n                (for example, GOOGLE_CLOUD_PROJECT). Applies to the Vertex AI API only.\n            location: The location to send API requests to (for example, us-central1). Can be obtained from environment variables.\n                Applies to the Vertex AI API only.\n            vertexai: Force the use of the Vertex AI API. If `False`, the Google Generative Language API will be used.\n                Defaults to `False` unless `location`, `project`, or `credentials` are provided.\n            client: A pre-initialized client to use.\n            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n            base_url: The base URL for the Google API.\n        \"\"\"\n        if client is None:\n            # NOTE: We are keeping GEMINI_API_KEY for backwards compatibility.\n            api_key = api_key or os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')\n\n            vertex_ai_args_used = bool(location or project or credentials)\n            if vertexai is None:\n                vertexai = vertex_ai_args_used\n\n            http_client = http_client or cached_async_http_client(\n                provider='google-vertex' if vertexai else 'google-gla'\n            )\n            http_options = HttpOptions(\n                base_url=base_url,\n                headers={'User-Agent': get_user_agent()},\n                httpx_async_client=http_client,\n            )\n            if not vertexai:\n                if api_key is None:\n                    raise UserError(\n                        'Set the `GOOGLE_API_KEY` environment variable or pass it via `GoogleProvider(api_key=...)`'\n                        'to use the Google Generative Language API.'\n                    )\n                self._client = Client(vertexai=False, api_key=api_key, http_options=http_options)\n            else:\n                if vertex_ai_args_used:\n                    api_key = None\n\n                if api_key is None:\n                    project = project or os.getenv('GOOGLE_CLOUD_PROJECT')\n                    # From https://github.com/pydantic/pydantic-ai/pull/2031/files#r2169682149:\n                    # Currently `us-central1` supports the most models by far of any region including `global`, but not\n                    # all of them. `us-central1` has all google models but is missing some Anthropic partner models,\n                    # which use `us-east5` instead. `global` has fewer models but higher availability.\n                    # For more details, check: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#available-regions\n                    location = location or os.getenv('GOOGLE_CLOUD_LOCATION') or 'us-central1'\n\n                self._client = Client(\n                    vertexai=True,\n                    api_key=api_key,\n                    project=project,\n                    location=location,\n                    credentials=credentials,\n                    http_options=http_options,\n                )\n        else:\n            self._client = client  # pragma: no cover\n\n```\n\n#### __init__\n\n```python\n__init__(\n    *,\n    api_key: str,\n    http_client: AsyncClient | None = None,\n    base_url: str | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    credentials: Credentials | None = None,\n    project: str | None = None,\n    location: (\n        VertexAILocation | Literal[\"global\"] | str | None\n    ) = None,\n    http_client: AsyncClient | None = None,\n    base_url: str | None = None\n) -> None\n\n```\n\n```python\n__init__(*, client: Client) -> None\n\n```\n\n```python\n__init__(\n    *,\n    vertexai: bool = False,\n    api_key: str | None = None,\n    http_client: AsyncClient | None = None,\n    base_url: str | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    credentials: Credentials | None = None,\n    project: str | None = None,\n    location: (\n        VertexAILocation | Literal[\"global\"] | str | None\n    ) = None,\n    vertexai: bool | None = None,\n    client: Client | None = None,\n    http_client: AsyncClient | None = None,\n    base_url: str | None = None\n) -> None\n\n```\n\nCreate a new Google provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `api_key` | `str | None` | The API key <https://ai.google.dev/gemini-api/docs/api-key>\\_ to use for authentication. It can also be set via the GOOGLE_API_KEY environment variable. | `None` | | `credentials` | `Credentials | None` | The credentials to use for authentication when calling the Vertex AI APIs. Credentials can be obtained from environment variables and default credentials. For more information, see Set up Application Default Credentials. Applies to the Vertex AI API only. | `None` | | `project` | `str | None` | The Google Cloud project ID to use for quota. Can be obtained from environment variables (for example, GOOGLE_CLOUD_PROJECT). Applies to the Vertex AI API only. | `None` | | `location` | `VertexAILocation | Literal['global'] | str | None` | The location to send API requests to (for example, us-central1). Can be obtained from environment variables. Applies to the Vertex AI API only. | `None` | | `vertexai` | `bool | None` | Force the use of the Vertex AI API. If False, the Google Generative Language API will be used. Defaults to False unless location, project, or credentials are provided. | `None` | | `client` | `Client | None` | A pre-initialized client to use. | `None` | | `http_client` | `AsyncClient | None` | An existing httpx.AsyncClient to use for making HTTP requests. | `None` | | `base_url` | `str | None` | The base URL for the Google API. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/google.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    credentials: Credentials | None = None,\n    project: str | None = None,\n    location: VertexAILocation | Literal['global'] | str | None = None,\n    vertexai: bool | None = None,\n    client: Client | None = None,\n    http_client: httpx.AsyncClient | None = None,\n    base_url: str | None = None,\n) -> None:\n    \"\"\"Create a new Google provider.\n\n    Args:\n        api_key: The `API key <https://ai.google.dev/gemini-api/docs/api-key>`_ to\n            use for authentication. It can also be set via the `GOOGLE_API_KEY` environment variable.\n        credentials: The credentials to use for authentication when calling the Vertex AI APIs. Credentials can be\n            obtained from environment variables and default credentials. For more information, see Set up\n            Application Default Credentials. Applies to the Vertex AI API only.\n        project: The Google Cloud project ID to use for quota. Can be obtained from environment variables\n            (for example, GOOGLE_CLOUD_PROJECT). Applies to the Vertex AI API only.\n        location: The location to send API requests to (for example, us-central1). Can be obtained from environment variables.\n            Applies to the Vertex AI API only.\n        vertexai: Force the use of the Vertex AI API. If `False`, the Google Generative Language API will be used.\n            Defaults to `False` unless `location`, `project`, or `credentials` are provided.\n        client: A pre-initialized client to use.\n        http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n        base_url: The base URL for the Google API.\n    \"\"\"\n    if client is None:\n        # NOTE: We are keeping GEMINI_API_KEY for backwards compatibility.\n        api_key = api_key or os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')\n\n        vertex_ai_args_used = bool(location or project or credentials)\n        if vertexai is None:\n            vertexai = vertex_ai_args_used\n\n        http_client = http_client or cached_async_http_client(\n            provider='google-vertex' if vertexai else 'google-gla'\n        )\n        http_options = HttpOptions(\n            base_url=base_url,\n            headers={'User-Agent': get_user_agent()},\n            httpx_async_client=http_client,\n        )\n        if not vertexai:\n            if api_key is None:\n                raise UserError(\n                    'Set the `GOOGLE_API_KEY` environment variable or pass it via `GoogleProvider(api_key=...)`'\n                    'to use the Google Generative Language API.'\n                )\n            self._client = Client(vertexai=False, api_key=api_key, http_options=http_options)\n        else:\n            if vertex_ai_args_used:\n                api_key = None\n\n            if api_key is None:\n                project = project or os.getenv('GOOGLE_CLOUD_PROJECT')\n                # From https://github.com/pydantic/pydantic-ai/pull/2031/files#r2169682149:\n                # Currently `us-central1` supports the most models by far of any region including `global`, but not\n                # all of them. `us-central1` has all google models but is missing some Anthropic partner models,\n                # which use `us-east5` instead. `global` has fewer models but higher availability.\n                # For more details, check: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#available-regions\n                location = location or os.getenv('GOOGLE_CLOUD_LOCATION') or 'us-central1'\n\n            self._client = Client(\n                vertexai=True,\n                api_key=api_key,\n                project=project,\n                location=location,\n                credentials=credentials,\n                http_options=http_options,\n            )\n    else:\n        self._client = client  # pragma: no cover\n\n```\n\n### VertexAILocation\n\n```python\nVertexAILocation = Literal[\n    \"asia-east1\",\n    \"asia-east2\",\n    \"asia-northeast1\",\n    \"asia-northeast3\",\n    \"asia-south1\",\n    \"asia-southeast1\",\n    \"australia-southeast1\",\n    \"europe-central2\",\n    \"europe-north1\",\n    \"europe-southwest1\",\n    \"europe-west1\",\n    \"europe-west2\",\n    \"europe-west3\",\n    \"europe-west4\",\n    \"europe-west6\",\n    \"europe-west8\",\n    \"europe-west9\",\n    \"me-central1\",\n    \"me-central2\",\n    \"me-west1\",\n    \"northamerica-northeast1\",\n    \"southamerica-east1\",\n    \"us-central1\",\n    \"us-east1\",\n    \"us-east4\",\n    \"us-east5\",\n    \"us-south1\",\n    \"us-west1\",\n    \"us-west4\",\n]\n\n```\n\nRegions available for Vertex AI. More details [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#genai-locations).\n\n### OpenAIProvider\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for OpenAI API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/openai.py`\n\n```python\nclass OpenAIProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for OpenAI API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'openai'\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.base_url)\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        return openai_model_profile(model_name)\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        base_url: str | None = None,\n        api_key: str | None = None,\n        openai_client: None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None: ...\n\n    def __init__(\n        self,\n        base_url: str | None = None,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        \"\"\"Create a new OpenAI provider.\n\n        Args:\n            base_url: The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable\n                will be used if available. Otherwise, defaults to OpenAI's base url.\n            api_key: The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable\n                will be used if available.\n            openai_client: An existing\n                [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)\n                client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\n            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n        \"\"\"\n        # This is a workaround for the OpenAI client requiring an API key, whilst locally served,\n        # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.\n        if api_key is None and 'OPENAI_API_KEY' not in os.environ and base_url is not None and openai_client is None:\n            api_key = 'api-key-not-set'\n\n        if openai_client is not None:\n            assert base_url is None, 'Cannot provide both `openai_client` and `base_url`'\n            assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'\n            assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='openai')\n            self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n\n```\n\n#### __init__\n\n```python\n__init__(*, openai_client: AsyncOpenAI) -> None\n\n```\n\n```python\n__init__(\n    base_url: str | None = None,\n    api_key: str | None = None,\n    openai_client: None = None,\n    http_client: AsyncClient | None = None,\n) -> None\n\n```\n\n```python\n__init__(\n    base_url: str | None = None,\n    api_key: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncClient | None = None,\n) -> None\n\n```\n\nCreate a new OpenAI provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `base_url` | `str | None` | The base url for the OpenAI requests. If not provided, the OPENAI_BASE_URL environment variable will be used if available. Otherwise, defaults to OpenAI's base url. | `None` | | `api_key` | `str | None` | The API key to use for authentication, if not provided, the OPENAI_API_KEY environment variable will be used if available. | `None` | | `openai_client` | `AsyncOpenAI | None` | An existing AsyncOpenAI client to use. If provided, base_url, api_key, and http_client must be None. | `None` | | `http_client` | `AsyncClient | None` | An existing httpx.AsyncClient to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/openai.py`\n\n```python\ndef __init__(\n    self,\n    base_url: str | None = None,\n    api_key: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: httpx.AsyncClient | None = None,\n) -> None:\n    \"\"\"Create a new OpenAI provider.\n\n    Args:\n        base_url: The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable\n            will be used if available. Otherwise, defaults to OpenAI's base url.\n        api_key: The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable\n            will be used if available.\n        openai_client: An existing\n            [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)\n            client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\n        http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n    \"\"\"\n    # This is a workaround for the OpenAI client requiring an API key, whilst locally served,\n    # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.\n    if api_key is None and 'OPENAI_API_KEY' not in os.environ and base_url is not None and openai_client is None:\n        api_key = 'api-key-not-set'\n\n    if openai_client is not None:\n        assert base_url is None, 'Cannot provide both `openai_client` and `base_url`'\n        assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'\n        assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'\n        self._client = openai_client\n    elif http_client is not None:\n        self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n    else:\n        http_client = cached_async_http_client(provider='openai')\n        self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n\n```\n\n### DeepSeekProvider\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for DeepSeek API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/deepseek.py`\n\n```python\nclass DeepSeekProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for DeepSeek API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'deepseek'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://api.deepseek.com'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        profile = deepseek_model_profile(model_name)\n\n        # As DeepSeekProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,\n        # we need to maintain that behavior unless json_schema_transformer is set explicitly.\n        # This was not the case when using a DeepSeek model with another model class (e.g. BedrockConverseModel or GroqModel),\n        # so we won't do this in `deepseek_model_profile` unless we learn it's always needed.\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        api_key = api_key or os.getenv('DEEPSEEK_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `DEEPSEEK_API_KEY` environment variable or pass it via `DeepSeekProvider(api_key=...)`'\n                'to use the DeepSeek provider.'\n            )\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='deepseek')\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```\n\n### BedrockModelProfile\n\nBases: `ModelProfile`\n\nProfile for models used with BedrockModel.\n\nALL FIELDS MUST BE `bedrock_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`\n\n```python\n@dataclass(kw_only=True)\nclass BedrockModelProfile(ModelProfile):\n    \"\"\"Profile for models used with BedrockModel.\n\n    ALL FIELDS MUST BE `bedrock_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n    \"\"\"\n\n    bedrock_supports_tool_choice: bool = False\n    bedrock_tool_result_format: Literal['text', 'json'] = 'text'\n    bedrock_send_back_thinking_parts: bool = False\n\n```\n\n### bedrock_amazon_model_profile\n\n```python\nbedrock_amazon_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n\n```\n\nGet the model profile for an Amazon model used via Bedrock.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`\n\n```python\ndef bedrock_amazon_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for an Amazon model used via Bedrock.\"\"\"\n    profile = amazon_model_profile(model_name)\n    if 'nova' in model_name:\n        return BedrockModelProfile(bedrock_supports_tool_choice=True).update(profile)\n    return profile\n\n```\n\n### bedrock_deepseek_model_profile\n\n```python\nbedrock_deepseek_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n\n```\n\nGet the model profile for a DeepSeek model used via Bedrock.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`\n\n```python\ndef bedrock_deepseek_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for a DeepSeek model used via Bedrock.\"\"\"\n    profile = deepseek_model_profile(model_name)\n    if 'r1' in model_name:\n        return BedrockModelProfile(bedrock_send_back_thinking_parts=True).update(profile)\n    return profile  # pragma: no cover\n\n```\n\n### BedrockProvider\n\nBases: `Provider[BaseClient]`\n\nProvider for AWS Bedrock.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`\n\n```python\nclass BedrockProvider(Provider[BaseClient]):\n    \"\"\"Provider for AWS Bedrock.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'bedrock'\n\n    @property\n    def base_url(self) -> str:\n        return self._client.meta.endpoint_url\n\n    @property\n    def client(self) -> BaseClient:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        provider_to_profile: dict[str, Callable[[str], ModelProfile | None]] = {\n            'anthropic': lambda model_name: BedrockModelProfile(\n                bedrock_supports_tool_choice=True, bedrock_send_back_thinking_parts=True\n            ).update(anthropic_model_profile(model_name)),\n            'mistral': lambda model_name: BedrockModelProfile(bedrock_tool_result_format='json').update(\n                mistral_model_profile(model_name)\n            ),\n            'cohere': cohere_model_profile,\n            'amazon': bedrock_amazon_model_profile,\n            'meta': meta_model_profile,\n            'deepseek': bedrock_deepseek_model_profile,\n        }\n\n        # Split the model name into parts\n        parts = model_name.split('.', 2)\n\n        # Handle regional prefixes (e.g. \"us.\")\n        if len(parts) > 2 and len(parts[0]) == 2:\n            parts = parts[1:]\n\n        if len(parts) < 2:\n            return None\n\n        provider = parts[0]\n        model_name_with_version = parts[1]\n\n        # Remove version suffix if it matches the format (e.g. \"-v1:0\" or \"-v14\")\n        version_match = re.match(r'(.+)-v\\d+(?::\\d+)?$', model_name_with_version)\n        if version_match:\n            model_name = version_match.group(1)\n        else:\n            model_name = model_name_with_version\n\n        if provider in provider_to_profile:\n            return provider_to_profile[provider](model_name)\n\n        return None\n\n    @overload\n    def __init__(self, *, bedrock_client: BaseClient) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        *,\n        api_key: str,\n        base_url: str | None = None,\n        region_name: str | None = None,\n        profile_name: str | None = None,\n        aws_read_timeout: float | None = None,\n        aws_connect_timeout: float | None = None,\n    ) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        *,\n        aws_access_key_id: str | None = None,\n        aws_secret_access_key: str | None = None,\n        aws_session_token: str | None = None,\n        base_url: str | None = None,\n        region_name: str | None = None,\n        profile_name: str | None = None,\n        aws_read_timeout: float | None = None,\n        aws_connect_timeout: float | None = None,\n    ) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        bedrock_client: BaseClient | None = None,\n        aws_access_key_id: str | None = None,\n        aws_secret_access_key: str | None = None,\n        aws_session_token: str | None = None,\n        base_url: str | None = None,\n        region_name: str | None = None,\n        profile_name: str | None = None,\n        api_key: str | None = None,\n        aws_read_timeout: float | None = None,\n        aws_connect_timeout: float | None = None,\n    ) -> None:\n        \"\"\"Initialize the Bedrock provider.\n\n        Args:\n            bedrock_client: A boto3 client for Bedrock Runtime. If provided, other arguments are ignored.\n            aws_access_key_id: The AWS access key ID. If not set, the `AWS_ACCESS_KEY_ID` environment variable will be used if available.\n            aws_secret_access_key: The AWS secret access key. If not set, the `AWS_SECRET_ACCESS_KEY` environment variable will be used if available.\n            aws_session_token: The AWS session token. If not set, the `AWS_SESSION_TOKEN` environment variable will be used if available.\n            api_key: The API key for Bedrock client. Can be used instead of `aws_access_key_id`, `aws_secret_access_key`, and `aws_session_token`. If not set, the `AWS_BEARER_TOKEN_BEDROCK` environment variable will be used if available.\n            base_url: The base URL for the Bedrock client.\n            region_name: The AWS region name. If not set, the `AWS_DEFAULT_REGION` environment variable will be used if available.\n            profile_name: The AWS profile name.\n            aws_read_timeout: The read timeout for Bedrock client.\n            aws_connect_timeout: The connect timeout for Bedrock client.\n        \"\"\"\n        if bedrock_client is not None:\n            self._client = bedrock_client\n        else:\n            read_timeout = aws_read_timeout or float(os.getenv('AWS_READ_TIMEOUT', 300))\n            connect_timeout = aws_connect_timeout or float(os.getenv('AWS_CONNECT_TIMEOUT', 60))\n            config: dict[str, Any] = {\n                'read_timeout': read_timeout,\n                'connect_timeout': connect_timeout,\n            }\n            try:\n                if api_key is not None:\n                    session = boto3.Session(\n                        botocore_session=_BearerTokenSession(api_key),\n                        region_name=region_name,\n                        profile_name=profile_name,\n                    )\n                    config['signature_version'] = 'bearer'\n                else:\n                    session = boto3.Session(\n                        aws_access_key_id=aws_access_key_id,\n                        aws_secret_access_key=aws_secret_access_key,\n                        aws_session_token=aws_session_token,\n                        region_name=region_name,\n                        profile_name=profile_name,\n                    )\n                self._client = session.client(  # type: ignore[reportUnknownMemberType]\n                    'bedrock-runtime',\n                    config=Config(**config),\n                    endpoint_url=base_url,\n                )\n            except NoRegionError as exc:  # pragma: no cover\n                raise UserError('You must provide a `region_name` or a boto3 client for Bedrock Runtime.') from exc\n\n```\n\n#### __init__\n\n```python\n__init__(*, bedrock_client: BaseClient) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str,\n    base_url: str | None = None,\n    region_name: str | None = None,\n    profile_name: str | None = None,\n    aws_read_timeout: float | None = None,\n    aws_connect_timeout: float | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    aws_access_key_id: str | None = None,\n    aws_secret_access_key: str | None = None,\n    aws_session_token: str | None = None,\n    base_url: str | None = None,\n    region_name: str | None = None,\n    profile_name: str | None = None,\n    aws_read_timeout: float | None = None,\n    aws_connect_timeout: float | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    bedrock_client: BaseClient | None = None,\n    aws_access_key_id: str | None = None,\n    aws_secret_access_key: str | None = None,\n    aws_session_token: str | None = None,\n    base_url: str | None = None,\n    region_name: str | None = None,\n    profile_name: str | None = None,\n    api_key: str | None = None,\n    aws_read_timeout: float | None = None,\n    aws_connect_timeout: float | None = None\n) -> None\n\n```\n\nInitialize the Bedrock provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `bedrock_client` | `BaseClient | None` | A boto3 client for Bedrock Runtime. If provided, other arguments are ignored. | `None` | | `aws_access_key_id` | `str | None` | The AWS access key ID. If not set, the AWS_ACCESS_KEY_ID environment variable will be used if available. | `None` | | `aws_secret_access_key` | `str | None` | The AWS secret access key. If not set, the AWS_SECRET_ACCESS_KEY environment variable will be used if available. | `None` | | `aws_session_token` | `str | None` | The AWS session token. If not set, the AWS_SESSION_TOKEN environment variable will be used if available. | `None` | | `api_key` | `str | None` | The API key for Bedrock client. Can be used instead of aws_access_key_id, aws_secret_access_key, and aws_session_token. If not set, the AWS_BEARER_TOKEN_BEDROCK environment variable will be used if available. | `None` | | `base_url` | `str | None` | The base URL for the Bedrock client. | `None` | | `region_name` | `str | None` | The AWS region name. If not set, the AWS_DEFAULT_REGION environment variable will be used if available. | `None` | | `profile_name` | `str | None` | The AWS profile name. | `None` | | `aws_read_timeout` | `float | None` | The read timeout for Bedrock client. | `None` | | `aws_connect_timeout` | `float | None` | The connect timeout for Bedrock client. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    bedrock_client: BaseClient | None = None,\n    aws_access_key_id: str | None = None,\n    aws_secret_access_key: str | None = None,\n    aws_session_token: str | None = None,\n    base_url: str | None = None,\n    region_name: str | None = None,\n    profile_name: str | None = None,\n    api_key: str | None = None,\n    aws_read_timeout: float | None = None,\n    aws_connect_timeout: float | None = None,\n) -> None:\n    \"\"\"Initialize the Bedrock provider.\n\n    Args:\n        bedrock_client: A boto3 client for Bedrock Runtime. If provided, other arguments are ignored.\n        aws_access_key_id: The AWS access key ID. If not set, the `AWS_ACCESS_KEY_ID` environment variable will be used if available.\n        aws_secret_access_key: The AWS secret access key. If not set, the `AWS_SECRET_ACCESS_KEY` environment variable will be used if available.\n        aws_session_token: The AWS session token. If not set, the `AWS_SESSION_TOKEN` environment variable will be used if available.\n        api_key: The API key for Bedrock client. Can be used instead of `aws_access_key_id`, `aws_secret_access_key`, and `aws_session_token`. If not set, the `AWS_BEARER_TOKEN_BEDROCK` environment variable will be used if available.\n        base_url: The base URL for the Bedrock client.\n        region_name: The AWS region name. If not set, the `AWS_DEFAULT_REGION` environment variable will be used if available.\n        profile_name: The AWS profile name.\n        aws_read_timeout: The read timeout for Bedrock client.\n        aws_connect_timeout: The connect timeout for Bedrock client.\n    \"\"\"\n    if bedrock_client is not None:\n        self._client = bedrock_client\n    else:\n        read_timeout = aws_read_timeout or float(os.getenv('AWS_READ_TIMEOUT', 300))\n        connect_timeout = aws_connect_timeout or float(os.getenv('AWS_CONNECT_TIMEOUT', 60))\n        config: dict[str, Any] = {\n            'read_timeout': read_timeout,\n            'connect_timeout': connect_timeout,\n        }\n        try:\n            if api_key is not None:\n                session = boto3.Session(\n                    botocore_session=_BearerTokenSession(api_key),\n                    region_name=region_name,\n                    profile_name=profile_name,\n                )\n                config['signature_version'] = 'bearer'\n            else:\n                session = boto3.Session(\n                    aws_access_key_id=aws_access_key_id,\n                    aws_secret_access_key=aws_secret_access_key,\n                    aws_session_token=aws_session_token,\n                    region_name=region_name,\n                    profile_name=profile_name,\n                )\n            self._client = session.client(  # type: ignore[reportUnknownMemberType]\n                'bedrock-runtime',\n                config=Config(**config),\n                endpoint_url=base_url,\n            )\n        except NoRegionError as exc:  # pragma: no cover\n            raise UserError('You must provide a `region_name` or a boto3 client for Bedrock Runtime.') from exc\n\n```\n\n### groq_moonshotai_model_profile\n\n```python\ngroq_moonshotai_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n\n```\n\nGet the model profile for an MoonshotAI model used with the Groq provider.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/groq.py`\n\n```python\ndef groq_moonshotai_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for an MoonshotAI model used with the Groq provider.\"\"\"\n    return ModelProfile(supports_json_object_output=True, supports_json_schema_output=True).update(\n        moonshotai_model_profile(model_name)\n    )\n\n```\n\n### meta_groq_model_profile\n\n```python\nmeta_groq_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n\n```\n\nGet the model profile for a Meta model used with the Groq provider.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/groq.py`\n\n```python\ndef meta_groq_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for a Meta model used with the Groq provider.\"\"\"\n    if model_name in {'llama-4-maverick-17b-128e-instruct', 'llama-4-scout-17b-16e-instruct'}:\n        return ModelProfile(supports_json_object_output=True, supports_json_schema_output=True).update(\n            meta_model_profile(model_name)\n        )\n    else:\n        return meta_model_profile(model_name)\n\n```\n\n### GroqProvider\n\nBases: `Provider[AsyncGroq]`\n\nProvider for Groq API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/groq.py`\n\n```python\nclass GroqProvider(Provider[AsyncGroq]):\n    \"\"\"Provider for Groq API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'groq'\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.base_url)\n\n    @property\n    def client(self) -> AsyncGroq:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        prefix_to_profile = {\n            'llama': meta_model_profile,\n            'meta-llama/': meta_groq_model_profile,\n            'gemma': google_model_profile,\n            'qwen': qwen_model_profile,\n            'deepseek': deepseek_model_profile,\n            'mistral': mistral_model_profile,\n            'moonshotai/': groq_moonshotai_model_profile,\n            'compound-': groq_model_profile,\n            'openai/': openai_model_profile,\n        }\n\n        for prefix, profile_func in prefix_to_profile.items():\n            model_name = model_name.lower()\n            if model_name.startswith(prefix):\n                if prefix.endswith('/'):\n                    model_name = model_name[len(prefix) :]\n                return profile_func(model_name)\n\n        return None\n\n    @overload\n    def __init__(self, *, groq_client: AsyncGroq | None = None) -> None: ...\n\n    @overload\n    def __init__(\n        self, *, api_key: str | None = None, base_url: str | None = None, http_client: httpx.AsyncClient | None = None\n    ) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        base_url: str | None = None,\n        groq_client: AsyncGroq | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        \"\"\"Create a new Groq provider.\n\n        Args:\n            api_key: The API key to use for authentication, if not provided, the `GROQ_API_KEY` environment variable\n                will be used if available.\n            base_url: The base url for the Groq requests. If not provided, the `GROQ_BASE_URL` environment variable\n                will be used if available. Otherwise, defaults to Groq's base url.\n            groq_client: An existing\n                [`AsyncGroq`](https://github.com/groq/groq-python?tab=readme-ov-file#async-usage)\n                client to use. If provided, `api_key` and `http_client` must be `None`.\n            http_client: An existing `AsyncHTTPClient` to use for making HTTP requests.\n        \"\"\"\n        if groq_client is not None:\n            assert http_client is None, 'Cannot provide both `groq_client` and `http_client`'\n            assert api_key is None, 'Cannot provide both `groq_client` and `api_key`'\n            assert base_url is None, 'Cannot provide both `groq_client` and `base_url`'\n            self._client = groq_client\n        else:\n            api_key = api_key or os.getenv('GROQ_API_KEY')\n            base_url = base_url or os.getenv('GROQ_BASE_URL', 'https://api.groq.com')\n\n            if not api_key:\n                raise UserError(\n                    'Set the `GROQ_API_KEY` environment variable or pass it via `GroqProvider(api_key=...)`'\n                    'to use the Groq provider.'\n                )\n            elif http_client is not None:\n                self._client = AsyncGroq(base_url=base_url, api_key=api_key, http_client=http_client)\n            else:\n                http_client = cached_async_http_client(provider='groq')\n                self._client = AsyncGroq(base_url=base_url, api_key=api_key, http_client=http_client)\n\n```\n\n#### __init__\n\n```python\n__init__(*, groq_client: AsyncGroq | None = None) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    groq_client: AsyncGroq | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\nCreate a new Groq provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `api_key` | `str | None` | The API key to use for authentication, if not provided, the GROQ_API_KEY environment variable will be used if available. | `None` | | `base_url` | `str | None` | The base url for the Groq requests. If not provided, the GROQ_BASE_URL environment variable will be used if available. Otherwise, defaults to Groq's base url. | `None` | | `groq_client` | `AsyncGroq | None` | An existing AsyncGroq client to use. If provided, api_key and http_client must be None. | `None` | | `http_client` | `AsyncClient | None` | An existing AsyncHTTPClient to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/groq.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    groq_client: AsyncGroq | None = None,\n    http_client: httpx.AsyncClient | None = None,\n) -> None:\n    \"\"\"Create a new Groq provider.\n\n    Args:\n        api_key: The API key to use for authentication, if not provided, the `GROQ_API_KEY` environment variable\n            will be used if available.\n        base_url: The base url for the Groq requests. If not provided, the `GROQ_BASE_URL` environment variable\n            will be used if available. Otherwise, defaults to Groq's base url.\n        groq_client: An existing\n            [`AsyncGroq`](https://github.com/groq/groq-python?tab=readme-ov-file#async-usage)\n            client to use. If provided, `api_key` and `http_client` must be `None`.\n        http_client: An existing `AsyncHTTPClient` to use for making HTTP requests.\n    \"\"\"\n    if groq_client is not None:\n        assert http_client is None, 'Cannot provide both `groq_client` and `http_client`'\n        assert api_key is None, 'Cannot provide both `groq_client` and `api_key`'\n        assert base_url is None, 'Cannot provide both `groq_client` and `base_url`'\n        self._client = groq_client\n    else:\n        api_key = api_key or os.getenv('GROQ_API_KEY')\n        base_url = base_url or os.getenv('GROQ_BASE_URL', 'https://api.groq.com')\n\n        if not api_key:\n            raise UserError(\n                'Set the `GROQ_API_KEY` environment variable or pass it via `GroqProvider(api_key=...)`'\n                'to use the Groq provider.'\n            )\n        elif http_client is not None:\n            self._client = AsyncGroq(base_url=base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='groq')\n            self._client = AsyncGroq(base_url=base_url, api_key=api_key, http_client=http_client)\n\n```\n\n### AzureProvider\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Azure OpenAI API.\n\nSee <https://azure.microsoft.com/en-us/products/ai-foundry> for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/azure.py`\n\n```python\nclass AzureProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for Azure OpenAI API.\n\n    See <https://azure.microsoft.com/en-us/products/ai-foundry> for more information.\n    \"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'azure'\n\n    @property\n    def base_url(self) -> str:\n        assert self._base_url is not None\n        return self._base_url\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        model_name = model_name.lower()\n\n        prefix_to_profile = {\n            'llama': meta_model_profile,\n            'meta-': meta_model_profile,\n            'deepseek': deepseek_model_profile,\n            'mistralai-': mistral_model_profile,\n            'mistral': mistral_model_profile,\n            'cohere-': cohere_model_profile,\n            'grok': grok_model_profile,\n        }\n\n        for prefix, profile_func in prefix_to_profile.items():\n            if model_name.startswith(prefix):\n                if prefix.endswith('-'):\n                    model_name = model_name[len(prefix) :]\n\n                profile = profile_func(model_name)\n\n                # As AzureProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,\n                # we need to maintain that behavior unless json_schema_transformer is set explicitly\n                return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n        # OpenAI models are unprefixed\n        return openai_model_profile(model_name)\n\n    @overload\n    def __init__(self, *, openai_client: AsyncAzureOpenAI) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        *,\n        azure_endpoint: str | None = None,\n        api_version: str | None = None,\n        api_key: str | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        azure_endpoint: str | None = None,\n        api_version: str | None = None,\n        api_key: str | None = None,\n        openai_client: AsyncAzureOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        \"\"\"Create a new Azure provider.\n\n        Args:\n            azure_endpoint: The Azure endpoint to use for authentication, if not provided, the `AZURE_OPENAI_ENDPOINT`\n                environment variable will be used if available.\n            api_version: The API version to use for authentication, if not provided, the `OPENAI_API_VERSION`\n                environment variable will be used if available.\n            api_key: The API key to use for authentication, if not provided, the `AZURE_OPENAI_API_KEY` environment variable\n                will be used if available.\n            openai_client: An existing\n                [`AsyncAzureOpenAI`](https://github.com/openai/openai-python#microsoft-azure-openai)\n                client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\n            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n        \"\"\"\n        if openai_client is not None:\n            assert azure_endpoint is None, 'Cannot provide both `openai_client` and `azure_endpoint`'\n            assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'\n            assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'\n            self._base_url = str(openai_client.base_url)\n            self._client = openai_client\n        else:\n            azure_endpoint = azure_endpoint or os.getenv('AZURE_OPENAI_ENDPOINT')\n            if not azure_endpoint:\n                raise UserError(\n                    'Must provide one of the `azure_endpoint` argument or the `AZURE_OPENAI_ENDPOINT` environment variable'\n                )\n\n            if not api_key and 'AZURE_OPENAI_API_KEY' not in os.environ:  # pragma: no cover\n                raise UserError(\n                    'Must provide one of the `api_key` argument or the `AZURE_OPENAI_API_KEY` environment variable'\n                )\n\n            if not api_version and 'OPENAI_API_VERSION' not in os.environ:  # pragma: no cover\n                raise UserError(\n                    'Must provide one of the `api_version` argument or the `OPENAI_API_VERSION` environment variable'\n                )\n\n            http_client = http_client or cached_async_http_client(provider='azure')\n            self._client = AsyncAzureOpenAI(\n                azure_endpoint=azure_endpoint,\n                api_key=api_key,\n                api_version=api_version,\n                http_client=http_client,\n            )\n            self._base_url = str(self._client.base_url)\n\n```\n\n#### __init__\n\n```python\n__init__(*, openai_client: AsyncAzureOpenAI) -> None\n\n```\n\n```python\n__init__(\n    *,\n    azure_endpoint: str | None = None,\n    api_version: str | None = None,\n    api_key: str | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    azure_endpoint: str | None = None,\n    api_version: str | None = None,\n    api_key: str | None = None,\n    openai_client: AsyncAzureOpenAI | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\nCreate a new Azure provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `azure_endpoint` | `str | None` | The Azure endpoint to use for authentication, if not provided, the AZURE_OPENAI_ENDPOINT environment variable will be used if available. | `None` | | `api_version` | `str | None` | The API version to use for authentication, if not provided, the OPENAI_API_VERSION environment variable will be used if available. | `None` | | `api_key` | `str | None` | The API key to use for authentication, if not provided, the AZURE_OPENAI_API_KEY environment variable will be used if available. | `None` | | `openai_client` | `AsyncAzureOpenAI | None` | An existing AsyncAzureOpenAI client to use. If provided, base_url, api_key, and http_client must be None. | `None` | | `http_client` | `AsyncClient | None` | An existing httpx.AsyncClient to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/azure.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    azure_endpoint: str | None = None,\n    api_version: str | None = None,\n    api_key: str | None = None,\n    openai_client: AsyncAzureOpenAI | None = None,\n    http_client: httpx.AsyncClient | None = None,\n) -> None:\n    \"\"\"Create a new Azure provider.\n\n    Args:\n        azure_endpoint: The Azure endpoint to use for authentication, if not provided, the `AZURE_OPENAI_ENDPOINT`\n            environment variable will be used if available.\n        api_version: The API version to use for authentication, if not provided, the `OPENAI_API_VERSION`\n            environment variable will be used if available.\n        api_key: The API key to use for authentication, if not provided, the `AZURE_OPENAI_API_KEY` environment variable\n            will be used if available.\n        openai_client: An existing\n            [`AsyncAzureOpenAI`](https://github.com/openai/openai-python#microsoft-azure-openai)\n            client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\n        http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n    \"\"\"\n    if openai_client is not None:\n        assert azure_endpoint is None, 'Cannot provide both `openai_client` and `azure_endpoint`'\n        assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'\n        assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'\n        self._base_url = str(openai_client.base_url)\n        self._client = openai_client\n    else:\n        azure_endpoint = azure_endpoint or os.getenv('AZURE_OPENAI_ENDPOINT')\n        if not azure_endpoint:\n            raise UserError(\n                'Must provide one of the `azure_endpoint` argument or the `AZURE_OPENAI_ENDPOINT` environment variable'\n            )\n\n        if not api_key and 'AZURE_OPENAI_API_KEY' not in os.environ:  # pragma: no cover\n            raise UserError(\n                'Must provide one of the `api_key` argument or the `AZURE_OPENAI_API_KEY` environment variable'\n            )\n\n        if not api_version and 'OPENAI_API_VERSION' not in os.environ:  # pragma: no cover\n            raise UserError(\n                'Must provide one of the `api_version` argument or the `OPENAI_API_VERSION` environment variable'\n            )\n\n        http_client = http_client or cached_async_http_client(provider='azure')\n        self._client = AsyncAzureOpenAI(\n            azure_endpoint=azure_endpoint,\n            api_key=api_key,\n            api_version=api_version,\n            http_client=http_client,\n        )\n        self._base_url = str(self._client.base_url)\n\n```\n\n### CohereProvider\n\nBases: `Provider[AsyncClientV2]`\n\nProvider for Cohere API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/cohere.py`\n\n```python\nclass CohereProvider(Provider[AsyncClientV2]):\n    \"\"\"Provider for Cohere API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'cohere'\n\n    @property\n    def base_url(self) -> str:\n        client_wrapper = self.client._client_wrapper  # type: ignore\n        return str(client_wrapper.get_base_url())\n\n    @property\n    def client(self) -> AsyncClientV2:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        return cohere_model_profile(model_name)\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        cohere_client: AsyncClientV2 | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        \"\"\"Create a new Cohere provider.\n\n        Args:\n            api_key: The API key to use for authentication, if not provided, the `CO_API_KEY` environment variable\n                will be used if available.\n            cohere_client: An existing\n                [AsyncClientV2](https://github.com/cohere-ai/cohere-python)\n                client to use. If provided, `api_key` and `http_client` must be `None`.\n            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n        \"\"\"\n        if cohere_client is not None:\n            assert http_client is None, 'Cannot provide both `cohere_client` and `http_client`'\n            assert api_key is None, 'Cannot provide both `cohere_client` and `api_key`'\n            self._client = cohere_client\n        else:\n            api_key = api_key or os.getenv('CO_API_KEY')\n            if not api_key:\n                raise UserError(\n                    'Set the `CO_API_KEY` environment variable or pass it via `CohereProvider(api_key=...)`'\n                    'to use the Cohere provider.'\n                )\n\n            base_url = os.getenv('CO_BASE_URL')\n            if http_client is not None:\n                self._client = AsyncClientV2(api_key=api_key, httpx_client=http_client, base_url=base_url)\n            else:\n                http_client = cached_async_http_client(provider='cohere')\n                self._client = AsyncClientV2(api_key=api_key, httpx_client=http_client, base_url=base_url)\n\n```\n\n#### __init__\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    cohere_client: AsyncClientV2 | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\nCreate a new Cohere provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `api_key` | `str | None` | The API key to use for authentication, if not provided, the CO_API_KEY environment variable will be used if available. | `None` | | `cohere_client` | `AsyncClientV2 | None` | An existing AsyncClientV2 client to use. If provided, api_key and http_client must be None. | `None` | | `http_client` | `AsyncClient | None` | An existing httpx.AsyncClient to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/cohere.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    cohere_client: AsyncClientV2 | None = None,\n    http_client: httpx.AsyncClient | None = None,\n) -> None:\n    \"\"\"Create a new Cohere provider.\n\n    Args:\n        api_key: The API key to use for authentication, if not provided, the `CO_API_KEY` environment variable\n            will be used if available.\n        cohere_client: An existing\n            [AsyncClientV2](https://github.com/cohere-ai/cohere-python)\n            client to use. If provided, `api_key` and `http_client` must be `None`.\n        http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n    \"\"\"\n    if cohere_client is not None:\n        assert http_client is None, 'Cannot provide both `cohere_client` and `http_client`'\n        assert api_key is None, 'Cannot provide both `cohere_client` and `api_key`'\n        self._client = cohere_client\n    else:\n        api_key = api_key or os.getenv('CO_API_KEY')\n        if not api_key:\n            raise UserError(\n                'Set the `CO_API_KEY` environment variable or pass it via `CohereProvider(api_key=...)`'\n                'to use the Cohere provider.'\n            )\n\n        base_url = os.getenv('CO_BASE_URL')\n        if http_client is not None:\n            self._client = AsyncClientV2(api_key=api_key, httpx_client=http_client, base_url=base_url)\n        else:\n            http_client = cached_async_http_client(provider='cohere')\n            self._client = AsyncClientV2(api_key=api_key, httpx_client=http_client, base_url=base_url)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Cerebras API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/cerebras.py`\n\n```python\nclass CerebrasProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for Cerebras API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'cerebras'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://api.cerebras.ai/v1'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        prefix_to_profile = {'llama': meta_model_profile, 'qwen': qwen_model_profile, 'gpt-oss': harmony_model_profile}\n\n        profile = None\n        for prefix, profile_func in prefix_to_profile.items():\n            model_name = model_name.lower()\n            if model_name.startswith(prefix):\n                profile = profile_func(model_name)\n\n        # According to https://inference-docs.cerebras.ai/resources/openai#currently-unsupported-openai-features,\n        # Cerebras doesn't support some model settings.\n        unsupported_model_settings = (\n            'frequency_penalty',\n            'logit_bias',\n            'presence_penalty',\n            'parallel_tool_calls',\n            'service_tier',\n        )\n        return OpenAIModelProfile(\n            json_schema_transformer=OpenAIJsonSchemaTransformer,\n            openai_unsupported_model_settings=unsupported_model_settings,\n        ).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        api_key = api_key or os.getenv('CEREBRAS_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `CEREBRAS_API_KEY` environment variable or pass it via `CerebrasProvider(api_key=...)` '\n                'to use the Cerebras provider.'\n            )\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='cerebras')\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```\n\nBases: `Provider[Mistral]`\n\nProvider for Mistral API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/mistral.py`\n\n```python\nclass MistralProvider(Provider[Mistral]):\n    \"\"\"Provider for Mistral API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'mistral'\n\n    @property\n    def base_url(self) -> str:\n        return self.client.sdk_configuration.get_server_details()[0]\n\n    @property\n    def client(self) -> Mistral:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        return mistral_model_profile(model_name)\n\n    @overload\n    def __init__(self, *, mistral_client: Mistral | None = None) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str | None = None, http_client: httpx.AsyncClient | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        mistral_client: Mistral | None = None,\n        base_url: str | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        \"\"\"Create a new Mistral provider.\n\n        Args:\n            api_key: The API key to use for authentication, if not provided, the `MISTRAL_API_KEY` environment variable\n                will be used if available.\n            mistral_client: An existing `Mistral` client to use, if provided, `api_key` and `http_client` must be `None`.\n            base_url: The base url for the Mistral requests.\n            http_client: An existing async client to use for making HTTP requests.\n        \"\"\"\n        if mistral_client is not None:\n            assert http_client is None, 'Cannot provide both `mistral_client` and `http_client`'\n            assert api_key is None, 'Cannot provide both `mistral_client` and `api_key`'\n            assert base_url is None, 'Cannot provide both `mistral_client` and `base_url`'\n            self._client = mistral_client\n        else:\n            api_key = api_key or os.getenv('MISTRAL_API_KEY')\n\n            if not api_key:\n                raise UserError(\n                    'Set the `MISTRAL_API_KEY` environment variable or pass it via `MistralProvider(api_key=...)`'\n                    'to use the Mistral provider.'\n                )\n            elif http_client is not None:\n                self._client = Mistral(api_key=api_key, async_client=http_client, server_url=base_url)\n            else:\n                http_client = cached_async_http_client(provider='mistral')\n                self._client = Mistral(api_key=api_key, async_client=http_client, server_url=base_url)\n\n```\n\n### __init__\n\n```python\n__init__(*, mistral_client: Mistral | None = None) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    mistral_client: Mistral | None = None,\n    base_url: str | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\nCreate a new Mistral provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `api_key` | `str | None` | The API key to use for authentication, if not provided, the MISTRAL_API_KEY environment variable will be used if available. | `None` | | `mistral_client` | `Mistral | None` | An existing Mistral client to use, if provided, api_key and http_client must be None. | `None` | | `base_url` | `str | None` | The base url for the Mistral requests. | `None` | | `http_client` | `AsyncClient | None` | An existing async client to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/mistral.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    mistral_client: Mistral | None = None,\n    base_url: str | None = None,\n    http_client: httpx.AsyncClient | None = None,\n) -> None:\n    \"\"\"Create a new Mistral provider.\n\n    Args:\n        api_key: The API key to use for authentication, if not provided, the `MISTRAL_API_KEY` environment variable\n            will be used if available.\n        mistral_client: An existing `Mistral` client to use, if provided, `api_key` and `http_client` must be `None`.\n        base_url: The base url for the Mistral requests.\n        http_client: An existing async client to use for making HTTP requests.\n    \"\"\"\n    if mistral_client is not None:\n        assert http_client is None, 'Cannot provide both `mistral_client` and `http_client`'\n        assert api_key is None, 'Cannot provide both `mistral_client` and `api_key`'\n        assert base_url is None, 'Cannot provide both `mistral_client` and `base_url`'\n        self._client = mistral_client\n    else:\n        api_key = api_key or os.getenv('MISTRAL_API_KEY')\n\n        if not api_key:\n            raise UserError(\n                'Set the `MISTRAL_API_KEY` environment variable or pass it via `MistralProvider(api_key=...)`'\n                'to use the Mistral provider.'\n            )\n        elif http_client is not None:\n            self._client = Mistral(api_key=api_key, async_client=http_client, server_url=base_url)\n        else:\n            http_client = cached_async_http_client(provider='mistral')\n            self._client = Mistral(api_key=api_key, async_client=http_client, server_url=base_url)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Fireworks AI API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/fireworks.py`\n\n```python\nclass FireworksProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for Fireworks AI API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'fireworks'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://api.fireworks.ai/inference/v1'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        prefix_to_profile = {\n            'llama': meta_model_profile,\n            'qwen': qwen_model_profile,\n            'deepseek': deepseek_model_profile,\n            'mistral': mistral_model_profile,\n            'gemma': google_model_profile,\n        }\n\n        prefix = 'accounts/fireworks/models/'\n\n        profile = None\n        if model_name.startswith(prefix):\n            model_name = model_name[len(prefix) :]\n            for provider, profile_func in prefix_to_profile.items():\n                if model_name.startswith(provider):\n                    profile = profile_func(model_name)\n                    break\n\n        # As the Fireworks API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer,\n        # unless json_schema_transformer is set explicitly\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        api_key = api_key or os.getenv('FIREWORKS_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `FIREWORKS_API_KEY` environment variable or pass it via `FireworksProvider(api_key=...)`'\n                'to use the Fireworks AI provider.'\n            )\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='fireworks')\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Grok API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/grok.py`\n\n```python\nclass GrokProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for Grok API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'grok'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://api.x.ai/v1'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        profile = grok_model_profile(model_name)\n\n        # As the Grok API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer,\n        # unless json_schema_transformer is set explicitly.\n        # Also, Grok does not support strict tool definitions: https://github.com/pydantic/pydantic-ai/issues/1846\n        return OpenAIModelProfile(\n            json_schema_transformer=OpenAIJsonSchemaTransformer, openai_supports_strict_tool_definition=False\n        ).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        api_key = api_key or os.getenv('GROK_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `GROK_API_KEY` environment variable or pass it via `GrokProvider(api_key=...)`'\n                'to use the Grok provider.'\n            )\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='grok')\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Together AI API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/together.py`\n\n```python\nclass TogetherProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for Together AI API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'together'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://api.together.xyz/v1'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        provider_to_profile = {\n            'deepseek-ai': deepseek_model_profile,\n            'google': google_model_profile,\n            'qwen': qwen_model_profile,\n            'meta-llama': meta_model_profile,\n            'mistralai': mistral_model_profile,\n        }\n\n        profile = None\n\n        model_name = model_name.lower()\n        provider, model_name = model_name.split('/', 1)\n        if provider in provider_to_profile:\n            profile = provider_to_profile[provider](model_name)\n\n        # As the Together API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer,\n        # unless json_schema_transformer is set explicitly\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        api_key = api_key or os.getenv('TOGETHER_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `TOGETHER_API_KEY` environment variable or pass it via `TogetherProvider(api_key=...)`'\n                'to use the Together AI provider.'\n            )\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='together')\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Heroku API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/heroku.py`\n\n```python\nclass HerokuProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for Heroku API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'heroku'\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.base_url)\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        # As the Heroku API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer.\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        base_url: str | None = None,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        if openai_client is not None:\n            assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'\n            assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'\n            self._client = openai_client\n        else:\n            api_key = api_key or os.getenv('HEROKU_INFERENCE_KEY')\n            if not api_key:\n                raise UserError(\n                    'Set the `HEROKU_INFERENCE_KEY` environment variable or pass it via `HerokuProvider(api_key=...)`'\n                    'to use the Heroku provider.'\n                )\n\n            base_url = base_url or os.getenv('HEROKU_INFERENCE_URL', 'https://us.inference.heroku.com')\n            base_url = base_url.rstrip('/') + '/v1'\n\n            if http_client is not None:\n                self._client = AsyncOpenAI(api_key=api_key, http_client=http_client, base_url=base_url)\n            else:\n                http_client = cached_async_http_client(provider='heroku')\n                self._client = AsyncOpenAI(api_key=api_key, http_client=http_client, base_url=base_url)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for GitHub Models API.\n\nGitHub Models provides access to various AI models through an OpenAI-compatible API. See <https://docs.github.com/en/github-models> for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/github.py`\n\n```python\nclass GitHubProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for GitHub Models API.\n\n    GitHub Models provides access to various AI models through an OpenAI-compatible API.\n    See <https://docs.github.com/en/github-models> for more information.\n    \"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'github'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://models.github.ai/inference'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        provider_to_profile = {\n            'xai': grok_model_profile,\n            'meta': meta_model_profile,\n            'microsoft': openai_model_profile,\n            'mistral-ai': mistral_model_profile,\n            'cohere': cohere_model_profile,\n            'deepseek': deepseek_model_profile,\n        }\n\n        profile = None\n\n        # If the model name does not contain a provider prefix, we assume it's an OpenAI model\n        if '/' not in model_name:\n            return openai_model_profile(model_name)\n\n        provider, model_name = model_name.lower().split('/', 1)\n        if provider in provider_to_profile:\n            model_name, *_ = model_name.split(':', 1)  # drop tags\n            profile = provider_to_profile[provider](model_name)\n\n        # As GitHubProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,\n        # we need to maintain that behavior unless json_schema_transformer is set explicitly\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        \"\"\"Create a new GitHub Models provider.\n\n        Args:\n            api_key: The GitHub token to use for authentication. If not provided, the `GITHUB_API_KEY`\n                environment variable will be used if available.\n            openai_client: An existing `AsyncOpenAI` client to use. If provided, `api_key` and `http_client` must be `None`.\n            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n        \"\"\"\n        api_key = api_key or os.getenv('GITHUB_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `GITHUB_API_KEY` environment variable or pass it via `GitHubProvider(api_key=...)`'\n                ' to use the GitHub Models provider.'\n            )\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='github')\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```\n\n### __init__\n\n```python\n__init__() -> None\n\n```\n\n```python\n__init__(*, api_key: str) -> None\n\n```\n\n```python\n__init__(*, api_key: str, http_client: AsyncClient) -> None\n\n```\n\n```python\n__init__(\n    *, openai_client: AsyncOpenAI | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\nCreate a new GitHub Models provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `api_key` | `str | None` | The GitHub token to use for authentication. If not provided, the GITHUB_API_KEY environment variable will be used if available. | `None` | | `openai_client` | `AsyncOpenAI | None` | An existing AsyncOpenAI client to use. If provided, api_key and http_client must be None. | `None` | | `http_client` | `AsyncClient | None` | An existing httpx.AsyncClient to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/github.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: httpx.AsyncClient | None = None,\n) -> None:\n    \"\"\"Create a new GitHub Models provider.\n\n    Args:\n        api_key: The GitHub token to use for authentication. If not provided, the `GITHUB_API_KEY`\n            environment variable will be used if available.\n        openai_client: An existing `AsyncOpenAI` client to use. If provided, `api_key` and `http_client` must be `None`.\n        http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n    \"\"\"\n    api_key = api_key or os.getenv('GITHUB_API_KEY')\n    if not api_key and openai_client is None:\n        raise UserError(\n            'Set the `GITHUB_API_KEY` environment variable or pass it via `GitHubProvider(api_key=...)`'\n            ' to use the GitHub Models provider.'\n        )\n\n    if openai_client is not None:\n        self._client = openai_client\n    elif http_client is not None:\n        self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n    else:\n        http_client = cached_async_http_client(provider='github')\n        self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for OpenRouter API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/openrouter.py`\n\n```python\nclass OpenRouterProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for OpenRouter API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'openrouter'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://openrouter.ai/api/v1'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        provider_to_profile = {\n            'google': google_model_profile,\n            'openai': openai_model_profile,\n            'anthropic': anthropic_model_profile,\n            'mistralai': mistral_model_profile,\n            'qwen': qwen_model_profile,\n            'x-ai': grok_model_profile,\n            'cohere': cohere_model_profile,\n            'amazon': amazon_model_profile,\n            'deepseek': deepseek_model_profile,\n            'meta-llama': meta_model_profile,\n            'moonshotai': moonshotai_model_profile,\n        }\n\n        profile = None\n\n        provider, model_name = model_name.split('/', 1)\n        if provider in provider_to_profile:\n            model_name, *_ = model_name.split(':', 1)  # drop tags\n            profile = provider_to_profile[provider](model_name)\n\n        # As OpenRouterProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,\n        # we need to maintain that behavior unless json_schema_transformer is set explicitly\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, app_url: str, app_title: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, app_url: str, app_title: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        app_url: str | None = None,\n        app_title: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        \"\"\"Configure the provider with either an API key or prebuilt client.\n\n        Args:\n            api_key: OpenRouter API key. Falls back to ``OPENROUTER_API_KEY``\n                when omitted and required unless ``openai_client`` is provided.\n            app_url: Optional url for app attribution. Falls back to\n                ``OPENROUTER_APP_URL`` when omitted.\n            app_title: Optional title for app attribution. Falls back to\n                ``OPENROUTER_APP_TITLE`` when omitted.\n            openai_client: Existing ``AsyncOpenAI`` client to reuse instead of\n                creating one internally.\n            http_client: Custom ``httpx.AsyncClient`` to pass into the\n                ``AsyncOpenAI`` constructor when building a client.\n\n        Raises:\n            UserError: If no API key is available and no ``openai_client`` is\n                provided.\n        \"\"\"\n        api_key = api_key or os.getenv('OPENROUTER_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `OPENROUTER_API_KEY` environment variable or pass it via `OpenRouterProvider(api_key=...)`'\n                'to use the OpenRouter provider.'\n            )\n\n        attribution_headers: dict[str, str] = {}\n        if http_referer := app_url or os.getenv('OPENROUTER_APP_URL'):\n            attribution_headers['HTTP-Referer'] = http_referer\n        if x_title := app_title or os.getenv('OPENROUTER_APP_TITLE'):\n            attribution_headers['X-Title'] = x_title\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(\n                base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=attribution_headers\n            )\n        else:\n            http_client = cached_async_http_client(provider='openrouter')\n            self._client = AsyncOpenAI(\n                base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=attribution_headers\n            )\n\n```\n\n### __init__\n\n```python\n__init__() -> None\n\n```\n\n```python\n__init__(*, api_key: str) -> None\n\n```\n\n```python\n__init__(*, api_key: str, http_client: AsyncClient) -> None\n\n```\n\n```python\n__init__(\n    *, api_key: str, app_url: str, app_title: str\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str,\n    app_url: str,\n    app_title: str,\n    http_client: AsyncClient\n) -> None\n\n```\n\n```python\n__init__(*, http_client: AsyncClient) -> None\n\n```\n\n```python\n__init__(\n    *, openai_client: AsyncOpenAI | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    app_url: str | None = None,\n    app_title: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\nConfigure the provider with either an API key or prebuilt client.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `api_key` | `str | None` | OpenRouter API key. Falls back to OPENROUTER_API_KEY when omitted and required unless openai_client is provided. | `None` | | `app_url` | `str | None` | Optional url for app attribution. Falls back to OPENROUTER_APP_URL when omitted. | `None` | | `app_title` | `str | None` | Optional title for app attribution. Falls back to OPENROUTER_APP_TITLE when omitted. | `None` | | `openai_client` | `AsyncOpenAI | None` | Existing AsyncOpenAI client to reuse instead of creating one internally. | `None` | | `http_client` | `AsyncClient | None` | Custom httpx.AsyncClient to pass into the AsyncOpenAI constructor when building a client. | `None` |\n\nRaises:\n\n| Type | Description | | --- | --- | | `UserError` | If no API key is available and no openai_client is provided. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/openrouter.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    app_url: str | None = None,\n    app_title: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: httpx.AsyncClient | None = None,\n) -> None:\n    \"\"\"Configure the provider with either an API key or prebuilt client.\n\n    Args:\n        api_key: OpenRouter API key. Falls back to ``OPENROUTER_API_KEY``\n            when omitted and required unless ``openai_client`` is provided.\n        app_url: Optional url for app attribution. Falls back to\n            ``OPENROUTER_APP_URL`` when omitted.\n        app_title: Optional title for app attribution. Falls back to\n            ``OPENROUTER_APP_TITLE`` when omitted.\n        openai_client: Existing ``AsyncOpenAI`` client to reuse instead of\n            creating one internally.\n        http_client: Custom ``httpx.AsyncClient`` to pass into the\n            ``AsyncOpenAI`` constructor when building a client.\n\n    Raises:\n        UserError: If no API key is available and no ``openai_client`` is\n            provided.\n    \"\"\"\n    api_key = api_key or os.getenv('OPENROUTER_API_KEY')\n    if not api_key and openai_client is None:\n        raise UserError(\n            'Set the `OPENROUTER_API_KEY` environment variable or pass it via `OpenRouterProvider(api_key=...)`'\n            'to use the OpenRouter provider.'\n        )\n\n    attribution_headers: dict[str, str] = {}\n    if http_referer := app_url or os.getenv('OPENROUTER_APP_URL'):\n        attribution_headers['HTTP-Referer'] = http_referer\n    if x_title := app_title or os.getenv('OPENROUTER_APP_TITLE'):\n        attribution_headers['X-Title'] = x_title\n\n    if openai_client is not None:\n        self._client = openai_client\n    elif http_client is not None:\n        self._client = AsyncOpenAI(\n            base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=attribution_headers\n        )\n    else:\n        http_client = cached_async_http_client(provider='openrouter')\n        self._client = AsyncOpenAI(\n            base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=attribution_headers\n        )\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Vercel AI Gateway API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/vercel.py`\n\n```python\nclass VercelProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for Vercel AI Gateway API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'vercel'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://ai-gateway.vercel.sh/v1'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        provider_to_profile = {\n            'anthropic': anthropic_model_profile,\n            'bedrock': amazon_model_profile,\n            'cohere': cohere_model_profile,\n            'deepseek': deepseek_model_profile,\n            'mistral': mistral_model_profile,\n            'openai': openai_model_profile,\n            'vertex': google_model_profile,\n            'xai': grok_model_profile,\n        }\n\n        profile = None\n\n        try:\n            provider, model_name = model_name.split('/', 1)\n        except ValueError:\n            raise UserError(f\"Model name must be in 'provider/model' format, got: {model_name!r}\")\n\n        if provider in provider_to_profile:\n            profile = provider_to_profile[provider](model_name)\n\n        # As VercelProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,\n        # we need to maintain that behavior unless json_schema_transformer is set explicitly\n        return OpenAIModelProfile(\n            json_schema_transformer=OpenAIJsonSchemaTransformer,\n        ).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        # Support Vercel AI Gateway's standard environment variables\n        api_key = api_key or os.getenv('VERCEL_AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')\n\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `VERCEL_AI_GATEWAY_API_KEY` or `VERCEL_OIDC_TOKEN` environment variable '\n                'or pass the API key via `VercelProvider(api_key=...)` to use the Vercel provider.'\n            )\n\n        default_headers = {'http-referer': 'https://ai.pydantic.dev/', 'x-title': 'pydantic-ai'}\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(\n                base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=default_headers\n            )\n        else:\n            http_client = cached_async_http_client(provider='vercel')\n            self._client = AsyncOpenAI(\n                base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=default_headers\n            )\n\n```\n\nBases: `Provider[AsyncInferenceClient]`\n\nProvider for Hugging Face.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/huggingface.py`\n\n```python\nclass HuggingFaceProvider(Provider[AsyncInferenceClient]):\n    \"\"\"Provider for Hugging Face.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'huggingface'\n\n    @property\n    def base_url(self) -> str:\n        return self.client.model  # type: ignore\n\n    @property\n    def client(self) -> AsyncInferenceClient:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        provider_to_profile = {\n            'deepseek-ai': deepseek_model_profile,\n            'google': google_model_profile,\n            'qwen': qwen_model_profile,\n            'meta-llama': meta_model_profile,\n            'mistralai': mistral_model_profile,\n            'moonshotai': moonshotai_model_profile,\n        }\n\n        if '/' not in model_name:\n            return None\n\n        model_name = model_name.lower()\n        provider, model_name = model_name.split('/', 1)\n        if provider in provider_to_profile:\n            return provider_to_profile[provider](model_name)\n\n        return None\n\n    @overload\n    def __init__(self, *, base_url: str, api_key: str | None = None) -> None: ...\n    @overload\n    def __init__(self, *, provider_name: str, api_key: str | None = None) -> None: ...\n    @overload\n    def __init__(self, *, hf_client: AsyncInferenceClient, api_key: str | None = None) -> None: ...\n    @overload\n    def __init__(self, *, hf_client: AsyncInferenceClient, base_url: str, api_key: str | None = None) -> None: ...\n    @overload\n    def __init__(self, *, hf_client: AsyncInferenceClient, provider_name: str, api_key: str | None = None) -> None: ...\n    @overload\n    def __init__(self, *, api_key: str | None = None) -> None: ...\n\n    def __init__(\n        self,\n        base_url: str | None = None,\n        api_key: str | None = None,\n        hf_client: AsyncInferenceClient | None = None,\n        http_client: AsyncClient | None = None,\n        provider_name: str | None = None,\n    ) -> None:\n        \"\"\"Create a new Hugging Face provider.\n\n        Args:\n            base_url: The base url for the Hugging Face requests.\n            api_key: The API key to use for authentication, if not provided, the `HF_TOKEN` environment variable\n                will be used if available.\n            hf_client: An existing\n                [`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)\n                client to use. If not provided, a new instance will be created.\n            http_client: (currently ignored) An existing `httpx.AsyncClient` to use for making HTTP requests.\n            provider_name : Name of the provider to use for inference. available providers can be found in the [HF Inference Providers documentation](https://huggingface.co/docs/inference-providers/index#partners).\n                defaults to \"auto\", which will select the first available provider for the model, the first of the providers available for the model, sorted by the user's order in https://hf.co/settings/inference-providers.\n                If `base_url` is passed, then `provider_name` is not used.\n        \"\"\"\n        api_key = api_key or os.getenv('HF_TOKEN')\n\n        if api_key is None:\n            raise UserError(\n                'Set the `HF_TOKEN` environment variable or pass it via `HuggingFaceProvider(api_key=...)`'\n                'to use the HuggingFace provider.'\n            )\n\n        if http_client is not None:\n            raise ValueError('`http_client` is ignored for HuggingFace provider, please use `hf_client` instead.')\n\n        if base_url is not None and provider_name is not None:\n            raise ValueError('Cannot provide both `base_url` and `provider_name`.')\n\n        if hf_client is None:\n            self._client = AsyncInferenceClient(api_key=api_key, provider=provider_name, base_url=base_url)  # type: ignore\n        else:\n            self._client = hf_client\n\n```\n\n### __init__\n\n```python\n__init__(\n    *, base_url: str, api_key: str | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *, provider_name: str, api_key: str | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    hf_client: AsyncInferenceClient,\n    api_key: str | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    hf_client: AsyncInferenceClient,\n    base_url: str,\n    api_key: str | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    hf_client: AsyncInferenceClient,\n    provider_name: str,\n    api_key: str | None = None\n) -> None\n\n```\n\n```python\n__init__(*, api_key: str | None = None) -> None\n\n```\n\n```python\n__init__(\n    base_url: str | None = None,\n    api_key: str | None = None,\n    hf_client: AsyncInferenceClient | None = None,\n    http_client: AsyncClient | None = None,\n    provider_name: str | None = None,\n) -> None\n\n```\n\nCreate a new Hugging Face provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `base_url` | `str | None` | The base url for the Hugging Face requests. | `None` | | `api_key` | `str | None` | The API key to use for authentication, if not provided, the HF_TOKEN environment variable will be used if available. | `None` | | `hf_client` | `AsyncInferenceClient | None` | An existing AsyncInferenceClient client to use. If not provided, a new instance will be created. | `None` | | `http_client` | `AsyncClient | None` | (currently ignored) An existing httpx.AsyncClient to use for making HTTP requests. | `None` | | `provider_name` | | Name of the provider to use for inference. available providers can be found in the HF Inference Providers documentation. defaults to \"auto\", which will select the first available provider for the model, the first of the providers available for the model, sorted by the user's order in https://hf.co/settings/inference-providers. If base_url is passed, then provider_name is not used. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/huggingface.py`\n\n```python\ndef __init__(\n    self,\n    base_url: str | None = None,\n    api_key: str | None = None,\n    hf_client: AsyncInferenceClient | None = None,\n    http_client: AsyncClient | None = None,\n    provider_name: str | None = None,\n) -> None:\n    \"\"\"Create a new Hugging Face provider.\n\n    Args:\n        base_url: The base url for the Hugging Face requests.\n        api_key: The API key to use for authentication, if not provided, the `HF_TOKEN` environment variable\n            will be used if available.\n        hf_client: An existing\n            [`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)\n            client to use. If not provided, a new instance will be created.\n        http_client: (currently ignored) An existing `httpx.AsyncClient` to use for making HTTP requests.\n        provider_name : Name of the provider to use for inference. available providers can be found in the [HF Inference Providers documentation](https://huggingface.co/docs/inference-providers/index#partners).\n            defaults to \"auto\", which will select the first available provider for the model, the first of the providers available for the model, sorted by the user's order in https://hf.co/settings/inference-providers.\n            If `base_url` is passed, then `provider_name` is not used.\n    \"\"\"\n    api_key = api_key or os.getenv('HF_TOKEN')\n\n    if api_key is None:\n        raise UserError(\n            'Set the `HF_TOKEN` environment variable or pass it via `HuggingFaceProvider(api_key=...)`'\n            'to use the HuggingFace provider.'\n        )\n\n    if http_client is not None:\n        raise ValueError('`http_client` is ignored for HuggingFace provider, please use `hf_client` instead.')\n\n    if base_url is not None and provider_name is not None:\n        raise ValueError('Cannot provide both `base_url` and `provider_name`.')\n\n    if hf_client is None:\n        self._client = AsyncInferenceClient(api_key=api_key, provider=provider_name, base_url=base_url)  # type: ignore\n    else:\n        self._client = hf_client\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for MoonshotAI platform (Kimi models).\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/moonshotai.py`\n\n```python\nclass MoonshotAIProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for MoonshotAI platform (Kimi models).\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'moonshotai'\n\n    @property\n    def base_url(self) -> str:\n        # OpenAI-compatible endpoint, see MoonshotAI docs\n        return 'https://api.moonshot.ai/v1'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        profile = moonshotai_model_profile(model_name)\n\n        # As the MoonshotAI API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer,\n        # unless json_schema_transformer is set explicitly.\n        # Also, MoonshotAI does not support strict tool definitions\n        # https://platform.moonshot.ai/docs/guide/migrating-from-openai-to-kimi#about-tool_choice\n        # \"Please note that the current version of Kimi API does not support the tool_choice=required parameter.\"\n        return OpenAIModelProfile(\n            json_schema_transformer=OpenAIJsonSchemaTransformer,\n            openai_supports_tool_choice_required=False,\n            supports_json_object_output=True,\n        ).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        api_key = api_key or os.getenv('MOONSHOTAI_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `MOONSHOTAI_API_KEY` environment variable or pass it via '\n                '`MoonshotAIProvider(api_key=...)` to use the MoonshotAI provider.'\n            )\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='moonshotai')\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for local or remote Ollama API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/ollama.py`\n\n```python\nclass OllamaProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for local or remote Ollama API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'ollama'\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.base_url)\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        prefix_to_profile = {\n            'llama': meta_model_profile,\n            'gemma': google_model_profile,\n            'qwen': qwen_model_profile,\n            'qwq': qwen_model_profile,\n            'deepseek': deepseek_model_profile,\n            'mistral': mistral_model_profile,\n            'command': cohere_model_profile,\n            'gpt-oss': harmony_model_profile,\n        }\n\n        profile = None\n        for prefix, profile_func in prefix_to_profile.items():\n            model_name = model_name.lower()\n            if model_name.startswith(prefix):\n                profile = profile_func(model_name)\n\n        # As OllamaProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,\n        # we need to maintain that behavior unless json_schema_transformer is set explicitly\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n    def __init__(\n        self,\n        base_url: str | None = None,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        \"\"\"Create a new Ollama provider.\n\n        Args:\n            base_url: The base url for the Ollama requests. If not provided, the `OLLAMA_BASE_URL` environment variable\n                will be used if available.\n            api_key: The API key to use for authentication, if not provided, the `OLLAMA_API_KEY` environment variable\n                will be used if available.\n            openai_client: An existing\n                [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)\n                client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\n            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n        \"\"\"\n        if openai_client is not None:\n            assert base_url is None, 'Cannot provide both `openai_client` and `base_url`'\n            assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'\n            assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'\n            self._client = openai_client\n        else:\n            base_url = base_url or os.getenv('OLLAMA_BASE_URL')\n            if not base_url:\n                raise UserError(\n                    'Set the `OLLAMA_BASE_URL` environment variable or pass it via `OllamaProvider(base_url=...)`'\n                    'to use the Ollama provider.'\n                )\n\n            # This is a workaround for the OpenAI client requiring an API key, whilst locally served,\n            # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.\n            api_key = api_key or os.getenv('OLLAMA_API_KEY') or 'api-key-not-set'\n\n            if http_client is not None:\n                self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n            else:\n                http_client = cached_async_http_client(provider='ollama')\n                self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n\n```\n\n### __init__\n\n```python\n__init__(\n    base_url: str | None = None,\n    api_key: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncClient | None = None,\n) -> None\n\n```\n\nCreate a new Ollama provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `base_url` | `str | None` | The base url for the Ollama requests. If not provided, the OLLAMA_BASE_URL environment variable will be used if available. | `None` | | `api_key` | `str | None` | The API key to use for authentication, if not provided, the OLLAMA_API_KEY environment variable will be used if available. | `None` | | `openai_client` | `AsyncOpenAI | None` | An existing AsyncOpenAI client to use. If provided, base_url, api_key, and http_client must be None. | `None` | | `http_client` | `AsyncClient | None` | An existing httpx.AsyncClient to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/ollama.py`\n\n```python\ndef __init__(\n    self,\n    base_url: str | None = None,\n    api_key: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: httpx.AsyncClient | None = None,\n) -> None:\n    \"\"\"Create a new Ollama provider.\n\n    Args:\n        base_url: The base url for the Ollama requests. If not provided, the `OLLAMA_BASE_URL` environment variable\n            will be used if available.\n        api_key: The API key to use for authentication, if not provided, the `OLLAMA_API_KEY` environment variable\n            will be used if available.\n        openai_client: An existing\n            [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)\n            client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\n        http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n    \"\"\"\n    if openai_client is not None:\n        assert base_url is None, 'Cannot provide both `openai_client` and `base_url`'\n        assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'\n        assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'\n        self._client = openai_client\n    else:\n        base_url = base_url or os.getenv('OLLAMA_BASE_URL')\n        if not base_url:\n            raise UserError(\n                'Set the `OLLAMA_BASE_URL` environment variable or pass it via `OllamaProvider(base_url=...)`'\n                'to use the Ollama provider.'\n            )\n\n        # This is a workaround for the OpenAI client requiring an API key, whilst locally served,\n        # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.\n        api_key = api_key or os.getenv('OLLAMA_API_KEY') or 'api-key-not-set'\n\n        if http_client is not None:\n            self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='ollama')\n            self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for LiteLLM API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/litellm.py`\n\n```python\nclass LiteLLMProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for LiteLLM API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'litellm'\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.base_url)\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        # Map provider prefixes to their profile functions\n        provider_to_profile = {\n            'anthropic': anthropic_model_profile,\n            'openai': openai_model_profile,\n            'google': google_model_profile,\n            'mistralai': mistral_model_profile,\n            'mistral': mistral_model_profile,\n            'cohere': cohere_model_profile,\n            'amazon': amazon_model_profile,\n            'bedrock': amazon_model_profile,\n            'meta-llama': meta_model_profile,\n            'meta': meta_model_profile,\n            'groq': groq_model_profile,\n            'deepseek': deepseek_model_profile,\n            'moonshotai': moonshotai_model_profile,\n            'x-ai': grok_model_profile,\n            'qwen': qwen_model_profile,\n        }\n\n        profile = None\n\n        # Check if model name contains a provider prefix (e.g., \"anthropic/claude-3\")\n        if '/' in model_name:\n            provider_prefix, model_suffix = model_name.split('/', 1)\n            if provider_prefix in provider_to_profile:\n                profile = provider_to_profile[provider_prefix](model_suffix)\n\n        # If no profile found, default to OpenAI profile\n        if profile is None:\n            profile = openai_model_profile(model_name)\n\n        # As LiteLLMProvider is used with OpenAIModel, which uses OpenAIJsonSchemaTransformer,\n        # we maintain that behavior\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n    @overload\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        api_base: str | None = None,\n    ) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        api_base: str | None = None,\n        http_client: AsyncHTTPClient,\n    ) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        api_base: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: AsyncHTTPClient | None = None,\n    ) -> None:\n        \"\"\"Initialize a LiteLLM provider.\n\n        Args:\n            api_key: API key for the model provider. If None, LiteLLM will try to get it from environment variables.\n            api_base: Base URL for the model provider. Use this for custom endpoints or self-hosted models.\n            openai_client: Pre-configured OpenAI client. If provided, other parameters are ignored.\n            http_client: Custom HTTP client to use.\n        \"\"\"\n        if openai_client is not None:\n            self._client = openai_client\n            return\n\n        # Create OpenAI client that will be used with LiteLLM's completion function\n        # The actual API calls will be intercepted and routed through LiteLLM\n        if http_client is not None:\n            self._client = AsyncOpenAI(\n                base_url=api_base, api_key=api_key or 'litellm-placeholder', http_client=http_client\n            )\n        else:\n            http_client = cached_async_http_client(provider='litellm')\n            self._client = AsyncOpenAI(\n                base_url=api_base, api_key=api_key or 'litellm-placeholder', http_client=http_client\n            )\n\n```\n\n### __init__\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    http_client: AsyncClient\n) -> None\n\n```\n\n```python\n__init__(*, openai_client: AsyncOpenAI) -> None\n\n```\n\n```python\n__init__(\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n```\n\nInitialize a LiteLLM provider.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `api_key` | `str | None` | API key for the model provider. If None, LiteLLM will try to get it from environment variables. | `None` | | `api_base` | `str | None` | Base URL for the model provider. Use this for custom endpoints or self-hosted models. | `None` | | `openai_client` | `AsyncOpenAI | None` | Pre-configured OpenAI client. If provided, other parameters are ignored. | `None` | | `http_client` | `AsyncClient | None` | Custom HTTP client to use. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/litellm.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncHTTPClient | None = None,\n) -> None:\n    \"\"\"Initialize a LiteLLM provider.\n\n    Args:\n        api_key: API key for the model provider. If None, LiteLLM will try to get it from environment variables.\n        api_base: Base URL for the model provider. Use this for custom endpoints or self-hosted models.\n        openai_client: Pre-configured OpenAI client. If provided, other parameters are ignored.\n        http_client: Custom HTTP client to use.\n    \"\"\"\n    if openai_client is not None:\n        self._client = openai_client\n        return\n\n    # Create OpenAI client that will be used with LiteLLM's completion function\n    # The actual API calls will be intercepted and routed through LiteLLM\n    if http_client is not None:\n        self._client = AsyncOpenAI(\n            base_url=api_base, api_key=api_key or 'litellm-placeholder', http_client=http_client\n        )\n    else:\n        http_client = cached_async_http_client(provider='litellm')\n        self._client = AsyncOpenAI(\n            base_url=api_base, api_key=api_key or 'litellm-placeholder', http_client=http_client\n        )\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Nebius AI Studio API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/nebius.py`\n\n```python\nclass NebiusProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for Nebius AI Studio API.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'nebius'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://api.studio.nebius.com/v1'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        provider_to_profile = {\n            'meta-llama': meta_model_profile,\n            'deepseek-ai': deepseek_model_profile,\n            'qwen': qwen_model_profile,\n            'google': google_model_profile,\n            'openai': harmony_model_profile,  # used for gpt-oss models on Nebius\n            'mistralai': mistral_model_profile,\n            'moonshotai': moonshotai_model_profile,\n        }\n\n        profile = None\n\n        try:\n            model_name = model_name.lower()\n            provider, model_name = model_name.split('/', 1)\n        except ValueError:\n            raise UserError(f\"Model name must be in 'provider/model' format, got: {model_name!r}\")\n        if provider in provider_to_profile:\n            profile = provider_to_profile[provider](model_name)\n\n        # As NebiusProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,\n        # we need to maintain that behavior unless json_schema_transformer is set explicitly\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        api_key = api_key or os.getenv('NEBIUS_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `NEBIUS_API_KEY` environment variable or pass it via '\n                '`NebiusProvider(api_key=...)` to use the Nebius AI Studio provider.'\n            )\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='nebius')\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for OVHcloud AI Endpoints.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/ovhcloud.py`\n\n```python\nclass OVHcloudProvider(Provider[AsyncOpenAI]):\n    \"\"\"Provider for OVHcloud AI Endpoints.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return 'ovhcloud'\n\n    @property\n    def base_url(self) -> str:\n        return 'https://oai.endpoints.kepler.ai.cloud.ovh.net/v1'\n\n    @property\n    def client(self) -> AsyncOpenAI:\n        return self._client\n\n    def model_profile(self, model_name: str) -> ModelProfile | None:\n        model_name = model_name.lower()\n\n        prefix_to_profile = {\n            'llama': meta_model_profile,\n            'meta-': meta_model_profile,\n            'deepseek': deepseek_model_profile,\n            'mistral': mistral_model_profile,\n            'gpt': harmony_model_profile,\n            'qwen': qwen_model_profile,\n        }\n\n        profile = None\n        for prefix, profile_func in prefix_to_profile.items():\n            if model_name.startswith(prefix):\n                profile = profile_func(model_name)\n\n        # As the OVHcloud AI Endpoints API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer.\n        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)\n\n    @overload\n    def __init__(self) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str) -> None: ...\n\n    @overload\n    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...\n\n    @overload\n    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        openai_client: AsyncOpenAI | None = None,\n        http_client: httpx.AsyncClient | None = None,\n    ) -> None:\n        api_key = api_key or os.getenv('OVHCLOUD_API_KEY')\n        if not api_key and openai_client is None:\n            raise UserError(\n                'Set the `OVHCLOUD_API_KEY` environment variable or pass it via '\n                '`OVHcloudProvider(api_key=...)` to use OVHcloud AI Endpoints provider.'\n            )\n\n        if openai_client is not None:\n            self._client = openai_client\n        elif http_client is not None:\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n        else:\n            http_client = cached_async_http_client(provider='ovhcloud')\n            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)\n\n```",
  "content_length": 128633
}