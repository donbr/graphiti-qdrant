{
  "title": "Messages and chat history",
  "source_url": null,
  "content": "Pydantic AI provides access to messages exchanged during an agent run. These messages can be used both to continue a coherent conversation, and to understand how an agent performed.\n\n### Accessing Messages from Results\n\nAfter running an agent, you can access the messages exchanged during that run from the `result` object.\n\nBoth RunResult (returned by Agent.run, Agent.run_sync) and StreamedRunResult (returned by Agent.run_stream) have the following methods:\n\n- all_messages(): returns all messages, including messages from prior runs. There's also a variant that returns JSON bytes, all_messages_json().\n- new_messages(): returns only the messages from the current run. There's also a variant that returns JSON bytes, new_messages_json().\n\nStreamedRunResult and complete messages\n\nOn StreamedRunResult, the messages returned from these methods will only include the final result message once the stream has finished.\n\nE.g. you've awaited one of the following coroutines:\n\n- StreamedRunResult.stream_output()\n- StreamedRunResult.stream_text()\n- StreamedRunResult.stream_responses()\n- StreamedRunResult.get_output()\n\n**Note:** The final result message will NOT be added to result messages if you use .stream_text(delta=True) since in this case the result content is never built as one string.\n\nExample of accessing methods on a RunResult :\n\n[Learn about Gateway](../gateway) run_result_messages.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult = agent.run_sync('Tell me a joke.')\nprint(result.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\n### all messages from the run\nprint(result.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n]\n\"\"\"\n\n```\n\nrun_result_messages.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult = agent.run_sync('Tell me a joke.')\nprint(result.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\n### all messages from the run\nprint(result.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n]\n\"\"\"\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nExample of accessing methods on a StreamedRunResult :\n\n[Learn about Gateway](../gateway) streamed_run_result_messages.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5', system_prompt='Be a helpful assistant.')\n\n\nasync def main():\n    async with agent.run_stream('Tell me a joke.') as result:\n        # incomplete messages before the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ],\n                run_id='...',\n            )\n        ]\n        \"\"\"\n\n        async for text in result.stream_text():\n            print(text)\n            #> Did you hear\n            #> Did you hear about the toothpaste\n            #> Did you hear about the toothpaste scandal? They called\n            #> Did you hear about the toothpaste scandal? They called it Colgate.\n\n        # complete messages once the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ],\n                run_id='...',\n            ),\n            ModelResponse(\n                parts=[\n                    TextPart(\n                        content='Did you hear about the toothpaste scandal? They called it Colgate.'\n                    )\n                ],\n                usage=RequestUsage(input_tokens=50, output_tokens=12),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            ),\n        ]\n        \"\"\"\n\n```\n\nstreamed_run_result_messages.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\n\nasync def main():\n    async with agent.run_stream('Tell me a joke.') as result:\n        # incomplete messages before the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ],\n                run_id='...',\n            )\n        ]\n        \"\"\"\n\n        async for text in result.stream_text():\n            print(text)\n            #> Did you hear\n            #> Did you hear about the toothpaste\n            #> Did you hear about the toothpaste scandal? They called\n            #> Did you hear about the toothpaste scandal? They called it Colgate.\n\n        # complete messages once the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ],\n                run_id='...',\n            ),\n            ModelResponse(\n                parts=[\n                    TextPart(\n                        content='Did you hear about the toothpaste scandal? They called it Colgate.'\n                    )\n                ],\n                usage=RequestUsage(input_tokens=50, output_tokens=12),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            ),\n        ]\n        \"\"\"\n\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\n### Using Messages as Input for Further Agent Runs\n\nThe primary use of message histories in Pydantic AI is to maintain context across multiple agent runs.\n\nTo use existing messages in a run, pass them to the `message_history` parameter of Agent.run, Agent.run_sync or Agent.run_stream.\n\nIf `message_history` is set and not empty, a new system prompt is not generated — we assume the existing message history includes a system prompt.\n\n[Learn about Gateway](../gateway) Reusing messages in a conversation\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync('Explain?', message_history=result1.new_messages())\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=61, output_tokens=26),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n]\n\"\"\"\n\n```\n\nReusing messages in a conversation\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync('Explain?', message_history=result1.new_messages())\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=61, output_tokens=26),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n]\n\"\"\"\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\n## Storing and loading messages (to JSON)\n\nWhile maintaining conversation state in memory is enough for many applications, often times you may want to store the messages history of an agent run on disk or in a database. This might be for evals, for sharing data between Python and JavaScript/TypeScript, or any number of other use cases.\n\nThe intended way to do this is using a `TypeAdapter`.\n\nWe export ModelMessagesTypeAdapter that can be used for this, or you can create your own.\n\nHere's an example showing how:\n\n[Learn about Gateway](../gateway) serialize messages to json\n\n```python\nfrom pydantic_core import to_jsonable_python\n\nfrom pydantic_ai import (\n    Agent,\n    ModelMessagesTypeAdapter,  # (1)!\n)\n\nagent = Agent('gateway/openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nhistory_step_1 = result1.all_messages()\nas_python_objects = to_jsonable_python(history_step_1)  # (2)!\nsame_history_as_step_1 = ModelMessagesTypeAdapter.validate_python(as_python_objects)\n\nresult2 = agent.run_sync(  # (3)!\n    'Tell me a different joke.', message_history=same_history_as_step_1\n)\n\n```\n\n1. Alternatively, you can create a `TypeAdapter` from scratch:\n   ```python\n   from pydantic import TypeAdapter\n   from pydantic_ai import ModelMessage\n   ModelMessagesTypeAdapter = TypeAdapter(list[ModelMessage])\n\n   ```\n1. Alternatively you can serialize to/from JSON directly:\n   ```python\n   from pydantic_core import to_json\n   ...\n   as_json_objects = to_json(history_step_1)\n   same_history_as_step_1 = ModelMessagesTypeAdapter.validate_json(as_json_objects)\n\n   ```\n1. You can now continue the conversation with history `same_history_as_step_1` despite creating a new agent run.\n\nserialize messages to json\n\n```python\nfrom pydantic_core import to_jsonable_python\n\nfrom pydantic_ai import (\n    Agent,\n    ModelMessagesTypeAdapter,  # (1)!\n)\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nhistory_step_1 = result1.all_messages()\nas_python_objects = to_jsonable_python(history_step_1)  # (2)!\nsame_history_as_step_1 = ModelMessagesTypeAdapter.validate_python(as_python_objects)\n\nresult2 = agent.run_sync(  # (3)!\n    'Tell me a different joke.', message_history=same_history_as_step_1\n)\n\n```\n\n1. Alternatively, you can create a `TypeAdapter` from scratch:\n   ```python\n   from pydantic import TypeAdapter\n   from pydantic_ai import ModelMessage\n   ModelMessagesTypeAdapter = TypeAdapter(list[ModelMessage])\n\n   ```\n1. Alternatively you can serialize to/from JSON directly:\n   ```python\n   from pydantic_core import to_json\n   ...\n   as_json_objects = to_json(history_step_1)\n   same_history_as_step_1 = ModelMessagesTypeAdapter.validate_json(as_json_objects)\n\n   ```\n1. You can now continue the conversation with history `same_history_as_step_1` despite creating a new agent run.\n\n*(This example is complete, it can be run \"as is\")*\n\n## Other ways of using messages\n\nSince messages are defined by simple dataclasses, you can manually create and manipulate, e.g. for testing.\n\nThe message format is independent of the model used, so you can use messages in different agents, or the same agent with different models.\n\nIn the example below, we reuse the message from the first agent run, which uses the `openai:gpt-5` model, in a second agent run using the `google-gla:gemini-2.5-pro` model.\n\n[Learn about Gateway](../gateway) Reusing messages with a different model\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync(\n    'Explain?',\n    model='google-gla:gemini-2.5-pro',\n    message_history=result1.new_messages(),\n)\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=61, output_tokens=26),\n        model_name='gemini-2.5-pro',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n]\n\"\"\"\n\n```\n\nReusing messages with a different model\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync(\n    'Explain?',\n    model='google-gla:gemini-2.5-pro',\n    message_history=result1.new_messages(),\n)\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ],\n        run_id='...',\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=61, output_tokens=26),\n        model_name='gemini-2.5-pro',\n        timestamp=datetime.datetime(...),\n        run_id='...',\n    ),\n]\n\"\"\"\n\n```\n\n## Processing Message History\n\nSometimes you may want to modify the message history before it's sent to the model. This could be for privacy reasons (filtering out sensitive information), to save costs on tokens, to give less context to the LLM, or custom processing logic.\n\nPydantic AI provides a `history_processors` parameter on `Agent` that allows you to intercept and modify the message history before each model request.\n\nHistory processors replace the message history\n\nHistory processors replace the message history in the state with the processed messages, including the new user prompt part. This means that if you want to keep the original message history, you need to make a copy of it.\n\n### Usage\n\nThe `history_processors` is a list of callables that take a list of ModelMessage and return a modified list of the same type.\n\nEach processor is applied in sequence, and processors can be either synchronous or asynchronous.\n\n[Learn about Gateway](../gateway) simple_history_processor.py\n\n```python\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Remove all ModelResponse messages, keeping only ModelRequest messages.\"\"\"\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n### Create agent with history processor\nagent = Agent('gateway/openai:gpt-5', history_processors=[filter_responses])\n\n### Example: Create some conversation history\nmessage_history = [\n    ModelRequest(parts=[UserPromptPart(content='What is 2+2?')]),\n    ModelResponse(parts=[TextPart(content='2+2 equals 4')]),  # This will be filtered out\n]\n\n### When you run the agent, the history processor will filter out ModelResponse messages\n### result = agent.run_sync('What about 3+3?', message_history=message_history)\n\n```\n\nsimple_history_processor.py\n\n```python\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Remove all ModelResponse messages, keeping only ModelRequest messages.\"\"\"\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n### Create agent with history processor\nagent = Agent('openai:gpt-5', history_processors=[filter_responses])\n\n### Example: Create some conversation history\nmessage_history = [\n    ModelRequest(parts=[UserPromptPart(content='What is 2+2?')]),\n    ModelResponse(parts=[TextPart(content='2+2 equals 4')]),  # This will be filtered out\n]\n\n### When you run the agent, the history processor will filter out ModelResponse messages\n### result = agent.run_sync('What about 3+3?', message_history=message_history)\n\n```\n\n#### Keep Only Recent Messages\n\nYou can use the `history_processor` to only keep the recent messages:\n\n[Learn about Gateway](../gateway) keep_recent_messages.py\n\n```python\nfrom pydantic_ai import Agent, ModelMessage\n\n\nasync def keep_recent_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Keep only the last 5 messages to manage token usage.\"\"\"\n    return messages[-5:] if len(messages) > 5 else messages\n\nagent = Agent('gateway/openai:gpt-5', history_processors=[keep_recent_messages])\n\n### Example: Even with a long conversation history, only the last 5 messages are sent to the model\nlong_conversation_history: list[ModelMessage] = []  # Your long conversation history here\n### result = agent.run_sync('What did we discuss?', message_history=long_conversation_history)\n\n```\n\nkeep_recent_messages.py\n\n```python\nfrom pydantic_ai import Agent, ModelMessage\n\n\nasync def keep_recent_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Keep only the last 5 messages to manage token usage.\"\"\"\n    return messages[-5:] if len(messages) > 5 else messages\n\nagent = Agent('openai:gpt-5', history_processors=[keep_recent_messages])\n\n### Example: Even with a long conversation history, only the last 5 messages are sent to the model\nlong_conversation_history: list[ModelMessage] = []  # Your long conversation history here\n### result = agent.run_sync('What did we discuss?', message_history=long_conversation_history)\n\n```\n\nBe careful when slicing the message history\n\nWhen slicing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269).\n\n#### `RunContext` parameter\n\nHistory processors can optionally accept a RunContext parameter to access additional information about the current run, such as dependencies, model information, and usage statistics:\n\n[Learn about Gateway](../gateway) context_aware_processor.py\n\n```python\nfrom pydantic_ai import Agent, ModelMessage, RunContext\n\n\ndef context_aware_processor(\n    ctx: RunContext[None],\n    messages: list[ModelMessage],\n) -> list[ModelMessage]:\n    # Access current usage\n    current_tokens = ctx.usage.total_tokens\n\n    # Filter messages based on context\n    if current_tokens > 1000:\n        return messages[-3:]  # Keep only recent messages when token usage is high\n    return messages\n\nagent = Agent('gateway/openai:gpt-5', history_processors=[context_aware_processor])\n\n```\n\ncontext_aware_processor.py\n\n```python\nfrom pydantic_ai import Agent, ModelMessage, RunContext\n\n\ndef context_aware_processor(\n    ctx: RunContext[None],\n    messages: list[ModelMessage],\n) -> list[ModelMessage]:\n    # Access current usage\n    current_tokens = ctx.usage.total_tokens\n\n    # Filter messages based on context\n    if current_tokens > 1000:\n        return messages[-3:]  # Keep only recent messages when token usage is high\n    return messages\n\nagent = Agent('openai:gpt-5', history_processors=[context_aware_processor])\n\n```\n\nThis allows for more sophisticated message processing based on the current state of the agent run.\n\n#### Summarize Old Messages\n\nUse an LLM to summarize older messages to preserve context while reducing tokens.\n\n[Learn about Gateway](../gateway) summarize_old_messages.py\n\n```python\nfrom pydantic_ai import Agent, ModelMessage\n\n### Use a cheaper model to summarize old messages.\nsummarize_agent = Agent(\n    'gateway/openai:gpt-5-mini',\n    instructions=\"\"\"\nSummarize this conversation, omitting small talk and unrelated topics.\nFocus on the technical discussion and next steps.\n\"\"\",\n)\n\n\nasync def summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    # Summarize the oldest 10 messages\n    if len(messages) > 10:\n        oldest_messages = messages[:10]\n        summary = await summarize_agent.run(message_history=oldest_messages)\n        # Return the last message and the summary\n        return summary.new_messages() + messages[-1:]\n\n    return messages\n\n\nagent = Agent('gateway/openai:gpt-5', history_processors=[summarize_old_messages])\n\n```\n\nsummarize_old_messages.py\n\n```python\nfrom pydantic_ai import Agent, ModelMessage\n\n### Use a cheaper model to summarize old messages.\nsummarize_agent = Agent(\n    'openai:gpt-5-mini',\n    instructions=\"\"\"\nSummarize this conversation, omitting small talk and unrelated topics.\nFocus on the technical discussion and next steps.\n\"\"\",\n)\n\n\nasync def summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    # Summarize the oldest 10 messages\n    if len(messages) > 10:\n        oldest_messages = messages[:10]\n        summary = await summarize_agent.run(message_history=oldest_messages)\n        # Return the last message and the summary\n        return summary.new_messages() + messages[-1:]\n\n    return messages\n\n\nagent = Agent('openai:gpt-5', history_processors=[summarize_old_messages])\n\n```\n\nBe careful when summarizing the message history\n\nWhen summarizing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269), where you can find examples of summarizing the message history.\n\n### Testing History Processors\n\nYou can test what messages are actually sent to the model provider using FunctionModel:\n\ntest_history_processor.py\n\n```python\nimport pytest\n\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\n\n@pytest.fixture\ndef received_messages() -> list[ModelMessage]:\n    return []\n\n\n@pytest.fixture\ndef function_model(received_messages: list[ModelMessage]) -> FunctionModel:\n    def capture_model_function(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n        # Capture the messages that the provider actually receives\n        received_messages.clear()\n        received_messages.extend(messages)\n        return ModelResponse(parts=[TextPart(content='Provider response')])\n\n    return FunctionModel(capture_model_function)\n\n\ndef test_history_processor(function_model: FunctionModel, received_messages: list[ModelMessage]):\n    def filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n        return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n    agent = Agent(function_model, history_processors=[filter_responses])\n\n    message_history = [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelResponse(parts=[TextPart(content='Answer 1')]),\n    ]\n\n    agent.run_sync('Question 2', message_history=message_history)\n    assert received_messages == [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelRequest(parts=[UserPromptPart(content='Question 2')]),\n    ]\n\n```\n\n### Multiple Processors\n\nYou can also use multiple processors:\n\n[Learn about Gateway](../gateway) multiple_history_processors.py\n\n```python\nfrom pydantic_ai import Agent, ModelMessage, ModelRequest\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n\ndef summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return messages[-5:]\n\n\nagent = Agent('gateway/openai:gpt-5', history_processors=[filter_responses, summarize_old_messages])\n\n```\n\nmultiple_history_processors.py\n\n```python\nfrom pydantic_ai import Agent, ModelMessage, ModelRequest\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n\ndef summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return messages[-5:]\n\n\nagent = Agent('openai:gpt-5', history_processors=[filter_responses, summarize_old_messages])\n\n```\n\nIn this case, the `filter_responses` processor will be applied first, and the `summarize_old_messages` processor will be applied second.\n\n## Examples\n\nFor a more complete example of using messages in conversations, see the [chat app](../examples/chat-app/) example.",
  "content_length": 30226
}