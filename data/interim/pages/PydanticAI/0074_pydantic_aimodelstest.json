{
  "title": "`pydantic_ai.models.test`",
  "source_url": null,
  "content": "Utility model for quickly testing apps built with Pydantic AI.\n\nHere's a minimal example:\n\n[Learn about Gateway](../../../gateway) test_model_usage.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nmy_agent = Agent('gateway/openai:gpt-5', system_prompt='...')\n\n\nasync def test_my_agent():\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\n    m = TestModel()\n    with my_agent.override(model=m):\n        result = await my_agent.run('Testing my agent...')\n        assert result.output == 'success (no tool calls)'\n    assert m.last_model_request_parameters.function_tools == []\n\n```\n\ntest_model_usage.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nmy_agent = Agent('openai:gpt-5', system_prompt='...')\n\n\nasync def test_my_agent():\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\n    m = TestModel()\n    with my_agent.override(model=m):\n        result = await my_agent.run('Testing my agent...')\n        assert result.output == 'success (no tool calls)'\n    assert m.last_model_request_parameters.function_tools == []\n\n```\n\nSee [Unit testing with `TestModel`](../../../testing/#unit-testing-with-testmodel) for detailed documentation.\n\n### TestModel\n\nBases: `Model`\n\nA model specifically for testing purposes.\n\nThis will (by default) call all tools in the agent, then return a tool response if possible, otherwise a plain response.\n\nHow useful this model is will vary significantly.\n\nApart from `__init__` derived by the `dataclass` decorator, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/test.py`\n\n```python\n@dataclass(init=False)\nclass TestModel(Model):\n    \"\"\"A model specifically for testing purposes.\n\n    This will (by default) call all tools in the agent, then return a tool response if possible,\n    otherwise a plain response.\n\n    How useful this model is will vary significantly.\n\n    Apart from `__init__` derived by the `dataclass` decorator, all methods are private or match those\n    of the base class.\n    \"\"\"\n\n    # NOTE: Avoid test discovery by pytest.\n    __test__ = False\n\n    call_tools: list[str] | Literal['all'] = 'all'\n    \"\"\"List of tools to call. If `'all'`, all tools will be called.\"\"\"\n    custom_output_text: str | None = None\n    \"\"\"If set, this text is returned as the final output.\"\"\"\n    custom_output_args: Any | None = None\n    \"\"\"If set, these args will be passed to the output tool.\"\"\"\n    seed: int = 0\n    \"\"\"Seed for generating random data.\"\"\"\n    last_model_request_parameters: ModelRequestParameters | None = field(default=None, init=False)\n    \"\"\"The last ModelRequestParameters passed to the model in a request.\n\n    The ModelRequestParameters contains information about the function and output tools available during request handling.\n\n    This is set when a request is made, so will reflect the function tools from the last step of the last run.\n    \"\"\"\n    _model_name: str = field(default='test', repr=False)\n    _system: str = field(default='test', repr=False)\n\n    def __init__(\n        self,\n        *,\n        call_tools: list[str] | Literal['all'] = 'all',\n        custom_output_text: str | None = None,\n        custom_output_args: Any | None = None,\n        seed: int = 0,\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize TestModel with optional settings and profile.\"\"\"\n        self.call_tools = call_tools\n        self.custom_output_text = custom_output_text\n        self.custom_output_args = custom_output_args\n        self.seed = seed\n        self.last_model_request_parameters = None\n        self._model_name = 'test'\n        self._system = 'test'\n        super().__init__(settings=settings, profile=profile)\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        self.last_model_request_parameters = model_request_parameters\n        model_response = self._request(messages, model_settings, model_request_parameters)\n        model_response.usage = _estimate_usage([*messages, model_response])\n        return model_response\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        self.last_model_request_parameters = model_request_parameters\n\n        model_response = self._request(messages, model_settings, model_request_parameters)\n        yield TestStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _model_name=self._model_name,\n            _structured_response=model_response,\n            _messages=messages,\n            _provider_name=self._system,\n        )\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The model provider.\"\"\"\n        return self._system\n\n    def gen_tool_args(self, tool_def: ToolDefinition) -> Any:\n        return _JsonSchemaTestData(tool_def.parameters_json_schema, self.seed).generate()\n\n    def _get_tool_calls(self, model_request_parameters: ModelRequestParameters) -> list[tuple[str, ToolDefinition]]:\n        if self.call_tools == 'all':\n            return [(r.name, r) for r in model_request_parameters.function_tools]\n        else:\n            function_tools_lookup = {t.name: t for t in model_request_parameters.function_tools}\n            tools_to_call = (function_tools_lookup[name] for name in self.call_tools)\n            return [(r.name, r) for r in tools_to_call]\n\n    def _get_output(self, model_request_parameters: ModelRequestParameters) -> _WrappedTextOutput | _WrappedToolOutput:\n        if self.custom_output_text is not None:\n            assert model_request_parameters.output_mode != 'tool', (\n                'Plain response not allowed, but `custom_output_text` is set.'\n            )\n            assert self.custom_output_args is None, 'Cannot set both `custom_output_text` and `custom_output_args`.'\n            return _WrappedTextOutput(self.custom_output_text)\n        elif self.custom_output_args is not None:\n            assert model_request_parameters.output_tools is not None, (\n                'No output tools provided, but `custom_output_args` is set.'\n            )\n            output_tool = model_request_parameters.output_tools[0]\n\n            if k := output_tool.outer_typed_dict_key:\n                return _WrappedToolOutput({k: self.custom_output_args})\n            else:\n                return _WrappedToolOutput(self.custom_output_args)\n        elif model_request_parameters.allow_text_output:\n            return _WrappedTextOutput(None)\n        elif model_request_parameters.output_tools:\n            return _WrappedToolOutput(None)\n        else:\n            return _WrappedTextOutput(None)\n\n    def _request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        if model_request_parameters.builtin_tools:\n            raise UserError('TestModel does not support built-in tools')\n\n        tool_calls = self._get_tool_calls(model_request_parameters)\n        output_wrapper = self._get_output(model_request_parameters)\n        output_tools = model_request_parameters.output_tools\n\n        # if there are tools, the first thing we want to do is call all of them\n        if tool_calls and not any(isinstance(m, ModelResponse) for m in messages):\n            return ModelResponse(\n                parts=[\n                    ToolCallPart(name, self.gen_tool_args(args), tool_call_id=f'pyd_ai_tool_call_id__{name}')\n                    for name, args in tool_calls\n                ],\n                model_name=self._model_name,\n            )\n\n        if messages:  # pragma: no branch\n            last_message = messages[-1]\n            assert isinstance(last_message, ModelRequest), 'Expected last message to be a `ModelRequest`.'\n\n            # check if there are any retry prompts, if so retry them\n            new_retry_names = {p.tool_name for p in last_message.parts if isinstance(p, RetryPromptPart)}\n            if new_retry_names:\n                # Handle retries for both function tools and output tools\n                # Check function tools first\n                retry_parts: list[ModelResponsePart] = [\n                    ToolCallPart(name, self.gen_tool_args(args)) for name, args in tool_calls if name in new_retry_names\n                ]\n                # Check output tools\n                if output_tools:\n                    retry_parts.extend(\n                        [\n                            ToolCallPart(\n                                tool.name,\n                                output_wrapper.value\n                                if isinstance(output_wrapper, _WrappedToolOutput) and output_wrapper.value is not None\n                                else self.gen_tool_args(tool),\n                                tool_call_id=f'pyd_ai_tool_call_id__{tool.name}',\n                            )\n                            for tool in output_tools\n                            if tool.name in new_retry_names\n                        ]\n                    )\n                return ModelResponse(parts=retry_parts, model_name=self._model_name)\n\n        if isinstance(output_wrapper, _WrappedTextOutput):\n            if (response_text := output_wrapper.value) is None:\n                # build up details of tool responses\n                output: dict[str, Any] = {}\n                for message in messages:\n                    if isinstance(message, ModelRequest):\n                        for part in message.parts:\n                            if isinstance(part, ToolReturnPart):\n                                output[part.tool_name] = part.content\n                if output:\n                    return ModelResponse(\n                        parts=[TextPart(pydantic_core.to_json(output).decode())], model_name=self._model_name\n                    )\n                else:\n                    return ModelResponse(parts=[TextPart('success (no tool calls)')], model_name=self._model_name)\n            else:\n                return ModelResponse(parts=[TextPart(response_text)], model_name=self._model_name)\n        else:\n            assert output_tools, 'No output tools provided'\n            custom_output_args = output_wrapper.value\n            output_tool = output_tools[self.seed % len(output_tools)]\n            if custom_output_args is not None:\n                return ModelResponse(\n                    parts=[\n                        ToolCallPart(\n                            output_tool.name,\n                            custom_output_args,\n                            tool_call_id=f'pyd_ai_tool_call_id__{output_tool.name}',\n                        )\n                    ],\n                    model_name=self._model_name,\n                )\n            else:\n                response_args = self.gen_tool_args(output_tool)\n                return ModelResponse(\n                    parts=[\n                        ToolCallPart(\n                            output_tool.name,\n                            response_args,\n                            tool_call_id=f'pyd_ai_tool_call_id__{output_tool.name}',\n                        )\n                    ],\n                    model_name=self._model_name,\n                )\n\n```\n\n#### __init__\n\n```python\n__init__(\n    *,\n    call_tools: list[str] | Literal[\"all\"] = \"all\",\n    custom_output_text: str | None = None,\n    custom_output_args: Any | None = None,\n    seed: int = 0,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize TestModel with optional settings and profile.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/test.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    call_tools: list[str] | Literal['all'] = 'all',\n    custom_output_text: str | None = None,\n    custom_output_args: Any | None = None,\n    seed: int = 0,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize TestModel with optional settings and profile.\"\"\"\n    self.call_tools = call_tools\n    self.custom_output_text = custom_output_text\n    self.custom_output_args = custom_output_args\n    self.seed = seed\n    self.last_model_request_parameters = None\n    self._model_name = 'test'\n    self._system = 'test'\n    super().__init__(settings=settings, profile=profile)\n\n```\n\n#### call_tools\n\n```python\ncall_tools: list[str] | Literal['all'] = call_tools\n\n```\n\nList of tools to call. If `'all'`, all tools will be called.\n\n#### custom_output_text\n\n```python\ncustom_output_text: str | None = custom_output_text\n\n```\n\nIf set, this text is returned as the final output.\n\n#### custom_output_args\n\n```python\ncustom_output_args: Any | None = custom_output_args\n\n```\n\nIf set, these args will be passed to the output tool.\n\n#### seed\n\n```python\nseed: int = seed\n\n```\n\nSeed for generating random data.\n\n#### last_model_request_parameters\n\n```python\nlast_model_request_parameters: (\n    ModelRequestParameters | None\n) = None\n\n```\n\nThe last ModelRequestParameters passed to the model in a request.\n\nThe ModelRequestParameters contains information about the function and output tools available during request handling.\n\nThis is set when a request is made, so will reflect the function tools from the last step of the last run.\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe model provider.\n\n### TestStreamedResponse\n\nBases: `StreamedResponse`\n\nA structured response that streams test data.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/test.py`\n\n```python\n@dataclass\nclass TestStreamedResponse(StreamedResponse):\n    \"\"\"A structured response that streams test data.\"\"\"\n\n    _model_name: str\n    _structured_response: ModelResponse\n    _messages: InitVar[Iterable[ModelMessage]]\n    _provider_name: str\n    _timestamp: datetime = field(default_factory=_utils.now_utc, init=False)\n\n    def __post_init__(self, _messages: Iterable[ModelMessage]):\n        self._usage = _estimate_usage(_messages)\n\n    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\n        for i, part in enumerate(self._structured_response.parts):\n            if isinstance(part, TextPart):\n                text = part.content\n                *words, last_word = text.split(' ')\n                words = [f'{word} ' for word in words]\n                words.append(last_word)\n                if len(words) == 1 and len(text) > 2:\n                    mid = len(text) // 2\n                    words = [text[:mid], text[mid:]]\n                self._usage += _get_string_usage('')\n                maybe_event = self._parts_manager.handle_text_delta(vendor_part_id=i, content='')\n                if maybe_event is not None:  # pragma: no branch\n                    yield maybe_event\n                for word in words:\n                    self._usage += _get_string_usage(word)\n                    maybe_event = self._parts_manager.handle_text_delta(vendor_part_id=i, content=word)\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n            elif isinstance(part, ToolCallPart):\n                yield self._parts_manager.handle_tool_call_part(\n                    vendor_part_id=i, tool_name=part.tool_name, args=part.args, tool_call_id=part.tool_call_id\n                )\n            elif isinstance(part, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover\n                # NOTE: These parts are not generated by TestModel, but we need to handle them for type checking\n                assert False, f'Unexpected part type in TestModel: {type(part).__name__}'\n            elif isinstance(part, ThinkingPart):  # pragma: no cover\n                # NOTE: There's no way to reach this part of the code, since we don't generate ThinkingPart on TestModel.\n                assert False, \"This should be unreachable — we don't generate ThinkingPart on TestModel.\"\n            elif isinstance(part, FilePart):  # pragma: no cover\n                # NOTE: There's no way to reach this part of the code, since we don't generate FilePart on TestModel.\n                assert False, \"This should be unreachable — we don't generate FilePart on TestModel.\"\n            else:\n                assert_never(part)\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"Get the model name of the response.\"\"\"\n        return self._model_name\n\n    @property\n    def provider_name(self) -> str:\n        \"\"\"Get the provider name.\"\"\"\n        return self._provider_name\n\n    @property\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._timestamp\n\n```\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nGet the model name of the response.\n\n#### provider_name\n\n```python\nprovider_name: str\n\n```\n\nGet the provider name.\n\n#### timestamp\n\n```python\ntimestamp: datetime\n\n```\n\nGet the timestamp of the response.",
  "content_length": 17704
}