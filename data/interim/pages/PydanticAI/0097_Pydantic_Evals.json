{
  "title": "Pydantic Evals",
  "source_url": null,
  "content": "**Pydantic Evals** is a powerful evaluation framework for systematically testing and evaluating AI systems, from simple LLM calls to complex multi-agent applications.\n\n## Design Philosophy\n\nCode-First Approach\n\nPydantic Evals follows a code-first philosophy where all evaluation components are defined in Python. This differs from platforms with web-based configuration. You write and run evals in code, and can write the results to disk or view them in your terminal or in [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/).\n\nEvals are an Emerging Practice\n\nUnlike unit tests, evals are an emerging art/science. Anyone who claims to know exactly how your evals should be defined can safely be ignored. We've designed Pydantic Evals to be flexible and useful without being too opinionated.\n\n## Quick Navigation\n\n**Getting Started:**\n\n- [Installation](#installation)\n- [Quick Start](quick-start/)\n- [Core Concepts](core-concepts/)\n\n**Evaluators:**\n\n- [Evaluators Overview](evaluators/overview/) - Compare evaluator types and learn when to use each approach\n- [Built-in Evaluators](evaluators/built-in/) - Complete reference for exact match, instance checks, and other ready-to-use evaluators\n- [LLM as a Judge](evaluators/llm-judge/) - Use LLMs to evaluate subjective qualities, complex criteria, and natural language outputs\n- [Custom Evaluators](evaluators/custom/) - Implement domain-specific scoring logic and custom evaluation metrics\n- [Span-Based Evaluation](evaluators/span-based/) - Evaluate internal agent behavior (tool calls, execution flow) using OpenTelemetry traces. Essential for complex agents where correctness depends on *how* the answer was reached, not just the final output. Also ensures eval assertions align with production telemetry.\n\n**How-To Guides:**\n\n- [Logfire Integration](how-to/logfire-integration/) - Visualize results\n- [Dataset Management](how-to/dataset-management/) - Save, load, generate\n- [Concurrency & Performance](how-to/concurrency/) - Control parallel execution\n- [Retry Strategies](how-to/retry-strategies/) - Handle transient failures\n- [Metrics & Attributes](how-to/metrics-attributes/) - Track custom data\n\n**Examples:**\n\n- [Simple Validation](examples/simple-validation/) - Basic example\n\n**Reference:**\n\n- [API Documentation](../api/pydantic_evals/dataset/)\n\n## Code-First Evaluation\n\nPydantic Evals follows a **code-first approach** where you define all evaluation components (datasets, experiments, tasks, cases and evaluators) in Python code, or as serialized data loaded by Python code. This differs from platforms with fully web-based configuration.\n\nWhen you run an *Experiment* you'll see a progress indicator and can print the results wherever you run your python code (IDE, terminal, etc). You also get a report object back that you can serialize and store or send to a notebook or other application for further visualization and analysis.\n\nIf you are using [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/), your experiment results automatically appear in the Logfire web interface for visualization, comparison, and collaborative analysis. Logfire serves as a observability layer - you write and run evals in code, then view and analyze results in the web UI.\n\n## Installation\n\nTo install the Pydantic Evals package, run:\n\n```bash\npip install pydantic-evals\n\n```\n\n```bash\nuv add pydantic-evals\n\n```\n\n`pydantic-evals` does not depend on `pydantic-ai`, but has an optional dependency on `logfire` if you'd like to use OpenTelemetry traces in your evals, or send evaluation results to [logfire](https://pydantic.dev/logfire).\n\n```bash\npip install 'pydantic-evals[logfire]'\n\n```\n\n```bash\nuv add 'pydantic-evals[logfire]'\n\n```\n\n## Pydantic Evals Data Model\n\nPydantic Evals is built around a simple data model:\n\n### Data Model Diagram\n\n```text\nDataset (1) ──────────── (Many) Case\n│                        │\n│                        │\n└─── (Many) Experiment ──┴─── (Many) Case results\n     │\n     └─── (1) Task\n     │\n     └─── (Many) Evaluator\n\n```\n\n### Key Relationships\n\n1. **Dataset → Cases**: One Dataset contains many Cases\n1. **Dataset → Experiments**: One Dataset can be used across many Experiments over time\n1. **Experiment → Case results**: One Experiment generates results by executing each Case\n1. **Experiment → Task**: One Experiment evaluates one defined Task\n1. **Experiment → Evaluators**: One Experiment uses multiple Evaluators. Dataset-wide Evaluators are run against all Cases, and Case-specific Evaluators against their respective Cases\n\n### Data Flow\n\n1. **Dataset creation**: Define cases and evaluators in YAML/JSON, or directly in Python\n1. **Experiment execution**: Run `dataset.evaluate_sync(task_function)`\n1. **Cases run**: Each Case is executed against the Task\n1. **Evaluation**: Evaluators score the Task outputs for each Case\n1. **Results**: All Case results are collected into a summary report\n\nA metaphor\n\nA useful metaphor (although not perfect) is to think of evals like a **Unit Testing** framework:\n\n- **Cases + Evaluators** are your individual unit tests - each one defines a specific scenario you want to test, complete with inputs and expected outcomes. Just like a unit test, a case asks: *\"Given this input, does my system produce the right output?\"*\n- **Datasets** are like test suites - they are the scaffolding that holds your unit tests together. They group related cases and define shared evaluation criteria that should apply across all tests in the suite.\n- **Experiments** are like running your entire test suite and getting a report. When you execute `dataset.evaluate_sync(my_ai_function)`, you're running all your cases against your AI system and collecting the results - just like running `pytest` and getting a summary of passes, failures, and performance metrics.\n\nThe key difference from traditional unit testing is that AI systems are probabilistic. If you're type checking you'll still get a simple pass/fail, but scores for text outputs are likely qualitative and/or categorical, and more open to interpretation.\n\nFor a deeper understanding, see [Core Concepts](core-concepts/).\n\n## Datasets and Cases\n\nIn Pydantic Evals, everything begins with Datasets and Cases:\n\n- **Dataset**: A collection of test Cases designed for the evaluation of a specific task or function\n- **Case**: A single test scenario corresponding to Task inputs, with optional expected outputs, metadata, and case-specific evaluators\n\nsimple_eval_dataset.py\n\n```python\nfrom pydantic_evals import Case, Dataset\n\ncase1 = Case(\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\ndataset = Dataset(cases=[case1])\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nSee [Dataset Management](how-to/dataset-management/) to learn about saving, loading, and generating datasets.\n\n## Evaluators\n\nEvaluators analyze and score the results of your Task when tested against a Case.\n\nThese can be deterministic, code-based checks (such as testing model output format with a regex, or checking for the appearance of PII or sensitive data), or they can assess non-deterministic model outputs for qualities like accuracy, precision/recall, hallucinations, or instruction-following.\n\nWhile both kinds of testing are useful in LLM systems, classical code-based tests are cheaper and easier than tests which require either human or machine review of model outputs.\n\nPydantic Evals includes several [built-in evaluators](evaluators/built-in/) and allows you to define [custom evaluators](evaluators/custom/):\n\nsimple_eval_evaluator.py\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\nfrom pydantic_evals.evaluators.common import IsInstance\n\nfrom simple_eval_dataset import dataset\n\ndataset.add_evaluator(IsInstance(type_name='str'))  # (1)!\n\n\n@dataclass\nclass MyEvaluator(Evaluator):\n    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:  # (2)!\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset.add_evaluator(MyEvaluator())\n\n```\n\n1. You can add built-in evaluators to a dataset using the add_evaluator method.\n1. This custom evaluator returns a simple score based on whether the output matches the expected output.\n\n*(This example is complete, it can be run \"as is\")*\n\nLearn more:\n\n- [Evaluators Overview](evaluators/overview/) - When to use different types\n- [Built-in Evaluators](evaluators/built-in/) - Complete reference\n- [LLM Judge](evaluators/llm-judge/) - Using LLMs as evaluators\n- [Custom Evaluators](evaluators/custom/) - Write your own logic\n- [Span-Based Evaluation](evaluators/span-based/) - Analyze execution traces\n\n## Running Experiments\n\nPerforming evaluations involves running a task against all cases in a dataset, also known as running an \"experiment\".\n\nPutting the above two examples together and using the more declarative `evaluators` kwarg to Dataset:\n\nsimple_eval_complete.py\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext, IsInstance\n\ncase1 = Case(  # (1)!\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\n\nclass MyEvaluator(Evaluator[str, str]):\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset = Dataset(\n    cases=[case1],\n    evaluators=[IsInstance(type_name='str'), MyEvaluator()],  # (2)!\n)\n\n\nasync def guess_city(question: str) -> str:  # (3)!\n    return 'Paris'\n\n\nreport = dataset.evaluate_sync(guess_city)  # (4)!\nreport.print(include_input=True, include_output=True, include_durations=False)  # (5)!\n\"\"\"\n                              Evaluation Summary: guess_city\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Case ID     ┃ Inputs                         ┃ Outputs ┃ Scores            ┃ Assertions ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ simple_case │ What is the capital of France? │ Paris   │ MyEvaluator: 1.00 │ ✔          │\n├─────────────┼────────────────────────────────┼─────────┼───────────────────┼────────────┤\n│ Averages    │                                │         │ MyEvaluator: 1.00 │ 100.0% ✔   │\n└─────────────┴────────────────────────────────┴─────────┴───────────────────┴────────────┘\n\"\"\"\n\n```\n\n1. Create a test case as above\n1. Create a Dataset with test cases and evaluators\n1. Our function to evaluate.\n1. Run the evaluation with evaluate_sync, which runs the function against all test cases in the dataset, and returns an EvaluationReport object.\n1. Print the report with print, which shows the results of the evaluation. We have omitted duration here just to keep the printed output from changing from run to run.\n\n*(This example is complete, it can be run \"as is\")*\n\nSee [Quick Start](quick-start/) for more examples and [Concurrency & Performance](how-to/concurrency/) to learn about controlling parallel execution.\n\n## API Reference\n\nFor comprehensive coverage of all classes, methods, and configuration options, see the detailed [API Reference documentation](https://ai.pydantic.dev/api/pydantic_evals/dataset/).\n\n## Next Steps\n\n1. **Start with simple evaluations** using [Quick Start](quick-start/)\n1. **Understand the data model** with [Core Concepts](core-concepts/)\n1. **Explore built-in evaluators** in [Built-in Evaluators](evaluators/built-in/)\n1. **Integrate with Logfire** for visualization: [Logfire Integration](how-to/logfire-integration/)\n1. **Build comprehensive test suites** with [Dataset Management](how-to/dataset-management/)\n1. **Implement custom evaluators** for domain-specific metrics: [Custom Evaluators](evaluators/custom/)",
  "content_length": 12279
}