{
  "title": "`pydantic_ai.profiles`",
  "source_url": null,
  "content": "Describes how requests to and responses from specific models or families of models need to be constructed and processed to get the best results, independent of the model and provider classes used.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/__init__.py`\n\n```python\n@dataclass(kw_only=True)\nclass ModelProfile:\n    \"\"\"Describes how requests to and responses from specific models or families of models need to be constructed and processed to get the best results, independent of the model and provider classes used.\"\"\"\n\n    supports_tools: bool = True\n    \"\"\"Whether the model supports tools.\"\"\"\n    supports_json_schema_output: bool = False\n    \"\"\"Whether the model supports JSON schema output.\n\n    This is also referred to as 'native' support for structured output.\n    Relates to the `NativeOutput` output type.\n    \"\"\"\n    supports_json_object_output: bool = False\n    \"\"\"Whether the model supports a dedicated mode to enforce JSON output, without necessarily sending a schema.\n\n    E.g. [OpenAI's JSON mode](https://platform.openai.com/docs/guides/structured-outputs#json-mode)\n    Relates to the `PromptedOutput` output type.\n    \"\"\"\n    supports_image_output: bool = False\n    \"\"\"Whether the model supports image output.\"\"\"\n    default_structured_output_mode: StructuredOutputMode = 'tool'\n    \"\"\"The default structured output mode to use for the model.\"\"\"\n    prompted_output_template: str = dedent(\n        \"\"\"\n        Always respond with a JSON object that's compatible with this schema:\n\n        {schema}\n\n        Don't include any text or Markdown fencing before or after.\n        \"\"\"\n    )\n    \"\"\"The instructions template to use for prompted structured output. The '{schema}' placeholder will be replaced with the JSON schema for the output.\"\"\"\n    json_schema_transformer: type[JsonSchemaTransformer] | None = None\n    \"\"\"The transformer to use to make JSON schemas for tools and structured output compatible with the model.\"\"\"\n\n    thinking_tags: tuple[str, str] = ('<think>', '</think>')\n    \"\"\"The tags used to indicate thinking parts in the model's output. Defaults to ('<think>', '</think>').\"\"\"\n\n    ignore_streamed_leading_whitespace: bool = False\n    \"\"\"Whether to ignore leading whitespace when streaming a response.\n\n    This is a workaround for models that emit `<think>\\n</think>\\n\\n` or an empty text part ahead of tool calls (e.g. Ollama + Qwen3),\n    which we don't want to end up treating as a final result when using `run_stream` with `str` a valid `output_type`.\n\n    This is currently only used by `OpenAIChatModel`, `HuggingFaceModel`, and `GroqModel`.\n    \"\"\"\n\n    @classmethod\n    def from_profile(cls, profile: ModelProfile | None) -> Self:\n        \"\"\"Build a ModelProfile subclass instance from a ModelProfile instance.\"\"\"\n        if isinstance(profile, cls):\n            return profile\n        return cls().update(profile)\n\n    def update(self, profile: ModelProfile | None) -> Self:\n        \"\"\"Update this ModelProfile (subclass) instance with the non-default values from another ModelProfile instance.\"\"\"\n        if not profile:\n            return self\n        field_names = set(f.name for f in fields(self))\n        non_default_attrs = {\n            f.name: getattr(profile, f.name)\n            for f in fields(profile)\n            if f.name in field_names and getattr(profile, f.name) != f.default\n        }\n        return replace(self, **non_default_attrs)\n\n```\n\n### supports_tools\n\n```python\nsupports_tools: bool = True\n\n```\n\nWhether the model supports tools.\n\n### supports_json_schema_output\n\n```python\nsupports_json_schema_output: bool = False\n\n```\n\nWhether the model supports JSON schema output.\n\nThis is also referred to as 'native' support for structured output. Relates to the `NativeOutput` output type.\n\n### supports_json_object_output\n\n```python\nsupports_json_object_output: bool = False\n\n```\n\nWhether the model supports a dedicated mode to enforce JSON output, without necessarily sending a schema.\n\nE.g. [OpenAI's JSON mode](https://platform.openai.com/docs/guides/structured-outputs#json-mode) Relates to the `PromptedOutput` output type.\n\n### supports_image_output\n\n```python\nsupports_image_output: bool = False\n\n```\n\nWhether the model supports image output.\n\n### default_structured_output_mode\n\n```python\ndefault_structured_output_mode: StructuredOutputMode = (\n    \"tool\"\n)\n\n```\n\nThe default structured output mode to use for the model.\n\n### prompted_output_template\n\n```python\nprompted_output_template: str = dedent(\n    \"\\n        Always respond with a JSON object that's compatible with this schema:\\n\\n        {schema}\\n\\n        Don't include any text or Markdown fencing before or after.\\n        \"\n)\n\n```\n\nThe instructions template to use for prompted structured output. The '{schema}' placeholder will be replaced with the JSON schema for the output.\n\n### json_schema_transformer\n\n```python\njson_schema_transformer: (\n    type[JsonSchemaTransformer] | None\n) = None\n\n```\n\nThe transformer to use to make JSON schemas for tools and structured output compatible with the model.\n\n### thinking_tags\n\n```python\nthinking_tags: tuple[str, str] = ('<think>', '</think>')\n\n```\n\nThe tags used to indicate thinking parts in the model's output. Defaults to ('', '').\n\n### ignore_streamed_leading_whitespace\n\n```python\nignore_streamed_leading_whitespace: bool = False\n\n```\n\nWhether to ignore leading whitespace when streaming a response.\n\n```text\nThis is a workaround for models that emit `<think>\n\n```\n\n`or an empty text part ahead of tool calls (e.g. Ollama + Qwen3), which we don't want to end up treating as a final result when using`run_stream`with`str`a valid`output_type\\`.\n\n```text\nThis is currently only used by `OpenAIChatModel`, `HuggingFaceModel`, and `GroqModel`.\n\n```\n\n### from_profile\n\n```python\nfrom_profile(profile: ModelProfile | None) -> Self\n\n```\n\nBuild a ModelProfile subclass instance from a ModelProfile instance.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/__init__.py`\n\n```python\n@classmethod\ndef from_profile(cls, profile: ModelProfile | None) -> Self:\n    \"\"\"Build a ModelProfile subclass instance from a ModelProfile instance.\"\"\"\n    if isinstance(profile, cls):\n        return profile\n    return cls().update(profile)\n\n```\n\n### update\n\n```python\nupdate(profile: ModelProfile | None) -> Self\n\n```\n\nUpdate this ModelProfile (subclass) instance with the non-default values from another ModelProfile instance.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/__init__.py`\n\n```python\ndef update(self, profile: ModelProfile | None) -> Self:\n    \"\"\"Update this ModelProfile (subclass) instance with the non-default values from another ModelProfile instance.\"\"\"\n    if not profile:\n        return self\n    field_names = set(f.name for f in fields(self))\n    non_default_attrs = {\n        f.name: getattr(profile, f.name)\n        for f in fields(profile)\n        if f.name in field_names and getattr(profile, f.name) != f.default\n    }\n    return replace(self, **non_default_attrs)\n\n```\n\n### OpenAIModelProfile\n\nBases: `ModelProfile`\n\nProfile for models used with `OpenAIChatModel`.\n\nALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/openai.py`\n\n```python\n@dataclass(kw_only=True)\nclass OpenAIModelProfile(ModelProfile):\n    \"\"\"Profile for models used with `OpenAIChatModel`.\n\n    ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n    \"\"\"\n\n    openai_supports_strict_tool_definition: bool = True\n    \"\"\"This can be set by a provider or user if the OpenAI-\"compatible\" API doesn't support strict tool definitions.\"\"\"\n\n    openai_supports_sampling_settings: bool = True\n    \"\"\"Turn off to don't send sampling settings like `temperature` and `top_p` to models that don't support them, like OpenAI's o-series reasoning models.\"\"\"\n\n    openai_unsupported_model_settings: Sequence[str] = ()\n    \"\"\"A list of model settings that are not supported by this model.\"\"\"\n\n    # Some OpenAI-compatible providers (e.g. MoonshotAI) currently do **not** accept\n    # `tool_choice=\"required\"`.  This flag lets the calling model know whether it's\n    # safe to pass that value along.  Default is `True` to preserve existing\n    # behaviour for OpenAI itself and most providers.\n    openai_supports_tool_choice_required: bool = True\n    \"\"\"Whether the provider accepts the value ``tool_choice='required'`` in the request payload.\"\"\"\n\n    openai_system_prompt_role: OpenAISystemPromptRole | None = None\n    \"\"\"The role to use for the system prompt message. If not provided, defaults to `'system'`.\"\"\"\n\n    openai_chat_supports_web_search: bool = False\n    \"\"\"Whether the model supports web search in Chat Completions API.\"\"\"\n\n    openai_supports_encrypted_reasoning_content: bool = False\n    \"\"\"Whether the model supports including encrypted reasoning content in the response.\"\"\"\n\n    openai_responses_requires_function_call_status_none: bool = False\n    \"\"\"Whether the Responses API requires the `status` field on function tool calls to be `None`.\n\n    This is required by vLLM Responses API versions before https://github.com/vllm-project/vllm/pull/26706.\n    See https://github.com/pydantic/pydantic-ai/issues/3245 for more details.\n    \"\"\"\n\n    def __post_init__(self):  # pragma: no cover\n        if not self.openai_supports_sampling_settings:\n            warnings.warn(\n                'The `openai_supports_sampling_settings` has no effect, and it will be removed in future versions. '\n                'Use `openai_unsupported_model_settings` instead.',\n                DeprecationWarning,\n            )\n\n```\n\n#### openai_supports_strict_tool_definition\n\n```python\nopenai_supports_strict_tool_definition: bool = True\n\n```\n\nThis can be set by a provider or user if the OpenAI-\"compatible\" API doesn't support strict tool definitions.\n\n#### openai_supports_sampling_settings\n\n```python\nopenai_supports_sampling_settings: bool = True\n\n```\n\nTurn off to don't send sampling settings like `temperature` and `top_p` to models that don't support them, like OpenAI's o-series reasoning models.\n\n#### openai_unsupported_model_settings\n\n```python\nopenai_unsupported_model_settings: Sequence[str] = ()\n\n```\n\nA list of model settings that are not supported by this model.\n\n#### openai_supports_tool_choice_required\n\n```python\nopenai_supports_tool_choice_required: bool = True\n\n```\n\nWhether the provider accepts the value `tool_choice='required'` in the request payload.\n\n#### openai_system_prompt_role\n\n```python\nopenai_system_prompt_role: OpenAISystemPromptRole | None = (\n    None\n)\n\n```\n\nThe role to use for the system prompt message. If not provided, defaults to `'system'`.\n\n#### openai_chat_supports_web_search\n\n```python\nopenai_chat_supports_web_search: bool = False\n\n```\n\nWhether the model supports web search in Chat Completions API.\n\n#### openai_supports_encrypted_reasoning_content\n\n```python\nopenai_supports_encrypted_reasoning_content: bool = False\n\n```\n\nWhether the model supports including encrypted reasoning content in the response.\n\n#### openai_responses_requires_function_call_status_none\n\n```python\nopenai_responses_requires_function_call_status_none: (\n    bool\n) = False\n\n```\n\nWhether the Responses API requires the `status` field on function tool calls to be `None`.\n\nThis is required by vLLM Responses API versions before https://github.com/vllm-project/vllm/pull/26706. See https://github.com/pydantic/pydantic-ai/issues/3245 for more details.\n\n### openai_model_profile\n\n```python\nopenai_model_profile(model_name: str) -> ModelProfile\n\n```\n\nGet the model profile for an OpenAI model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/openai.py`\n\n```python\ndef openai_model_profile(model_name: str) -> ModelProfile:\n    \"\"\"Get the model profile for an OpenAI model.\"\"\"\n    is_gpt_5 = model_name.startswith('gpt-5')\n    is_o_series = model_name.startswith('o')\n    is_reasoning_model = is_o_series or (is_gpt_5 and 'gpt-5-chat' not in model_name)\n\n    # Check if the model supports web search (only specific search-preview models)\n    supports_web_search = '-search-preview' in model_name\n\n    # Structured Outputs (output mode 'native') is only supported with the gpt-4o-mini, gpt-4o-mini-2024-07-18, and gpt-4o-2024-08-06 model snapshots and later.\n    # We leave it in here for all models because the `default_structured_output_mode` is `'tool'`, so `native` is only used\n    # when the user specifically uses the `NativeOutput` marker, so an error from the API is acceptable.\n\n    if is_reasoning_model:\n        openai_unsupported_model_settings = (\n            'temperature',\n            'top_p',\n            'presence_penalty',\n            'frequency_penalty',\n            'logit_bias',\n            'logprobs',\n            'top_logprobs',\n        )\n    else:\n        openai_unsupported_model_settings = ()\n\n    # The o1-mini model doesn't support the `system` role, so we default to `user`.\n    # See https://github.com/pydantic/pydantic-ai/issues/974 for more details.\n    openai_system_prompt_role = 'user' if model_name.startswith('o1-mini') else None\n\n    return OpenAIModelProfile(\n        json_schema_transformer=OpenAIJsonSchemaTransformer,\n        supports_json_schema_output=True,\n        supports_json_object_output=True,\n        supports_image_output=is_gpt_5 or 'o3' in model_name or '4.1' in model_name or '4o' in model_name,\n        openai_unsupported_model_settings=openai_unsupported_model_settings,\n        openai_system_prompt_role=openai_system_prompt_role,\n        openai_chat_supports_web_search=supports_web_search,\n        openai_supports_encrypted_reasoning_content=is_reasoning_model,\n    )\n\n```\n\n### OpenAIJsonSchemaTransformer\n\nBases: `JsonSchemaTransformer`\n\nRecursively handle the schema to make it compatible with OpenAI strict mode.\n\nSee https://platform.openai.com/docs/guides/function-calling?api-mode=responses#strict-mode for more details, but this basically just requires: * `additionalProperties` must be set to false for each object in the parameters * all fields in properties must be marked as required\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/openai.py`\n\n```python\n@dataclass(init=False)\nclass OpenAIJsonSchemaTransformer(JsonSchemaTransformer):\n    \"\"\"Recursively handle the schema to make it compatible with OpenAI strict mode.\n\n    See https://platform.openai.com/docs/guides/function-calling?api-mode=responses#strict-mode for more details,\n    but this basically just requires:\n    * `additionalProperties` must be set to false for each object in the parameters\n    * all fields in properties must be marked as required\n    \"\"\"\n\n    def __init__(self, schema: JsonSchema, *, strict: bool | None = None):\n        super().__init__(schema, strict=strict)\n        self.root_ref = schema.get('$ref')\n\n    def walk(self) -> JsonSchema:\n        # Note: OpenAI does not support anyOf at the root in strict mode\n        # However, we don't need to check for it here because we ensure in pydantic_ai._utils.check_object_json_schema\n        # that the root schema either has type 'object' or is recursive.\n        result = super().walk()\n\n        # For recursive models, we need to tweak the schema to make it compatible with strict mode.\n        # Because the following should never change the semantics of the schema we apply it unconditionally.\n        if self.root_ref is not None:\n            result.pop('$ref', None)  # We replace references to the self.root_ref with just '#' in the transform method\n            root_key = re.sub(r'^#/\\$defs/', '', self.root_ref)\n            result.update(self.defs.get(root_key) or {})\n\n        return result\n\n    def transform(self, schema: JsonSchema) -> JsonSchema:  # noqa: C901\n        # Remove unnecessary keys\n        schema.pop('title', None)\n        schema.pop('$schema', None)\n        schema.pop('discriminator', None)\n\n        default = schema.get('default', _sentinel)\n        if default is not _sentinel:\n            # the \"default\" keyword is not allowed in strict mode, but including it makes some Ollama models behave\n            # better, so we keep it around when not strict\n            if self.strict is True:\n                schema.pop('default', None)\n            elif self.strict is None:  # pragma: no branch\n                self.is_strict_compatible = False\n\n        if schema_ref := schema.get('$ref'):\n            if schema_ref == self.root_ref:\n                schema['$ref'] = '#'\n            if len(schema) > 1:\n                # OpenAI Strict mode doesn't support siblings to \"$ref\", but _does_ allow siblings to \"anyOf\".\n                # So if there is a \"description\" field or any other extra info, we move the \"$ref\" into an \"anyOf\":\n                schema['anyOf'] = [{'$ref': schema.pop('$ref')}]\n\n        # Track strict-incompatible keys\n        incompatible_values: dict[str, Any] = {}\n        for key in _STRICT_INCOMPATIBLE_KEYS:\n            value = schema.get(key, _sentinel)\n            if value is not _sentinel:\n                incompatible_values[key] = value\n        if format := schema.get('format'):\n            if format not in _STRICT_COMPATIBLE_STRING_FORMATS:\n                incompatible_values['format'] = format\n        description = schema.get('description')\n        if incompatible_values:\n            if self.strict is True:\n                notes: list[str] = []\n                for key, value in incompatible_values.items():\n                    schema.pop(key)\n                    notes.append(f'{key}={value}')\n                notes_string = ', '.join(notes)\n                schema['description'] = notes_string if not description else f'{description} ({notes_string})'\n            elif self.strict is None:  # pragma: no branch\n                self.is_strict_compatible = False\n\n        schema_type = schema.get('type')\n        if 'oneOf' in schema:\n            # OpenAI does not support oneOf in strict mode\n            if self.strict is True:\n                schema['anyOf'] = schema.pop('oneOf')\n            else:\n                self.is_strict_compatible = False\n\n        if schema_type == 'object':\n            if self.strict is True:\n                # additional properties are disallowed\n                schema['additionalProperties'] = False\n\n                # all properties are required\n                if 'properties' not in schema:\n                    schema['properties'] = dict[str, Any]()\n                schema['required'] = list(schema['properties'].keys())\n\n            elif self.strict is None:\n                if schema.get('additionalProperties', None) not in (None, False):\n                    self.is_strict_compatible = False\n                else:\n                    # additional properties are disallowed by default\n                    schema['additionalProperties'] = False\n\n                if 'properties' not in schema or 'required' not in schema:\n                    self.is_strict_compatible = False\n                else:\n                    required = schema['required']\n                    for k in schema['properties'].keys():\n                        if k not in required:\n                            self.is_strict_compatible = False\n        return schema\n\n```\n\n### anthropic_model_profile\n\n```python\nanthropic_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n\n```\n\nGet the model profile for an Anthropic model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/anthropic.py`\n\n```python\ndef anthropic_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for an Anthropic model.\"\"\"\n    models_that_support_json_schema_output = ('claude-sonnet-4-5', 'claude-opus-4-1', 'claude-opus-4-5')\n    \"\"\"These models support both structured outputs and strict tool calling.\"\"\"\n    # TODO update when new models are released that support structured outputs\n    # https://docs.claude.com/en/docs/build-with-claude/structured-outputs#example-usage\n\n    supports_json_schema_output = model_name.startswith(models_that_support_json_schema_output)\n    return ModelProfile(\n        thinking_tags=('<thinking>', '</thinking>'),\n        supports_json_schema_output=supports_json_schema_output,\n        json_schema_transformer=AnthropicJsonSchemaTransformer,\n    )\n\n```\n\n### AnthropicJsonSchemaTransformer\n\nBases: `JsonSchemaTransformer`\n\nTransforms schemas to the subset supported by Anthropic structured outputs.\n\nTransformation is applied when:\n\n- `NativeOutput` is used as the `output_type` of the Agent\n- `strict=True` is set on the `Tool`\n\nThe behavior of this transformer differs from the OpenAI one in that it sets `Tool.strict=False` by default when not explicitly set to True.\n\nExample\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('anthropic:claude-sonnet-4-5')\n\n@agent.tool_plain  # -> defaults to strict=False\ndef my_tool(x: str) -> dict[str, int]:\n    ...\n\n```\n\nAnthropic's SDK `transform_schema()` automatically:\n\n- Adds `additionalProperties: false` to all objects (required by API)\n- Removes unsupported constraints (minLength, pattern, etc.)\n- Moves removed constraints to description field\n- Removes title and $schema fields\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/anthropic.py`\n\n````python\n@dataclass(init=False)\nclass AnthropicJsonSchemaTransformer(JsonSchemaTransformer):\n    \"\"\"Transforms schemas to the subset supported by Anthropic structured outputs.\n\n    Transformation is applied when:\n    - `NativeOutput` is used as the `output_type` of the Agent\n    - `strict=True` is set on the `Tool`\n\n    The behavior of this transformer differs from the OpenAI one in that it sets `Tool.strict=False` by default when not explicitly set to True.\n\n    Example:\n        ```python\n        from pydantic_ai import Agent\n\n        agent = Agent('anthropic:claude-sonnet-4-5')\n\n        @agent.tool_plain  # -> defaults to strict=False\n        def my_tool(x: str) -> dict[str, int]:\n            ...\n        ```\n\n    Anthropic's SDK `transform_schema()` automatically:\n    - Adds `additionalProperties: false` to all objects (required by API)\n    - Removes unsupported constraints (minLength, pattern, etc.)\n    - Moves removed constraints to description field\n    - Removes title and $schema fields\n    \"\"\"\n\n    def walk(self) -> JsonSchema:\n        from anthropic import transform_schema\n\n        schema = super().walk()\n\n        # The caller (pydantic_ai.models._customize_tool_def or _customize_output_object) coalesces\n        # - output_object.strict = self.is_strict_compatible\n        # - tool_def.strict = self.is_strict_compatible\n        # the reason we don't default to `strict=True` is that the transformation could be lossy\n        # so in order to change the behavior (default to True), we need to come up with logic that will check for lossiness\n        # https://github.com/pydantic/pydantic-ai/issues/3541\n        self.is_strict_compatible = self.strict is True  # not compatible when strict is False/None\n\n        return transform_schema(schema) if self.strict is True else schema\n\n    def transform(self, schema: JsonSchema) -> JsonSchema:\n        schema.pop('title', None)\n        schema.pop('$schema', None)\n        return schema\n\n````\n\n### GoogleModelProfile\n\nBases: `ModelProfile`\n\nProfile for models used with `GoogleModel`.\n\nALL FIELDS MUST BE `google_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/google.py`\n\n```python\n@dataclass(kw_only=True)\nclass GoogleModelProfile(ModelProfile):\n    \"\"\"Profile for models used with `GoogleModel`.\n\n    ALL FIELDS MUST BE `google_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n    \"\"\"\n\n    google_supports_native_output_with_builtin_tools: bool = False\n    \"\"\"Whether the model supports native output with builtin tools.\n    See https://ai.google.dev/gemini-api/docs/structured-output?example=recipe#structured_outputs_with_tools\"\"\"\n\n```\n\n#### google_supports_native_output_with_builtin_tools\n\n```python\ngoogle_supports_native_output_with_builtin_tools: bool = (\n    False\n)\n\n```\n\nWhether the model supports native output with builtin tools. See https://ai.google.dev/gemini-api/docs/structured-output?example=recipe#structured_outputs_with_tools\n\n### google_model_profile\n\n```python\ngoogle_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n\n```\n\nGet the model profile for a Google model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/google.py`\n\n```python\ndef google_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for a Google model.\"\"\"\n    is_image_model = 'image' in model_name\n    is_3_or_newer = 'gemini-3' in model_name\n    return GoogleModelProfile(\n        json_schema_transformer=GoogleJsonSchemaTransformer,\n        supports_image_output=is_image_model,\n        supports_json_schema_output=is_3_or_newer or not is_image_model,\n        supports_json_object_output=is_3_or_newer or not is_image_model,\n        supports_tools=not is_image_model,\n        google_supports_native_output_with_builtin_tools=is_3_or_newer,\n    )\n\n```\n\n### GoogleJsonSchemaTransformer\n\nBases: `JsonSchemaTransformer`\n\nTransforms the JSON Schema from Pydantic to be suitable for Gemini.\n\nGemini supports [a subset of OpenAPI v3.0.3](https://ai.google.dev/gemini-api/docs/function-calling#function_declarations).\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/google.py`\n\n```python\nclass GoogleJsonSchemaTransformer(JsonSchemaTransformer):\n    \"\"\"Transforms the JSON Schema from Pydantic to be suitable for Gemini.\n\n    Gemini supports [a subset of OpenAPI v3.0.3](https://ai.google.dev/gemini-api/docs/function-calling#function_declarations).\n    \"\"\"\n\n    def transform(self, schema: JsonSchema) -> JsonSchema:\n        # Remove properties not supported by Gemini\n        schema.pop('$schema', None)\n        if (const := schema.pop('const', None)) is not None:\n            # Gemini doesn't support const, but it does support enum with a single value\n            schema['enum'] = [const]\n        schema.pop('discriminator', None)\n        schema.pop('examples', None)\n\n        # Remove 'title' due to https://github.com/googleapis/python-genai/issues/1732\n        schema.pop('title', None)\n\n        type_ = schema.get('type')\n        if type_ == 'string' and (fmt := schema.pop('format', None)):\n            description = schema.get('description')\n            if description:\n                schema['description'] = f'{description} (format: {fmt})'\n            else:\n                schema['description'] = f'Format: {fmt}'\n\n        # Note: exclusiveMinimum/exclusiveMaximum are NOT yet supported\n        schema.pop('exclusiveMinimum', None)\n        schema.pop('exclusiveMaximum', None)\n\n        return schema\n\n```\n\n### meta_model_profile\n\n```python\nmeta_model_profile(model_name: str) -> ModelProfile | None\n\n```\n\nGet the model profile for a Meta model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/meta.py`\n\n```python\ndef meta_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for a Meta model.\"\"\"\n    return ModelProfile(json_schema_transformer=InlineDefsJsonSchemaTransformer)\n\n```\n\n### amazon_model_profile\n\n```python\namazon_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n\n```\n\nGet the model profile for an Amazon model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/amazon.py`\n\n```python\ndef amazon_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for an Amazon model.\"\"\"\n    return ModelProfile(json_schema_transformer=InlineDefsJsonSchemaTransformer)\n\n```\n\n### deepseek_model_profile\n\n```python\ndeepseek_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n\n```\n\nGet the model profile for a DeepSeek model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/deepseek.py`\n\n```python\ndef deepseek_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for a DeepSeek model.\"\"\"\n    return ModelProfile(ignore_streamed_leading_whitespace='r1' in model_name)\n\n```\n\n### grok_model_profile\n\n```python\ngrok_model_profile(model_name: str) -> ModelProfile | None\n\n```\n\nGet the model profile for a Grok model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/grok.py`\n\n```python\ndef grok_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for a Grok model.\"\"\"\n    return None\n\n```\n\n### mistral_model_profile\n\n```python\nmistral_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n\n```\n\nGet the model profile for a Mistral model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/mistral.py`\n\n```python\ndef mistral_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for a Mistral model.\"\"\"\n    return None\n\n```\n\n### qwen_model_profile\n\n```python\nqwen_model_profile(model_name: str) -> ModelProfile | None\n\n```\n\nGet the model profile for a Qwen model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/qwen.py`\n\n```python\ndef qwen_model_profile(model_name: str) -> ModelProfile | None:\n    \"\"\"Get the model profile for a Qwen model.\"\"\"\n    if model_name.startswith('qwen-3-coder'):\n        return OpenAIModelProfile(\n            json_schema_transformer=InlineDefsJsonSchemaTransformer,\n            openai_supports_tool_choice_required=False,\n            openai_supports_strict_tool_definition=False,\n            ignore_streamed_leading_whitespace=True,\n        )\n    return ModelProfile(\n        json_schema_transformer=InlineDefsJsonSchemaTransformer,\n        ignore_streamed_leading_whitespace=True,\n    )\n\n```",
  "content_length": 29485
}