{
  "title": "`pydantic_ai.ui`",
  "source_url": null,
  "content": "### StateDeps\n\nBases: `Generic[StateT]`\n\nDependency type that holds state.\n\nThis class is used to manage the state of an agent run. It allows setting the state of the agent run with a specific type of state model, which must be a subclass of `BaseModel`.\n\nThe state is set using the `state` setter by the `Adapter` when the run starts.\n\nImplements the `StateHandler` protocol.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\n@dataclass\nclass StateDeps(Generic[StateT]):\n    \"\"\"Dependency type that holds state.\n\n    This class is used to manage the state of an agent run. It allows setting\n    the state of the agent run with a specific type of state model, which must\n    be a subclass of `BaseModel`.\n\n    The state is set using the `state` setter by the `Adapter` when the run starts.\n\n    Implements the `StateHandler` protocol.\n    \"\"\"\n\n    state: StateT\n\n```\n\n### StateHandler\n\nBases: `Protocol`\n\nProtocol for state handlers in agent runs. Requires the class to be a dataclass with a `state` field.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\n@runtime_checkable\nclass StateHandler(Protocol):\n    \"\"\"Protocol for state handlers in agent runs. Requires the class to be a dataclass with a `state` field.\"\"\"\n\n    # Has to be a dataclass so we can use `replace` to update the state.\n    # From https://github.com/python/typeshed/blob/9ab7fde0a0cd24ed7a72837fcb21093b811b80d8/stdlib/_typeshed/__init__.pyi#L352\n    __dataclass_fields__: ClassVar[dict[str, Field[Any]]]\n\n    @property\n    def state(self) -> Any:\n        \"\"\"Get the current state of the agent run.\"\"\"\n        ...\n\n    @state.setter\n    def state(self, state: Any) -> None:\n        \"\"\"Set the state of the agent run.\n\n        This method is called to update the state of the agent run with the\n        provided state.\n\n        Args:\n            state: The run state.\n        \"\"\"\n        ...\n\n```\n\n#### state\n\n```python\nstate: Any\n\n```\n\nGet the current state of the agent run.\n\n### UIAdapter\n\nBases: `ABC`, `Generic[RunInputT, MessageT, EventT, AgentDepsT, OutputDataT]`\n\nBase class for UI adapters.\n\nThis class is responsible for transforming agent run input received from the frontend into arguments for Agent.run_stream_events(), running the agent, and then transforming Pydantic AI events into protocol-specific events.\n\nThe event stream transformation is handled by a protocol-specific UIEventStream subclass.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\n@dataclass\nclass UIAdapter(ABC, Generic[RunInputT, MessageT, EventT, AgentDepsT, OutputDataT]):\n    \"\"\"Base class for UI adapters.\n\n    This class is responsible for transforming agent run input received from the frontend into arguments for [`Agent.run_stream_events()`][pydantic_ai.Agent.run_stream_events], running the agent, and then transforming Pydantic AI events into protocol-specific events.\n\n    The event stream transformation is handled by a protocol-specific [`UIEventStream`][pydantic_ai.ui.UIEventStream] subclass.\n    \"\"\"\n\n    agent: AbstractAgent[AgentDepsT, OutputDataT]\n    \"\"\"The Pydantic AI agent to run.\"\"\"\n\n    run_input: RunInputT\n    \"\"\"The protocol-specific run input object.\"\"\"\n\n    _: KW_ONLY\n\n    accept: str | None = None\n    \"\"\"The `Accept` header value of the request, used to determine how to encode the protocol-specific events for the streaming response.\"\"\"\n\n    @classmethod\n    async def from_request(\n        cls, request: Request, *, agent: AbstractAgent[AgentDepsT, OutputDataT]\n    ) -> UIAdapter[RunInputT, MessageT, EventT, AgentDepsT, OutputDataT]:\n        \"\"\"Create an adapter from a request.\"\"\"\n        return cls(\n            agent=agent,\n            run_input=cls.build_run_input(await request.body()),\n            accept=request.headers.get('accept'),\n        )\n\n    @classmethod\n    @abstractmethod\n    def build_run_input(cls, body: bytes) -> RunInputT:\n        \"\"\"Build a protocol-specific run input object from the request body.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def load_messages(cls, messages: Sequence[MessageT]) -> list[ModelMessage]:\n        \"\"\"Transform protocol-specific messages into Pydantic AI messages.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def build_event_stream(self) -> UIEventStream[RunInputT, EventT, AgentDepsT, OutputDataT]:\n        \"\"\"Build a protocol-specific event stream transformer.\"\"\"\n        raise NotImplementedError\n\n    @cached_property\n    @abstractmethod\n    def messages(self) -> list[ModelMessage]:\n        \"\"\"Pydantic AI messages from the protocol-specific run input.\"\"\"\n        raise NotImplementedError\n\n    @cached_property\n    def toolset(self) -> AbstractToolset[AgentDepsT] | None:\n        \"\"\"Toolset representing frontend tools from the protocol-specific run input.\"\"\"\n        return None\n\n    @cached_property\n    def state(self) -> dict[str, Any] | None:\n        \"\"\"Frontend state from the protocol-specific run input.\"\"\"\n        return None\n\n    def transform_stream(\n        self,\n        stream: AsyncIterator[NativeEvent],\n        on_complete: OnCompleteFunc[EventT] | None = None,\n    ) -> AsyncIterator[EventT]:\n        \"\"\"Transform a stream of Pydantic AI events into protocol-specific events.\n\n        Args:\n            stream: The stream of Pydantic AI events to transform.\n            on_complete: Optional callback function called when the agent run completes successfully.\n                The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n        \"\"\"\n        return self.build_event_stream().transform_stream(stream, on_complete=on_complete)\n\n    def encode_stream(self, stream: AsyncIterator[EventT]) -> AsyncIterator[str]:\n        \"\"\"Encode a stream of protocol-specific events as strings according to the `Accept` header value.\n\n        Args:\n            stream: The stream of protocol-specific events to encode.\n        \"\"\"\n        return self.build_event_stream().encode_stream(stream)\n\n    def streaming_response(self, stream: AsyncIterator[EventT]) -> StreamingResponse:\n        \"\"\"Generate a streaming response from a stream of protocol-specific events.\n\n        Args:\n            stream: The stream of protocol-specific events to encode.\n        \"\"\"\n        return self.build_event_stream().streaming_response(stream)\n\n    def run_stream_native(\n        self,\n        *,\n        output_type: OutputSpec[Any] | None = None,\n        message_history: Sequence[ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: Model | KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: UsageLimits | None = None,\n        usage: RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    ) -> AsyncIterator[NativeEvent]:\n        \"\"\"Run the agent with the protocol-specific run input and stream Pydantic AI events.\n\n        Args:\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools to use for this run.\n        \"\"\"\n        message_history = [*(message_history or []), *self.messages]\n\n        toolset = self.toolset\n        if toolset:\n            output_type = [output_type or self.agent.output_type, DeferredToolRequests]\n            toolsets = [*(toolsets or []), toolset]\n\n        if isinstance(deps, StateHandler):\n            raw_state = self.state or {}\n            if isinstance(deps.state, BaseModel):\n                state = type(deps.state).model_validate(raw_state)\n            else:\n                state = raw_state\n\n            deps.state = state\n        elif self.state:\n            warnings.warn(\n                f'State was provided but `deps` of type `{type(deps).__name__}` does not implement the `StateHandler` protocol, so the state was ignored. Use `StateDeps[...]` or implement `StateHandler` to receive AG-UI state.',\n                UserWarning,\n                stacklevel=2,\n            )\n\n        return self.agent.run_stream_events(\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            deps=deps,\n            model_settings=model_settings,\n            instructions=instructions,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n        )\n\n    def run_stream(\n        self,\n        *,\n        output_type: OutputSpec[Any] | None = None,\n        message_history: Sequence[ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: Model | KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: UsageLimits | None = None,\n        usage: RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        on_complete: OnCompleteFunc[EventT] | None = None,\n    ) -> AsyncIterator[EventT]:\n        \"\"\"Run the agent with the protocol-specific run input and stream protocol-specific events.\n\n        Args:\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools to use for this run.\n            on_complete: Optional callback function called when the agent run completes successfully.\n                The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n        \"\"\"\n        return self.transform_stream(\n            self.run_stream_native(\n                output_type=output_type,\n                message_history=message_history,\n                deferred_tool_results=deferred_tool_results,\n                model=model,\n                instructions=instructions,\n                deps=deps,\n                model_settings=model_settings,\n                usage_limits=usage_limits,\n                usage=usage,\n                infer_name=infer_name,\n                toolsets=toolsets,\n                builtin_tools=builtin_tools,\n            ),\n            on_complete=on_complete,\n        )\n\n    @classmethod\n    async def dispatch_request(\n        cls,\n        request: Request,\n        *,\n        agent: AbstractAgent[AgentDepsT, OutputDataT],\n        message_history: Sequence[ModelMessage] | None = None,\n        deferred_tool_results: DeferredToolResults | None = None,\n        model: Model | KnownModelName | str | None = None,\n        instructions: Instructions[AgentDepsT] = None,\n        deps: AgentDepsT = None,\n        output_type: OutputSpec[Any] | None = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: UsageLimits | None = None,\n        usage: RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n        on_complete: OnCompleteFunc[EventT] | None = None,\n    ) -> Response:\n        \"\"\"Handle a protocol-specific HTTP request by running the agent and returning a streaming response of protocol-specific events.\n\n        Args:\n            request: The incoming Starlette/FastAPI request.\n            agent: The agent to run.\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n                output validators since output validators would expect an argument that matches the agent's output type.\n            message_history: History of the conversation so far.\n            deferred_tool_results: Optional results for deferred tool calls in the message history.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            instructions: Optional additional instructions to use for this run.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n            builtin_tools: Optional additional builtin tools to use for this run.\n            on_complete: Optional callback function called when the agent run completes successfully.\n                The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n\n        Returns:\n            A streaming Starlette response with protocol-specific events encoded per the request's `Accept` header value.\n        \"\"\"\n        try:\n            from starlette.responses import Response\n        except ImportError as e:  # pragma: no cover\n            raise ImportError(\n                'Please install the `starlette` package to use `dispatch_request()` method, '\n                'you can use the `ui` optional group — `pip install \"pydantic-ai-slim[ui]\"`'\n            ) from e\n\n        try:\n            adapter = await cls.from_request(request, agent=agent)\n        except ValidationError as e:  # pragma: no cover\n            return Response(\n                content=e.json(),\n                media_type='application/json',\n                status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n            )\n\n        return adapter.streaming_response(\n            adapter.run_stream(\n                message_history=message_history,\n                deferred_tool_results=deferred_tool_results,\n                deps=deps,\n                output_type=output_type,\n                model=model,\n                instructions=instructions,\n                model_settings=model_settings,\n                usage_limits=usage_limits,\n                usage=usage,\n                infer_name=infer_name,\n                toolsets=toolsets,\n                builtin_tools=builtin_tools,\n                on_complete=on_complete,\n            ),\n        )\n\n```\n\n#### agent\n\n```python\nagent: AbstractAgent[AgentDepsT, OutputDataT]\n\n```\n\nThe Pydantic AI agent to run.\n\n#### run_input\n\n```python\nrun_input: RunInputT\n\n```\n\nThe protocol-specific run input object.\n\n#### accept\n\n```python\naccept: str | None = None\n\n```\n\nThe `Accept` header value of the request, used to determine how to encode the protocol-specific events for the streaming response.\n\n#### from_request\n\n```python\nfrom_request(\n    request: Request,\n    *,\n    agent: AbstractAgent[AgentDepsT, OutputDataT]\n) -> UIAdapter[\n    RunInputT, MessageT, EventT, AgentDepsT, OutputDataT\n]\n\n```\n\nCreate an adapter from a request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\n@classmethod\nasync def from_request(\n    cls, request: Request, *, agent: AbstractAgent[AgentDepsT, OutputDataT]\n) -> UIAdapter[RunInputT, MessageT, EventT, AgentDepsT, OutputDataT]:\n    \"\"\"Create an adapter from a request.\"\"\"\n    return cls(\n        agent=agent,\n        run_input=cls.build_run_input(await request.body()),\n        accept=request.headers.get('accept'),\n    )\n\n```\n\n#### build_run_input\n\n```python\nbuild_run_input(body: bytes) -> RunInputT\n\n```\n\nBuild a protocol-specific run input object from the request body.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\n@classmethod\n@abstractmethod\ndef build_run_input(cls, body: bytes) -> RunInputT:\n    \"\"\"Build a protocol-specific run input object from the request body.\"\"\"\n    raise NotImplementedError\n\n```\n\n#### load_messages\n\n```python\nload_messages(\n    messages: Sequence[MessageT],\n) -> list[ModelMessage]\n\n```\n\nTransform protocol-specific messages into Pydantic AI messages.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\n@classmethod\n@abstractmethod\ndef load_messages(cls, messages: Sequence[MessageT]) -> list[ModelMessage]:\n    \"\"\"Transform protocol-specific messages into Pydantic AI messages.\"\"\"\n    raise NotImplementedError\n\n```\n\n#### build_event_stream\n\n```python\nbuild_event_stream() -> (\n    UIEventStream[\n        RunInputT, EventT, AgentDepsT, OutputDataT\n    ]\n)\n\n```\n\nBuild a protocol-specific event stream transformer.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\n@abstractmethod\ndef build_event_stream(self) -> UIEventStream[RunInputT, EventT, AgentDepsT, OutputDataT]:\n    \"\"\"Build a protocol-specific event stream transformer.\"\"\"\n    raise NotImplementedError\n\n```\n\n#### messages\n\n```python\nmessages: list[ModelMessage]\n\n```\n\nPydantic AI messages from the protocol-specific run input.\n\n#### toolset\n\n```python\ntoolset: AbstractToolset[AgentDepsT] | None\n\n```\n\nToolset representing frontend tools from the protocol-specific run input.\n\n#### state\n\n```python\nstate: dict[str, Any] | None\n\n```\n\nFrontend state from the protocol-specific run input.\n\n#### transform_stream\n\n```python\ntransform_stream(\n    stream: AsyncIterator[NativeEvent],\n    on_complete: OnCompleteFunc[EventT] | None = None,\n) -> AsyncIterator[EventT]\n\n```\n\nTransform a stream of Pydantic AI events into protocol-specific events.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `stream` | `AsyncIterator[NativeEvent]` | The stream of Pydantic AI events to transform. | *required* | | `on_complete` | `OnCompleteFunc[EventT] | None` | Optional callback function called when the agent run completes successfully. The callback receives the completed AgentRunResult and can optionally yield additional protocol-specific events. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\ndef transform_stream(\n    self,\n    stream: AsyncIterator[NativeEvent],\n    on_complete: OnCompleteFunc[EventT] | None = None,\n) -> AsyncIterator[EventT]:\n    \"\"\"Transform a stream of Pydantic AI events into protocol-specific events.\n\n    Args:\n        stream: The stream of Pydantic AI events to transform.\n        on_complete: Optional callback function called when the agent run completes successfully.\n            The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n    \"\"\"\n    return self.build_event_stream().transform_stream(stream, on_complete=on_complete)\n\n```\n\n#### encode_stream\n\n```python\nencode_stream(\n    stream: AsyncIterator[EventT],\n) -> AsyncIterator[str]\n\n```\n\nEncode a stream of protocol-specific events as strings according to the `Accept` header value.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `stream` | `AsyncIterator[EventT]` | The stream of protocol-specific events to encode. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\ndef encode_stream(self, stream: AsyncIterator[EventT]) -> AsyncIterator[str]:\n    \"\"\"Encode a stream of protocol-specific events as strings according to the `Accept` header value.\n\n    Args:\n        stream: The stream of protocol-specific events to encode.\n    \"\"\"\n    return self.build_event_stream().encode_stream(stream)\n\n```\n\n#### streaming_response\n\n```python\nstreaming_response(\n    stream: AsyncIterator[EventT],\n) -> StreamingResponse\n\n```\n\nGenerate a streaming response from a stream of protocol-specific events.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `stream` | `AsyncIterator[EventT]` | The stream of protocol-specific events to encode. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\ndef streaming_response(self, stream: AsyncIterator[EventT]) -> StreamingResponse:\n    \"\"\"Generate a streaming response from a stream of protocol-specific events.\n\n    Args:\n        stream: The stream of protocol-specific events to encode.\n    \"\"\"\n    return self.build_event_stream().streaming_response(stream)\n\n```\n\n#### run_stream_native\n\n```python\nrun_stream_native(\n    *,\n    output_type: OutputSpec[Any] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[NativeEvent]\n\n```\n\nRun the agent with the protocol-specific run input and stream Pydantic AI events.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_type` | `OutputSpec[Any] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools to use for this run. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\ndef run_stream_native(\n    self,\n    *,\n    output_type: OutputSpec[Any] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n) -> AsyncIterator[NativeEvent]:\n    \"\"\"Run the agent with the protocol-specific run input and stream Pydantic AI events.\n\n    Args:\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools to use for this run.\n    \"\"\"\n    message_history = [*(message_history or []), *self.messages]\n\n    toolset = self.toolset\n    if toolset:\n        output_type = [output_type or self.agent.output_type, DeferredToolRequests]\n        toolsets = [*(toolsets or []), toolset]\n\n    if isinstance(deps, StateHandler):\n        raw_state = self.state or {}\n        if isinstance(deps.state, BaseModel):\n            state = type(deps.state).model_validate(raw_state)\n        else:\n            state = raw_state\n\n        deps.state = state\n    elif self.state:\n        warnings.warn(\n            f'State was provided but `deps` of type `{type(deps).__name__}` does not implement the `StateHandler` protocol, so the state was ignored. Use `StateDeps[...]` or implement `StateHandler` to receive AG-UI state.',\n            UserWarning,\n            stacklevel=2,\n        )\n\n    return self.agent.run_stream_events(\n        output_type=output_type,\n        message_history=message_history,\n        deferred_tool_results=deferred_tool_results,\n        model=model,\n        deps=deps,\n        model_settings=model_settings,\n        instructions=instructions,\n        usage_limits=usage_limits,\n        usage=usage,\n        infer_name=infer_name,\n        toolsets=toolsets,\n        builtin_tools=builtin_tools,\n    )\n\n```\n\n#### run_stream\n\n```python\nrun_stream(\n    *,\n    output_type: OutputSpec[Any] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    on_complete: OnCompleteFunc[EventT] | None = None\n) -> AsyncIterator[EventT]\n\n```\n\nRun the agent with the protocol-specific run input and stream protocol-specific events.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `output_type` | `OutputSpec[Any] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools to use for this run. | `None` | | `on_complete` | `OnCompleteFunc[EventT] | None` | Optional callback function called when the agent run completes successfully. The callback receives the completed AgentRunResult and can optionally yield additional protocol-specific events. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\ndef run_stream(\n    self,\n    *,\n    output_type: OutputSpec[Any] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    on_complete: OnCompleteFunc[EventT] | None = None,\n) -> AsyncIterator[EventT]:\n    \"\"\"Run the agent with the protocol-specific run input and stream protocol-specific events.\n\n    Args:\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools to use for this run.\n        on_complete: Optional callback function called when the agent run completes successfully.\n            The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n    \"\"\"\n    return self.transform_stream(\n        self.run_stream_native(\n            output_type=output_type,\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            model=model,\n            instructions=instructions,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n        ),\n        on_complete=on_complete,\n    )\n\n```\n\n#### dispatch_request\n\n```python\ndispatch_request(\n    request: Request,\n    *,\n    agent: AbstractAgent[AgentDepsT, OutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    output_type: OutputSpec[Any] | None = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    on_complete: OnCompleteFunc[EventT] | None = None\n) -> Response\n\n```\n\nHandle a protocol-specific HTTP request by running the agent and returning a streaming response of protocol-specific events.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `request` | `Request` | The incoming Starlette/FastAPI request. | *required* | | `agent` | `AbstractAgent[AgentDepsT, OutputDataT]` | The agent to run. | *required* | | `output_type` | `OutputSpec[Any] | None` | Custom output type to use for this run, output_type may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` | | `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` | | `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` | | `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if model was not set when creating the agent. | `None` | | `instructions` | `Instructions[AgentDepsT]` | Optional additional instructions to use for this run. | `None` | | `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` | | `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` | | `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` | | `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` | | `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` | | `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` | | `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools to use for this run. | `None` | | `on_complete` | `OnCompleteFunc[EventT] | None` | Optional callback function called when the agent run completes successfully. The callback receives the completed AgentRunResult and can optionally yield additional protocol-specific events. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `Response` | A streaming Starlette response with protocol-specific events encoded per the request's Accept header value. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_adapter.py`\n\n```python\n@classmethod\nasync def dispatch_request(\n    cls,\n    request: Request,\n    *,\n    agent: AbstractAgent[AgentDepsT, OutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: DeferredToolResults | None = None,\n    model: Model | KnownModelName | str | None = None,\n    instructions: Instructions[AgentDepsT] = None,\n    deps: AgentDepsT = None,\n    output_type: OutputSpec[Any] | None = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,\n    on_complete: OnCompleteFunc[EventT] | None = None,\n) -> Response:\n    \"\"\"Handle a protocol-specific HTTP request by running the agent and returning a streaming response of protocol-specific events.\n\n    Args:\n        request: The incoming Starlette/FastAPI request.\n        agent: The agent to run.\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        message_history: History of the conversation so far.\n        deferred_tool_results: Optional results for deferred tool calls in the message history.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        instructions: Optional additional instructions to use for this run.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        builtin_tools: Optional additional builtin tools to use for this run.\n        on_complete: Optional callback function called when the agent run completes successfully.\n            The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n\n    Returns:\n        A streaming Starlette response with protocol-specific events encoded per the request's `Accept` header value.\n    \"\"\"\n    try:\n        from starlette.responses import Response\n    except ImportError as e:  # pragma: no cover\n        raise ImportError(\n            'Please install the `starlette` package to use `dispatch_request()` method, '\n            'you can use the `ui` optional group — `pip install \"pydantic-ai-slim[ui]\"`'\n        ) from e\n\n    try:\n        adapter = await cls.from_request(request, agent=agent)\n    except ValidationError as e:  # pragma: no cover\n        return Response(\n            content=e.json(),\n            media_type='application/json',\n            status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n        )\n\n    return adapter.streaming_response(\n        adapter.run_stream(\n            message_history=message_history,\n            deferred_tool_results=deferred_tool_results,\n            deps=deps,\n            output_type=output_type,\n            model=model,\n            instructions=instructions,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            builtin_tools=builtin_tools,\n            on_complete=on_complete,\n        ),\n    )\n\n```\n\n### SSE_CONTENT_TYPE\n\n```python\nSSE_CONTENT_TYPE = 'text/event-stream'\n\n```\n\nContent type header value for Server-Sent Events (SSE).\n\n### NativeEvent\n\n```python\nNativeEvent: TypeAlias = (\n    AgentStreamEvent | AgentRunResultEvent[Any]\n)\n\n```\n\nType alias for the native event type, which is either an `AgentStreamEvent` or an `AgentRunResultEvent`.\n\n### OnCompleteFunc\n\n```python\nOnCompleteFunc: TypeAlias = (\n    Callable[[AgentRunResult[Any]], None]\n    | Callable[[AgentRunResult[Any]], Awaitable[None]]\n    | Callable[[AgentRunResult[Any]], AsyncIterator[EventT]]\n)\n\n```\n\nCallback function type that receives the `AgentRunResult` of the completed run. Can be sync, async, or an async generator of protocol-specific events.\n\n### UIEventStream\n\nBases: `ABC`, `Generic[RunInputT, EventT, AgentDepsT, OutputDataT]`\n\nBase class for UI event stream transformers.\n\nThis class is responsible for transforming Pydantic AI events into protocol-specific events.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\n@dataclass\nclass UIEventStream(ABC, Generic[RunInputT, EventT, AgentDepsT, OutputDataT]):\n    \"\"\"Base class for UI event stream transformers.\n\n    This class is responsible for transforming Pydantic AI events into protocol-specific events.\n    \"\"\"\n\n    run_input: RunInputT\n\n    accept: str | None = None\n    \"\"\"The `Accept` header value of the request, used to determine how to encode the protocol-specific events for the streaming response.\"\"\"\n\n    message_id: str = field(default_factory=lambda: str(uuid4()))\n    \"\"\"The message ID to use for the next event.\"\"\"\n\n    _turn: Literal['request', 'response'] | None = None\n\n    _result: AgentRunResult[OutputDataT] | None = None\n    _final_result_event: FinalResultEvent | None = None\n\n    def new_message_id(self) -> str:\n        \"\"\"Generate and store a new message ID.\"\"\"\n        self.message_id = str(uuid4())\n        return self.message_id\n\n    @property\n    def response_headers(self) -> Mapping[str, str] | None:\n        \"\"\"Response headers to return to the frontend.\"\"\"\n        return None\n\n    @property\n    def content_type(self) -> str:\n        \"\"\"Get the content type for the event stream, compatible with the `Accept` header value.\n\n        By default, this returns the Server-Sent Events content type (`text/event-stream`).\n        If a subclass supports other types as well, it should consider `self.accept` in [`encode_event()`][pydantic_ai.ui.UIEventStream.encode_event] and return the resulting content type.\n        \"\"\"\n        return SSE_CONTENT_TYPE\n\n    @abstractmethod\n    def encode_event(self, event: EventT) -> str:\n        \"\"\"Encode a protocol-specific event as a string.\"\"\"\n        raise NotImplementedError\n\n    async def encode_stream(self, stream: AsyncIterator[EventT]) -> AsyncIterator[str]:\n        \"\"\"Encode a stream of protocol-specific events as strings according to the `Accept` header value.\"\"\"\n        async for event in stream:\n            yield self.encode_event(event)\n\n    def streaming_response(self, stream: AsyncIterator[EventT]) -> StreamingResponse:\n        \"\"\"Generate a streaming response from a stream of protocol-specific events.\"\"\"\n        try:\n            from starlette.responses import StreamingResponse\n        except ImportError as e:  # pragma: no cover\n            raise ImportError(\n                'Please install the `starlette` package to use the `streaming_response()` method, '\n                'you can use the `ui` optional group — `pip install \"pydantic-ai-slim[ui]\"`'\n            ) from e\n\n        return StreamingResponse(\n            self.encode_stream(stream),\n            headers=self.response_headers,\n            media_type=self.content_type,\n        )\n\n    async def transform_stream(  # noqa: C901\n        self, stream: AsyncIterator[NativeEvent], on_complete: OnCompleteFunc[EventT] | None = None\n    ) -> AsyncIterator[EventT]:\n        \"\"\"Transform a stream of Pydantic AI events into protocol-specific events.\n\n        This method dispatches to specific hooks and `handle_*` methods that subclasses can override:\n        - [`before_stream()`][pydantic_ai.ui.UIEventStream.before_stream]\n        - [`after_stream()`][pydantic_ai.ui.UIEventStream.after_stream]\n        - [`on_error()`][pydantic_ai.ui.UIEventStream.on_error]\n        - [`before_request()`][pydantic_ai.ui.UIEventStream.before_request]\n        - [`after_request()`][pydantic_ai.ui.UIEventStream.after_request]\n        - [`before_response()`][pydantic_ai.ui.UIEventStream.before_response]\n        - [`after_response()`][pydantic_ai.ui.UIEventStream.after_response]\n        - [`handle_event()`][pydantic_ai.ui.UIEventStream.handle_event]\n\n        Args:\n            stream: The stream of Pydantic AI events to transform.\n            on_complete: Optional callback function called when the agent run completes successfully.\n                The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n        \"\"\"\n        async for e in self.before_stream():\n            yield e\n\n        try:\n            async for event in stream:\n                if isinstance(event, PartStartEvent):\n                    async for e in self._turn_to('response'):\n                        yield e\n                elif isinstance(event, FunctionToolCallEvent):\n                    async for e in self._turn_to('request'):\n                        yield e\n                elif isinstance(event, AgentRunResultEvent):\n                    if (\n                        self._final_result_event\n                        and (tool_call_id := self._final_result_event.tool_call_id)\n                        and (tool_name := self._final_result_event.tool_name)\n                    ):\n                        async for e in self._turn_to('request'):\n                            yield e\n\n                        self._final_result_event = None\n                        # Ensure the stream does not end on a dangling tool call without a result.\n                        output_tool_result_event = FunctionToolResultEvent(\n                            result=ToolReturnPart(\n                                tool_call_id=tool_call_id,\n                                tool_name=tool_name,\n                                content='Final result processed.',\n                            )\n                        )\n                        async for e in self.handle_function_tool_result(output_tool_result_event):\n                            yield e\n\n                    result = cast(AgentRunResult[OutputDataT], event.result)\n                    self._result = result\n\n                    async for e in self._turn_to(None):\n                        yield e\n\n                    if on_complete is not None:\n                        if inspect.isasyncgenfunction(on_complete):\n                            async for e in on_complete(result):\n                                yield e\n                        elif _utils.is_async_callable(on_complete):\n                            await on_complete(result)\n                        else:\n                            await _utils.run_in_executor(on_complete, result)\n                elif isinstance(event, FinalResultEvent):\n                    self._final_result_event = event\n\n                if isinstance(event, BuiltinToolCallEvent | BuiltinToolResultEvent):  # pyright: ignore[reportDeprecated]\n                    # These events were deprecated before this feature was introduced\n                    continue\n\n                async for e in self.handle_event(event):\n                    yield e\n        except Exception as e:\n            async for e in self.on_error(e):\n                yield e\n        finally:\n            async for e in self._turn_to(None):\n                yield e\n\n            async for e in self.after_stream():\n                yield e\n\n    async def _turn_to(self, to_turn: Literal['request', 'response'] | None) -> AsyncIterator[EventT]:\n        \"\"\"Fire hooks when turning from request to response or vice versa.\"\"\"\n        if to_turn == self._turn:\n            return\n\n        if self._turn == 'request':\n            async for e in self.after_request():\n                yield e\n        elif self._turn == 'response':\n            async for e in self.after_response():\n                yield e\n\n        self._turn = to_turn\n\n        if to_turn == 'request':\n            async for e in self.before_request():\n                yield e\n        elif to_turn == 'response':\n            async for e in self.before_response():\n                yield e\n\n    async def handle_event(self, event: NativeEvent) -> AsyncIterator[EventT]:\n        \"\"\"Transform a Pydantic AI event into one or more protocol-specific events.\n\n        This method dispatches to specific `handle_*` methods based on event type:\n\n        - [`PartStartEvent`][pydantic_ai.messages.PartStartEvent] -> [`handle_part_start()`][pydantic_ai.ui.UIEventStream.handle_part_start]\n        - [`PartDeltaEvent`][pydantic_ai.messages.PartDeltaEvent] -> `handle_part_delta`\n        - [`PartEndEvent`][pydantic_ai.messages.PartEndEvent] -> `handle_part_end`\n        - [`FinalResultEvent`][pydantic_ai.messages.FinalResultEvent] -> `handle_final_result`\n        - [`FunctionToolCallEvent`][pydantic_ai.messages.FunctionToolCallEvent] -> `handle_function_tool_call`\n        - [`FunctionToolResultEvent`][pydantic_ai.messages.FunctionToolResultEvent] -> `handle_function_tool_result`\n        - [`AgentRunResultEvent`][pydantic_ai.run.AgentRunResultEvent] -> `handle_run_result`\n\n        Subclasses are encouraged to override the individual `handle_*` methods rather than this one.\n        If you need specific behavior for all events, make sure you call the super method.\n        \"\"\"\n        match event:\n            case PartStartEvent():\n                async for e in self.handle_part_start(event):\n                    yield e\n            case PartDeltaEvent():\n                async for e in self.handle_part_delta(event):\n                    yield e\n            case PartEndEvent():\n                async for e in self.handle_part_end(event):\n                    yield e\n            case FinalResultEvent():\n                async for e in self.handle_final_result(event):\n                    yield e\n            case FunctionToolCallEvent():\n                async for e in self.handle_function_tool_call(event):\n                    yield e\n            case FunctionToolResultEvent():\n                async for e in self.handle_function_tool_result(event):\n                    yield e\n            case AgentRunResultEvent():\n                async for e in self.handle_run_result(event):\n                    yield e\n            case _:\n                pass\n\n    async def handle_part_start(self, event: PartStartEvent) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `PartStartEvent`.\n\n        This method dispatches to specific `handle_*` methods based on part type:\n\n        - [`TextPart`][pydantic_ai.messages.TextPart] -> [`handle_text_start()`][pydantic_ai.ui.UIEventStream.handle_text_start]\n        - [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] -> [`handle_thinking_start()`][pydantic_ai.ui.UIEventStream.handle_thinking_start]\n        - [`ToolCallPart`][pydantic_ai.messages.ToolCallPart] -> [`handle_tool_call_start()`][pydantic_ai.ui.UIEventStream.handle_tool_call_start]\n        - [`BuiltinToolCallPart`][pydantic_ai.messages.BuiltinToolCallPart] -> [`handle_builtin_tool_call_start()`][pydantic_ai.ui.UIEventStream.handle_builtin_tool_call_start]\n        - [`BuiltinToolReturnPart`][pydantic_ai.messages.BuiltinToolReturnPart] -> [`handle_builtin_tool_return()`][pydantic_ai.ui.UIEventStream.handle_builtin_tool_return]\n        - [`FilePart`][pydantic_ai.messages.FilePart] -> [`handle_file()`][pydantic_ai.ui.UIEventStream.handle_file]\n\n        Subclasses are encouraged to override the individual `handle_*` methods rather than this one.\n        If you need specific behavior for all part start events, make sure you call the super method.\n\n        Args:\n            event: The part start event.\n        \"\"\"\n        part = event.part\n        previous_part_kind = event.previous_part_kind\n        match part:\n            case TextPart():\n                async for e in self.handle_text_start(part, follows_text=previous_part_kind == 'text'):\n                    yield e\n            case ThinkingPart():\n                async for e in self.handle_thinking_start(part, follows_thinking=previous_part_kind == 'thinking'):\n                    yield e\n            case ToolCallPart():\n                async for e in self.handle_tool_call_start(part):\n                    yield e\n            case BuiltinToolCallPart():\n                async for e in self.handle_builtin_tool_call_start(part):\n                    yield e\n            case BuiltinToolReturnPart():\n                async for e in self.handle_builtin_tool_return(part):\n                    yield e\n            case FilePart():  # pragma: no branch\n                async for e in self.handle_file(part):\n                    yield e\n\n    async def handle_part_delta(self, event: PartDeltaEvent) -> AsyncIterator[EventT]:\n        \"\"\"Handle a PartDeltaEvent.\n\n        This method dispatches to specific `handle_*_delta` methods based on part delta type:\n\n        - [`TextPartDelta`][pydantic_ai.messages.TextPartDelta] -> [`handle_text_delta()`][pydantic_ai.ui.UIEventStream.handle_text_delta]\n        - [`ThinkingPartDelta`][pydantic_ai.messages.ThinkingPartDelta] -> [`handle_thinking_delta()`][pydantic_ai.ui.UIEventStream.handle_thinking_delta]\n        - [`ToolCallPartDelta`][pydantic_ai.messages.ToolCallPartDelta] -> [`handle_tool_call_delta()`][pydantic_ai.ui.UIEventStream.handle_tool_call_delta]\n\n        Subclasses are encouraged to override the individual `handle_*_delta` methods rather than this one.\n        If you need specific behavior for all part delta events, make sure you call the super method.\n\n        Args:\n            event: The PartDeltaEvent.\n        \"\"\"\n        delta = event.delta\n        match delta:\n            case TextPartDelta():\n                async for e in self.handle_text_delta(delta):\n                    yield e\n            case ThinkingPartDelta():\n                async for e in self.handle_thinking_delta(delta):\n                    yield e\n            case ToolCallPartDelta():  # pragma: no branch\n                async for e in self.handle_tool_call_delta(delta):\n                    yield e\n\n    async def handle_part_end(self, event: PartEndEvent) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `PartEndEvent`.\n\n        This method dispatches to specific `handle_*_end` methods based on part type:\n\n        - [`TextPart`][pydantic_ai.messages.TextPart] -> [`handle_text_end()`][pydantic_ai.ui.UIEventStream.handle_text_end]\n        - [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] -> [`handle_thinking_end()`][pydantic_ai.ui.UIEventStream.handle_thinking_end]\n        - [`ToolCallPart`][pydantic_ai.messages.ToolCallPart] -> [`handle_tool_call_end()`][pydantic_ai.ui.UIEventStream.handle_tool_call_end]\n        - [`BuiltinToolCallPart`][pydantic_ai.messages.BuiltinToolCallPart] -> [`handle_builtin_tool_call_end()`][pydantic_ai.ui.UIEventStream.handle_builtin_tool_call_end]\n\n        Subclasses are encouraged to override the individual `handle_*_end` methods rather than this one.\n        If you need specific behavior for all part end events, make sure you call the super method.\n\n        Args:\n            event: The part end event.\n        \"\"\"\n        part = event.part\n        next_part_kind = event.next_part_kind\n        match part:\n            case TextPart():\n                async for e in self.handle_text_end(part, followed_by_text=next_part_kind == 'text'):\n                    yield e\n            case ThinkingPart():\n                async for e in self.handle_thinking_end(part, followed_by_thinking=next_part_kind == 'thinking'):\n                    yield e\n            case ToolCallPart():\n                async for e in self.handle_tool_call_end(part):\n                    yield e\n            case BuiltinToolCallPart():\n                async for e in self.handle_builtin_tool_call_end(part):\n                    yield e\n            case BuiltinToolReturnPart() | FilePart():  # pragma: no cover\n                # These don't have deltas, so they don't need to be ended.\n                pass\n\n    async def before_stream(self) -> AsyncIterator[EventT]:\n        \"\"\"Yield events before agent streaming starts.\n\n        This hook is called before any agent events are processed.\n        Override this to inject custom events at the start of the stream.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def after_stream(self) -> AsyncIterator[EventT]:\n        \"\"\"Yield events after agent streaming completes.\n\n        This hook is called after all agent events have been processed.\n        Override this to inject custom events at the end of the stream.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def on_error(self, error: Exception) -> AsyncIterator[EventT]:\n        \"\"\"Handle errors that occur during streaming.\n\n        Args:\n            error: The error that occurred during streaming.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def before_request(self) -> AsyncIterator[EventT]:\n        \"\"\"Yield events before a model request is processed.\n\n        Override this to inject custom events at the start of the request.\n        \"\"\"\n        return  # pragma: lax no cover\n        yield  # Make this an async generator\n\n    async def after_request(self) -> AsyncIterator[EventT]:\n        \"\"\"Yield events after a model request is processed.\n\n        Override this to inject custom events at the end of the request.\n        \"\"\"\n        return  # pragma: lax no cover\n        yield  # Make this an async generator\n\n    async def before_response(self) -> AsyncIterator[EventT]:\n        \"\"\"Yield events before a model response is processed.\n\n        Override this to inject custom events at the start of the response.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def after_response(self) -> AsyncIterator[EventT]:\n        \"\"\"Yield events after a model response is processed.\n\n        Override this to inject custom events at the end of the response.\n        \"\"\"\n        return  # pragma: lax no cover\n        yield  # Make this an async generator\n\n    async def handle_text_start(self, part: TextPart, follows_text: bool = False) -> AsyncIterator[EventT]:\n        \"\"\"Handle the start of a `TextPart`.\n\n        Args:\n            part: The text part.\n            follows_text: Whether the part is directly preceded by another text part. In this case, you may want to yield a \"text-delta\" event instead of a \"text-start\" event.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `TextPartDelta`.\n\n        Args:\n            delta: The text part delta.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_text_end(self, part: TextPart, followed_by_text: bool = False) -> AsyncIterator[EventT]:\n        \"\"\"Handle the end of a `TextPart`.\n\n        Args:\n            part: The text part.\n            followed_by_text: Whether the part is directly followed by another text part. In this case, you may not want to yield a \"text-end\" event yet.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_thinking_start(self, part: ThinkingPart, follows_thinking: bool = False) -> AsyncIterator[EventT]:\n        \"\"\"Handle the start of a `ThinkingPart`.\n\n        Args:\n            part: The thinking part.\n            follows_thinking: Whether the part is directly preceded by another thinking part. In this case, you may want to yield a \"thinking-delta\" event instead of a \"thinking-start\" event.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_thinking_delta(self, delta: ThinkingPartDelta) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `ThinkingPartDelta`.\n\n        Args:\n            delta: The thinking part delta.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_thinking_end(\n        self, part: ThinkingPart, followed_by_thinking: bool = False\n    ) -> AsyncIterator[EventT]:\n        \"\"\"Handle the end of a `ThinkingPart`.\n\n        Args:\n            part: The thinking part.\n            followed_by_thinking: Whether the part is directly followed by another thinking part. In this case, you may not want to yield a \"thinking-end\" event yet.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_tool_call_start(self, part: ToolCallPart) -> AsyncIterator[EventT]:\n        \"\"\"Handle the start of a `ToolCallPart`.\n\n        Args:\n            part: The tool call part.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_tool_call_delta(self, delta: ToolCallPartDelta) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `ToolCallPartDelta`.\n\n        Args:\n            delta: The tool call part delta.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_tool_call_end(self, part: ToolCallPart) -> AsyncIterator[EventT]:\n        \"\"\"Handle the end of a `ToolCallPart`.\n\n        Args:\n            part: The tool call part.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_builtin_tool_call_start(self, part: BuiltinToolCallPart) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `BuiltinToolCallPart` at start.\n\n        Args:\n            part: The builtin tool call part.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_builtin_tool_call_end(self, part: BuiltinToolCallPart) -> AsyncIterator[EventT]:\n        \"\"\"Handle the end of a `BuiltinToolCallPart`.\n\n        Args:\n            part: The builtin tool call part.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_builtin_tool_return(self, part: BuiltinToolReturnPart) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `BuiltinToolReturnPart`.\n\n        Args:\n            part: The builtin tool return part.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_file(self, part: FilePart) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `FilePart`.\n\n        Args:\n            part: The file part.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_final_result(self, event: FinalResultEvent) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `FinalResultEvent`.\n\n        Args:\n            event: The final result event.\n        \"\"\"\n        return\n        yield  # Make this an async generator\n\n    async def handle_function_tool_call(self, event: FunctionToolCallEvent) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `FunctionToolCallEvent`.\n\n        Args:\n            event: The function tool call event.\n        \"\"\"\n        return\n        yield  # Make this an async generator\n\n    async def handle_function_tool_result(self, event: FunctionToolResultEvent) -> AsyncIterator[EventT]:\n        \"\"\"Handle a `FunctionToolResultEvent`.\n\n        Args:\n            event: The function tool result event.\n        \"\"\"\n        return  # pragma: no cover\n        yield  # Make this an async generator\n\n    async def handle_run_result(self, event: AgentRunResultEvent) -> AsyncIterator[EventT]:\n        \"\"\"Handle an `AgentRunResultEvent`.\n\n        Args:\n            event: The agent run result event.\n        \"\"\"\n        return\n        yield  # Make this an async generator\n\n```\n\n#### accept\n\n```python\naccept: str | None = None\n\n```\n\nThe `Accept` header value of the request, used to determine how to encode the protocol-specific events for the streaming response.\n\n#### message_id\n\n```python\nmessage_id: str = field(\n    default_factory=lambda: str(uuid4())\n)\n\n```\n\nThe message ID to use for the next event.\n\n#### new_message_id\n\n```python\nnew_message_id() -> str\n\n```\n\nGenerate and store a new message ID.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\ndef new_message_id(self) -> str:\n    \"\"\"Generate and store a new message ID.\"\"\"\n    self.message_id = str(uuid4())\n    return self.message_id\n\n```\n\n#### response_headers\n\n```python\nresponse_headers: Mapping[str, str] | None\n\n```\n\nResponse headers to return to the frontend.\n\n#### content_type\n\n```python\ncontent_type: str\n\n```\n\nGet the content type for the event stream, compatible with the `Accept` header value.\n\nBy default, this returns the Server-Sent Events content type (`text/event-stream`). If a subclass supports other types as well, it should consider `self.accept` in encode_event() and return the resulting content type.\n\n#### encode_event\n\n```python\nencode_event(event: EventT) -> str\n\n```\n\nEncode a protocol-specific event as a string.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\n@abstractmethod\ndef encode_event(self, event: EventT) -> str:\n    \"\"\"Encode a protocol-specific event as a string.\"\"\"\n    raise NotImplementedError\n\n```\n\n#### encode_stream\n\n```python\nencode_stream(\n    stream: AsyncIterator[EventT],\n) -> AsyncIterator[str]\n\n```\n\nEncode a stream of protocol-specific events as strings according to the `Accept` header value.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def encode_stream(self, stream: AsyncIterator[EventT]) -> AsyncIterator[str]:\n    \"\"\"Encode a stream of protocol-specific events as strings according to the `Accept` header value.\"\"\"\n    async for event in stream:\n        yield self.encode_event(event)\n\n```\n\n#### streaming_response\n\n```python\nstreaming_response(\n    stream: AsyncIterator[EventT],\n) -> StreamingResponse\n\n```\n\nGenerate a streaming response from a stream of protocol-specific events.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\ndef streaming_response(self, stream: AsyncIterator[EventT]) -> StreamingResponse:\n    \"\"\"Generate a streaming response from a stream of protocol-specific events.\"\"\"\n    try:\n        from starlette.responses import StreamingResponse\n    except ImportError as e:  # pragma: no cover\n        raise ImportError(\n            'Please install the `starlette` package to use the `streaming_response()` method, '\n            'you can use the `ui` optional group — `pip install \"pydantic-ai-slim[ui]\"`'\n        ) from e\n\n    return StreamingResponse(\n        self.encode_stream(stream),\n        headers=self.response_headers,\n        media_type=self.content_type,\n    )\n\n```\n\n#### transform_stream\n\n```python\ntransform_stream(\n    stream: AsyncIterator[NativeEvent],\n    on_complete: OnCompleteFunc[EventT] | None = None,\n) -> AsyncIterator[EventT]\n\n```\n\nTransform a stream of Pydantic AI events into protocol-specific events.\n\nThis method dispatches to specific hooks and `handle_*` methods that subclasses can override:\n\n- before_stream()\n- after_stream()\n- on_error()\n- before_request()\n- after_request()\n- before_response()\n- after_response()\n- handle_event()\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `stream` | `AsyncIterator[NativeEvent]` | The stream of Pydantic AI events to transform. | *required* | | `on_complete` | `OnCompleteFunc[EventT] | None` | Optional callback function called when the agent run completes successfully. The callback receives the completed AgentRunResult and can optionally yield additional protocol-specific events. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def transform_stream(  # noqa: C901\n    self, stream: AsyncIterator[NativeEvent], on_complete: OnCompleteFunc[EventT] | None = None\n) -> AsyncIterator[EventT]:\n    \"\"\"Transform a stream of Pydantic AI events into protocol-specific events.\n\n    This method dispatches to specific hooks and `handle_*` methods that subclasses can override:\n    - [`before_stream()`][pydantic_ai.ui.UIEventStream.before_stream]\n    - [`after_stream()`][pydantic_ai.ui.UIEventStream.after_stream]\n    - [`on_error()`][pydantic_ai.ui.UIEventStream.on_error]\n    - [`before_request()`][pydantic_ai.ui.UIEventStream.before_request]\n    - [`after_request()`][pydantic_ai.ui.UIEventStream.after_request]\n    - [`before_response()`][pydantic_ai.ui.UIEventStream.before_response]\n    - [`after_response()`][pydantic_ai.ui.UIEventStream.after_response]\n    - [`handle_event()`][pydantic_ai.ui.UIEventStream.handle_event]\n\n    Args:\n        stream: The stream of Pydantic AI events to transform.\n        on_complete: Optional callback function called when the agent run completes successfully.\n            The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n    \"\"\"\n    async for e in self.before_stream():\n        yield e\n\n    try:\n        async for event in stream:\n            if isinstance(event, PartStartEvent):\n                async for e in self._turn_to('response'):\n                    yield e\n            elif isinstance(event, FunctionToolCallEvent):\n                async for e in self._turn_to('request'):\n                    yield e\n            elif isinstance(event, AgentRunResultEvent):\n                if (\n                    self._final_result_event\n                    and (tool_call_id := self._final_result_event.tool_call_id)\n                    and (tool_name := self._final_result_event.tool_name)\n                ):\n                    async for e in self._turn_to('request'):\n                        yield e\n\n                    self._final_result_event = None\n                    # Ensure the stream does not end on a dangling tool call without a result.\n                    output_tool_result_event = FunctionToolResultEvent(\n                        result=ToolReturnPart(\n                            tool_call_id=tool_call_id,\n                            tool_name=tool_name,\n                            content='Final result processed.',\n                        )\n                    )\n                    async for e in self.handle_function_tool_result(output_tool_result_event):\n                        yield e\n\n                result = cast(AgentRunResult[OutputDataT], event.result)\n                self._result = result\n\n                async for e in self._turn_to(None):\n                    yield e\n\n                if on_complete is not None:\n                    if inspect.isasyncgenfunction(on_complete):\n                        async for e in on_complete(result):\n                            yield e\n                    elif _utils.is_async_callable(on_complete):\n                        await on_complete(result)\n                    else:\n                        await _utils.run_in_executor(on_complete, result)\n            elif isinstance(event, FinalResultEvent):\n                self._final_result_event = event\n\n            if isinstance(event, BuiltinToolCallEvent | BuiltinToolResultEvent):  # pyright: ignore[reportDeprecated]\n                # These events were deprecated before this feature was introduced\n                continue\n\n            async for e in self.handle_event(event):\n                yield e\n    except Exception as e:\n        async for e in self.on_error(e):\n            yield e\n    finally:\n        async for e in self._turn_to(None):\n            yield e\n\n        async for e in self.after_stream():\n            yield e\n\n```\n\n#### handle_event\n\n```python\nhandle_event(event: NativeEvent) -> AsyncIterator[EventT]\n\n```\n\nTransform a Pydantic AI event into one or more protocol-specific events.\n\nThis method dispatches to specific `handle_*` methods based on event type:\n\n- PartStartEvent -> handle_part_start()\n- PartDeltaEvent -> `handle_part_delta`\n- PartEndEvent -> `handle_part_end`\n- FinalResultEvent -> `handle_final_result`\n- FunctionToolCallEvent -> `handle_function_tool_call`\n- FunctionToolResultEvent -> `handle_function_tool_result`\n- AgentRunResultEvent -> `handle_run_result`\n\nSubclasses are encouraged to override the individual `handle_*` methods rather than this one. If you need specific behavior for all events, make sure you call the super method.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_event(self, event: NativeEvent) -> AsyncIterator[EventT]:\n    \"\"\"Transform a Pydantic AI event into one or more protocol-specific events.\n\n    This method dispatches to specific `handle_*` methods based on event type:\n\n    - [`PartStartEvent`][pydantic_ai.messages.PartStartEvent] -> [`handle_part_start()`][pydantic_ai.ui.UIEventStream.handle_part_start]\n    - [`PartDeltaEvent`][pydantic_ai.messages.PartDeltaEvent] -> `handle_part_delta`\n    - [`PartEndEvent`][pydantic_ai.messages.PartEndEvent] -> `handle_part_end`\n    - [`FinalResultEvent`][pydantic_ai.messages.FinalResultEvent] -> `handle_final_result`\n    - [`FunctionToolCallEvent`][pydantic_ai.messages.FunctionToolCallEvent] -> `handle_function_tool_call`\n    - [`FunctionToolResultEvent`][pydantic_ai.messages.FunctionToolResultEvent] -> `handle_function_tool_result`\n    - [`AgentRunResultEvent`][pydantic_ai.run.AgentRunResultEvent] -> `handle_run_result`\n\n    Subclasses are encouraged to override the individual `handle_*` methods rather than this one.\n    If you need specific behavior for all events, make sure you call the super method.\n    \"\"\"\n    match event:\n        case PartStartEvent():\n            async for e in self.handle_part_start(event):\n                yield e\n        case PartDeltaEvent():\n            async for e in self.handle_part_delta(event):\n                yield e\n        case PartEndEvent():\n            async for e in self.handle_part_end(event):\n                yield e\n        case FinalResultEvent():\n            async for e in self.handle_final_result(event):\n                yield e\n        case FunctionToolCallEvent():\n            async for e in self.handle_function_tool_call(event):\n                yield e\n        case FunctionToolResultEvent():\n            async for e in self.handle_function_tool_result(event):\n                yield e\n        case AgentRunResultEvent():\n            async for e in self.handle_run_result(event):\n                yield e\n        case _:\n            pass\n\n```\n\n#### handle_part_start\n\n```python\nhandle_part_start(\n    event: PartStartEvent,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `PartStartEvent`.\n\nThis method dispatches to specific `handle_*` methods based on part type:\n\n- TextPart -> handle_text_start()\n- ThinkingPart -> handle_thinking_start()\n- ToolCallPart -> handle_tool_call_start()\n- BuiltinToolCallPart -> handle_builtin_tool_call_start()\n- BuiltinToolReturnPart -> handle_builtin_tool_return()\n- FilePart -> handle_file()\n\nSubclasses are encouraged to override the individual `handle_*` methods rather than this one. If you need specific behavior for all part start events, make sure you call the super method.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `event` | `PartStartEvent` | The part start event. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_part_start(self, event: PartStartEvent) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `PartStartEvent`.\n\n    This method dispatches to specific `handle_*` methods based on part type:\n\n    - [`TextPart`][pydantic_ai.messages.TextPart] -> [`handle_text_start()`][pydantic_ai.ui.UIEventStream.handle_text_start]\n    - [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] -> [`handle_thinking_start()`][pydantic_ai.ui.UIEventStream.handle_thinking_start]\n    - [`ToolCallPart`][pydantic_ai.messages.ToolCallPart] -> [`handle_tool_call_start()`][pydantic_ai.ui.UIEventStream.handle_tool_call_start]\n    - [`BuiltinToolCallPart`][pydantic_ai.messages.BuiltinToolCallPart] -> [`handle_builtin_tool_call_start()`][pydantic_ai.ui.UIEventStream.handle_builtin_tool_call_start]\n    - [`BuiltinToolReturnPart`][pydantic_ai.messages.BuiltinToolReturnPart] -> [`handle_builtin_tool_return()`][pydantic_ai.ui.UIEventStream.handle_builtin_tool_return]\n    - [`FilePart`][pydantic_ai.messages.FilePart] -> [`handle_file()`][pydantic_ai.ui.UIEventStream.handle_file]\n\n    Subclasses are encouraged to override the individual `handle_*` methods rather than this one.\n    If you need specific behavior for all part start events, make sure you call the super method.\n\n    Args:\n        event: The part start event.\n    \"\"\"\n    part = event.part\n    previous_part_kind = event.previous_part_kind\n    match part:\n        case TextPart():\n            async for e in self.handle_text_start(part, follows_text=previous_part_kind == 'text'):\n                yield e\n        case ThinkingPart():\n            async for e in self.handle_thinking_start(part, follows_thinking=previous_part_kind == 'thinking'):\n                yield e\n        case ToolCallPart():\n            async for e in self.handle_tool_call_start(part):\n                yield e\n        case BuiltinToolCallPart():\n            async for e in self.handle_builtin_tool_call_start(part):\n                yield e\n        case BuiltinToolReturnPart():\n            async for e in self.handle_builtin_tool_return(part):\n                yield e\n        case FilePart():  # pragma: no branch\n            async for e in self.handle_file(part):\n                yield e\n\n```\n\n#### handle_part_delta\n\n```python\nhandle_part_delta(\n    event: PartDeltaEvent,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a PartDeltaEvent.\n\nThis method dispatches to specific `handle_*_delta` methods based on part delta type:\n\n- TextPartDelta -> handle_text_delta()\n- ThinkingPartDelta -> handle_thinking_delta()\n- ToolCallPartDelta -> handle_tool_call_delta()\n\nSubclasses are encouraged to override the individual `handle_*_delta` methods rather than this one. If you need specific behavior for all part delta events, make sure you call the super method.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `event` | `PartDeltaEvent` | The PartDeltaEvent. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_part_delta(self, event: PartDeltaEvent) -> AsyncIterator[EventT]:\n    \"\"\"Handle a PartDeltaEvent.\n\n    This method dispatches to specific `handle_*_delta` methods based on part delta type:\n\n    - [`TextPartDelta`][pydantic_ai.messages.TextPartDelta] -> [`handle_text_delta()`][pydantic_ai.ui.UIEventStream.handle_text_delta]\n    - [`ThinkingPartDelta`][pydantic_ai.messages.ThinkingPartDelta] -> [`handle_thinking_delta()`][pydantic_ai.ui.UIEventStream.handle_thinking_delta]\n    - [`ToolCallPartDelta`][pydantic_ai.messages.ToolCallPartDelta] -> [`handle_tool_call_delta()`][pydantic_ai.ui.UIEventStream.handle_tool_call_delta]\n\n    Subclasses are encouraged to override the individual `handle_*_delta` methods rather than this one.\n    If you need specific behavior for all part delta events, make sure you call the super method.\n\n    Args:\n        event: The PartDeltaEvent.\n    \"\"\"\n    delta = event.delta\n    match delta:\n        case TextPartDelta():\n            async for e in self.handle_text_delta(delta):\n                yield e\n        case ThinkingPartDelta():\n            async for e in self.handle_thinking_delta(delta):\n                yield e\n        case ToolCallPartDelta():  # pragma: no branch\n            async for e in self.handle_tool_call_delta(delta):\n                yield e\n\n```\n\n#### handle_part_end\n\n```python\nhandle_part_end(\n    event: PartEndEvent,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `PartEndEvent`.\n\nThis method dispatches to specific `handle_*_end` methods based on part type:\n\n- TextPart -> handle_text_end()\n- ThinkingPart -> handle_thinking_end()\n- ToolCallPart -> handle_tool_call_end()\n- BuiltinToolCallPart -> handle_builtin_tool_call_end()\n\nSubclasses are encouraged to override the individual `handle_*_end` methods rather than this one. If you need specific behavior for all part end events, make sure you call the super method.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `event` | `PartEndEvent` | The part end event. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_part_end(self, event: PartEndEvent) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `PartEndEvent`.\n\n    This method dispatches to specific `handle_*_end` methods based on part type:\n\n    - [`TextPart`][pydantic_ai.messages.TextPart] -> [`handle_text_end()`][pydantic_ai.ui.UIEventStream.handle_text_end]\n    - [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] -> [`handle_thinking_end()`][pydantic_ai.ui.UIEventStream.handle_thinking_end]\n    - [`ToolCallPart`][pydantic_ai.messages.ToolCallPart] -> [`handle_tool_call_end()`][pydantic_ai.ui.UIEventStream.handle_tool_call_end]\n    - [`BuiltinToolCallPart`][pydantic_ai.messages.BuiltinToolCallPart] -> [`handle_builtin_tool_call_end()`][pydantic_ai.ui.UIEventStream.handle_builtin_tool_call_end]\n\n    Subclasses are encouraged to override the individual `handle_*_end` methods rather than this one.\n    If you need specific behavior for all part end events, make sure you call the super method.\n\n    Args:\n        event: The part end event.\n    \"\"\"\n    part = event.part\n    next_part_kind = event.next_part_kind\n    match part:\n        case TextPart():\n            async for e in self.handle_text_end(part, followed_by_text=next_part_kind == 'text'):\n                yield e\n        case ThinkingPart():\n            async for e in self.handle_thinking_end(part, followed_by_thinking=next_part_kind == 'thinking'):\n                yield e\n        case ToolCallPart():\n            async for e in self.handle_tool_call_end(part):\n                yield e\n        case BuiltinToolCallPart():\n            async for e in self.handle_builtin_tool_call_end(part):\n                yield e\n        case BuiltinToolReturnPart() | FilePart():  # pragma: no cover\n            # These don't have deltas, so they don't need to be ended.\n            pass\n\n```\n\n#### before_stream\n\n```python\nbefore_stream() -> AsyncIterator[EventT]\n\n```\n\nYield events before agent streaming starts.\n\nThis hook is called before any agent events are processed. Override this to inject custom events at the start of the stream.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def before_stream(self) -> AsyncIterator[EventT]:\n    \"\"\"Yield events before agent streaming starts.\n\n    This hook is called before any agent events are processed.\n    Override this to inject custom events at the start of the stream.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### after_stream\n\n```python\nafter_stream() -> AsyncIterator[EventT]\n\n```\n\nYield events after agent streaming completes.\n\nThis hook is called after all agent events have been processed. Override this to inject custom events at the end of the stream.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def after_stream(self) -> AsyncIterator[EventT]:\n    \"\"\"Yield events after agent streaming completes.\n\n    This hook is called after all agent events have been processed.\n    Override this to inject custom events at the end of the stream.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### on_error\n\n```python\non_error(error: Exception) -> AsyncIterator[EventT]\n\n```\n\nHandle errors that occur during streaming.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `error` | `Exception` | The error that occurred during streaming. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def on_error(self, error: Exception) -> AsyncIterator[EventT]:\n    \"\"\"Handle errors that occur during streaming.\n\n    Args:\n        error: The error that occurred during streaming.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### before_request\n\n```python\nbefore_request() -> AsyncIterator[EventT]\n\n```\n\nYield events before a model request is processed.\n\nOverride this to inject custom events at the start of the request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def before_request(self) -> AsyncIterator[EventT]:\n    \"\"\"Yield events before a model request is processed.\n\n    Override this to inject custom events at the start of the request.\n    \"\"\"\n    return  # pragma: lax no cover\n    yield  # Make this an async generator\n\n```\n\n#### after_request\n\n```python\nafter_request() -> AsyncIterator[EventT]\n\n```\n\nYield events after a model request is processed.\n\nOverride this to inject custom events at the end of the request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def after_request(self) -> AsyncIterator[EventT]:\n    \"\"\"Yield events after a model request is processed.\n\n    Override this to inject custom events at the end of the request.\n    \"\"\"\n    return  # pragma: lax no cover\n    yield  # Make this an async generator\n\n```\n\n#### before_response\n\n```python\nbefore_response() -> AsyncIterator[EventT]\n\n```\n\nYield events before a model response is processed.\n\nOverride this to inject custom events at the start of the response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def before_response(self) -> AsyncIterator[EventT]:\n    \"\"\"Yield events before a model response is processed.\n\n    Override this to inject custom events at the start of the response.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### after_response\n\n```python\nafter_response() -> AsyncIterator[EventT]\n\n```\n\nYield events after a model response is processed.\n\nOverride this to inject custom events at the end of the response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def after_response(self) -> AsyncIterator[EventT]:\n    \"\"\"Yield events after a model response is processed.\n\n    Override this to inject custom events at the end of the response.\n    \"\"\"\n    return  # pragma: lax no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_text_start\n\n```python\nhandle_text_start(\n    part: TextPart, follows_text: bool = False\n) -> AsyncIterator[EventT]\n\n```\n\nHandle the start of a `TextPart`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `TextPart` | The text part. | *required* | | `follows_text` | `bool` | Whether the part is directly preceded by another text part. In this case, you may want to yield a \"text-delta\" event instead of a \"text-start\" event. | `False` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_text_start(self, part: TextPart, follows_text: bool = False) -> AsyncIterator[EventT]:\n    \"\"\"Handle the start of a `TextPart`.\n\n    Args:\n        part: The text part.\n        follows_text: Whether the part is directly preceded by another text part. In this case, you may want to yield a \"text-delta\" event instead of a \"text-start\" event.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_text_delta\n\n```python\nhandle_text_delta(\n    delta: TextPartDelta,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `TextPartDelta`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `delta` | `TextPartDelta` | The text part delta. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `TextPartDelta`.\n\n    Args:\n        delta: The text part delta.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_text_end\n\n```python\nhandle_text_end(\n    part: TextPart, followed_by_text: bool = False\n) -> AsyncIterator[EventT]\n\n```\n\nHandle the end of a `TextPart`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `TextPart` | The text part. | *required* | | `followed_by_text` | `bool` | Whether the part is directly followed by another text part. In this case, you may not want to yield a \"text-end\" event yet. | `False` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_text_end(self, part: TextPart, followed_by_text: bool = False) -> AsyncIterator[EventT]:\n    \"\"\"Handle the end of a `TextPart`.\n\n    Args:\n        part: The text part.\n        followed_by_text: Whether the part is directly followed by another text part. In this case, you may not want to yield a \"text-end\" event yet.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_thinking_start\n\n```python\nhandle_thinking_start(\n    part: ThinkingPart, follows_thinking: bool = False\n) -> AsyncIterator[EventT]\n\n```\n\nHandle the start of a `ThinkingPart`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `ThinkingPart` | The thinking part. | *required* | | `follows_thinking` | `bool` | Whether the part is directly preceded by another thinking part. In this case, you may want to yield a \"thinking-delta\" event instead of a \"thinking-start\" event. | `False` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_thinking_start(self, part: ThinkingPart, follows_thinking: bool = False) -> AsyncIterator[EventT]:\n    \"\"\"Handle the start of a `ThinkingPart`.\n\n    Args:\n        part: The thinking part.\n        follows_thinking: Whether the part is directly preceded by another thinking part. In this case, you may want to yield a \"thinking-delta\" event instead of a \"thinking-start\" event.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_thinking_delta\n\n```python\nhandle_thinking_delta(\n    delta: ThinkingPartDelta,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `ThinkingPartDelta`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `delta` | `ThinkingPartDelta` | The thinking part delta. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_thinking_delta(self, delta: ThinkingPartDelta) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `ThinkingPartDelta`.\n\n    Args:\n        delta: The thinking part delta.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_thinking_end\n\n```python\nhandle_thinking_end(\n    part: ThinkingPart, followed_by_thinking: bool = False\n) -> AsyncIterator[EventT]\n\n```\n\nHandle the end of a `ThinkingPart`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `ThinkingPart` | The thinking part. | *required* | | `followed_by_thinking` | `bool` | Whether the part is directly followed by another thinking part. In this case, you may not want to yield a \"thinking-end\" event yet. | `False` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_thinking_end(\n    self, part: ThinkingPart, followed_by_thinking: bool = False\n) -> AsyncIterator[EventT]:\n    \"\"\"Handle the end of a `ThinkingPart`.\n\n    Args:\n        part: The thinking part.\n        followed_by_thinking: Whether the part is directly followed by another thinking part. In this case, you may not want to yield a \"thinking-end\" event yet.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_tool_call_start\n\n```python\nhandle_tool_call_start(\n    part: ToolCallPart,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle the start of a `ToolCallPart`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `ToolCallPart` | The tool call part. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_tool_call_start(self, part: ToolCallPart) -> AsyncIterator[EventT]:\n    \"\"\"Handle the start of a `ToolCallPart`.\n\n    Args:\n        part: The tool call part.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_tool_call_delta\n\n```python\nhandle_tool_call_delta(\n    delta: ToolCallPartDelta,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `ToolCallPartDelta`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `delta` | `ToolCallPartDelta` | The tool call part delta. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_tool_call_delta(self, delta: ToolCallPartDelta) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `ToolCallPartDelta`.\n\n    Args:\n        delta: The tool call part delta.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_tool_call_end\n\n```python\nhandle_tool_call_end(\n    part: ToolCallPart,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle the end of a `ToolCallPart`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `ToolCallPart` | The tool call part. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_tool_call_end(self, part: ToolCallPart) -> AsyncIterator[EventT]:\n    \"\"\"Handle the end of a `ToolCallPart`.\n\n    Args:\n        part: The tool call part.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_builtin_tool_call_start\n\n```python\nhandle_builtin_tool_call_start(\n    part: BuiltinToolCallPart,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `BuiltinToolCallPart` at start.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `BuiltinToolCallPart` | The builtin tool call part. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_builtin_tool_call_start(self, part: BuiltinToolCallPart) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `BuiltinToolCallPart` at start.\n\n    Args:\n        part: The builtin tool call part.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_builtin_tool_call_end\n\n```python\nhandle_builtin_tool_call_end(\n    part: BuiltinToolCallPart,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle the end of a `BuiltinToolCallPart`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `BuiltinToolCallPart` | The builtin tool call part. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_builtin_tool_call_end(self, part: BuiltinToolCallPart) -> AsyncIterator[EventT]:\n    \"\"\"Handle the end of a `BuiltinToolCallPart`.\n\n    Args:\n        part: The builtin tool call part.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_builtin_tool_return\n\n```python\nhandle_builtin_tool_return(\n    part: BuiltinToolReturnPart,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `BuiltinToolReturnPart`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `BuiltinToolReturnPart` | The builtin tool return part. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_builtin_tool_return(self, part: BuiltinToolReturnPart) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `BuiltinToolReturnPart`.\n\n    Args:\n        part: The builtin tool return part.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_file\n\n```python\nhandle_file(part: FilePart) -> AsyncIterator[EventT]\n\n```\n\nHandle a `FilePart`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `part` | `FilePart` | The file part. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_file(self, part: FilePart) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `FilePart`.\n\n    Args:\n        part: The file part.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_final_result\n\n```python\nhandle_final_result(\n    event: FinalResultEvent,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `FinalResultEvent`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `event` | `FinalResultEvent` | The final result event. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_final_result(self, event: FinalResultEvent) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `FinalResultEvent`.\n\n    Args:\n        event: The final result event.\n    \"\"\"\n    return\n    yield  # Make this an async generator\n\n```\n\n#### handle_function_tool_call\n\n```python\nhandle_function_tool_call(\n    event: FunctionToolCallEvent,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `FunctionToolCallEvent`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `event` | `FunctionToolCallEvent` | The function tool call event. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_function_tool_call(self, event: FunctionToolCallEvent) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `FunctionToolCallEvent`.\n\n    Args:\n        event: The function tool call event.\n    \"\"\"\n    return\n    yield  # Make this an async generator\n\n```\n\n#### handle_function_tool_result\n\n```python\nhandle_function_tool_result(\n    event: FunctionToolResultEvent,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle a `FunctionToolResultEvent`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `event` | `FunctionToolResultEvent` | The function tool result event. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_function_tool_result(self, event: FunctionToolResultEvent) -> AsyncIterator[EventT]:\n    \"\"\"Handle a `FunctionToolResultEvent`.\n\n    Args:\n        event: The function tool result event.\n    \"\"\"\n    return  # pragma: no cover\n    yield  # Make this an async generator\n\n```\n\n#### handle_run_result\n\n```python\nhandle_run_result(\n    event: AgentRunResultEvent,\n) -> AsyncIterator[EventT]\n\n```\n\nHandle an `AgentRunResultEvent`.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `event` | `AgentRunResultEvent` | The agent run result event. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_event_stream.py`\n\n```python\nasync def handle_run_result(self, event: AgentRunResultEvent) -> AsyncIterator[EventT]:\n    \"\"\"Handle an `AgentRunResultEvent`.\n\n    Args:\n        event: The agent run result event.\n    \"\"\"\n    return\n    yield  # Make this an async generator\n\n```\n\n### MessagesBuilder\n\nHelper class to build Pydantic AI messages from request/response parts.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_messages_builder.py`\n\n```python\n@dataclass\nclass MessagesBuilder:\n    \"\"\"Helper class to build Pydantic AI messages from request/response parts.\"\"\"\n\n    messages: list[ModelMessage] = field(default_factory=list)\n\n    def add(self, part: ModelRequestPart | ModelResponsePart) -> None:\n        \"\"\"Add a new part, creating a new request or response message if necessary.\"\"\"\n        last_message = self.messages[-1] if self.messages else None\n        if isinstance(part, get_union_args(ModelRequestPart)):\n            part = cast(ModelRequestPart, part)\n            if isinstance(last_message, ModelRequest):\n                last_message.parts = [*last_message.parts, part]\n            else:\n                self.messages.append(ModelRequest(parts=[part]))\n        else:\n            part = cast(ModelResponsePart, part)\n            if isinstance(last_message, ModelResponse):\n                last_message.parts = [*last_message.parts, part]\n            else:\n                self.messages.append(ModelResponse(parts=[part]))\n\n```\n\n#### add\n\n```python\nadd(part: ModelRequestPart | ModelResponsePart) -> None\n\n```\n\nAdd a new part, creating a new request or response message if necessary.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ui/_messages_builder.py`\n\n```python\ndef add(self, part: ModelRequestPart | ModelResponsePart) -> None:\n    \"\"\"Add a new part, creating a new request or response message if necessary.\"\"\"\n    last_message = self.messages[-1] if self.messages else None\n    if isinstance(part, get_union_args(ModelRequestPart)):\n        part = cast(ModelRequestPart, part)\n        if isinstance(last_message, ModelRequest):\n            last_message.parts = [*last_message.parts, part]\n        else:\n            self.messages.append(ModelRequest(parts=[part]))\n    else:\n        part = cast(ModelResponsePart, part)\n        if isinstance(last_message, ModelResponse):\n            last_message.parts = [*last_message.parts, part]\n        else:\n            self.messages.append(ModelResponse(parts=[part]))\n\n```",
  "content_length": 102557
}