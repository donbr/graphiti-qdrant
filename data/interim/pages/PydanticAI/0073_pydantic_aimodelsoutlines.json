{
  "title": "`pydantic_ai.models.outlines`",
  "source_url": null,
  "content": "## Setup\n\nFor details on how to set up this model, see [model configuration for Outlines](../../../models/outlines/).\n\n### OutlinesModel\n\nBases: `Model`\n\nA model that relies on the Outlines library to run non API-based models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\n@dataclass(init=False)\nclass OutlinesModel(Model):\n    \"\"\"A model that relies on the Outlines library to run non API-based models.\"\"\"\n\n    def __init__(\n        self,\n        model: OutlinesBaseModel | OutlinesAsyncBaseModel,\n        *,\n        provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize an Outlines model.\n\n        Args:\n            model: The Outlines model used for the model.\n            provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n                instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n            profile: The model profile to use. Defaults to a profile picked by the provider.\n            settings: Default model settings for this model instance.\n        \"\"\"\n        self.model: OutlinesBaseModel | OutlinesAsyncBaseModel = model\n        self._model_name: str = 'outlines-model'\n\n        if isinstance(provider, str):\n            provider = infer_provider(provider)\n\n        super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n    @classmethod\n    def from_transformers(\n        cls,\n        hf_model: transformers.modeling_utils.PreTrainedModel,\n        hf_tokenizer_or_processor: transformers.tokenization_utils.PreTrainedTokenizer\n        | transformers.processing_utils.ProcessorMixin,\n        *,\n        provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Create an Outlines model from a Hugging Face model and tokenizer.\n\n        Args:\n            hf_model: The Hugging Face PreTrainedModel or any model that is compatible with the\n                `transformers` API.\n            hf_tokenizer_or_processor: Either a HuggingFace `PreTrainedTokenizer` or any tokenizer that is compatible\n                with the `transformers` API, or a HuggingFace processor inheriting from `ProcessorMixin`. If a\n                tokenizer is provided, a regular model will be used, while if you provide a processor, it will be a\n                multimodal model.\n            provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n                instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n            profile: The model profile to use. Defaults to a profile picked by the provider.\n            settings: Default model settings for this model instance.\n        \"\"\"\n        outlines_model: OutlinesBaseModel = from_transformers(hf_model, hf_tokenizer_or_processor)\n        return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n    @classmethod\n    def from_llamacpp(\n        cls,\n        llama_model: llama_cpp.Llama,\n        *,\n        provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Create an Outlines model from a LlamaCpp model.\n\n        Args:\n            llama_model: The llama_cpp.Llama model to use.\n            provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n                instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n            profile: The model profile to use. Defaults to a profile picked by the provider.\n            settings: Default model settings for this model instance.\n        \"\"\"\n        outlines_model: OutlinesBaseModel = from_llamacpp(llama_model)\n        return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n    @classmethod\n    def from_mlxlm(  # pragma: no cover\n        cls,\n        mlx_model: nn.Module,\n        mlx_tokenizer: transformers.tokenization_utils.PreTrainedTokenizer,\n        *,\n        provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Create an Outlines model from a MLXLM model.\n\n        Args:\n            mlx_model: The nn.Module model to use.\n            mlx_tokenizer: The PreTrainedTokenizer to use.\n            provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n                instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n            profile: The model profile to use. Defaults to a profile picked by the provider.\n            settings: Default model settings for this model instance.\n        \"\"\"\n        outlines_model: OutlinesBaseModel = from_mlxlm(mlx_model, mlx_tokenizer)\n        return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n    @classmethod\n    def from_sglang(\n        cls,\n        base_url: str,\n        api_key: str | None = None,\n        model_name: str | None = None,\n        *,\n        provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Create an Outlines model to send requests to an SGLang server.\n\n        Args:\n            base_url: The url of the SGLang server.\n            api_key: The API key to use for authenticating requests to the SGLang server.\n            model_name: The name of the model to use.\n            provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n                instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n            profile: The model profile to use. Defaults to a profile picked by the provider.\n            settings: Default model settings for this model instance.\n        \"\"\"\n        try:\n            from openai import AsyncOpenAI\n        except ImportError as _import_error:\n            raise ImportError(\n                'Please install `openai` to use the Outlines SGLang model, '\n                'you can use the `openai` optional group — `pip install \"pydantic-ai-slim[openai]\"`'\n            ) from _import_error\n\n        openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n        outlines_model: OutlinesBaseModel | OutlinesAsyncBaseModel = from_sglang(openai_client, model_name)\n        return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n    @classmethod\n    def from_vllm_offline(  # pragma: no cover\n        cls,\n        vllm_model: Any,\n        *,\n        provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Create an Outlines model from a vLLM offline inference model.\n\n        Args:\n            vllm_model: The vllm.LLM local model to use.\n            provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n                instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n            profile: The model profile to use. Defaults to a profile picked by the provider.\n            settings: Default model settings for this model instance.\n        \"\"\"\n        outlines_model: OutlinesBaseModel | OutlinesAsyncBaseModel = from_vllm_offline(vllm_model)\n        return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n    @property\n    def model_name(self) -> str:\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        return 'outlines'\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        \"\"\"Make a request to the model.\"\"\"\n        prompt, output_type, inference_kwargs = await self._build_generation_arguments(\n            messages, model_settings, model_request_parameters\n        )\n        # Async is available for SgLang\n        response: str\n        if isinstance(self.model, OutlinesAsyncBaseModel):\n            response = await self.model(prompt, output_type, None, **inference_kwargs)\n        else:\n            response = self.model(prompt, output_type, None, **inference_kwargs)\n        return self._process_response(response)\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n\n        prompt, output_type, inference_kwargs = await self._build_generation_arguments(\n            messages, model_settings, model_request_parameters\n        )\n        # Async is available for SgLang\n        if isinstance(self.model, OutlinesAsyncBaseModel):\n            response = self.model.stream(prompt, output_type, None, **inference_kwargs)\n            yield await self._process_streamed_response(response, model_request_parameters)\n        else:\n            response = self.model.stream(prompt, output_type, None, **inference_kwargs)\n\n            async def async_response():\n                for chunk in response:\n                    yield chunk\n\n            yield await self._process_streamed_response(async_response(), model_request_parameters)\n\n    async def _build_generation_arguments(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> tuple[Chat, JsonSchema | None, dict[str, Any]]:\n        \"\"\"Build the generation arguments for the model.\"\"\"\n        if (\n            model_request_parameters.function_tools\n            or model_request_parameters.builtin_tools\n            or model_request_parameters.output_tools\n        ):\n            raise UserError('Outlines does not support function tools and builtin tools yet.')\n\n        if model_request_parameters.output_object:\n            output_type = JsonSchema(model_request_parameters.output_object.json_schema)\n        else:\n            output_type = None\n\n        prompt = await self._format_prompt(messages, model_request_parameters)\n        inference_kwargs = self.format_inference_kwargs(model_settings)\n\n        return prompt, output_type, inference_kwargs\n\n    def format_inference_kwargs(self, model_settings: ModelSettings | None) -> dict[str, Any]:\n        \"\"\"Format the model settings for the inference kwargs.\"\"\"\n        settings_dict: dict[str, Any] = dict(model_settings) if model_settings else {}\n\n        if isinstance(self.model, Transformers):\n            settings_dict = self._format_transformers_inference_kwargs(settings_dict)\n        elif isinstance(self.model, LlamaCpp):\n            settings_dict = self._format_llama_cpp_inference_kwargs(settings_dict)\n        elif isinstance(self.model, MLXLM):  # pragma: no cover\n            settings_dict = self._format_mlxlm_inference_kwargs(settings_dict)\n        elif isinstance(self.model, SGLang | AsyncSGLang):\n            settings_dict = self._format_sglang_inference_kwargs(settings_dict)\n        elif isinstance(self.model, VLLMOffline):  # pragma: no cover\n            settings_dict = self._format_vllm_offline_inference_kwargs(settings_dict)\n\n        extra_body = settings_dict.pop('extra_body', {})\n        settings_dict.update(extra_body)\n\n        return settings_dict\n\n    def _format_transformers_inference_kwargs(self, model_settings: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"Select the model settings supported by the Transformers model.\"\"\"\n        supported_args = [\n            'max_tokens',\n            'temperature',\n            'top_p',\n            'logit_bias',\n            'extra_body',\n        ]\n        filtered_settings = {k: model_settings[k] for k in supported_args if k in model_settings}\n\n        return filtered_settings\n\n    def _format_llama_cpp_inference_kwargs(self, model_settings: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"Select the model settings supported by the LlamaCpp model.\"\"\"\n        supported_args = [\n            'max_tokens',\n            'temperature',\n            'top_p',\n            'seed',\n            'presence_penalty',\n            'frequency_penalty',\n            'logit_bias',\n            'extra_body',\n        ]\n        filtered_settings = {k: model_settings[k] for k in supported_args if k in model_settings}\n\n        return filtered_settings\n\n    def _format_mlxlm_inference_kwargs(  # pragma: no cover\n        self, model_settings: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Select the model settings supported by the MLXLM model.\"\"\"\n        supported_args = [\n            'extra_body',\n        ]\n        filtered_settings = {k: model_settings[k] for k in supported_args if k in model_settings}\n\n        return filtered_settings\n\n    def _format_sglang_inference_kwargs(self, model_settings: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"Select the model settings supported by the SGLang model.\"\"\"\n        supported_args = [\n            'max_tokens',\n            'temperature',\n            'top_p',\n            'presence_penalty',\n            'frequency_penalty',\n            'extra_body',\n        ]\n        filtered_settings = {k: model_settings[k] for k in supported_args if k in model_settings}\n\n        return filtered_settings\n\n    def _format_vllm_offline_inference_kwargs(  # pragma: no cover\n        self, model_settings: dict[str, Any]\n    ) -> dict[str, Any]:\n        \"\"\"Select the model settings supported by the vLLMOffline model.\"\"\"\n        from vllm.sampling_params import SamplingParams\n\n        supported_args = [\n            'max_tokens',\n            'temperature',\n            'top_p',\n            'seed',\n            'presence_penalty',\n            'frequency_penalty',\n            'logit_bias',\n            'extra_body',\n        ]\n        # The arguments that are part of the fields of `ModelSettings` must be put in a `SamplingParams` object and\n        # provided through the `sampling_params` argument to vLLM\n        sampling_params = model_settings.get('extra_body', {}).pop('sampling_params', SamplingParams())\n\n        for key in supported_args:\n            setattr(sampling_params, key, model_settings.get(key, None))\n\n        filtered_settings = {\n            'sampling_params': sampling_params,\n            **model_settings.get('extra_body', {}),\n        }\n\n        return filtered_settings\n\n    async def _format_prompt(  # noqa: C901\n        self, messages: list[ModelMessage], model_request_parameters: ModelRequestParameters\n    ) -> Chat:\n        \"\"\"Turn the model messages into an Outlines Chat instance.\"\"\"\n        chat = Chat()\n\n        if instructions := self._get_instructions(messages, model_request_parameters):\n            chat.add_system_message(instructions)\n\n        for message in messages:\n            if isinstance(message, ModelRequest):\n                for part in message.parts:\n                    if isinstance(part, SystemPromptPart):\n                        chat.add_system_message(part.content)\n                    elif isinstance(part, UserPromptPart):\n                        if isinstance(part.content, str):\n                            chat.add_user_message(part.content)\n                        elif isinstance(part.content, Sequence):\n                            outlines_input: Sequence[str | Image] = []\n                            for item in part.content:\n                                if isinstance(item, str):\n                                    outlines_input.append(item)\n                                elif isinstance(item, ImageUrl):\n                                    image_content: DownloadedItem[bytes] = await download_item(\n                                        item, data_format='bytes', type_format='mime'\n                                    )\n                                    image = self._create_PIL_image(image_content['data'], image_content['data_type'])\n                                    outlines_input.append(Image(image))\n                                elif isinstance(item, BinaryContent) and item.is_image:\n                                    image = self._create_PIL_image(item.data, item.media_type)\n                                    outlines_input.append(Image(image))\n                                else:\n                                    raise UserError(\n                                        'Each element of the content sequence must be a string, an `ImageUrl`'\n                                        + ' or a `BinaryImage`.'\n                                    )\n                            chat.add_user_message(outlines_input)\n                        else:\n                            assert_never(part.content)\n                    elif isinstance(part, RetryPromptPart):\n                        chat.add_user_message(part.model_response())\n                    elif isinstance(part, ToolReturnPart):\n                        raise UserError('Tool calls are not supported for Outlines models yet.')\n                    else:\n                        assert_never(part)\n            elif isinstance(message, ModelResponse):\n                text_parts: list[str] = []\n                image_parts: list[Image] = []\n                for part in message.parts:\n                    if isinstance(part, TextPart):\n                        text_parts.append(part.content)\n                    elif isinstance(part, ThinkingPart):\n                        # NOTE: We don't send ThinkingPart to the providers yet.\n                        pass\n                    elif isinstance(part, ToolCallPart | BuiltinToolCallPart | BuiltinToolReturnPart):\n                        raise UserError('Tool calls are not supported for Outlines models yet.')\n                    elif isinstance(part, FilePart):\n                        if isinstance(part.content, BinaryContent) and part.content.is_image:\n                            image = self._create_PIL_image(part.content.data, part.content.media_type)\n                            image_parts.append(Image(image))\n                        else:\n                            raise UserError(\n                                'File parts other than `BinaryImage` are not supported for Outlines models yet.'\n                            )\n                    else:\n                        assert_never(part)\n                if len(text_parts) == 1 and len(image_parts) == 0:\n                    chat.add_assistant_message(text_parts[0])\n                else:\n                    chat.add_assistant_message([*text_parts, *image_parts])\n            else:\n                assert_never(message)\n        return chat\n\n    def _create_PIL_image(self, data: bytes, data_type: str) -> PILImage.Image:\n        \"\"\"Create a PIL Image from the data and data type.\"\"\"\n        image = PILImage.open(io.BytesIO(data))\n        image.format = data_type.split('/')[-1]\n        return image\n\n    def _process_response(self, response: str) -> ModelResponse:\n        \"\"\"Turn the Outlines text response into a Pydantic AI model response instance.\"\"\"\n        return ModelResponse(\n            parts=cast(\n                list[ModelResponsePart], split_content_into_text_and_thinking(response, self.profile.thinking_tags)\n            ),\n        )\n\n    async def _process_streamed_response(\n        self, response: AsyncIterable[str], model_request_parameters: ModelRequestParameters\n    ) -> StreamedResponse:\n        \"\"\"Turn the Outlines text response into a Pydantic AI streamed response instance.\"\"\"\n        peekable_response = _utils.PeekableAsyncStream(response)\n        first_chunk = await peekable_response.peek()\n        if isinstance(first_chunk, _utils.Unset):  # pragma: no cover\n            raise UnexpectedModelBehavior('Streamed response ended without content or tool calls')\n\n        timestamp = datetime.now(tz=timezone.utc)\n        return OutlinesStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _model_name=self._model_name,\n            _model_profile=self.profile,\n            _response=peekable_response,\n            _timestamp=timestamp,\n            _provider_name='outlines',\n        )\n\n    def customize_request_parameters(self, model_request_parameters: ModelRequestParameters) -> ModelRequestParameters:\n        \"\"\"Customize the model request parameters for the model.\"\"\"\n        if model_request_parameters.output_mode in ('auto', 'native'):\n            # This way the JSON schema will be included in the instructions.\n            return replace(model_request_parameters, output_mode='prompted')\n        else:\n            return model_request_parameters\n\n```\n\n#### __init__\n\n```python\n__init__(\n    model: Model | AsyncModel,\n    *,\n    provider: (\n        Literal[\"outlines\"] | Provider[Model]\n    ) = \"outlines\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize an Outlines model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model` | `Model | AsyncModel` | The Outlines model used for the model. | *required* | | `provider` | `Literal['outlines'] | Provider[Model]` | The provider to use for OutlinesModel. Can be either the string 'outlines' or an instance of Provider[OutlinesBaseModel]. If not provided, the other parameters will be used. | `'outlines'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider. | `None` | | `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\ndef __init__(\n    self,\n    model: OutlinesBaseModel | OutlinesAsyncBaseModel,\n    *,\n    provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize an Outlines model.\n\n    Args:\n        model: The Outlines model used for the model.\n        provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n            instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n        profile: The model profile to use. Defaults to a profile picked by the provider.\n        settings: Default model settings for this model instance.\n    \"\"\"\n    self.model: OutlinesBaseModel | OutlinesAsyncBaseModel = model\n    self._model_name: str = 'outlines-model'\n\n    if isinstance(provider, str):\n        provider = infer_provider(provider)\n\n    super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n```\n\n#### from_transformers\n\n```python\nfrom_transformers(\n    hf_model: PreTrainedModel,\n    hf_tokenizer_or_processor: (\n        PreTrainedTokenizer | ProcessorMixin\n    ),\n    *,\n    provider: (\n        Literal[\"outlines\"] | Provider[Model]\n    ) = \"outlines\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nCreate an Outlines model from a Hugging Face model and tokenizer.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `hf_model` | `PreTrainedModel` | The Hugging Face PreTrainedModel or any model that is compatible with the transformers API. | *required* | | `hf_tokenizer_or_processor` | `PreTrainedTokenizer | ProcessorMixin` | Either a HuggingFace PreTrainedTokenizer or any tokenizer that is compatible with the transformers API, or a HuggingFace processor inheriting from ProcessorMixin. If a tokenizer is provided, a regular model will be used, while if you provide a processor, it will be a multimodal model. | *required* | | `provider` | `Literal['outlines'] | Provider[Model]` | The provider to use for OutlinesModel. Can be either the string 'outlines' or an instance of Provider[OutlinesBaseModel]. If not provided, the other parameters will be used. | `'outlines'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider. | `None` | | `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\n@classmethod\ndef from_transformers(\n    cls,\n    hf_model: transformers.modeling_utils.PreTrainedModel,\n    hf_tokenizer_or_processor: transformers.tokenization_utils.PreTrainedTokenizer\n    | transformers.processing_utils.ProcessorMixin,\n    *,\n    provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Create an Outlines model from a Hugging Face model and tokenizer.\n\n    Args:\n        hf_model: The Hugging Face PreTrainedModel or any model that is compatible with the\n            `transformers` API.\n        hf_tokenizer_or_processor: Either a HuggingFace `PreTrainedTokenizer` or any tokenizer that is compatible\n            with the `transformers` API, or a HuggingFace processor inheriting from `ProcessorMixin`. If a\n            tokenizer is provided, a regular model will be used, while if you provide a processor, it will be a\n            multimodal model.\n        provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n            instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n        profile: The model profile to use. Defaults to a profile picked by the provider.\n        settings: Default model settings for this model instance.\n    \"\"\"\n    outlines_model: OutlinesBaseModel = from_transformers(hf_model, hf_tokenizer_or_processor)\n    return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n```\n\n#### from_llamacpp\n\n```python\nfrom_llamacpp(\n    llama_model: Llama,\n    *,\n    provider: (\n        Literal[\"outlines\"] | Provider[Model]\n    ) = \"outlines\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nCreate an Outlines model from a LlamaCpp model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `llama_model` | `Llama` | The llama_cpp.Llama model to use. | *required* | | `provider` | `Literal['outlines'] | Provider[Model]` | The provider to use for OutlinesModel. Can be either the string 'outlines' or an instance of Provider[OutlinesBaseModel]. If not provided, the other parameters will be used. | `'outlines'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider. | `None` | | `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\n@classmethod\ndef from_llamacpp(\n    cls,\n    llama_model: llama_cpp.Llama,\n    *,\n    provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Create an Outlines model from a LlamaCpp model.\n\n    Args:\n        llama_model: The llama_cpp.Llama model to use.\n        provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n            instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n        profile: The model profile to use. Defaults to a profile picked by the provider.\n        settings: Default model settings for this model instance.\n    \"\"\"\n    outlines_model: OutlinesBaseModel = from_llamacpp(llama_model)\n    return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n```\n\n#### from_mlxlm\n\n```python\nfrom_mlxlm(\n    mlx_model: Module,\n    mlx_tokenizer: PreTrainedTokenizer,\n    *,\n    provider: (\n        Literal[\"outlines\"] | Provider[Model]\n    ) = \"outlines\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nCreate an Outlines model from a MLXLM model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `mlx_model` | `Module` | The nn.Module model to use. | *required* | | `mlx_tokenizer` | `PreTrainedTokenizer` | The PreTrainedTokenizer to use. | *required* | | `provider` | `Literal['outlines'] | Provider[Model]` | The provider to use for OutlinesModel. Can be either the string 'outlines' or an instance of Provider[OutlinesBaseModel]. If not provided, the other parameters will be used. | `'outlines'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider. | `None` | | `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\n@classmethod\ndef from_mlxlm(  # pragma: no cover\n    cls,\n    mlx_model: nn.Module,\n    mlx_tokenizer: transformers.tokenization_utils.PreTrainedTokenizer,\n    *,\n    provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Create an Outlines model from a MLXLM model.\n\n    Args:\n        mlx_model: The nn.Module model to use.\n        mlx_tokenizer: The PreTrainedTokenizer to use.\n        provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n            instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n        profile: The model profile to use. Defaults to a profile picked by the provider.\n        settings: Default model settings for this model instance.\n    \"\"\"\n    outlines_model: OutlinesBaseModel = from_mlxlm(mlx_model, mlx_tokenizer)\n    return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n```\n\n#### from_sglang\n\n```python\nfrom_sglang(\n    base_url: str,\n    api_key: str | None = None,\n    model_name: str | None = None,\n    *,\n    provider: (\n        Literal[\"outlines\"] | Provider[Model]\n    ) = \"outlines\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nCreate an Outlines model to send requests to an SGLang server.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `base_url` | `str` | The url of the SGLang server. | *required* | | `api_key` | `str | None` | The API key to use for authenticating requests to the SGLang server. | `None` | | `model_name` | `str | None` | The name of the model to use. | `None` | | `provider` | `Literal['outlines'] | Provider[Model]` | The provider to use for OutlinesModel. Can be either the string 'outlines' or an instance of Provider[OutlinesBaseModel]. If not provided, the other parameters will be used. | `'outlines'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider. | `None` | | `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\n@classmethod\ndef from_sglang(\n    cls,\n    base_url: str,\n    api_key: str | None = None,\n    model_name: str | None = None,\n    *,\n    provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Create an Outlines model to send requests to an SGLang server.\n\n    Args:\n        base_url: The url of the SGLang server.\n        api_key: The API key to use for authenticating requests to the SGLang server.\n        model_name: The name of the model to use.\n        provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n            instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n        profile: The model profile to use. Defaults to a profile picked by the provider.\n        settings: Default model settings for this model instance.\n    \"\"\"\n    try:\n        from openai import AsyncOpenAI\n    except ImportError as _import_error:\n        raise ImportError(\n            'Please install `openai` to use the Outlines SGLang model, '\n            'you can use the `openai` optional group — `pip install \"pydantic-ai-slim[openai]\"`'\n        ) from _import_error\n\n    openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n    outlines_model: OutlinesBaseModel | OutlinesAsyncBaseModel = from_sglang(openai_client, model_name)\n    return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n```\n\n#### from_vllm_offline\n\n```python\nfrom_vllm_offline(\n    vllm_model: Any,\n    *,\n    provider: (\n        Literal[\"outlines\"] | Provider[Model]\n    ) = \"outlines\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nCreate an Outlines model from a vLLM offline inference model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `vllm_model` | `Any` | The vllm.LLM local model to use. | *required* | | `provider` | `Literal['outlines'] | Provider[Model]` | The provider to use for OutlinesModel. Can be either the string 'outlines' or an instance of Provider[OutlinesBaseModel]. If not provided, the other parameters will be used. | `'outlines'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider. | `None` | | `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\n@classmethod\ndef from_vllm_offline(  # pragma: no cover\n    cls,\n    vllm_model: Any,\n    *,\n    provider: Literal['outlines'] | Provider[OutlinesBaseModel] = 'outlines',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Create an Outlines model from a vLLM offline inference model.\n\n    Args:\n        vllm_model: The vllm.LLM local model to use.\n        provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n            instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n        profile: The model profile to use. Defaults to a profile picked by the provider.\n        settings: Default model settings for this model instance.\n    \"\"\"\n    outlines_model: OutlinesBaseModel | OutlinesAsyncBaseModel = from_vllm_offline(vllm_model)\n    return cls(outlines_model, provider=provider, profile=profile, settings=settings)\n\n```\n\n#### format_inference_kwargs\n\n```python\nformat_inference_kwargs(\n    model_settings: ModelSettings | None,\n) -> dict[str, Any]\n\n```\n\nFormat the model settings for the inference kwargs.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\ndef format_inference_kwargs(self, model_settings: ModelSettings | None) -> dict[str, Any]:\n    \"\"\"Format the model settings for the inference kwargs.\"\"\"\n    settings_dict: dict[str, Any] = dict(model_settings) if model_settings else {}\n\n    if isinstance(self.model, Transformers):\n        settings_dict = self._format_transformers_inference_kwargs(settings_dict)\n    elif isinstance(self.model, LlamaCpp):\n        settings_dict = self._format_llama_cpp_inference_kwargs(settings_dict)\n    elif isinstance(self.model, MLXLM):  # pragma: no cover\n        settings_dict = self._format_mlxlm_inference_kwargs(settings_dict)\n    elif isinstance(self.model, SGLang | AsyncSGLang):\n        settings_dict = self._format_sglang_inference_kwargs(settings_dict)\n    elif isinstance(self.model, VLLMOffline):  # pragma: no cover\n        settings_dict = self._format_vllm_offline_inference_kwargs(settings_dict)\n\n    extra_body = settings_dict.pop('extra_body', {})\n    settings_dict.update(extra_body)\n\n    return settings_dict\n\n```\n\n#### customize_request_parameters\n\n```python\ncustomize_request_parameters(\n    model_request_parameters: ModelRequestParameters,\n) -> ModelRequestParameters\n\n```\n\nCustomize the model request parameters for the model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\ndef customize_request_parameters(self, model_request_parameters: ModelRequestParameters) -> ModelRequestParameters:\n    \"\"\"Customize the model request parameters for the model.\"\"\"\n    if model_request_parameters.output_mode in ('auto', 'native'):\n        # This way the JSON schema will be included in the instructions.\n        return replace(model_request_parameters, output_mode='prompted')\n    else:\n        return model_request_parameters\n\n```\n\n### OutlinesStreamedResponse\n\nBases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for Outlines models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/outlines.py`\n\n```python\n@dataclass\nclass OutlinesStreamedResponse(StreamedResponse):\n    \"\"\"Implementation of `StreamedResponse` for Outlines models.\"\"\"\n\n    _model_name: str\n    _model_profile: ModelProfile\n    _response: AsyncIterable[str]\n    _timestamp: datetime\n    _provider_name: str\n\n    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\n        async for event in self._response:\n            event = self._parts_manager.handle_text_delta(\n                vendor_part_id='content',\n                content=event,\n                thinking_tags=self._model_profile.thinking_tags,\n                ignore_leading_whitespace=self._model_profile.ignore_streamed_leading_whitespace,\n            )\n            if event is not None:  # pragma: no branch\n                yield event\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"Get the model name of the response.\"\"\"\n        return self._model_name\n\n    @property\n    def provider_name(self) -> str:\n        \"\"\"Get the provider name.\"\"\"\n        return self._provider_name\n\n    @property\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._timestamp\n\n```\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nGet the model name of the response.\n\n#### provider_name\n\n```python\nprovider_name: str\n\n```\n\nGet the provider name.\n\n#### timestamp\n\n```python\ntimestamp: datetime\n\n```\n\nGet the timestamp of the response.",
  "content_length": 39044
}