{
  "title": "`pydantic_ai.usage`",
  "source_url": null,
  "content": "### RequestUsage\n\nBases: `UsageBase`\n\nLLM usage associated with a single request.\n\nThis is an implementation of `genai_prices.types.AbstractUsage` so it can be used to calculate the price of the request using [genai-prices](https://github.com/pydantic/genai-prices).\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\n@dataclass(repr=False, kw_only=True)\nclass RequestUsage(UsageBase):\n    \"\"\"LLM usage associated with a single request.\n\n    This is an implementation of `genai_prices.types.AbstractUsage` so it can be used to calculate the price of the\n    request using [genai-prices](https://github.com/pydantic/genai-prices).\n    \"\"\"\n\n    @property\n    def requests(self):\n        return 1\n\n    def incr(self, incr_usage: RequestUsage) -> None:\n        \"\"\"Increment the usage in place.\n\n        Args:\n            incr_usage: The usage to increment by.\n        \"\"\"\n        return _incr_usage_tokens(self, incr_usage)\n\n    def __add__(self, other: RequestUsage) -> RequestUsage:\n        \"\"\"Add two RequestUsages together.\n\n        This is provided so it's trivial to sum usage information from multiple parts of a response.\n\n        **WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations.\n        \"\"\"\n        new_usage = copy(self)\n        new_usage.incr(other)\n        return new_usage\n\n    @classmethod\n    def extract(\n        cls,\n        data: Any,\n        *,\n        provider: str,\n        provider_url: str,\n        provider_fallback: str,\n        api_flavor: str = 'default',\n        details: dict[str, Any] | None = None,\n    ) -> RequestUsage:\n        \"\"\"Extract usage information from the response data using genai-prices.\n\n        Args:\n            data: The response data from the model API.\n            provider: The actual provider ID\n            provider_url: The provider base_url\n            provider_fallback: The fallback provider ID to use if the actual provider is not found in genai-prices.\n                For example, an OpenAI model should set this to \"openai\" in case it has an obscure provider ID.\n            api_flavor: The API flavor to use when extracting usage information,\n                e.g. 'chat' or 'responses' for OpenAI.\n            details: Becomes the `details` field on the returned `RequestUsage` for convenience.\n        \"\"\"\n        details = details or {}\n        for provider_id, provider_api_url in [(None, provider_url), (provider, None), (provider_fallback, None)]:\n            try:\n                provider_obj = get_snapshot().find_provider(None, provider_id, provider_api_url)\n                _model_ref, extracted_usage = provider_obj.extract_usage(data, api_flavor=api_flavor)\n                return cls(**{k: v for k, v in extracted_usage.__dict__.items() if v is not None}, details=details)\n            except Exception:\n                pass\n        return cls(details=details)\n\n```\n\n#### incr\n\n```python\nincr(incr_usage: RequestUsage) -> None\n\n```\n\nIncrement the usage in place.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `incr_usage` | `RequestUsage` | The usage to increment by. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\ndef incr(self, incr_usage: RequestUsage) -> None:\n    \"\"\"Increment the usage in place.\n\n    Args:\n        incr_usage: The usage to increment by.\n    \"\"\"\n    return _incr_usage_tokens(self, incr_usage)\n\n```\n\n#### __add__\n\n```python\n__add__(other: RequestUsage) -> RequestUsage\n\n```\n\nAdd two RequestUsages together.\n\nThis is provided so it's trivial to sum usage information from multiple parts of a response.\n\n**WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\ndef __add__(self, other: RequestUsage) -> RequestUsage:\n    \"\"\"Add two RequestUsages together.\n\n    This is provided so it's trivial to sum usage information from multiple parts of a response.\n\n    **WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations.\n    \"\"\"\n    new_usage = copy(self)\n    new_usage.incr(other)\n    return new_usage\n\n```\n\n#### extract\n\n```python\nextract(\n    data: Any,\n    *,\n    provider: str,\n    provider_url: str,\n    provider_fallback: str,\n    api_flavor: str = \"default\",\n    details: dict[str, Any] | None = None\n) -> RequestUsage\n\n```\n\nExtract usage information from the response data using genai-prices.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `data` | `Any` | The response data from the model API. | *required* | | `provider` | `str` | The actual provider ID | *required* | | `provider_url` | `str` | The provider base_url | *required* | | `provider_fallback` | `str` | The fallback provider ID to use if the actual provider is not found in genai-prices. For example, an OpenAI model should set this to \"openai\" in case it has an obscure provider ID. | *required* | | `api_flavor` | `str` | The API flavor to use when extracting usage information, e.g. 'chat' or 'responses' for OpenAI. | `'default'` | | `details` | `dict[str, Any] | None` | Becomes the details field on the returned RequestUsage for convenience. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\n@classmethod\ndef extract(\n    cls,\n    data: Any,\n    *,\n    provider: str,\n    provider_url: str,\n    provider_fallback: str,\n    api_flavor: str = 'default',\n    details: dict[str, Any] | None = None,\n) -> RequestUsage:\n    \"\"\"Extract usage information from the response data using genai-prices.\n\n    Args:\n        data: The response data from the model API.\n        provider: The actual provider ID\n        provider_url: The provider base_url\n        provider_fallback: The fallback provider ID to use if the actual provider is not found in genai-prices.\n            For example, an OpenAI model should set this to \"openai\" in case it has an obscure provider ID.\n        api_flavor: The API flavor to use when extracting usage information,\n            e.g. 'chat' or 'responses' for OpenAI.\n        details: Becomes the `details` field on the returned `RequestUsage` for convenience.\n    \"\"\"\n    details = details or {}\n    for provider_id, provider_api_url in [(None, provider_url), (provider, None), (provider_fallback, None)]:\n        try:\n            provider_obj = get_snapshot().find_provider(None, provider_id, provider_api_url)\n            _model_ref, extracted_usage = provider_obj.extract_usage(data, api_flavor=api_flavor)\n            return cls(**{k: v for k, v in extracted_usage.__dict__.items() if v is not None}, details=details)\n        except Exception:\n            pass\n    return cls(details=details)\n\n```\n\n### RunUsage\n\nBases: `UsageBase`\n\nLLM usage associated with an agent run.\n\nResponsibility for calculating request usage is on the model; Pydantic AI simply sums the usage information across requests.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\n@dataclass(repr=False, kw_only=True)\nclass RunUsage(UsageBase):\n    \"\"\"LLM usage associated with an agent run.\n\n    Responsibility for calculating request usage is on the model; Pydantic AI simply sums the usage information across requests.\n    \"\"\"\n\n    requests: int = 0\n    \"\"\"Number of requests made to the LLM API.\"\"\"\n\n    tool_calls: int = 0\n    \"\"\"Number of successful tool calls executed during the run.\"\"\"\n\n    input_tokens: int = 0\n    \"\"\"Total number of input/prompt tokens.\"\"\"\n\n    cache_write_tokens: int = 0\n    \"\"\"Total number of tokens written to the cache.\"\"\"\n\n    cache_read_tokens: int = 0\n    \"\"\"Total number of tokens read from the cache.\"\"\"\n\n    input_audio_tokens: int = 0\n    \"\"\"Total number of audio input tokens.\"\"\"\n\n    cache_audio_read_tokens: int = 0\n    \"\"\"Total number of audio tokens read from the cache.\"\"\"\n\n    output_tokens: int = 0\n    \"\"\"Total number of output/completion tokens.\"\"\"\n\n    details: dict[str, int] = dataclasses.field(default_factory=dict)\n    \"\"\"Any extra details returned by the model.\"\"\"\n\n    def incr(self, incr_usage: RunUsage | RequestUsage) -> None:\n        \"\"\"Increment the usage in place.\n\n        Args:\n            incr_usage: The usage to increment by.\n        \"\"\"\n        if isinstance(incr_usage, RunUsage):\n            self.requests += incr_usage.requests\n            self.tool_calls += incr_usage.tool_calls\n        return _incr_usage_tokens(self, incr_usage)\n\n    def __add__(self, other: RunUsage | RequestUsage) -> RunUsage:\n        \"\"\"Add two RunUsages together.\n\n        This is provided so it's trivial to sum usage information from multiple runs.\n        \"\"\"\n        new_usage = copy(self)\n        new_usage.incr(other)\n        return new_usage\n\n```\n\n#### requests\n\n```python\nrequests: int = 0\n\n```\n\nNumber of requests made to the LLM API.\n\n#### tool_calls\n\n```python\ntool_calls: int = 0\n\n```\n\nNumber of successful tool calls executed during the run.\n\n#### input_tokens\n\n```python\ninput_tokens: int = 0\n\n```\n\nTotal number of input/prompt tokens.\n\n#### cache_write_tokens\n\n```python\ncache_write_tokens: int = 0\n\n```\n\nTotal number of tokens written to the cache.\n\n#### cache_read_tokens\n\n```python\ncache_read_tokens: int = 0\n\n```\n\nTotal number of tokens read from the cache.\n\n#### input_audio_tokens\n\n```python\ninput_audio_tokens: int = 0\n\n```\n\nTotal number of audio input tokens.\n\n#### cache_audio_read_tokens\n\n```python\ncache_audio_read_tokens: int = 0\n\n```\n\nTotal number of audio tokens read from the cache.\n\n#### output_tokens\n\n```python\noutput_tokens: int = 0\n\n```\n\nTotal number of output/completion tokens.\n\n#### details\n\n```python\ndetails: dict[str, int] = field(default_factory=dict)\n\n```\n\nAny extra details returned by the model.\n\n#### incr\n\n```python\nincr(incr_usage: RunUsage | RequestUsage) -> None\n\n```\n\nIncrement the usage in place.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `incr_usage` | `RunUsage | RequestUsage` | The usage to increment by. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\ndef incr(self, incr_usage: RunUsage | RequestUsage) -> None:\n    \"\"\"Increment the usage in place.\n\n    Args:\n        incr_usage: The usage to increment by.\n    \"\"\"\n    if isinstance(incr_usage, RunUsage):\n        self.requests += incr_usage.requests\n        self.tool_calls += incr_usage.tool_calls\n    return _incr_usage_tokens(self, incr_usage)\n\n```\n\n#### __add__\n\n```python\n__add__(other: RunUsage | RequestUsage) -> RunUsage\n\n```\n\nAdd two RunUsages together.\n\nThis is provided so it's trivial to sum usage information from multiple runs.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\ndef __add__(self, other: RunUsage | RequestUsage) -> RunUsage:\n    \"\"\"Add two RunUsages together.\n\n    This is provided so it's trivial to sum usage information from multiple runs.\n    \"\"\"\n    new_usage = copy(self)\n    new_usage.incr(other)\n    return new_usage\n\n```\n\n### Usage\n\nBases: `RunUsage`\n\nDeprecated\n\n`Usage` is deprecated, use `RunUsage` instead\n\nDeprecated alias for `RunUsage`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\n@dataclass(repr=False, kw_only=True)\n@deprecated('`Usage` is deprecated, use `RunUsage` instead')\nclass Usage(RunUsage):\n    \"\"\"Deprecated alias for `RunUsage`.\"\"\"\n\n```\n\n### UsageLimits\n\nLimits on model usage.\n\nThe request count is tracked by pydantic_ai, and the request limit is checked before each request to the model. Token counts are provided in responses from the model, and the token limits are checked after each response.\n\nEach of the limits can be set to `None` to disable that limit.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\n@dataclass(repr=False, kw_only=True)\nclass UsageLimits:\n    \"\"\"Limits on model usage.\n\n    The request count is tracked by pydantic_ai, and the request limit is checked before each request to the model.\n    Token counts are provided in responses from the model, and the token limits are checked after each response.\n\n    Each of the limits can be set to `None` to disable that limit.\n    \"\"\"\n\n    request_limit: int | None = 50\n    \"\"\"The maximum number of requests allowed to the model.\"\"\"\n    tool_calls_limit: int | None = None\n    \"\"\"The maximum number of successful tool calls allowed to be executed.\"\"\"\n    input_tokens_limit: int | None = None\n    \"\"\"The maximum number of input/prompt tokens allowed.\"\"\"\n    output_tokens_limit: int | None = None\n    \"\"\"The maximum number of output/response tokens allowed.\"\"\"\n    total_tokens_limit: int | None = None\n    \"\"\"The maximum number of tokens allowed in requests and responses combined.\"\"\"\n    count_tokens_before_request: bool = False\n    \"\"\"If True, perform a token counting pass before sending the request to the model,\n    to enforce `request_tokens_limit` ahead of time. This may incur additional overhead\n    (from calling the model's `count_tokens` API before making the actual request) and is disabled by default.\"\"\"\n\n    @property\n    @deprecated('`request_tokens_limit` is deprecated, use `input_tokens_limit` instead')\n    def request_tokens_limit(self) -> int | None:\n        return self.input_tokens_limit\n\n    @property\n    @deprecated('`response_tokens_limit` is deprecated, use `output_tokens_limit` instead')\n    def response_tokens_limit(self) -> int | None:\n        return self.output_tokens_limit\n\n    @overload\n    def __init__(\n        self,\n        *,\n        request_limit: int | None = 50,\n        tool_calls_limit: int | None = None,\n        input_tokens_limit: int | None = None,\n        output_tokens_limit: int | None = None,\n        total_tokens_limit: int | None = None,\n        count_tokens_before_request: bool = False,\n    ) -> None:\n        self.request_limit = request_limit\n        self.tool_calls_limit = tool_calls_limit\n        self.input_tokens_limit = input_tokens_limit\n        self.output_tokens_limit = output_tokens_limit\n        self.total_tokens_limit = total_tokens_limit\n        self.count_tokens_before_request = count_tokens_before_request\n\n    @overload\n    @deprecated(\n        'Use `input_tokens_limit` instead of `request_tokens_limit` and `output_tokens_limit` and `total_tokens_limit`'\n    )\n    def __init__(\n        self,\n        *,\n        request_limit: int | None = 50,\n        tool_calls_limit: int | None = None,\n        request_tokens_limit: int | None = None,\n        response_tokens_limit: int | None = None,\n        total_tokens_limit: int | None = None,\n        count_tokens_before_request: bool = False,\n    ) -> None:\n        self.request_limit = request_limit\n        self.tool_calls_limit = tool_calls_limit\n        self.input_tokens_limit = request_tokens_limit\n        self.output_tokens_limit = response_tokens_limit\n        self.total_tokens_limit = total_tokens_limit\n        self.count_tokens_before_request = count_tokens_before_request\n\n    def __init__(\n        self,\n        *,\n        request_limit: int | None = 50,\n        tool_calls_limit: int | None = None,\n        input_tokens_limit: int | None = None,\n        output_tokens_limit: int | None = None,\n        total_tokens_limit: int | None = None,\n        count_tokens_before_request: bool = False,\n        # deprecated:\n        request_tokens_limit: int | None = None,\n        response_tokens_limit: int | None = None,\n    ):\n        self.request_limit = request_limit\n        self.tool_calls_limit = tool_calls_limit\n        self.input_tokens_limit = input_tokens_limit or request_tokens_limit\n        self.output_tokens_limit = output_tokens_limit or response_tokens_limit\n        self.total_tokens_limit = total_tokens_limit\n        self.count_tokens_before_request = count_tokens_before_request\n\n    def has_token_limits(self) -> bool:\n        \"\"\"Returns `True` if this instance places any limits on token counts.\n\n        If this returns `False`, the `check_tokens` method will never raise an error.\n\n        This is useful because if we have token limits, we need to check them after receiving each streamed message.\n        If there are no limits, we can skip that processing in the streaming response iterator.\n        \"\"\"\n        return any(\n            limit is not None for limit in (self.input_tokens_limit, self.output_tokens_limit, self.total_tokens_limit)\n        )\n\n    def check_before_request(self, usage: RunUsage) -> None:\n        \"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed any of the limits.\"\"\"\n        request_limit = self.request_limit\n        if request_limit is not None and usage.requests >= request_limit:\n            raise UsageLimitExceeded(f'The next request would exceed the request_limit of {request_limit}')\n\n        input_tokens = usage.input_tokens\n        if self.input_tokens_limit is not None and input_tokens > self.input_tokens_limit:\n            raise UsageLimitExceeded(\n                f'The next request would exceed the input_tokens_limit of {self.input_tokens_limit} ({input_tokens=})'\n            )\n\n        total_tokens = usage.total_tokens\n        if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\n            raise UsageLimitExceeded(  # pragma: lax no cover\n                f'The next request would exceed the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})'\n            )\n\n    def check_tokens(self, usage: RunUsage) -> None:\n        \"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"\n        input_tokens = usage.input_tokens\n        if self.input_tokens_limit is not None and input_tokens > self.input_tokens_limit:\n            raise UsageLimitExceeded(f'Exceeded the input_tokens_limit of {self.input_tokens_limit} ({input_tokens=})')\n\n        output_tokens = usage.output_tokens\n        if self.output_tokens_limit is not None and output_tokens > self.output_tokens_limit:\n            raise UsageLimitExceeded(\n                f'Exceeded the output_tokens_limit of {self.output_tokens_limit} ({output_tokens=})'\n            )\n\n        total_tokens = usage.total_tokens\n        if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\n            raise UsageLimitExceeded(f'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})')\n\n    def check_before_tool_call(self, projected_usage: RunUsage) -> None:\n        \"\"\"Raises a `UsageLimitExceeded` exception if the next tool call(s) would exceed the tool call limit.\"\"\"\n        tool_calls_limit = self.tool_calls_limit\n        tool_calls = projected_usage.tool_calls\n        if tool_calls_limit is not None and tool_calls > tool_calls_limit:\n            raise UsageLimitExceeded(\n                f'The next tool call(s) would exceed the tool_calls_limit of {tool_calls_limit} ({tool_calls=}).'\n            )\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n```\n\n#### request_limit\n\n```python\nrequest_limit: int | None = request_limit\n\n```\n\nThe maximum number of requests allowed to the model.\n\n#### tool_calls_limit\n\n```python\ntool_calls_limit: int | None = tool_calls_limit\n\n```\n\nThe maximum number of successful tool calls allowed to be executed.\n\n#### input_tokens_limit\n\n```python\ninput_tokens_limit: int | None = (\n    input_tokens_limit or request_tokens_limit\n)\n\n```\n\nThe maximum number of input/prompt tokens allowed.\n\n#### output_tokens_limit\n\n```python\noutput_tokens_limit: int | None = (\n    output_tokens_limit or response_tokens_limit\n)\n\n```\n\nThe maximum number of output/response tokens allowed.\n\n#### total_tokens_limit\n\n```python\ntotal_tokens_limit: int | None = total_tokens_limit\n\n```\n\nThe maximum number of tokens allowed in requests and responses combined.\n\n#### count_tokens_before_request\n\n```python\ncount_tokens_before_request: bool = (\n    count_tokens_before_request\n)\n\n```\n\nIf True, perform a token counting pass before sending the request to the model, to enforce `request_tokens_limit` ahead of time. This may incur additional overhead (from calling the model's `count_tokens` API before making the actual request) and is disabled by default.\n\n#### has_token_limits\n\n```python\nhas_token_limits() -> bool\n\n```\n\nReturns `True` if this instance places any limits on token counts.\n\nIf this returns `False`, the `check_tokens` method will never raise an error.\n\nThis is useful because if we have token limits, we need to check them after receiving each streamed message. If there are no limits, we can skip that processing in the streaming response iterator.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\ndef has_token_limits(self) -> bool:\n    \"\"\"Returns `True` if this instance places any limits on token counts.\n\n    If this returns `False`, the `check_tokens` method will never raise an error.\n\n    This is useful because if we have token limits, we need to check them after receiving each streamed message.\n    If there are no limits, we can skip that processing in the streaming response iterator.\n    \"\"\"\n    return any(\n        limit is not None for limit in (self.input_tokens_limit, self.output_tokens_limit, self.total_tokens_limit)\n    )\n\n```\n\n#### check_before_request\n\n```python\ncheck_before_request(usage: RunUsage) -> None\n\n```\n\nRaises a `UsageLimitExceeded` exception if the next request would exceed any of the limits.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\ndef check_before_request(self, usage: RunUsage) -> None:\n    \"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed any of the limits.\"\"\"\n    request_limit = self.request_limit\n    if request_limit is not None and usage.requests >= request_limit:\n        raise UsageLimitExceeded(f'The next request would exceed the request_limit of {request_limit}')\n\n    input_tokens = usage.input_tokens\n    if self.input_tokens_limit is not None and input_tokens > self.input_tokens_limit:\n        raise UsageLimitExceeded(\n            f'The next request would exceed the input_tokens_limit of {self.input_tokens_limit} ({input_tokens=})'\n        )\n\n    total_tokens = usage.total_tokens\n    if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\n        raise UsageLimitExceeded(  # pragma: lax no cover\n            f'The next request would exceed the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})'\n        )\n\n```\n\n#### check_tokens\n\n```python\ncheck_tokens(usage: RunUsage) -> None\n\n```\n\nRaises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\ndef check_tokens(self, usage: RunUsage) -> None:\n    \"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"\n    input_tokens = usage.input_tokens\n    if self.input_tokens_limit is not None and input_tokens > self.input_tokens_limit:\n        raise UsageLimitExceeded(f'Exceeded the input_tokens_limit of {self.input_tokens_limit} ({input_tokens=})')\n\n    output_tokens = usage.output_tokens\n    if self.output_tokens_limit is not None and output_tokens > self.output_tokens_limit:\n        raise UsageLimitExceeded(\n            f'Exceeded the output_tokens_limit of {self.output_tokens_limit} ({output_tokens=})'\n        )\n\n    total_tokens = usage.total_tokens\n    if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:\n        raise UsageLimitExceeded(f'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})')\n\n```\n\n#### check_before_tool_call\n\n```python\ncheck_before_tool_call(projected_usage: RunUsage) -> None\n\n```\n\nRaises a `UsageLimitExceeded` exception if the next tool call(s) would exceed the tool call limit.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n```python\ndef check_before_tool_call(self, projected_usage: RunUsage) -> None:\n    \"\"\"Raises a `UsageLimitExceeded` exception if the next tool call(s) would exceed the tool call limit.\"\"\"\n    tool_calls_limit = self.tool_calls_limit\n    tool_calls = projected_usage.tool_calls\n    if tool_calls_limit is not None and tool_calls > tool_calls_limit:\n        raise UsageLimitExceeded(\n            f'The next tool call(s) would exceed the tool_calls_limit of {tool_calls_limit} ({tool_calls=}).'\n        )\n\n```",
  "content_length": 24189
}