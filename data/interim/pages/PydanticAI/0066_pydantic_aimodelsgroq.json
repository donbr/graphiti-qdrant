{
  "title": "`pydantic_ai.models.groq`",
  "source_url": null,
  "content": "## Setup\n\nFor details on how to set up authentication with this model, see [model configuration for Groq](../../../models/groq/).\n\n### ProductionGroqModelNames\n\n```python\nProductionGroqModelNames = Literal[\n    \"distil-whisper-large-v3-en\",\n    \"gemma2-9b-it\",\n    \"llama-3.3-70b-versatile\",\n    \"llama-3.1-8b-instant\",\n    \"llama-guard-3-8b\",\n    \"llama3-70b-8192\",\n    \"llama3-8b-8192\",\n    \"whisper-large-v3\",\n    \"whisper-large-v3-turbo\",\n]\n\n```\n\nProduction Groq models from <https://console.groq.com/docs/models#production-models>.\n\n### PreviewGroqModelNames\n\n```python\nPreviewGroqModelNames = Literal[\n    \"playai-tts\",\n    \"playai-tts-arabic\",\n    \"qwen-qwq-32b\",\n    \"mistral-saba-24b\",\n    \"qwen-2.5-coder-32b\",\n    \"qwen-2.5-32b\",\n    \"deepseek-r1-distill-qwen-32b\",\n    \"deepseek-r1-distill-llama-70b\",\n    \"llama-3.3-70b-specdec\",\n    \"llama-3.2-1b-preview\",\n    \"llama-3.2-3b-preview\",\n    \"llama-3.2-11b-vision-preview\",\n    \"llama-3.2-90b-vision-preview\",\n    \"moonshotai/kimi-k2-instruct\",\n]\n\n```\n\nPreview Groq models from <https://console.groq.com/docs/models#preview-models>.\n\n### GroqModelName\n\n```python\nGroqModelName = (\n    str | ProductionGroqModelNames | PreviewGroqModelNames\n)\n\n```\n\nPossible Groq model names.\n\nSince Groq supports a variety of models and the list changes frequencly, we explicitly list the named models as of 2025-03-31 but allow any name in the type hints.\n\nSee <https://console.groq.com/docs/models> for an up to date date list of models and more details.\n\n### GroqModelSettings\n\nBases: `ModelSettings`\n\nSettings used for a Groq model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\n\n```python\nclass GroqModelSettings(ModelSettings, total=False):\n    \"\"\"Settings used for a Groq model request.\"\"\"\n\n    # ALL FIELDS MUST BE `groq_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\n    groq_reasoning_format: Literal['hidden', 'raw', 'parsed']\n    \"\"\"The format of the reasoning output.\n\n    See [the Groq docs](https://console.groq.com/docs/reasoning#reasoning-format) for more details.\n    \"\"\"\n\n```\n\n#### groq_reasoning_format\n\n```python\ngroq_reasoning_format: Literal['hidden', 'raw', 'parsed']\n\n```\n\nThe format of the reasoning output.\n\nSee [the Groq docs](https://console.groq.com/docs/reasoning#reasoning-format) for more details.\n\n### GroqModel\n\nBases: `Model`\n\nA model that uses the Groq API.\n\nInternally, this uses the [Groq Python client](https://github.com/groq/groq-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\n\n```python\n@dataclass(init=False)\nclass GroqModel(Model):\n    \"\"\"A model that uses the Groq API.\n\n    Internally, this uses the [Groq Python client](https://github.com/groq/groq-python) to interact with the API.\n\n    Apart from `__init__`, all methods are private or match those of the base class.\n    \"\"\"\n\n    client: AsyncGroq = field(repr=False)\n\n    _model_name: GroqModelName = field(repr=False)\n    _provider: Provider[AsyncGroq] = field(repr=False)\n\n    def __init__(\n        self,\n        model_name: GroqModelName,\n        *,\n        provider: Literal['groq', 'gateway'] | Provider[AsyncGroq] = 'groq',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize a Groq model.\n\n        Args:\n            model_name: The name of the Groq model to use. List of model names available\n                [here](https://console.groq.com/docs/models).\n            provider: The provider to use for authentication and API access. Can be either the string\n                'groq' or an instance of `Provider[AsyncGroq]`. If not provided, a new provider will be\n                created using the other parameters.\n            profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n            settings: Model-specific settings that will be used as defaults for this model.\n        \"\"\"\n        self._model_name = model_name\n\n        if isinstance(provider, str):\n            provider = infer_provider('gateway/groq' if provider == 'gateway' else provider)\n        self._provider = provider\n        self.client = provider.client\n\n        super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.base_url)\n\n    @property\n    def model_name(self) -> GroqModelName:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The model provider.\"\"\"\n        return self._provider.name\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        try:\n            response = await self._completions_create(\n                messages, False, cast(GroqModelSettings, model_settings or {}), model_request_parameters\n            )\n        except ModelHTTPError as e:\n            if isinstance(e.body, dict):  # pragma: no branch\n                # The Groq SDK tries to be helpful by raising an exception when generated tool arguments don't match the schema,\n                # but we'd rather handle it ourselves so we can tell the model to retry the tool call.\n                try:\n                    error = _GroqToolUseFailedError.model_validate(e.body)  # pyright: ignore[reportUnknownMemberType]\n                    tool_call_part = ToolCallPart(\n                        tool_name=error.error.failed_generation.name,\n                        args=error.error.failed_generation.arguments,\n                    )\n                    return ModelResponse(\n                        parts=[tool_call_part],\n                        model_name=e.model_name,\n                        timestamp=_utils.now_utc(),\n                        provider_name=self._provider.name,\n                        finish_reason='error',\n                    )\n                except ValidationError:\n                    pass\n            raise\n        model_response = self._process_response(response)\n        return model_response\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._completions_create(\n            messages, True, cast(GroqModelSettings, model_settings or {}), model_request_parameters\n        )\n        async with response:\n            yield await self._process_streamed_response(response, model_request_parameters)\n\n    @overload\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[True],\n        model_settings: GroqModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> AsyncStream[chat.ChatCompletionChunk]:\n        pass\n\n    @overload\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[False],\n        model_settings: GroqModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> chat.ChatCompletion:\n        pass\n\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        stream: bool,\n        model_settings: GroqModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> chat.ChatCompletion | AsyncStream[chat.ChatCompletionChunk]:\n        tools = self._get_tools(model_request_parameters)\n        tools += self._get_builtin_tools(model_request_parameters)\n        if not tools:\n            tool_choice: Literal['none', 'required', 'auto'] | None = None\n        elif not model_request_parameters.allow_text_output:\n            tool_choice = 'required'\n        else:\n            tool_choice = 'auto'\n\n        groq_messages = self._map_messages(messages, model_request_parameters)\n\n        response_format: chat.completion_create_params.ResponseFormat | None = None\n        if model_request_parameters.output_mode == 'native':\n            output_object = model_request_parameters.output_object\n            assert output_object is not None\n            response_format = self._map_json_schema(output_object)\n        elif (\n            model_request_parameters.output_mode == 'prompted'\n            and not tools\n            and self.profile.supports_json_object_output\n        ):  # pragma: no branch\n            response_format = {'type': 'json_object'}\n\n        try:\n            extra_headers = model_settings.get('extra_headers', {})\n            extra_headers.setdefault('User-Agent', get_user_agent())\n            return await self.client.chat.completions.create(\n                model=self._model_name,\n                messages=groq_messages,\n                n=1,\n                parallel_tool_calls=model_settings.get('parallel_tool_calls', NOT_GIVEN),\n                tools=tools or NOT_GIVEN,\n                tool_choice=tool_choice or NOT_GIVEN,\n                stop=model_settings.get('stop_sequences', NOT_GIVEN),\n                stream=stream,\n                response_format=response_format or NOT_GIVEN,\n                max_tokens=model_settings.get('max_tokens', NOT_GIVEN),\n                temperature=model_settings.get('temperature', NOT_GIVEN),\n                top_p=model_settings.get('top_p', NOT_GIVEN),\n                timeout=model_settings.get('timeout', NOT_GIVEN),\n                seed=model_settings.get('seed', NOT_GIVEN),\n                presence_penalty=model_settings.get('presence_penalty', NOT_GIVEN),\n                reasoning_format=model_settings.get('groq_reasoning_format', NOT_GIVEN),\n                frequency_penalty=model_settings.get('frequency_penalty', NOT_GIVEN),\n                logit_bias=model_settings.get('logit_bias', NOT_GIVEN),\n                extra_headers=extra_headers,\n                extra_body=model_settings.get('extra_body'),\n            )\n        except APIStatusError as e:\n            if (status_code := e.status_code) >= 400:\n                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n            raise ModelAPIError(model_name=self.model_name, message=e.message) from e  # pragma: no cover\n        except APIConnectionError as e:\n            raise ModelAPIError(model_name=self.model_name, message=e.message) from e\n\n    def _process_response(self, response: chat.ChatCompletion) -> ModelResponse:\n        \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\n        timestamp = number_to_datetime(response.created)\n        choice = response.choices[0]\n        items: list[ModelResponsePart] = []\n        if choice.message.reasoning is not None:\n            # NOTE: The `reasoning` field is only present if `groq_reasoning_format` is set to `parsed`.\n            items.append(ThinkingPart(content=choice.message.reasoning))\n        if choice.message.executed_tools:\n            for tool in choice.message.executed_tools:\n                call_part, return_part = _map_executed_tool(tool, self.system)\n                if call_part and return_part:  # pragma: no branch\n                    items.append(call_part)\n                    items.append(return_part)\n        if choice.message.content:\n            # NOTE: The `<think>` tag is only present if `groq_reasoning_format` is set to `raw`.\n            items.extend(split_content_into_text_and_thinking(choice.message.content, self.profile.thinking_tags))\n        if choice.message.tool_calls is not None:\n            for c in choice.message.tool_calls:\n                items.append(ToolCallPart(tool_name=c.function.name, args=c.function.arguments, tool_call_id=c.id))\n\n        raw_finish_reason = choice.finish_reason\n        provider_details = {'finish_reason': raw_finish_reason}\n        finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n        return ModelResponse(\n            parts=items,\n            usage=_map_usage(response),\n            model_name=response.model,\n            timestamp=timestamp,\n            provider_response_id=response.id,\n            provider_name=self._provider.name,\n            finish_reason=finish_reason,\n            provider_details=provider_details,\n        )\n\n    async def _process_streamed_response(\n        self, response: AsyncStream[chat.ChatCompletionChunk], model_request_parameters: ModelRequestParameters\n    ) -> GroqStreamedResponse:\n        \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\n        peekable_response = _utils.PeekableAsyncStream(response)\n        first_chunk = await peekable_response.peek()\n        if isinstance(first_chunk, _utils.Unset):\n            raise UnexpectedModelBehavior(  # pragma: no cover\n                'Streamed response ended without content or tool calls'\n            )\n\n        return GroqStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _response=peekable_response,\n            _model_name=first_chunk.model,\n            _model_profile=self.profile,\n            _timestamp=number_to_datetime(first_chunk.created),\n            _provider_name=self._provider.name,\n        )\n\n    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:\n        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]\n\n    def _get_builtin_tools(\n        self, model_request_parameters: ModelRequestParameters\n    ) -> list[chat.ChatCompletionToolParam]:\n        tools: list[chat.ChatCompletionToolParam] = []\n        for tool in model_request_parameters.builtin_tools:\n            if isinstance(tool, WebSearchTool):\n                if not GroqModelProfile.from_profile(self.profile).groq_always_has_web_search_builtin_tool:\n                    raise UserError('`WebSearchTool` is not supported by Groq')  # pragma: no cover\n            else:\n                raise UserError(\n                    f'`{tool.__class__.__name__}` is not supported by `GroqModel`. If it should be, please file an issue.'\n                )\n        return tools\n\n    def _map_messages(\n        self, messages: list[ModelMessage], model_request_parameters: ModelRequestParameters\n    ) -> list[chat.ChatCompletionMessageParam]:\n        \"\"\"Just maps a `pydantic_ai.Message` to a `groq.types.ChatCompletionMessageParam`.\"\"\"\n        groq_messages: list[chat.ChatCompletionMessageParam] = []\n        for message in messages:\n            if isinstance(message, ModelRequest):\n                groq_messages.extend(self._map_user_message(message))\n            elif isinstance(message, ModelResponse):\n                texts: list[str] = []\n                tool_calls: list[chat.ChatCompletionMessageToolCallParam] = []\n                for item in message.parts:\n                    if isinstance(item, TextPart):\n                        texts.append(item.content)\n                    elif isinstance(item, ToolCallPart):\n                        tool_calls.append(self._map_tool_call(item))\n                    elif isinstance(item, ThinkingPart):\n                        start_tag, end_tag = self.profile.thinking_tags\n                        texts.append('\\n'.join([start_tag, item.content, end_tag]))\n                    elif isinstance(item, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover\n                        # These are not currently sent back\n                        pass\n                    elif isinstance(item, FilePart):  # pragma: no cover\n                        # Files generated by models are not sent back to models that don't themselves generate files.\n                        pass\n                    else:\n                        assert_never(item)\n                message_param = chat.ChatCompletionAssistantMessageParam(role='assistant')\n                if texts:\n                    # Note: model responses from this model should only have one text item, so the following\n                    # shouldn't merge multiple texts into one unless you switch models between runs:\n                    message_param['content'] = '\\n\\n'.join(texts)\n                if tool_calls:\n                    message_param['tool_calls'] = tool_calls\n                groq_messages.append(message_param)\n            else:\n                assert_never(message)\n        if instructions := self._get_instructions(messages, model_request_parameters):\n            groq_messages.insert(0, chat.ChatCompletionSystemMessageParam(role='system', content=instructions))\n        return groq_messages\n\n    @staticmethod\n    def _map_tool_call(t: ToolCallPart) -> chat.ChatCompletionMessageToolCallParam:\n        return chat.ChatCompletionMessageToolCallParam(\n            id=_guard_tool_call_id(t=t),\n            type='function',\n            function={'name': t.tool_name, 'arguments': t.args_as_json_str()},\n        )\n\n    @staticmethod\n    def _map_tool_definition(f: ToolDefinition) -> chat.ChatCompletionToolParam:\n        return {\n            'type': 'function',\n            'function': {\n                'name': f.name,\n                'description': f.description or '',\n                'parameters': f.parameters_json_schema,\n            },\n        }\n\n    def _map_json_schema(self, o: OutputObjectDefinition) -> chat.completion_create_params.ResponseFormat:\n        response_format_param: chat.completion_create_params.ResponseFormatResponseFormatJsonSchema = {\n            'type': 'json_schema',\n            'json_schema': {\n                'name': o.name or DEFAULT_OUTPUT_TOOL_NAME,\n                'schema': o.json_schema,\n                'strict': o.strict,\n            },\n        }\n        if o.description:  # pragma: no branch\n            response_format_param['json_schema']['description'] = o.description\n        return response_format_param\n\n    @classmethod\n    def _map_user_message(cls, message: ModelRequest) -> Iterable[chat.ChatCompletionMessageParam]:\n        for part in message.parts:\n            if isinstance(part, SystemPromptPart):\n                yield chat.ChatCompletionSystemMessageParam(role='system', content=part.content)\n            elif isinstance(part, UserPromptPart):\n                yield cls._map_user_prompt(part)\n            elif isinstance(part, ToolReturnPart):\n                yield chat.ChatCompletionToolMessageParam(\n                    role='tool',\n                    tool_call_id=_guard_tool_call_id(t=part),\n                    content=part.model_response_str(),\n                )\n            elif isinstance(part, RetryPromptPart):  # pragma: no branch\n                if part.tool_name is None:\n                    yield chat.ChatCompletionUserMessageParam(  # pragma: no cover\n                        role='user', content=part.model_response()\n                    )\n                else:\n                    yield chat.ChatCompletionToolMessageParam(\n                        role='tool',\n                        tool_call_id=_guard_tool_call_id(t=part),\n                        content=part.model_response(),\n                    )\n\n    @staticmethod\n    def _map_user_prompt(part: UserPromptPart) -> chat.ChatCompletionUserMessageParam:\n        content: str | list[chat.ChatCompletionContentPartParam]\n        if isinstance(part.content, str):\n            content = part.content\n        else:\n            content = []\n            for item in part.content:\n                if isinstance(item, str):\n                    content.append(chat.ChatCompletionContentPartTextParam(text=item, type='text'))\n                elif isinstance(item, ImageUrl):\n                    image_url = ImageURL(url=item.url)\n                    content.append(chat.ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))\n                elif isinstance(item, BinaryContent):\n                    if item.is_image:\n                        image_url = ImageURL(url=item.data_uri)\n                        content.append(chat.ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))\n                    else:\n                        raise RuntimeError('Only images are supported for binary content in Groq.')\n                elif isinstance(item, DocumentUrl):  # pragma: no cover\n                    raise RuntimeError('DocumentUrl is not supported in Groq.')\n                else:  # pragma: no cover\n                    raise RuntimeError(f'Unsupported content type: {type(item)}')\n\n        return chat.ChatCompletionUserMessageParam(role='user', content=content)\n\n```\n\n#### __init__\n\n```python\n__init__(\n    model_name: GroqModelName,\n    *,\n    provider: (\n        Literal[\"groq\", \"gateway\"] | Provider[AsyncGroq]\n    ) = \"groq\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize a Groq model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model_name` | `GroqModelName` | The name of the Groq model to use. List of model names available here. | *required* | | `provider` | `Literal['groq', 'gateway'] | Provider[AsyncGroq]` | The provider to use for authentication and API access. Can be either the string 'groq' or an instance of Provider[AsyncGroq]. If not provided, a new provider will be created using the other parameters. | `'groq'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` | | `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\n\n```python\ndef __init__(\n    self,\n    model_name: GroqModelName,\n    *,\n    provider: Literal['groq', 'gateway'] | Provider[AsyncGroq] = 'groq',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize a Groq model.\n\n    Args:\n        model_name: The name of the Groq model to use. List of model names available\n            [here](https://console.groq.com/docs/models).\n        provider: The provider to use for authentication and API access. Can be either the string\n            'groq' or an instance of `Provider[AsyncGroq]`. If not provided, a new provider will be\n            created using the other parameters.\n        profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n        settings: Model-specific settings that will be used as defaults for this model.\n    \"\"\"\n    self._model_name = model_name\n\n    if isinstance(provider, str):\n        provider = infer_provider('gateway/groq' if provider == 'gateway' else provider)\n    self._provider = provider\n    self.client = provider.client\n\n    super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n```\n\n#### model_name\n\n```python\nmodel_name: GroqModelName\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe model provider.\n\n### GroqStreamedResponse\n\nBases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for Groq models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\n\n```python\n@dataclass\nclass GroqStreamedResponse(StreamedResponse):\n    \"\"\"Implementation of `StreamedResponse` for Groq models.\"\"\"\n\n    _model_name: GroqModelName\n    _model_profile: ModelProfile\n    _response: AsyncIterable[chat.ChatCompletionChunk]\n    _timestamp: datetime\n    _provider_name: str\n\n    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:  # noqa: C901\n        try:\n            executed_tool_call_id: str | None = None\n            reasoning_index = 0\n            reasoning = False\n            async for chunk in self._response:\n                self._usage += _map_usage(chunk)\n\n                if chunk.id:  # pragma: no branch\n                    self.provider_response_id = chunk.id\n\n                try:\n                    choice = chunk.choices[0]\n                except IndexError:\n                    continue\n\n                if raw_finish_reason := choice.finish_reason:\n                    self.provider_details = {'finish_reason': raw_finish_reason}\n                    self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n\n                if choice.delta.reasoning is not None:\n                    if not reasoning:\n                        reasoning_index += 1\n                        reasoning = True\n\n                    # NOTE: The `reasoning` field is only present if `groq_reasoning_format` is set to `parsed`.\n                    yield self._parts_manager.handle_thinking_delta(\n                        vendor_part_id=f'reasoning-{reasoning_index}', content=choice.delta.reasoning\n                    )\n                else:\n                    reasoning = False\n\n                if choice.delta.executed_tools:\n                    for tool in choice.delta.executed_tools:\n                        call_part, return_part = _map_executed_tool(\n                            tool, self.provider_name, streaming=True, tool_call_id=executed_tool_call_id\n                        )\n                        if call_part:\n                            executed_tool_call_id = call_part.tool_call_id\n                            yield self._parts_manager.handle_part(\n                                vendor_part_id=f'executed_tools-{tool.index}-call', part=call_part\n                            )\n                        if return_part:\n                            executed_tool_call_id = None\n                            yield self._parts_manager.handle_part(\n                                vendor_part_id=f'executed_tools-{tool.index}-return', part=return_part\n                            )\n\n                # Handle the text part of the response\n                content = choice.delta.content\n                if content:\n                    maybe_event = self._parts_manager.handle_text_delta(\n                        vendor_part_id='content',\n                        content=content,\n                        thinking_tags=self._model_profile.thinking_tags,\n                        ignore_leading_whitespace=self._model_profile.ignore_streamed_leading_whitespace,\n                    )\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n\n                # Handle the tool calls\n                for dtc in choice.delta.tool_calls or []:\n                    maybe_event = self._parts_manager.handle_tool_call_delta(\n                        vendor_part_id=dtc.index,\n                        tool_name=dtc.function and dtc.function.name,\n                        args=dtc.function and dtc.function.arguments,\n                        tool_call_id=dtc.id,\n                    )\n                    if maybe_event is not None:\n                        yield maybe_event\n        except APIError as e:\n            if isinstance(e.body, dict):  # pragma: no branch\n                # The Groq SDK tries to be helpful by raising an exception when generated tool arguments don't match the schema,\n                # but we'd rather handle it ourselves so we can tell the model to retry the tool call\n                try:\n                    error = _GroqToolUseFailedInnerError.model_validate(e.body)  # pyright: ignore[reportUnknownMemberType]\n                    yield self._parts_manager.handle_tool_call_part(\n                        vendor_part_id='tool_use_failed',\n                        tool_name=error.failed_generation.name,\n                        args=error.failed_generation.arguments,\n                    )\n                    return\n                except ValidationError as e:  # pragma: no cover\n                    pass\n            raise  # pragma: no cover\n\n    @property\n    def model_name(self) -> GroqModelName:\n        \"\"\"Get the model name of the response.\"\"\"\n        return self._model_name\n\n    @property\n    def provider_name(self) -> str:\n        \"\"\"Get the provider name.\"\"\"\n        return self._provider_name\n\n    @property\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._timestamp\n\n```\n\n#### model_name\n\n```python\nmodel_name: GroqModelName\n\n```\n\nGet the model name of the response.\n\n#### provider_name\n\n```python\nprovider_name: str\n\n```\n\nGet the provider name.\n\n#### timestamp\n\n```python\ntimestamp: datetime\n\n```\n\nGet the timestamp of the response.",
  "content_length": 28906
}