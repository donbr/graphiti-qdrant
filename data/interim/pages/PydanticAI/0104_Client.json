{
  "title": "Client",
  "source_url": null,
  "content": "Pydantic AI can act as an [MCP client](https://modelcontextprotocol.io/quickstart/client), connecting to MCP servers to use their tools.\n\n## Install\n\nYou need to either install [`pydantic-ai`](../../install/), or [`pydantic-ai-slim`](../../install/#slim-install) with the `mcp` optional group:\n\n```bash\npip install \"pydantic-ai-slim[mcp]\"\n\n```\n\n```bash\nuv add \"pydantic-ai-slim[mcp]\"\n\n```\n\n## Usage\n\nPydantic AI comes with three ways to connect to MCP servers:\n\n- MCPServerStreamableHTTP which connects to an MCP server using the [Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport\n- MCPServerSSE which connects to an MCP server using the [HTTP SSE](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) transport\n- MCPServerStdio which runs the server as a subprocess and connects to it using the [stdio](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) transport\n\nExamples of all three are shown below.\n\nEach MCP server instance is a [toolset](../../toolsets/) and can be registered with an Agent using the `toolsets` argument.\n\nYou can use the async with agent context manager to open and close connections to all registered servers (and in the case of stdio servers, start and stop the subprocesses) around the context where they'll be used in agent runs. You can also use async with server to manage the connection or subprocess of a specific server, for example if you'd like to use it with multiple agents. If you don't explicitly enter one of these context managers to set up the server, this will be done automatically when it's needed (e.g. to list the available tools or call a specific tool), but it's more efficient to do so around the entire context where you expect the servers to be used.\n\n### Streamable HTTP Client\n\nMCPServerStreamableHTTP connects over HTTP using the [Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport to a server.\n\nNote\n\nMCPServerStreamableHTTP requires an MCP server to be running and accepting HTTP connections before running the agent. Running the server is not managed by Pydantic AI.\n\nBefore creating the Streamable HTTP client, we need to run a server that supports the Streamable HTTP transport.\n\nstreamable_http_server.py\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP()\n\n@app.tool()\ndef add(a: int, b: int) -> int:\n    return a + b\n\nif __name__ == '__main__':\n    app.run(transport='streamable-http')\n\n```\n\nThen we can create the client:\n\n[Learn about Gateway](../../gateway) mcp_streamable_http_client.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')  # (1)!\nagent = Agent('gateway/openai:gpt-5', toolsets=[server])  # (2)!\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n\n```\n\n1. Define the MCP server with the URL used to connect.\n1. Create an agent with the MCP server attached.\n\nmcp_streamable_http_client.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')  # (1)!\nagent = Agent('openai:gpt-5', toolsets=[server])  # (2)!\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n\n```\n\n1. Define the MCP server with the URL used to connect.\n1. Create an agent with the MCP server attached.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\n**What's happening here?**\n\n- The model receives the prompt \"What is 7 plus 5?\"\n- The model decides \"Oh, I've got this `add` tool, that will be a good way to answer this question\"\n- The model returns a tool call\n- Pydantic AI sends the tool call to the MCP server using the Streamable HTTP transport\n- The model is called again with the return value of running the `add` tool (12)\n- The model returns the final answer\n\nYou can visualise this clearly, and even see the tool call, by adding three lines of code to instrument the example with [logfire](https://logfire.pydantic.dev/docs):\n\nmcp_sse_client_logfire.py\n\n```python\nimport logfire\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\n\n```\n\n### SSE Client\n\nMCPServerSSE connects over HTTP using the [HTTP + Server Sent Events transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) to a server.\n\nNote\n\nThe SSE transport in MCP is deprecated, you should use Streamable HTTP instead.\n\nBefore creating the SSE client, we need to run a server that supports the SSE transport.\n\nsse_server.py\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP()\n\n@app.tool()\ndef add(a: int, b: int) -> int:\n    return a + b\n\nif __name__ == '__main__':\n    app.run(transport='sse')\n\n```\n\nThen we can create the client:\n\n[Learn about Gateway](../../gateway) mcp_sse_client.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\nserver = MCPServerSSE('http://localhost:3001/sse')  # (1)!\nagent = Agent('gateway/openai:gpt-5', toolsets=[server])  # (2)!\n\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n\n```\n\n1. Define the MCP server with the URL used to connect.\n1. Create an agent with the MCP server attached.\n\nmcp_sse_client.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\nserver = MCPServerSSE('http://localhost:3001/sse')  # (1)!\nagent = Agent('openai:gpt-5', toolsets=[server])  # (2)!\n\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n\n```\n\n1. Define the MCP server with the URL used to connect.\n1. Create an agent with the MCP server attached.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\n### MCP \"stdio\" Server\n\nMCP also offers [stdio transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) where the server is run as a subprocess and communicates with the client over `stdin` and `stdout`. In this case, you'd use the MCPServerStdio class.\n\nIn this example [mcp-run-python](https://github.com/pydantic/mcp-run-python) is used as the MCP server.\n\n[Learn about Gateway](../../gateway) mcp_stdio_client.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(  # (1)!\n    'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10\n)\nagent = Agent('gateway/openai:gpt-5', toolsets=[server])\n\n\nasync def main():\n    result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n\n```\n\n1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.\n\nmcp_stdio_client.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(  # (1)!\n    'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10\n)\nagent = Agent('openai:gpt-5', toolsets=[server])\n\n\nasync def main():\n    result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n\n```\n\n1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.\n\n## Loading MCP Servers from Configuration\n\nInstead of creating MCP server instances individually in code, you can load multiple servers from a JSON configuration file using load_mcp_servers().\n\nThis is particularly useful when you need to manage multiple MCP servers or want to configure servers externally without modifying code.\n\n### Configuration Format\n\nThe configuration file should be a JSON file with an `mcpServers` object containing server definitions. Each server is identified by a unique key and contains the configuration for that server type:\n\nmcp_config.json\n\n```json\n{\n  \"mcpServers\": {\n    \"python-runner\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"mcp-run-python\", \"stdio\"]\n    },\n    \"weather-api\": {\n      \"url\": \"http://localhost:3001/sse\"\n    },\n    \"calculator\": {\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n\n```\n\nNote\n\nThe MCP server is only inferred to be an SSE server because of the `/sse` suffix. Any other server with the \"url\" field will be inferred to be a Streamable HTTP server.\n\nWe made this decision given that the SSE transport is deprecated.\n\n### Environment Variables\n\nThe configuration file supports environment variable expansion using the `${VAR}` and `${VAR:-default}` syntax, [like Claude Code](https://code.claude.com/docs/en/mcp#environment-variable-expansion-in-mcp-json). This is useful for keeping sensitive information like API keys or host names out of your configuration files:\n\nmcp_config_with_env.json\n\n```json\n{\n  \"mcpServers\": {\n    \"python-runner\": {\n      \"command\": \"${PYTHON_CMD:-python3}\",\n      \"args\": [\"run\", \"${MCP_MODULE}\", \"stdio\"],\n      \"env\": {\n        \"API_KEY\": \"${MY_API_KEY}\"\n      }\n    },\n    \"weather-api\": {\n      \"url\": \"https://${SERVER_HOST:-localhost}:${SERVER_PORT:-8080}/sse\"\n    }\n  }\n}\n\n```\n\nWhen loading this configuration with load_mcp_servers():\n\n- `${VAR}` references will be replaced with the corresponding environment variable values.\n- `${VAR:-default}` references will use the environment variable value if set, otherwise the default value.\n\nWarning\n\nIf a referenced environment variable using `${VAR}` syntax is not defined, a `ValueError` will be raised. Use the `${VAR:-default}` syntax to provide a fallback value.\n\n### Usage\n\n[Learn about Gateway](../../gateway) mcp_config_loader.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import load_mcp_servers\n\n### Load all servers from configuration file\nservers = load_mcp_servers('mcp_config.json')\n\n### Create agent with all loaded servers\nagent = Agent('gateway/openai:gpt-5', toolsets=servers)\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n\n```\n\nmcp_config_loader.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import load_mcp_servers\n\n### Load all servers from configuration file\nservers = load_mcp_servers('mcp_config.json')\n\n### Create agent with all loaded servers\nagent = Agent('openai:gpt-5', toolsets=servers)\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\n## Tool call customization\n\nThe MCP servers provide the ability to set a `process_tool_call` which allows the customization of tool call requests and their responses.\n\nA common use case for this is to inject metadata to the requests which the server call needs:\n\nmcp_process_tool_call.py\n\n```python\nfrom typing import Any\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.mcp import CallToolFunc, MCPServerStdio, ToolResult\nfrom pydantic_ai.models.test import TestModel\n\n\nasync def process_tool_call(\n    ctx: RunContext[int],\n    call_tool: CallToolFunc,\n    name: str,\n    tool_args: dict[str, Any],\n) -> ToolResult:\n    \"\"\"A tool call processor that passes along the deps.\"\"\"\n    return await call_tool(name, tool_args, {'deps': ctx.deps})\n\n\nserver = MCPServerStdio('python', args=['mcp_server.py'], process_tool_call=process_tool_call)\nagent = Agent(\n    model=TestModel(call_tools=['echo_deps']),\n    deps_type=int,\n    toolsets=[server]\n)\n\n\nasync def main():\n    result = await agent.run('Echo with deps set to 42', deps=42)\n    print(result.output)\n    #> {\"echo_deps\":{\"echo\":\"This is an echo message\",\"deps\":42}}\n\n```\n\nHow to access the metadata is MCP server SDK specific. For example with the [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk), it is accessible via the [`ctx: Context`](https://github.com/modelcontextprotocol/python-sdk#context) argument that can be included on tool call handlers:\n\nmcp_server.py\n\n```python\nfrom typing import Any\n\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.server.session import ServerSession\n\nmcp = FastMCP('Pydantic AI MCP Server')\nlog_level = 'unset'\n\n\n@mcp.tool()\nasync def echo_deps(ctx: Context[ServerSession, None]) -> dict[str, Any]:\n    \"\"\"Echo the run context.\n\n    Args:\n        ctx: Context object containing request and session information.\n\n    Returns:\n        Dictionary with an echo message and the deps.\n    \"\"\"\n    await ctx.info('This is an info message')\n\n    deps: Any = getattr(ctx.request_context.meta, 'deps')\n    return {'echo': 'This is an echo message', 'deps': deps}\n\nif __name__ == '__main__':\n    mcp.run()\n\n```\n\n## Using Tool Prefixes to Avoid Naming Conflicts\n\nWhen connecting to multiple MCP servers that might provide tools with the same name, you can use the `tool_prefix` parameter to avoid naming conflicts. This parameter adds a prefix to all tool names from a specific server.\n\nThis allows you to use multiple servers that might have overlapping tool names without conflicts:\n\n[Learn about Gateway](../../gateway) mcp_tool_prefix_http_client.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\n### Create two servers with different prefixes\nweather_server = MCPServerSSE(\n    'http://localhost:3001/sse',\n    tool_prefix='weather'  # Tools will be prefixed with 'weather_'\n)\n\ncalculator_server = MCPServerSSE(\n    'http://localhost:3002/sse',\n    tool_prefix='calc'  # Tools will be prefixed with 'calc_'\n)\n\n### Both servers might have a tool named 'get_data', but they'll be exposed as:\n### - 'weather_get_data'\n### - 'calc_get_data'\nagent = Agent('gateway/openai:gpt-5', toolsets=[weather_server, calculator_server])\n\n```\n\nmcp_tool_prefix_http_client.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\n### Create two servers with different prefixes\nweather_server = MCPServerSSE(\n    'http://localhost:3001/sse',\n    tool_prefix='weather'  # Tools will be prefixed with 'weather_'\n)\n\ncalculator_server = MCPServerSSE(\n    'http://localhost:3002/sse',\n    tool_prefix='calc'  # Tools will be prefixed with 'calc_'\n)\n\n### Both servers might have a tool named 'get_data', but they'll be exposed as:\n### - 'weather_get_data'\n### - 'calc_get_data'\nagent = Agent('openai:gpt-5', toolsets=[weather_server, calculator_server])\n\n```\n\n## Server Instructions\n\nMCP servers can provide instructions during initialization that give context about how to best interact with the server's tools. These instructions are accessible via the instructions property after the server connection is established.\n\n[Learn about Gateway](../../gateway) mcp_server_instructions.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')\nagent = Agent('gateway/openai:gpt-5', toolsets=[server])\n\n@agent.instructions\nasync def mcp_server_instructions():\n    return server.instructions  # (1)!\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n\n```\n\n1. The server connection is guaranteed to be established by this point, so `server.instructions` is available.\n\nmcp_server_instructions.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')\nagent = Agent('openai:gpt-5', toolsets=[server])\n\n@agent.instructions\nasync def mcp_server_instructions():\n    return server.instructions  # (1)!\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n\n```\n\n1. The server connection is guaranteed to be established by this point, so `server.instructions` is available.\n\n## Tool metadata\n\nMCP tools can include metadata that provides additional information about the tool's characteristics, which can be useful when filtering tools. The `meta`, `annotations`, and `output_schema` fields can be found on the `metadata` dict on the ToolDefinition object that's passed to filter functions.\n\n## Resources\n\nMCP servers can provide [resources](https://modelcontextprotocol.io/docs/concepts/resources) - files, data, or content that can be accessed by the client. Resources in MCP are application-driven, with host applications determining how to incorporate context manually, based on their needs. This means they will *not* be exposed to the LLM automatically (unless a tool returns a `ResourceLink` or `EmbeddedResource`).\n\nPydantic AI provides methods to discover and read resources from MCP servers:\n\n- list_resources() - List all available resources on the server\n- list_resource_templates() - List resource templates with parameter placeholders\n- read_resource(uri) - Read the contents of a specific resource by URI\n\nResources are automatically converted: text content is returned as `str`, and binary content is returned as BinaryContent.\n\nBefore consuming resources, we need to run a server that exposes some:\n\nmcp_resource_server.py\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP('Pydantic AI MCP Server')\nlog_level = 'unset'\n\n\n@mcp.resource('resource://user_name.txt', mime_type='text/plain')\nasync def user_name_resource() -> str:\n    return 'Alice'\n\n\nif __name__ == '__main__':\n    mcp.run()\n\n```\n\nThen we can create the client:\n\nmcp_resources.py\n\n```python\nimport asyncio\n\nfrom pydantic_ai.mcp import MCPServerStdio\n\n\nasync def main():\n    server = MCPServerStdio('python', args=['-m', 'mcp_resource_server'])\n\n    async with server:\n        # List all available resources\n        resources = await server.list_resources()\n        for resource in resources:\n            print(f' - {resource.name}: {resource.uri} ({resource.mime_type})')\n            #>  - user_name_resource: resource://user_name.txt (text/plain)\n\n        # Read a text resource\n        user_name = await server.read_resource('resource://user_name.txt')\n        print(f'Text content: {user_name}')\n        #> Text content: Alice\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\n## Custom TLS / SSL configuration\n\nIn some environments you need to tweak how HTTPS connections are established – for example to trust an internal Certificate Authority, present a client certificate for **mTLS**, or (during local development only!) disable certificate verification altogether. All HTTP-based MCP client classes (MCPServerStreamableHTTP and MCPServerSSE) expose an `http_client` parameter that lets you pass your own pre-configured [`httpx.AsyncClient`](https://www.python-httpx.org/async/).\n\n[Learn about Gateway](../../gateway) mcp_custom_tls_client.py\n\n```python\nimport ssl\n\nimport httpx\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\n### Trust an internal / self-signed CA\nssl_ctx = ssl.create_default_context(cafile='/etc/ssl/private/my_company_ca.pem')\n\n### OPTIONAL: if the server requires **mutual TLS** load your client certificate\nssl_ctx.load_cert_chain(certfile='/etc/ssl/certs/client.crt', keyfile='/etc/ssl/private/client.key',)\n\nhttp_client = httpx.AsyncClient(\n    verify=ssl_ctx,\n    timeout=httpx.Timeout(10.0),\n)\n\nserver = MCPServerSSE(\n    'http://localhost:3001/sse',\n    http_client=http_client,  # (1)!\n)\nagent = Agent('gateway/openai:gpt-5', toolsets=[server])\n\nasync def main():\n    result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n\n```\n\n1. When you supply `http_client`, Pydantic AI re-uses this client for every request. Anything supported by **httpx** (`verify`, `cert`, custom proxies, timeouts, etc.) therefore applies to all MCP traffic.\n\nmcp_custom_tls_client.py\n\n```python\nimport ssl\n\nimport httpx\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\n### Trust an internal / self-signed CA\nssl_ctx = ssl.create_default_context(cafile='/etc/ssl/private/my_company_ca.pem')\n\n### OPTIONAL: if the server requires **mutual TLS** load your client certificate\nssl_ctx.load_cert_chain(certfile='/etc/ssl/certs/client.crt', keyfile='/etc/ssl/private/client.key',)\n\nhttp_client = httpx.AsyncClient(\n    verify=ssl_ctx,\n    timeout=httpx.Timeout(10.0),\n)\n\nserver = MCPServerSSE(\n    'http://localhost:3001/sse',\n    http_client=http_client,  # (1)!\n)\nagent = Agent('openai:gpt-5', toolsets=[server])\n\nasync def main():\n    result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n\n```\n\n1. When you supply `http_client`, Pydantic AI re-uses this client for every request. Anything supported by **httpx** (`verify`, `cert`, custom proxies, timeouts, etc.) therefore applies to all MCP traffic.\n\n## MCP Sampling\n\nWhat is MCP Sampling?\n\nIn MCP [sampling](https://modelcontextprotocol.io/docs/concepts/sampling) is a system by which an MCP server can make LLM calls via the MCP client - effectively proxying requests to an LLM via the client over whatever transport is being used.\n\nSampling is extremely useful when MCP servers need to use Gen AI but you don't want to provision them each with their own LLM credentials or when a public MCP server would like the connecting client to pay for LLM calls.\n\nConfusingly it has nothing to do with the concept of \"sampling\" in observability, or frankly the concept of \"sampling\" in any other domain.\n\nSampling Diagram\n\nHere's a mermaid diagram that may or may not make the data flow clearer:\n\n```\nsequenceDiagram\n    participant LLM\n    participant MCP_Client as MCP client\n    participant MCP_Server as MCP server\n\n    MCP_Client->>LLM: LLM call\n    LLM->>MCP_Client: LLM tool call response\n\n    MCP_Client->>MCP_Server: tool call\n    MCP_Server->>MCP_Client: sampling \"create message\"\n\n    MCP_Client->>LLM: LLM call\n    LLM->>MCP_Client: LLM text response\n\n    MCP_Client->>MCP_Server: sampling response\n    MCP_Server->>MCP_Client: tool call response\n```\n\nPydantic AI supports sampling as both a client and server. See the [server](../server/#mcp-sampling) documentation for details on how to use sampling within a server.\n\nSampling is automatically supported by Pydantic AI agents when they act as a client.\n\nTo be able to use sampling, an MCP server instance needs to have a sampling_model set. This can be done either directly on the server using the constructor keyword argument or the property, or by using agent.set_mcp_sampling_model() to set the agent's model or one specified as an argument as the sampling model on all MCP servers registered with that agent.\n\nLet's say we have an MCP server that wants to use sampling (in this case to generate an SVG as per the tool arguments).\n\nSampling MCP Server generate_svg.py\n\n````python\nimport re\nfrom pathlib import Path\n\nfrom mcp import SamplingMessage\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.types import TextContent\n\napp = FastMCP()\n\n\n@app.tool()\nasync def image_generator(ctx: Context, subject: str, style: str) -> str:\n    prompt = f'{subject=} {style=}'\n    # `ctx.session.create_message` is the sampling call\n    result = await ctx.session.create_message(\n        [SamplingMessage(role='user', content=TextContent(type='text', text=prompt))],\n        max_tokens=1_024,\n        system_prompt='Generate an SVG image as per the user input',\n    )\n    assert isinstance(result.content, TextContent)\n\n    path = Path(f'{subject}_{style}.svg')\n    # remove triple backticks if the svg was returned within markdown\n    if m := re.search(r'^```\\w*$(.+?)```$', result.content.text, re.S | re.M):\n        path.write_text(m.group(1), encoding='utf-8')\n    else:\n        path.write_text(result.content.text, encoding='utf-8')\n    return f'See {path}'\n\n\nif __name__ == '__main__':\n    # run the server via stdio\n    app.run()\n\n````\n\nUsing this server with an `Agent` will automatically allow sampling:\n\nsampling_mcp_client.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio('python', args=['generate_svg.py'])\nagent = Agent('openai:gpt-5', toolsets=[server])\n\n\nasync def main():\n    agent.set_mcp_sampling_model()\n    result = await agent.run('Create an image of a robot in a punk style.')\n    print(result.output)\n    #> Image file written to robot_punk.svg.\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nYou can disallow sampling by setting allow_sampling=False when creating the server reference, e.g.:\n\nsampling_disallowed.py\n\n```python\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(\n    'python',\n    args=['generate_svg.py'],\n    allow_sampling=False,\n)\n\n```\n\n## Elicitation\n\nIn MCP, [elicitation](https://modelcontextprotocol.io/docs/concepts/elicitation) allows a server to request for [structured input](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) from the client for missing or additional context during a session.\n\nElicitation let models essentially say \"Hold on - I need to know X before i can continue\" rather than requiring everything upfront or taking a shot in the dark.\n\n### How Elicitation works\n\nElicitation introduces a new protocol message type called [`ElicitRequest`](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitrequest), which is sent from the server to the client when it needs additional information. The client can then respond with an [`ElicitResult`](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitresult) or an `ErrorData` message.\n\nHere's a typical interaction:\n\n- User makes a request to the MCP server (e.g. \"Book a table at that Italian place\")\n- The server identifies that it needs more information (e.g. \"Which Italian place?\", \"What date and time?\")\n- The server sends an `ElicitRequest` to the client asking for the missing information.\n- The client receives the request, presents it to the user (e.g. via a terminal prompt, GUI dialog, or web interface).\n- User provides the requested information, `decline` or `cancel` the request.\n- The client sends an `ElicitResult` back to the server with the user's response.\n- With the structured data, the server can continue processing the original request.\n\nThis allows for a more interactive and user-friendly experience, especially for multi-staged workflows. Instead of requiring all information upfront, the server can ask for it as needed, making the interaction feel more natural.\n\n### Setting up Elicitation\n\nTo enable elicitation, provide an elicitation_callback function when creating your MCP server instance:\n\nrestaurant_server.py\n\n```python\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom pydantic import BaseModel, Field\n\nmcp = FastMCP(name='Restaurant Booking')\n\n\nclass BookingDetails(BaseModel):\n    \"\"\"Schema for restaurant booking information.\"\"\"\n\n    restaurant: str = Field(description='Choose a restaurant')\n    party_size: int = Field(description='Number of people', ge=1, le=8)\n    date: str = Field(description='Reservation date (DD-MM-YYYY)')\n\n\n@mcp.tool()\nasync def book_table(ctx: Context) -> str:\n    \"\"\"Book a restaurant table with user input.\"\"\"\n    # Ask user for booking details using Pydantic schema\n    result = await ctx.elicit(message='Please provide your booking details:', schema=BookingDetails)\n\n    if result.action == 'accept' and result.data:\n        booking = result.data\n        return f'✅ Booked table for {booking.party_size} at {booking.restaurant} on {booking.date}'\n    elif result.action == 'decline':\n        return 'No problem! Maybe another time.'\n    else:  # cancel\n        return 'Booking cancelled.'\n\n\nif __name__ == '__main__':\n    mcp.run(transport='stdio')\n\n```\n\nThis server demonstrates elicitation by requesting structured booking details from the client when the `book_table` tool is called. Here's how to create a client that handles these elicitation requests:\n\n[Learn about Gateway](../../gateway) client_example.py\n\n```python\nimport asyncio\nfrom typing import Any\n\nfrom mcp.client.session import ClientSession\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import ElicitRequestParams, ElicitResult\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\n\nasync def handle_elicitation(\n    context: RequestContext[ClientSession, Any, Any],\n    params: ElicitRequestParams,\n) -> ElicitResult:\n    \"\"\"Handle elicitation requests from MCP server.\"\"\"\n    print(f'\\n{params.message}')\n\n    if not params.requestedSchema:\n        response = input('Response: ')\n        return ElicitResult(action='accept', content={'response': response})\n\n    # Collect data for each field\n    properties = params.requestedSchema['properties']\n    data = {}\n\n    for field, info in properties.items():\n        description = info.get('description', field)\n\n        value = input(f'{description}: ')\n\n        # Convert to proper type based on JSON schema\n        if info.get('type') == 'integer':\n            data[field] = int(value)\n        else:\n            data[field] = value\n\n    # Confirm\n    confirm = input('\\nConfirm booking? (y/n/c): ').lower()\n\n    if confirm == 'y':\n        print('Booking details:', data)\n        return ElicitResult(action='accept', content=data)\n    elif confirm == 'n':\n        return ElicitResult(action='decline')\n    else:\n        return ElicitResult(action='cancel')\n\n\n### Set up MCP server connection\nrestaurant_server = MCPServerStdio(\n    'python', args=['restaurant_server.py'], elicitation_callback=handle_elicitation\n)\n\n### Create agent\nagent = Agent('gateway/openai:gpt-5', toolsets=[restaurant_server])\n\n\nasync def main():\n    \"\"\"Run the agent to book a restaurant table.\"\"\"\n    result = await agent.run('Book me a table')\n    print(f'\\nResult: {result.output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\nclient_example.py\n\n```python\nimport asyncio\nfrom typing import Any\n\nfrom mcp.client.session import ClientSession\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import ElicitRequestParams, ElicitResult\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\n\nasync def handle_elicitation(\n    context: RequestContext[ClientSession, Any, Any],\n    params: ElicitRequestParams,\n) -> ElicitResult:\n    \"\"\"Handle elicitation requests from MCP server.\"\"\"\n    print(f'\\n{params.message}')\n\n    if not params.requestedSchema:\n        response = input('Response: ')\n        return ElicitResult(action='accept', content={'response': response})\n\n    # Collect data for each field\n    properties = params.requestedSchema['properties']\n    data = {}\n\n    for field, info in properties.items():\n        description = info.get('description', field)\n\n        value = input(f'{description}: ')\n\n        # Convert to proper type based on JSON schema\n        if info.get('type') == 'integer':\n            data[field] = int(value)\n        else:\n            data[field] = value\n\n    # Confirm\n    confirm = input('\\nConfirm booking? (y/n/c): ').lower()\n\n    if confirm == 'y':\n        print('Booking details:', data)\n        return ElicitResult(action='accept', content=data)\n    elif confirm == 'n':\n        return ElicitResult(action='decline')\n    else:\n        return ElicitResult(action='cancel')\n\n\n### Set up MCP server connection\nrestaurant_server = MCPServerStdio(\n    'python', args=['restaurant_server.py'], elicitation_callback=handle_elicitation\n)\n\n### Create agent\nagent = Agent('openai:gpt-5', toolsets=[restaurant_server])\n\n\nasync def main():\n    \"\"\"Run the agent to book a restaurant table.\"\"\"\n    result = await agent.run('Book me a table')\n    print(f'\\nResult: {result.output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\n### Supported Schema Types\n\nMCP elicitation supports string, number, boolean, and enum types with flat object structures only. These limitations ensure reliable cross-client compatibility. See [supported schema types](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) for details.\n\n### Security\n\nMCP Elicitation requires careful handling - servers must not request sensitive information, and clients must implement user approval controls with clear explanations. See [security considerations](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#security-considerations) for details.",
  "content_length": 32674
}