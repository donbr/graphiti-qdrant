{
  "title": "`pydantic_ai.models.anthropic`",
  "source_url": null,
  "content": "## Setup\n\nFor details on how to set up authentication with this model, see [model configuration for Anthropic](../../../models/anthropic/).\n\n### LatestAnthropicModelNames\n\n```python\nLatestAnthropicModelNames = ModelParam\n\n```\n\nLatest Anthropic models.\n\n### AnthropicModelName\n\n```python\nAnthropicModelName = str | LatestAnthropicModelNames\n\n```\n\nPossible Anthropic model names.\n\nSince Anthropic supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Anthropic docs](https://docs.anthropic.com/en/docs/about-claude/models) for a full list.\n\n### AnthropicModelSettings\n\nBases: `ModelSettings`\n\nSettings used for an Anthropic model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\n\n```python\nclass AnthropicModelSettings(ModelSettings, total=False):\n    \"\"\"Settings used for an Anthropic model request.\"\"\"\n\n    # ALL FIELDS MUST BE `anthropic_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\n    anthropic_metadata: BetaMetadataParam\n    \"\"\"An object describing metadata about the request.\n\n    Contains `user_id`, an external identifier for the user who is associated with the request.\n    \"\"\"\n\n    anthropic_thinking: BetaThinkingConfigParam\n    \"\"\"Determine whether the model should generate a thinking block.\n\n    See [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for more information.\n    \"\"\"\n\n    anthropic_cache_tool_definitions: bool | Literal['5m', '1h']\n    \"\"\"Whether to add `cache_control` to the last tool definition.\n\n    When enabled, the last tool in the `tools` array will have `cache_control` set,\n    allowing Anthropic to cache tool definitions and reduce costs.\n    If `True`, uses TTL='5m'. You can also specify '5m' or '1h' directly.\n    See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.\n    \"\"\"\n\n    anthropic_cache_instructions: bool | Literal['5m', '1h']\n    \"\"\"Whether to add `cache_control` to the last system prompt block.\n\n    When enabled, the last system prompt will have `cache_control` set,\n    allowing Anthropic to cache system instructions and reduce costs.\n    If `True`, uses TTL='5m'. You can also specify '5m' or '1h' directly.\n    See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.\n    \"\"\"\n\n    anthropic_cache_messages: bool | Literal['5m', '1h']\n    \"\"\"Convenience setting to enable caching for the last user message.\n\n    When enabled, this automatically adds a cache point to the last content block\n    in the final user message, which is useful for caching conversation history\n    or context in multi-turn conversations.\n    If `True`, uses TTL='5m'. You can also specify '5m' or '1h' directly.\n\n    Note: Uses 1 of Anthropic's 4 available cache points per request. Any additional CachePoint\n    markers in messages will be automatically limited to respect the 4-cache-point maximum.\n    See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.\n    \"\"\"\n\n```\n\n#### anthropic_metadata\n\n```python\nanthropic_metadata: BetaMetadataParam\n\n```\n\nAn object describing metadata about the request.\n\nContains `user_id`, an external identifier for the user who is associated with the request.\n\n#### anthropic_thinking\n\n```python\nanthropic_thinking: BetaThinkingConfigParam\n\n```\n\nDetermine whether the model should generate a thinking block.\n\nSee [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for more information.\n\n#### anthropic_cache_tool_definitions\n\n```python\nanthropic_cache_tool_definitions: bool | Literal[\"5m\", \"1h\"]\n\n```\n\nWhether to add `cache_control` to the last tool definition.\n\nWhen enabled, the last tool in the `tools` array will have `cache_control` set, allowing Anthropic to cache tool definitions and reduce costs. If `True`, uses TTL='5m'. You can also specify '5m' or '1h' directly. See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.\n\n#### anthropic_cache_instructions\n\n```python\nanthropic_cache_instructions: bool | Literal['5m', '1h']\n\n```\n\nWhether to add `cache_control` to the last system prompt block.\n\nWhen enabled, the last system prompt will have `cache_control` set, allowing Anthropic to cache system instructions and reduce costs. If `True`, uses TTL='5m'. You can also specify '5m' or '1h' directly. See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.\n\n#### anthropic_cache_messages\n\n```python\nanthropic_cache_messages: bool | Literal['5m', '1h']\n\n```\n\nConvenience setting to enable caching for the last user message.\n\nWhen enabled, this automatically adds a cache point to the last content block in the final user message, which is useful for caching conversation history or context in multi-turn conversations. If `True`, uses TTL='5m'. You can also specify '5m' or '1h' directly.\n\nNote: Uses 1 of Anthropic's 4 available cache points per request. Any additional CachePoint markers in messages will be automatically limited to respect the 4-cache-point maximum. See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.\n\n### AnthropicModel\n\nBases: `Model`\n\nA model that uses the Anthropic API.\n\nInternally, this uses the [Anthropic Python client](https://github.com/anthropics/anthropic-sdk-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\n\n```python\n@dataclass(init=False)\nclass AnthropicModel(Model):\n    \"\"\"A model that uses the Anthropic API.\n\n    Internally, this uses the [Anthropic Python client](https://github.com/anthropics/anthropic-sdk-python) to interact with the API.\n\n    Apart from `__init__`, all methods are private or match those of the base class.\n    \"\"\"\n\n    client: AsyncAnthropicClient = field(repr=False)\n\n    _model_name: AnthropicModelName = field(repr=False)\n    _provider: Provider[AsyncAnthropicClient] = field(repr=False)\n\n    def __init__(\n        self,\n        model_name: AnthropicModelName,\n        *,\n        provider: Literal['anthropic', 'gateway'] | Provider[AsyncAnthropicClient] = 'anthropic',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize an Anthropic model.\n\n        Args:\n            model_name: The name of the Anthropic model to use. List of model names available\n                [here](https://docs.anthropic.com/en/docs/about-claude/models).\n            provider: The provider to use for the Anthropic API. Can be either the string 'anthropic' or an\n                instance of `Provider[AsyncAnthropicClient]`. Defaults to 'anthropic'.\n            profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n                The default 'anthropic' provider will use the default `..profiles.anthropic.anthropic_model_profile`.\n            settings: Default model settings for this model instance.\n        \"\"\"\n        self._model_name = model_name\n\n        if isinstance(provider, str):\n            provider = infer_provider('gateway/anthropic' if provider == 'gateway' else provider)\n        self._provider = provider\n        self.client = provider.client\n\n        super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.base_url)\n\n    @property\n    def model_name(self) -> AnthropicModelName:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The model provider.\"\"\"\n        return self._provider.name\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._messages_create(\n            messages, False, cast(AnthropicModelSettings, model_settings or {}), model_request_parameters\n        )\n        model_response = self._process_response(response)\n        return model_response\n\n    async def count_tokens(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> usage.RequestUsage:\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n\n        response = await self._messages_count_tokens(\n            messages, cast(AnthropicModelSettings, model_settings or {}), model_request_parameters\n        )\n\n        return usage.RequestUsage(input_tokens=response.input_tokens)\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._messages_create(\n            messages, True, cast(AnthropicModelSettings, model_settings or {}), model_request_parameters\n        )\n        async with response:\n            yield await self._process_streamed_response(response, model_request_parameters)\n\n    def prepare_request(\n        self, model_settings: ModelSettings | None, model_request_parameters: ModelRequestParameters\n    ) -> tuple[ModelSettings | None, ModelRequestParameters]:\n        settings = merge_model_settings(self.settings, model_settings)\n        if (\n            model_request_parameters.output_tools\n            and settings\n            and (thinking := settings.get('anthropic_thinking'))\n            and thinking.get('type') == 'enabled'\n        ):\n            if model_request_parameters.output_mode == 'auto':\n                output_mode = 'native' if self.profile.supports_json_schema_output else 'prompted'\n                model_request_parameters = replace(model_request_parameters, output_mode=output_mode)\n            elif (\n                model_request_parameters.output_mode == 'tool' and not model_request_parameters.allow_text_output\n            ):  # pragma: no branch\n                # This would result in `tool_choice=required`, which Anthropic does not support with thinking.\n                suggested_output_type = 'NativeOutput' if self.profile.supports_json_schema_output else 'PromptedOutput'\n                raise UserError(\n                    f'Anthropic does not support thinking and output tools at the same time. Use `output_type={suggested_output_type}(...)` instead.'\n                )\n\n        if model_request_parameters.output_mode == 'native':\n            assert model_request_parameters.output_object is not None\n            if model_request_parameters.output_object.strict is False:\n                raise UserError(\n                    'Setting `strict=False` on `output_type=NativeOutput(...)` is not allowed for Anthropic models.'\n                )\n            model_request_parameters = replace(\n                model_request_parameters, output_object=replace(model_request_parameters.output_object, strict=True)\n            )\n        return super().prepare_request(model_settings, model_request_parameters)\n\n    @overload\n    async def _messages_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[True],\n        model_settings: AnthropicModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> AsyncStream[BetaRawMessageStreamEvent]:\n        pass\n\n    @overload\n    async def _messages_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[False],\n        model_settings: AnthropicModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> BetaMessage:\n        pass\n\n    async def _messages_create(\n        self,\n        messages: list[ModelMessage],\n        stream: bool,\n        model_settings: AnthropicModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> BetaMessage | AsyncStream[BetaRawMessageStreamEvent]:\n        \"\"\"Calls the Anthropic API to create a message.\n\n        This is the last step before sending the request to the API.\n        Most preprocessing has happened in `prepare_request()`.\n        \"\"\"\n        tools = self._get_tools(model_request_parameters, model_settings)\n        tools, mcp_servers, builtin_tool_betas = self._add_builtin_tools(tools, model_request_parameters)\n\n        tool_choice = self._infer_tool_choice(tools, model_settings, model_request_parameters)\n\n        system_prompt, anthropic_messages = await self._map_message(messages, model_request_parameters, model_settings)\n        self._limit_cache_points(system_prompt, anthropic_messages, tools)\n        output_format = self._native_output_format(model_request_parameters)\n        betas, extra_headers = self._get_betas_and_extra_headers(tools, model_request_parameters, model_settings)\n        betas.update(builtin_tool_betas)\n        try:\n            return await self.client.beta.messages.create(\n                max_tokens=model_settings.get('max_tokens', 4096),\n                system=system_prompt or OMIT,\n                messages=anthropic_messages,\n                model=self._model_name,\n                tools=tools or OMIT,\n                tool_choice=tool_choice or OMIT,\n                mcp_servers=mcp_servers or OMIT,\n                output_format=output_format or OMIT,\n                betas=sorted(betas) or OMIT,\n                stream=stream,\n                thinking=model_settings.get('anthropic_thinking', OMIT),\n                stop_sequences=model_settings.get('stop_sequences', OMIT),\n                temperature=model_settings.get('temperature', OMIT),\n                top_p=model_settings.get('top_p', OMIT),\n                timeout=model_settings.get('timeout', NOT_GIVEN),\n                metadata=model_settings.get('anthropic_metadata', OMIT),\n                extra_headers=extra_headers,\n                extra_body=model_settings.get('extra_body'),\n            )\n        except APIStatusError as e:\n            if (status_code := e.status_code) >= 400:\n                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n            raise ModelAPIError(model_name=self.model_name, message=e.message) from e  # pragma: lax no cover\n        except APIConnectionError as e:\n            raise ModelAPIError(model_name=self.model_name, message=e.message) from e\n\n    def _get_betas_and_extra_headers(\n        self,\n        tools: list[BetaToolUnionParam],\n        model_request_parameters: ModelRequestParameters,\n        model_settings: AnthropicModelSettings,\n    ) -> tuple[set[str], dict[str, str]]:\n        \"\"\"Prepare beta features list and extra headers for API request.\n\n        Handles merging custom `anthropic-beta` header from `extra_headers` into betas set\n        and ensuring `User-Agent` is set.\n        \"\"\"\n        extra_headers = model_settings.get('extra_headers', {})\n        extra_headers.setdefault('User-Agent', get_user_agent())\n\n        betas: set[str] = set()\n\n        has_strict_tools = any(tool.get('strict') for tool in tools)\n\n        if has_strict_tools or model_request_parameters.output_mode == 'native':\n            betas.add('structured-outputs-2025-11-13')\n\n        if beta_header := extra_headers.pop('anthropic-beta', None):\n            betas.update({stripped_beta for beta in beta_header.split(',') if (stripped_beta := beta.strip())})\n\n        return betas, extra_headers\n\n    async def _messages_count_tokens(\n        self,\n        messages: list[ModelMessage],\n        model_settings: AnthropicModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> BetaMessageTokensCount:\n        if isinstance(self.client, AsyncAnthropicBedrock):\n            raise UserError('AsyncAnthropicBedrock client does not support `count_tokens` api.')\n\n        # standalone function to make it easier to override\n        tools = self._get_tools(model_request_parameters, model_settings)\n        tools, mcp_servers, builtin_tool_betas = self._add_builtin_tools(tools, model_request_parameters)\n\n        tool_choice = self._infer_tool_choice(tools, model_settings, model_request_parameters)\n\n        system_prompt, anthropic_messages = await self._map_message(messages, model_request_parameters, model_settings)\n        self._limit_cache_points(system_prompt, anthropic_messages, tools)\n        output_format = self._native_output_format(model_request_parameters)\n        betas, extra_headers = self._get_betas_and_extra_headers(tools, model_request_parameters, model_settings)\n        betas.update(builtin_tool_betas)\n        try:\n            return await self.client.beta.messages.count_tokens(\n                system=system_prompt or OMIT,\n                messages=anthropic_messages,\n                model=self._model_name,\n                tools=tools or OMIT,\n                tool_choice=tool_choice or OMIT,\n                mcp_servers=mcp_servers or OMIT,\n                betas=sorted(betas) or OMIT,\n                output_format=output_format or OMIT,\n                thinking=model_settings.get('anthropic_thinking', OMIT),\n                timeout=model_settings.get('timeout', NOT_GIVEN),\n                extra_headers=extra_headers,\n                extra_body=model_settings.get('extra_body'),\n            )\n        except APIStatusError as e:\n            if (status_code := e.status_code) >= 400:\n                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n            raise ModelAPIError(model_name=self.model_name, message=e.message) from e  # pragma: lax no cover\n        except APIConnectionError as e:\n            raise ModelAPIError(model_name=self.model_name, message=e.message) from e\n\n    def _process_response(self, response: BetaMessage) -> ModelResponse:\n        \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\n        items: list[ModelResponsePart] = []\n        builtin_tool_calls: dict[str, BuiltinToolCallPart] = {}\n        for item in response.content:\n            if isinstance(item, BetaTextBlock):\n                items.append(TextPart(content=item.text))\n            elif isinstance(item, BetaServerToolUseBlock):\n                call_part = _map_server_tool_use_block(item, self.system)\n                builtin_tool_calls[call_part.tool_call_id] = call_part\n                items.append(call_part)\n            elif isinstance(item, BetaWebSearchToolResultBlock):\n                items.append(_map_web_search_tool_result_block(item, self.system))\n            elif isinstance(item, BetaCodeExecutionToolResultBlock):\n                items.append(_map_code_execution_tool_result_block(item, self.system))\n            elif isinstance(item, BetaWebFetchToolResultBlock):\n                items.append(_map_web_fetch_tool_result_block(item, self.system))\n            elif isinstance(item, BetaRedactedThinkingBlock):\n                items.append(\n                    ThinkingPart(id='redacted_thinking', content='', signature=item.data, provider_name=self.system)\n                )\n            elif isinstance(item, BetaThinkingBlock):\n                items.append(ThinkingPart(content=item.thinking, signature=item.signature, provider_name=self.system))\n            elif isinstance(item, BetaMCPToolUseBlock):\n                call_part = _map_mcp_server_use_block(item, self.system)\n                builtin_tool_calls[call_part.tool_call_id] = call_part\n                items.append(call_part)\n            elif isinstance(item, BetaMCPToolResultBlock):\n                call_part = builtin_tool_calls.get(item.tool_use_id)\n                items.append(_map_mcp_server_result_block(item, call_part, self.system))\n            else:\n                assert isinstance(item, BetaToolUseBlock), f'unexpected item type {type(item)}'\n                items.append(\n                    ToolCallPart(\n                        tool_name=item.name,\n                        args=cast(dict[str, Any], item.input),\n                        tool_call_id=item.id,\n                    )\n                )\n\n        finish_reason: FinishReason | None = None\n        provider_details: dict[str, Any] | None = None\n        if raw_finish_reason := response.stop_reason:  # pragma: no branch\n            provider_details = {'finish_reason': raw_finish_reason}\n            finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n\n        return ModelResponse(\n            parts=items,\n            usage=_map_usage(response, self._provider.name, self._provider.base_url, self._model_name),\n            model_name=response.model,\n            provider_response_id=response.id,\n            provider_name=self._provider.name,\n            finish_reason=finish_reason,\n            provider_details=provider_details,\n        )\n\n    async def _process_streamed_response(\n        self, response: AsyncStream[BetaRawMessageStreamEvent], model_request_parameters: ModelRequestParameters\n    ) -> StreamedResponse:\n        peekable_response = _utils.PeekableAsyncStream(response)\n        first_chunk = await peekable_response.peek()\n        if isinstance(first_chunk, _utils.Unset):\n            raise UnexpectedModelBehavior('Streamed response ended without content or tool calls')  # pragma: no cover\n\n        assert isinstance(first_chunk, BetaRawMessageStartEvent)\n\n        return AnthropicStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _model_name=first_chunk.message.model,\n            _response=peekable_response,\n            _timestamp=_utils.now_utc(),\n            _provider_name=self._provider.name,\n            _provider_url=self._provider.base_url,\n        )\n\n    def _get_tools(\n        self, model_request_parameters: ModelRequestParameters, model_settings: AnthropicModelSettings\n    ) -> list[BetaToolUnionParam]:\n        tools: list[BetaToolUnionParam] = [\n            self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()\n        ]\n\n        # Add cache_control to the last tool if enabled\n        if tools and (cache_tool_defs := model_settings.get('anthropic_cache_tool_definitions')):\n            # If True, use '5m'; otherwise use the specified ttl value\n            ttl: Literal['5m', '1h'] = '5m' if cache_tool_defs is True else cache_tool_defs\n            last_tool = tools[-1]\n            last_tool['cache_control'] = BetaCacheControlEphemeralParam(type='ephemeral', ttl=ttl)\n\n        return tools\n\n    def _add_builtin_tools(\n        self, tools: list[BetaToolUnionParam], model_request_parameters: ModelRequestParameters\n    ) -> tuple[list[BetaToolUnionParam], list[BetaRequestMCPServerURLDefinitionParam], set[str]]:\n        beta_features: set[str] = set()\n        mcp_servers: list[BetaRequestMCPServerURLDefinitionParam] = []\n        for tool in model_request_parameters.builtin_tools:\n            if isinstance(tool, WebSearchTool):\n                user_location = UserLocation(type='approximate', **tool.user_location) if tool.user_location else None\n                tools.append(\n                    BetaWebSearchTool20250305Param(\n                        name='web_search',\n                        type='web_search_20250305',\n                        max_uses=tool.max_uses,\n                        allowed_domains=tool.allowed_domains,\n                        blocked_domains=tool.blocked_domains,\n                        user_location=user_location,\n                    )\n                )\n            elif isinstance(tool, CodeExecutionTool):  # pragma: no branch\n                tools.append(BetaCodeExecutionTool20250522Param(name='code_execution', type='code_execution_20250522'))\n                beta_features.add('code-execution-2025-05-22')\n            elif isinstance(tool, WebFetchTool):  # pragma: no branch\n                citations = BetaCitationsConfigParam(enabled=tool.enable_citations) if tool.enable_citations else None\n                tools.append(\n                    BetaWebFetchTool20250910Param(\n                        name='web_fetch',\n                        type='web_fetch_20250910',\n                        max_uses=tool.max_uses,\n                        allowed_domains=tool.allowed_domains,\n                        blocked_domains=tool.blocked_domains,\n                        citations=citations,\n                        max_content_tokens=tool.max_content_tokens,\n                    )\n                )\n                beta_features.add('web-fetch-2025-09-10')\n            elif isinstance(tool, MemoryTool):  # pragma: no branch\n                if 'memory' not in model_request_parameters.tool_defs:\n                    raise UserError(\"Built-in `MemoryTool` requires a 'memory' tool to be defined.\")\n                # Replace the memory tool definition with the built-in memory tool\n                tools = [tool for tool in tools if tool.get('name') != 'memory']\n                tools.append(BetaMemoryTool20250818Param(name='memory', type='memory_20250818'))\n                beta_features.add('context-management-2025-06-27')\n            elif isinstance(tool, MCPServerTool) and tool.url:\n                mcp_server_url_definition_param = BetaRequestMCPServerURLDefinitionParam(\n                    type='url',\n                    name=tool.id,\n                    url=tool.url,\n                )\n                if tool.allowed_tools is not None:  # pragma: no branch\n                    mcp_server_url_definition_param['tool_configuration'] = BetaRequestMCPServerToolConfigurationParam(\n                        enabled=bool(tool.allowed_tools),\n                        allowed_tools=tool.allowed_tools,\n                    )\n                if tool.authorization_token:  # pragma: no cover\n                    mcp_server_url_definition_param['authorization_token'] = tool.authorization_token\n                mcp_servers.append(mcp_server_url_definition_param)\n                beta_features.add('mcp-client-2025-04-04')\n            else:  # pragma: no cover\n                raise UserError(\n                    f'`{tool.__class__.__name__}` is not supported by `AnthropicModel`. If it should be, please file an issue.'\n                )\n        return tools, mcp_servers, beta_features\n\n    def _infer_tool_choice(\n        self,\n        tools: list[BetaToolUnionParam],\n        model_settings: AnthropicModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> BetaToolChoiceParam | None:\n        if not tools:\n            return None\n        else:\n            tool_choice: BetaToolChoiceParam\n\n            if not model_request_parameters.allow_text_output:\n                tool_choice = {'type': 'any'}\n            else:\n                tool_choice = {'type': 'auto'}\n\n            if 'parallel_tool_calls' in model_settings:\n                tool_choice['disable_parallel_tool_use'] = not model_settings['parallel_tool_calls']\n\n            return tool_choice\n\n    async def _map_message(  # noqa: C901\n        self,\n        messages: list[ModelMessage],\n        model_request_parameters: ModelRequestParameters,\n        model_settings: AnthropicModelSettings,\n    ) -> tuple[str | list[BetaTextBlockParam], list[BetaMessageParam]]:\n        \"\"\"Just maps a `pydantic_ai.Message` to a `anthropic.types.MessageParam`.\"\"\"\n        system_prompt_parts: list[str] = []\n        anthropic_messages: list[BetaMessageParam] = []\n        for m in messages:\n            if isinstance(m, ModelRequest):\n                user_content_params: list[BetaContentBlockParam] = []\n                for request_part in m.parts:\n                    if isinstance(request_part, SystemPromptPart):\n                        system_prompt_parts.append(request_part.content)\n                    elif isinstance(request_part, UserPromptPart):\n                        async for content in self._map_user_prompt(request_part):\n                            if isinstance(content, CachePoint):\n                                self._add_cache_control_to_last_param(user_content_params, ttl=content.ttl)\n                            else:\n                                user_content_params.append(content)\n                    elif isinstance(request_part, ToolReturnPart):\n                        tool_result_block_param = BetaToolResultBlockParam(\n                            tool_use_id=_guard_tool_call_id(t=request_part),\n                            type='tool_result',\n                            content=request_part.model_response_str(),\n                            is_error=False,\n                        )\n                        user_content_params.append(tool_result_block_param)\n                    elif isinstance(request_part, RetryPromptPart):  # pragma: no branch\n                        if request_part.tool_name is None:\n                            text = request_part.model_response()  # pragma: no cover\n                            retry_param = BetaTextBlockParam(type='text', text=text)  # pragma: no cover\n                        else:\n                            retry_param = BetaToolResultBlockParam(\n                                tool_use_id=_guard_tool_call_id(t=request_part),\n                                type='tool_result',\n                                content=request_part.model_response(),\n                                is_error=True,\n                            )\n                        user_content_params.append(retry_param)\n                if len(user_content_params) > 0:\n                    anthropic_messages.append(BetaMessageParam(role='user', content=user_content_params))\n            elif isinstance(m, ModelResponse):\n                assistant_content_params: list[\n                    BetaTextBlockParam\n                    | BetaToolUseBlockParam\n                    | BetaServerToolUseBlockParam\n                    | BetaWebSearchToolResultBlockParam\n                    | BetaCodeExecutionToolResultBlockParam\n                    | BetaWebFetchToolResultBlockParam\n                    | BetaThinkingBlockParam\n                    | BetaRedactedThinkingBlockParam\n                    | BetaMCPToolUseBlockParam\n                    | BetaMCPToolResultBlock\n                ] = []\n                for response_part in m.parts:\n                    if isinstance(response_part, TextPart):\n                        if response_part.content:\n                            assistant_content_params.append(BetaTextBlockParam(text=response_part.content, type='text'))\n                    elif isinstance(response_part, ToolCallPart):\n                        tool_use_block_param = BetaToolUseBlockParam(\n                            id=_guard_tool_call_id(t=response_part),\n                            type='tool_use',\n                            name=response_part.tool_name,\n                            input=response_part.args_as_dict(),\n                        )\n                        assistant_content_params.append(tool_use_block_param)\n                    elif isinstance(response_part, ThinkingPart):\n                        if (\n                            response_part.provider_name == self.system and response_part.signature is not None\n                        ):  # pragma: no branch\n                            if response_part.id == 'redacted_thinking':\n                                assistant_content_params.append(\n                                    BetaRedactedThinkingBlockParam(\n                                        data=response_part.signature,\n                                        type='redacted_thinking',\n                                    )\n                                )\n                            else:\n                                assistant_content_params.append(\n                                    BetaThinkingBlockParam(\n                                        thinking=response_part.content,\n                                        signature=response_part.signature,\n                                        type='thinking',\n                                    )\n                                )\n                        elif response_part.content:  # pragma: no branch\n                            start_tag, end_tag = self.profile.thinking_tags\n                            assistant_content_params.append(\n                                BetaTextBlockParam(\n                                    text='\\n'.join([start_tag, response_part.content, end_tag]), type='text'\n                                )\n                            )\n                    elif isinstance(response_part, BuiltinToolCallPart):\n                        if response_part.provider_name == self.system:\n                            tool_use_id = _guard_tool_call_id(t=response_part)\n                            if response_part.tool_name == WebSearchTool.kind:\n                                server_tool_use_block_param = BetaServerToolUseBlockParam(\n                                    id=tool_use_id,\n                                    type='server_tool_use',\n                                    name='web_search',\n                                    input=response_part.args_as_dict(),\n                                )\n                                assistant_content_params.append(server_tool_use_block_param)\n                            elif response_part.tool_name == CodeExecutionTool.kind:\n                                server_tool_use_block_param = BetaServerToolUseBlockParam(\n                                    id=tool_use_id,\n                                    type='server_tool_use',\n                                    name='code_execution',\n                                    input=response_part.args_as_dict(),\n                                )\n                                assistant_content_params.append(server_tool_use_block_param)\n                            elif response_part.tool_name == WebFetchTool.kind:\n                                server_tool_use_block_param = BetaServerToolUseBlockParam(\n                                    id=tool_use_id,\n                                    type='server_tool_use',\n                                    name='web_fetch',\n                                    input=response_part.args_as_dict(),\n                                )\n                                assistant_content_params.append(server_tool_use_block_param)\n                            elif (\n                                response_part.tool_name.startswith(MCPServerTool.kind)\n                                and (server_id := response_part.tool_name.split(':', 1)[1])\n                                and (args := response_part.args_as_dict())\n                                and (tool_name := args.get('tool_name'))\n                                and (tool_args := args.get('tool_args'))\n                            ):  # pragma: no branch\n                                mcp_tool_use_block_param = BetaMCPToolUseBlockParam(\n                                    id=tool_use_id,\n                                    type='mcp_tool_use',\n                                    server_name=server_id,\n                                    name=tool_name,\n                                    input=tool_args,\n                                )\n                                assistant_content_params.append(mcp_tool_use_block_param)\n                    elif isinstance(response_part, BuiltinToolReturnPart):\n                        if response_part.provider_name == self.system:\n                            tool_use_id = _guard_tool_call_id(t=response_part)\n                            if response_part.tool_name in (\n                                WebSearchTool.kind,\n                                'web_search_tool_result',  # Backward compatibility\n                            ) and isinstance(response_part.content, dict | list):\n                                assistant_content_params.append(\n                                    BetaWebSearchToolResultBlockParam(\n                                        tool_use_id=tool_use_id,\n                                        type='web_search_tool_result',\n                                        content=cast(\n                                            BetaWebSearchToolResultBlockParamContentParam,\n                                            response_part.content,  # pyright: ignore[reportUnknownMemberType]\n                                        ),\n                                    )\n                                )\n                            elif response_part.tool_name in (  # pragma: no branch\n                                CodeExecutionTool.kind,\n                                'code_execution_tool_result',  # Backward compatibility\n                            ) and isinstance(response_part.content, dict):\n                                assistant_content_params.append(\n                                    BetaCodeExecutionToolResultBlockParam(\n                                        tool_use_id=tool_use_id,\n                                        type='code_execution_tool_result',\n                                        content=cast(\n                                            BetaCodeExecutionToolResultBlockParamContentParam,\n                                            response_part.content,  # pyright: ignore[reportUnknownMemberType]\n                                        ),\n                                    )\n                                )\n                            elif response_part.tool_name == WebFetchTool.kind and isinstance(\n                                response_part.content, dict\n                            ):\n                                assistant_content_params.append(\n                                    BetaWebFetchToolResultBlockParam(\n                                        tool_use_id=tool_use_id,\n                                        type='web_fetch_tool_result',\n                                        content=cast(\n                                            WebFetchToolResultBlockParamContent,\n                                            response_part.content,  # pyright: ignore[reportUnknownMemberType]\n                                        ),\n                                    )\n                                )\n                            elif response_part.tool_name.startswith(MCPServerTool.kind) and isinstance(\n                                response_part.content, dict\n                            ):  # pragma: no branch\n                                assistant_content_params.append(\n                                    BetaMCPToolResultBlock(\n                                        tool_use_id=tool_use_id,\n                                        type='mcp_tool_result',\n                                        **cast(dict[str, Any], response_part.content),  # pyright: ignore[reportUnknownMemberType]\n                                    )\n                                )\n                    elif isinstance(response_part, FilePart):  # pragma: no cover\n                        # Files generated by models are not sent back to models that don't themselves generate files.\n                        pass\n                    else:\n                        assert_never(response_part)\n                if len(assistant_content_params) > 0:\n                    anthropic_messages.append(BetaMessageParam(role='assistant', content=assistant_content_params))\n            else:\n                assert_never(m)\n        if instructions := self._get_instructions(messages, model_request_parameters):\n            system_prompt_parts.insert(0, instructions)\n        system_prompt = '\\n\\n'.join(system_prompt_parts)\n\n        # Add cache_control to the last message content if anthropic_cache_messages is enabled\n        if anthropic_messages and (cache_messages := model_settings.get('anthropic_cache_messages')):\n            ttl: Literal['5m', '1h'] = '5m' if cache_messages is True else cache_messages\n            m = anthropic_messages[-1]\n            content = m['content']\n            if isinstance(content, str):\n                # Convert string content to list format with cache_control\n                m['content'] = [  # pragma: no cover\n                    BetaTextBlockParam(\n                        text=content,\n                        type='text',\n                        cache_control=BetaCacheControlEphemeralParam(type='ephemeral', ttl=ttl),\n                    )\n                ]\n            else:\n                # Add cache_control to the last content block\n                content = cast(list[BetaContentBlockParam], content)\n                self._add_cache_control_to_last_param(content, ttl)\n\n        # If anthropic_cache_instructions is enabled, return system prompt as a list with cache_control\n        if system_prompt and (cache_instructions := model_settings.get('anthropic_cache_instructions')):\n            # If True, use '5m'; otherwise use the specified ttl value\n            ttl: Literal['5m', '1h'] = '5m' if cache_instructions is True else cache_instructions\n            system_prompt_blocks = [\n                BetaTextBlockParam(\n                    type='text',\n                    text=system_prompt,\n                    cache_control=BetaCacheControlEphemeralParam(type='ephemeral', ttl=ttl),\n                )\n            ]\n            return system_prompt_blocks, anthropic_messages\n\n        return system_prompt, anthropic_messages\n\n    @staticmethod\n    def _limit_cache_points(\n        system_prompt: str | list[BetaTextBlockParam],\n        anthropic_messages: list[BetaMessageParam],\n        tools: list[BetaToolUnionParam],\n    ) -> None:\n        \"\"\"Limit the number of cache points in the request to Anthropic's maximum.\n\n        Anthropic enforces a maximum of 4 cache points per request. This method ensures\n        compliance by counting existing cache points and removing excess ones from messages.\n\n        Strategy:\n        1. Count cache points in system_prompt (can be multiple if list of blocks)\n        2. Count cache points in tools (can be in any position, not just last)\n        3. Raise UserError if system + tools already exceed MAX_CACHE_POINTS\n        4. Calculate remaining budget for message cache points\n        5. Traverse messages from newest to oldest, keeping the most recent cache points\n           within the remaining budget\n        6. Remove excess cache points from older messages to stay within limit\n\n        Cache point priority (always preserved):\n        - System prompt cache points\n        - Tool definition cache points\n        - Message cache points (newest first, oldest removed if needed)\n\n        Raises:\n            UserError: If system_prompt and tools combined already exceed MAX_CACHE_POINTS (4).\n                      This indicates a configuration error that cannot be auto-fixed.\n        \"\"\"\n        MAX_CACHE_POINTS = 4\n\n        # Count existing cache points in system prompt\n        used_cache_points = (\n            sum(1 for block in system_prompt if 'cache_control' in cast(dict[str, Any], block))\n            if isinstance(system_prompt, list)\n            else 0\n        )\n\n        # Count existing cache points in tools (any tool may have cache_control)\n        # Note: cache_control can be in the middle of tools list if builtin tools are added after\n        for tool in tools:\n            if 'cache_control' in tool:\n                used_cache_points += 1\n\n        # Calculate remaining cache points budget for messages\n        remaining_budget = MAX_CACHE_POINTS - used_cache_points\n        if remaining_budget < 0:  # pragma: no cover\n            raise UserError(\n                f'Too many cache points for Anthropic request. '\n                f'System prompt and tool definitions already use {used_cache_points} cache points, '\n                f'which exceeds the maximum of {MAX_CACHE_POINTS}.'\n            )\n        # Remove excess cache points from messages (newest to oldest)\n        for message in reversed(anthropic_messages):\n            content = message['content']\n            if isinstance(content, str):  # pragma: no cover\n                continue\n\n            # Process content blocks in reverse order (newest first)\n            for block in reversed(cast(list[BetaContentBlockParam], content)):\n                block_dict = cast(dict[str, Any], block)\n\n                if 'cache_control' in block_dict:\n                    if remaining_budget > 0:\n                        remaining_budget -= 1\n                    else:\n                        # Exceeded limit, remove this cache point\n                        del block_dict['cache_control']\n\n    @staticmethod\n    def _add_cache_control_to_last_param(params: list[BetaContentBlockParam], ttl: Literal['5m', '1h'] = '5m') -> None:\n        \"\"\"Add cache control to the last content block param.\n\n        See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.\n        \"\"\"\n        if not params:\n            raise UserError(\n                'CachePoint cannot be the first content in a user message - there must be previous content to attach the CachePoint to. '\n                'To cache system instructions or tool definitions, use the `anthropic_cache_instructions` or `anthropic_cache_tool_definitions` settings instead.'\n            )\n\n        # Only certain types support cache_control\n        # See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#what-can-be-cached\n        cacheable_types = {'text', 'tool_use', 'server_tool_use', 'image', 'tool_result', 'document'}\n        # Cast needed because BetaContentBlockParam is a union including response Block types (Pydantic models)\n        # that don't support dict operations, even though at runtime we only have request Param types (TypedDicts).\n        last_param = cast(dict[str, Any], params[-1])\n        if last_param['type'] not in cacheable_types:\n            raise UserError(f'Cache control not supported for param type: {last_param[\"type\"]}')\n\n        # Add cache_control to the last param\n        last_param['cache_control'] = BetaCacheControlEphemeralParam(type='ephemeral', ttl=ttl)\n\n    @staticmethod\n    async def _map_user_prompt(\n        part: UserPromptPart,\n    ) -> AsyncGenerator[BetaContentBlockParam | CachePoint]:\n        if isinstance(part.content, str):\n            if part.content:  # Only yield non-empty text\n                yield BetaTextBlockParam(text=part.content, type='text')\n        else:\n            for item in part.content:\n                if isinstance(item, str):\n                    if item:  # Only yield non-empty text\n                        yield BetaTextBlockParam(text=item, type='text')\n                elif isinstance(item, CachePoint):\n                    yield item\n                elif isinstance(item, BinaryContent):\n                    if item.is_image:\n                        yield BetaImageBlockParam(\n                            source={'data': io.BytesIO(item.data), 'media_type': item.media_type, 'type': 'base64'},  # type: ignore\n                            type='image',\n                        )\n                    elif item.media_type == 'application/pdf':\n                        yield BetaBase64PDFBlockParam(\n                            source=BetaBase64PDFSourceParam(\n                                data=io.BytesIO(item.data),\n                                media_type='application/pdf',\n                                type='base64',\n                            ),\n                            type='document',\n                        )\n                    else:\n                        raise RuntimeError('Only images and PDFs are supported for binary content')\n                elif isinstance(item, ImageUrl):\n                    yield BetaImageBlockParam(source={'type': 'url', 'url': item.url}, type='image')\n                elif isinstance(item, DocumentUrl):\n                    if item.media_type == 'application/pdf':\n                        yield BetaBase64PDFBlockParam(source={'url': item.url, 'type': 'url'}, type='document')\n                    elif item.media_type == 'text/plain':\n                        downloaded_item = await download_item(item, data_format='text')\n                        yield BetaBase64PDFBlockParam(\n                            source=BetaPlainTextSourceParam(\n                                data=downloaded_item['data'], media_type=item.media_type, type='text'\n                            ),\n                            type='document',\n                        )\n                    else:  # pragma: no cover\n                        raise RuntimeError(f'Unsupported media type: {item.media_type}')\n                else:\n                    raise RuntimeError(f'Unsupported content type: {type(item)}')  # pragma: no cover\n\n    def _map_tool_definition(self, f: ToolDefinition) -> BetaToolParam:\n        \"\"\"Maps a `ToolDefinition` dataclass to an Anthropic `BetaToolParam` dictionary.\"\"\"\n        tool_param: BetaToolParam = {\n            'name': f.name,\n            'description': f.description or '',\n            'input_schema': f.parameters_json_schema,\n        }\n        if f.strict and self.profile.supports_json_schema_output:\n            tool_param['strict'] = f.strict\n        return tool_param\n\n    @staticmethod\n    def _native_output_format(model_request_parameters: ModelRequestParameters) -> BetaJSONOutputFormatParam | None:\n        if model_request_parameters.output_mode != 'native':\n            return None\n        assert model_request_parameters.output_object is not None\n        return {'type': 'json_schema', 'schema': model_request_parameters.output_object.json_schema}\n\n```\n\n#### __init__\n\n```python\n__init__(\n    model_name: AnthropicModelName,\n    *,\n    provider: (\n        Literal[\"anthropic\", \"gateway\"]\n        | Provider[AsyncAnthropicClient]\n    ) = \"anthropic\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize an Anthropic model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model_name` | `AnthropicModelName` | The name of the Anthropic model to use. List of model names available here. | *required* | | `provider` | `Literal['anthropic', 'gateway'] | Provider[AsyncAnthropicClient]` | The provider to use for the Anthropic API. Can be either the string 'anthropic' or an instance of Provider[AsyncAnthropicClient]. Defaults to 'anthropic'. | `'anthropic'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. The default 'anthropic' provider will use the default ..profiles.anthropic.anthropic_model_profile. | `None` | | `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\n\n```python\ndef __init__(\n    self,\n    model_name: AnthropicModelName,\n    *,\n    provider: Literal['anthropic', 'gateway'] | Provider[AsyncAnthropicClient] = 'anthropic',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize an Anthropic model.\n\n    Args:\n        model_name: The name of the Anthropic model to use. List of model names available\n            [here](https://docs.anthropic.com/en/docs/about-claude/models).\n        provider: The provider to use for the Anthropic API. Can be either the string 'anthropic' or an\n            instance of `Provider[AsyncAnthropicClient]`. Defaults to 'anthropic'.\n        profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n            The default 'anthropic' provider will use the default `..profiles.anthropic.anthropic_model_profile`.\n        settings: Default model settings for this model instance.\n    \"\"\"\n    self._model_name = model_name\n\n    if isinstance(provider, str):\n        provider = infer_provider('gateway/anthropic' if provider == 'gateway' else provider)\n    self._provider = provider\n    self.client = provider.client\n\n    super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n```\n\n#### model_name\n\n```python\nmodel_name: AnthropicModelName\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe model provider.\n\n### AnthropicStreamedResponse\n\nBases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for Anthropic models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\n\n```python\n@dataclass\nclass AnthropicStreamedResponse(StreamedResponse):\n    \"\"\"Implementation of `StreamedResponse` for Anthropic models.\"\"\"\n\n    _model_name: AnthropicModelName\n    _response: AsyncIterable[BetaRawMessageStreamEvent]\n    _timestamp: datetime\n    _provider_name: str\n    _provider_url: str\n\n    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:  # noqa: C901\n        current_block: BetaContentBlock | None = None\n\n        builtin_tool_calls: dict[str, BuiltinToolCallPart] = {}\n        async for event in self._response:\n            if isinstance(event, BetaRawMessageStartEvent):\n                self._usage = _map_usage(event, self._provider_name, self._provider_url, self._model_name)\n                self.provider_response_id = event.message.id\n\n            elif isinstance(event, BetaRawContentBlockStartEvent):\n                current_block = event.content_block\n                if isinstance(current_block, BetaTextBlock) and current_block.text:\n                    maybe_event = self._parts_manager.handle_text_delta(\n                        vendor_part_id=event.index, content=current_block.text\n                    )\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n                elif isinstance(current_block, BetaThinkingBlock):\n                    yield self._parts_manager.handle_thinking_delta(\n                        vendor_part_id=event.index,\n                        content=current_block.thinking,\n                        signature=current_block.signature,\n                        provider_name=self.provider_name,\n                    )\n                elif isinstance(current_block, BetaRedactedThinkingBlock):\n                    yield self._parts_manager.handle_thinking_delta(\n                        vendor_part_id=event.index,\n                        id='redacted_thinking',\n                        signature=current_block.data,\n                        provider_name=self.provider_name,\n                    )\n                elif isinstance(current_block, BetaToolUseBlock):\n                    maybe_event = self._parts_manager.handle_tool_call_delta(\n                        vendor_part_id=event.index,\n                        tool_name=current_block.name,\n                        args=cast(dict[str, Any], current_block.input) or None,\n                        tool_call_id=current_block.id,\n                    )\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n                elif isinstance(current_block, BetaServerToolUseBlock):\n                    call_part = _map_server_tool_use_block(current_block, self.provider_name)\n                    builtin_tool_calls[call_part.tool_call_id] = call_part\n                    yield self._parts_manager.handle_part(\n                        vendor_part_id=event.index,\n                        part=call_part,\n                    )\n                elif isinstance(current_block, BetaWebSearchToolResultBlock):\n                    yield self._parts_manager.handle_part(\n                        vendor_part_id=event.index,\n                        part=_map_web_search_tool_result_block(current_block, self.provider_name),\n                    )\n                elif isinstance(current_block, BetaCodeExecutionToolResultBlock):\n                    yield self._parts_manager.handle_part(\n                        vendor_part_id=event.index,\n                        part=_map_code_execution_tool_result_block(current_block, self.provider_name),\n                    )\n                elif isinstance(current_block, BetaWebFetchToolResultBlock):  # pragma: lax no cover\n                    yield self._parts_manager.handle_part(\n                        vendor_part_id=event.index,\n                        part=_map_web_fetch_tool_result_block(current_block, self.provider_name),\n                    )\n                elif isinstance(current_block, BetaMCPToolUseBlock):\n                    call_part = _map_mcp_server_use_block(current_block, self.provider_name)\n                    builtin_tool_calls[call_part.tool_call_id] = call_part\n\n                    args_json = call_part.args_as_json_str()\n                    # Drop the final `{}}` so that we can add tool args deltas\n                    args_json_delta = args_json[:-3]\n                    assert args_json_delta.endswith('\"tool_args\":'), (\n                        f'Expected {args_json_delta!r} to end in `\"tool_args\":`'\n                    )\n\n                    yield self._parts_manager.handle_part(\n                        vendor_part_id=event.index, part=replace(call_part, args=None)\n                    )\n                    maybe_event = self._parts_manager.handle_tool_call_delta(\n                        vendor_part_id=event.index,\n                        args=args_json_delta,\n                    )\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n                elif isinstance(current_block, BetaMCPToolResultBlock):\n                    call_part = builtin_tool_calls.get(current_block.tool_use_id)\n                    yield self._parts_manager.handle_part(\n                        vendor_part_id=event.index,\n                        part=_map_mcp_server_result_block(current_block, call_part, self.provider_name),\n                    )\n\n            elif isinstance(event, BetaRawContentBlockDeltaEvent):\n                if isinstance(event.delta, BetaTextDelta):\n                    maybe_event = self._parts_manager.handle_text_delta(\n                        vendor_part_id=event.index, content=event.delta.text\n                    )\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n                elif isinstance(event.delta, BetaThinkingDelta):\n                    yield self._parts_manager.handle_thinking_delta(\n                        vendor_part_id=event.index,\n                        content=event.delta.thinking,\n                        provider_name=self.provider_name,\n                    )\n                elif isinstance(event.delta, BetaSignatureDelta):\n                    yield self._parts_manager.handle_thinking_delta(\n                        vendor_part_id=event.index,\n                        signature=event.delta.signature,\n                        provider_name=self.provider_name,\n                    )\n                elif isinstance(event.delta, BetaInputJSONDelta):\n                    maybe_event = self._parts_manager.handle_tool_call_delta(\n                        vendor_part_id=event.index,\n                        args=event.delta.partial_json,\n                    )\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n                # TODO(Marcelo): We need to handle citations.\n                elif isinstance(event.delta, BetaCitationsDelta):\n                    pass\n\n            elif isinstance(event, BetaRawMessageDeltaEvent):\n                self._usage = _map_usage(event, self._provider_name, self._provider_url, self._model_name, self._usage)\n                if raw_finish_reason := event.delta.stop_reason:  # pragma: no branch\n                    self.provider_details = {'finish_reason': raw_finish_reason}\n                    self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n\n            elif isinstance(event, BetaRawContentBlockStopEvent):  # pragma: no branch\n                if isinstance(current_block, BetaMCPToolUseBlock):\n                    maybe_event = self._parts_manager.handle_tool_call_delta(\n                        vendor_part_id=event.index,\n                        args='}',\n                    )\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n                current_block = None\n            elif isinstance(event, BetaRawMessageStopEvent):  # pragma: no branch\n                current_block = None\n\n    @property\n    def model_name(self) -> AnthropicModelName:\n        \"\"\"Get the model name of the response.\"\"\"\n        return self._model_name\n\n    @property\n    def provider_name(self) -> str:\n        \"\"\"Get the provider name.\"\"\"\n        return self._provider_name\n\n    @property\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._timestamp\n\n```\n\n#### model_name\n\n```python\nmodel_name: AnthropicModelName\n\n```\n\nGet the model name of the response.\n\n#### provider_name\n\n```python\nprovider_name: str\n\n```\n\nGet the provider name.\n\n#### timestamp\n\n```python\ntimestamp: datetime\n\n```\n\nGet the timestamp of the response.",
  "content_length": 61666
}