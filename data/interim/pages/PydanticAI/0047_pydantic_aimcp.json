{
  "title": "`pydantic_ai.mcp`",
  "source_url": null,
  "content": "### MCPError\n\nBases: `RuntimeError`\n\nRaised when an MCP server returns an error response.\n\nThis exception wraps error responses from MCP servers, following the ErrorData schema from the MCP specification.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\nclass MCPError(RuntimeError):\n    \"\"\"Raised when an MCP server returns an error response.\n\n    This exception wraps error responses from MCP servers, following the ErrorData schema\n    from the MCP specification.\n    \"\"\"\n\n    message: str\n    \"\"\"The error message.\"\"\"\n\n    code: int\n    \"\"\"The error code returned by the server.\"\"\"\n\n    data: dict[str, Any] | None\n    \"\"\"Additional information about the error, if provided by the server.\"\"\"\n\n    def __init__(self, message: str, code: int, data: dict[str, Any] | None = None):\n        self.message = message\n        self.code = code\n        self.data = data\n        super().__init__(message)\n\n    @classmethod\n    def from_mcp_sdk(cls, error: mcp_exceptions.McpError) -> MCPError:\n        \"\"\"Create an MCPError from an MCP SDK McpError.\n\n        Args:\n            error: An McpError from the MCP SDK.\n        \"\"\"\n        # Extract error data from the McpError.error attribute\n        error_data = error.error\n        return cls(message=error_data.message, code=error_data.code, data=error_data.data)\n\n    def __str__(self) -> str:\n        if self.data:\n            return f'{self.message} (code: {self.code}, data: {self.data})'\n        return f'{self.message} (code: {self.code})'\n\n```\n\n#### message\n\n```python\nmessage: str = message\n\n```\n\nThe error message.\n\n#### code\n\n```python\ncode: int = code\n\n```\n\nThe error code returned by the server.\n\n#### data\n\n```python\ndata: dict[str, Any] | None = data\n\n```\n\nAdditional information about the error, if provided by the server.\n\n#### from_mcp_sdk\n\n```python\nfrom_mcp_sdk(error: McpError) -> MCPError\n\n```\n\nCreate an MCPError from an MCP SDK McpError.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `error` | `McpError` | An McpError from the MCP SDK. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@classmethod\ndef from_mcp_sdk(cls, error: mcp_exceptions.McpError) -> MCPError:\n    \"\"\"Create an MCPError from an MCP SDK McpError.\n\n    Args:\n        error: An McpError from the MCP SDK.\n    \"\"\"\n    # Extract error data from the McpError.error attribute\n    error_data = error.error\n    return cls(message=error_data.message, code=error_data.code, data=error_data.data)\n\n```\n\n### ResourceAnnotations\n\nAdditional properties describing MCP entities.\n\nSee the [resource annotations in the MCP specification](https://modelcontextprotocol.io/specification/2025-06-18/server/resources#annotations).\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@dataclass(repr=False, kw_only=True)\nclass ResourceAnnotations:\n    \"\"\"Additional properties describing MCP entities.\n\n    See the [resource annotations in the MCP specification](https://modelcontextprotocol.io/specification/2025-06-18/server/resources#annotations).\n    \"\"\"\n\n    audience: list[mcp_types.Role] | None = None\n    \"\"\"Intended audience for this entity.\"\"\"\n\n    priority: Annotated[float, Field(ge=0.0, le=1.0)] | None = None\n    \"\"\"Priority level for this entity, ranging from 0.0 to 1.0.\"\"\"\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n    @classmethod\n    def from_mcp_sdk(cls, mcp_annotations: mcp_types.Annotations) -> ResourceAnnotations:\n        \"\"\"Convert from MCP SDK Annotations to ResourceAnnotations.\n\n        Args:\n            mcp_annotations: The MCP SDK annotations object.\n        \"\"\"\n        return cls(audience=mcp_annotations.audience, priority=mcp_annotations.priority)\n\n```\n\n#### audience\n\n```python\naudience: list[Role] | None = None\n\n```\n\nIntended audience for this entity.\n\n#### priority\n\n```python\npriority: Annotated[float, Field(ge=0.0, le=1.0)] | None = (\n    None\n)\n\n```\n\nPriority level for this entity, ranging from 0.0 to 1.0.\n\n#### from_mcp_sdk\n\n```python\nfrom_mcp_sdk(\n    mcp_annotations: Annotations,\n) -> ResourceAnnotations\n\n```\n\nConvert from MCP SDK Annotations to ResourceAnnotations.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `mcp_annotations` | `Annotations` | The MCP SDK annotations object. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@classmethod\ndef from_mcp_sdk(cls, mcp_annotations: mcp_types.Annotations) -> ResourceAnnotations:\n    \"\"\"Convert from MCP SDK Annotations to ResourceAnnotations.\n\n    Args:\n        mcp_annotations: The MCP SDK annotations object.\n    \"\"\"\n    return cls(audience=mcp_annotations.audience, priority=mcp_annotations.priority)\n\n```\n\n### Resource\n\nBases: `BaseResource`\n\nA resource that can be read from an MCP server.\n\nSee the [resources in the MCP specification](https://modelcontextprotocol.io/specification/2025-06-18/server/resources).\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@dataclass(repr=False, kw_only=True)\nclass Resource(BaseResource):\n    \"\"\"A resource that can be read from an MCP server.\n\n    See the [resources in the MCP specification](https://modelcontextprotocol.io/specification/2025-06-18/server/resources).\n    \"\"\"\n\n    uri: str\n    \"\"\"The URI of the resource.\"\"\"\n\n    size: int | None = None\n    \"\"\"The size of the raw resource content in bytes (before base64 encoding), if known.\"\"\"\n\n    @classmethod\n    def from_mcp_sdk(cls, mcp_resource: mcp_types.Resource) -> Resource:\n        \"\"\"Convert from MCP SDK Resource to PydanticAI Resource.\n\n        Args:\n            mcp_resource: The MCP SDK Resource object.\n        \"\"\"\n        return cls(\n            uri=str(mcp_resource.uri),\n            name=mcp_resource.name,\n            title=mcp_resource.title,\n            description=mcp_resource.description,\n            mime_type=mcp_resource.mimeType,\n            size=mcp_resource.size,\n            annotations=ResourceAnnotations.from_mcp_sdk(mcp_resource.annotations)\n            if mcp_resource.annotations\n            else None,\n            metadata=mcp_resource.meta,\n        )\n\n```\n\n#### uri\n\n```python\nuri: str\n\n```\n\nThe URI of the resource.\n\n#### size\n\n```python\nsize: int | None = None\n\n```\n\nThe size of the raw resource content in bytes (before base64 encoding), if known.\n\n#### from_mcp_sdk\n\n```python\nfrom_mcp_sdk(mcp_resource: Resource) -> Resource\n\n```\n\nConvert from MCP SDK Resource to PydanticAI Resource.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `mcp_resource` | `Resource` | The MCP SDK Resource object. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@classmethod\ndef from_mcp_sdk(cls, mcp_resource: mcp_types.Resource) -> Resource:\n    \"\"\"Convert from MCP SDK Resource to PydanticAI Resource.\n\n    Args:\n        mcp_resource: The MCP SDK Resource object.\n    \"\"\"\n    return cls(\n        uri=str(mcp_resource.uri),\n        name=mcp_resource.name,\n        title=mcp_resource.title,\n        description=mcp_resource.description,\n        mime_type=mcp_resource.mimeType,\n        size=mcp_resource.size,\n        annotations=ResourceAnnotations.from_mcp_sdk(mcp_resource.annotations)\n        if mcp_resource.annotations\n        else None,\n        metadata=mcp_resource.meta,\n    )\n\n```\n\n### ResourceTemplate\n\nBases: `BaseResource`\n\nA template for parameterized resources on an MCP server.\n\nSee the [resource templates in the MCP specification](https://modelcontextprotocol.io/specification/2025-06-18/server/resources#resource-templates).\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@dataclass(repr=False, kw_only=True)\nclass ResourceTemplate(BaseResource):\n    \"\"\"A template for parameterized resources on an MCP server.\n\n    See the [resource templates in the MCP specification](https://modelcontextprotocol.io/specification/2025-06-18/server/resources#resource-templates).\n    \"\"\"\n\n    uri_template: str\n    \"\"\"URI template (RFC 6570) for constructing resource URIs.\"\"\"\n\n    @classmethod\n    def from_mcp_sdk(cls, mcp_template: mcp_types.ResourceTemplate) -> ResourceTemplate:\n        \"\"\"Convert from MCP SDK ResourceTemplate to PydanticAI ResourceTemplate.\n\n        Args:\n            mcp_template: The MCP SDK ResourceTemplate object.\n        \"\"\"\n        return cls(\n            uri_template=mcp_template.uriTemplate,\n            name=mcp_template.name,\n            title=mcp_template.title,\n            description=mcp_template.description,\n            mime_type=mcp_template.mimeType,\n            annotations=ResourceAnnotations.from_mcp_sdk(mcp_template.annotations)\n            if mcp_template.annotations\n            else None,\n            metadata=mcp_template.meta,\n        )\n\n```\n\n#### uri_template\n\n```python\nuri_template: str\n\n```\n\nURI template (RFC 6570) for constructing resource URIs.\n\n#### from_mcp_sdk\n\n```python\nfrom_mcp_sdk(\n    mcp_template: ResourceTemplate,\n) -> ResourceTemplate\n\n```\n\nConvert from MCP SDK ResourceTemplate to PydanticAI ResourceTemplate.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `mcp_template` | `ResourceTemplate` | The MCP SDK ResourceTemplate object. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@classmethod\ndef from_mcp_sdk(cls, mcp_template: mcp_types.ResourceTemplate) -> ResourceTemplate:\n    \"\"\"Convert from MCP SDK ResourceTemplate to PydanticAI ResourceTemplate.\n\n    Args:\n        mcp_template: The MCP SDK ResourceTemplate object.\n    \"\"\"\n    return cls(\n        uri_template=mcp_template.uriTemplate,\n        name=mcp_template.name,\n        title=mcp_template.title,\n        description=mcp_template.description,\n        mime_type=mcp_template.mimeType,\n        annotations=ResourceAnnotations.from_mcp_sdk(mcp_template.annotations)\n        if mcp_template.annotations\n        else None,\n        metadata=mcp_template.meta,\n    )\n\n```\n\n### ServerCapabilities\n\nCapabilities that an MCP server supports.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@dataclass(repr=False, kw_only=True)\nclass ServerCapabilities:\n    \"\"\"Capabilities that an MCP server supports.\"\"\"\n\n    experimental: list[str] | None = None\n    \"\"\"Experimental, non-standard capabilities that the server supports.\"\"\"\n\n    logging: bool = False\n    \"\"\"Whether the server supports sending log messages to the client.\"\"\"\n\n    prompts: bool = False\n    \"\"\"Whether the server offers any prompt templates.\"\"\"\n\n    resources: bool = False\n    \"\"\"Whether the server offers any resources to read.\"\"\"\n\n    tools: bool = False\n    \"\"\"Whether the server offers any tools to call.\"\"\"\n\n    completions: bool = False\n    \"\"\"Whether the server offers autocompletion suggestions for prompts and resources.\"\"\"\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n    @classmethod\n    def from_mcp_sdk(cls, mcp_capabilities: mcp_types.ServerCapabilities) -> ServerCapabilities:\n        \"\"\"Convert from MCP SDK ServerCapabilities to PydanticAI ServerCapabilities.\n\n        Args:\n            mcp_capabilities: The MCP SDK ServerCapabilities object.\n        \"\"\"\n        return cls(\n            experimental=list(mcp_capabilities.experimental.keys()) if mcp_capabilities.experimental else None,\n            logging=mcp_capabilities.logging is not None,\n            prompts=mcp_capabilities.prompts is not None,\n            resources=mcp_capabilities.resources is not None,\n            tools=mcp_capabilities.tools is not None,\n            completions=mcp_capabilities.completions is not None,\n        )\n\n```\n\n#### experimental\n\n```python\nexperimental: list[str] | None = None\n\n```\n\nExperimental, non-standard capabilities that the server supports.\n\n#### logging\n\n```python\nlogging: bool = False\n\n```\n\nWhether the server supports sending log messages to the client.\n\n#### prompts\n\n```python\nprompts: bool = False\n\n```\n\nWhether the server offers any prompt templates.\n\n#### resources\n\n```python\nresources: bool = False\n\n```\n\nWhether the server offers any resources to read.\n\n#### tools\n\n```python\ntools: bool = False\n\n```\n\nWhether the server offers any tools to call.\n\n#### completions\n\n```python\ncompletions: bool = False\n\n```\n\nWhether the server offers autocompletion suggestions for prompts and resources.\n\n#### from_mcp_sdk\n\n```python\nfrom_mcp_sdk(\n    mcp_capabilities: ServerCapabilities,\n) -> ServerCapabilities\n\n```\n\nConvert from MCP SDK ServerCapabilities to PydanticAI ServerCapabilities.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `mcp_capabilities` | `ServerCapabilities` | The MCP SDK ServerCapabilities object. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@classmethod\ndef from_mcp_sdk(cls, mcp_capabilities: mcp_types.ServerCapabilities) -> ServerCapabilities:\n    \"\"\"Convert from MCP SDK ServerCapabilities to PydanticAI ServerCapabilities.\n\n    Args:\n        mcp_capabilities: The MCP SDK ServerCapabilities object.\n    \"\"\"\n    return cls(\n        experimental=list(mcp_capabilities.experimental.keys()) if mcp_capabilities.experimental else None,\n        logging=mcp_capabilities.logging is not None,\n        prompts=mcp_capabilities.prompts is not None,\n        resources=mcp_capabilities.resources is not None,\n        tools=mcp_capabilities.tools is not None,\n        completions=mcp_capabilities.completions is not None,\n    )\n\n```\n\n### MCPServer\n\nBases: `AbstractToolset[Any]`, `ABC`\n\nBase class for attaching agents to MCP servers.\n\nSee <https://modelcontextprotocol.io> for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\nclass MCPServer(AbstractToolset[Any], ABC):\n    \"\"\"Base class for attaching agents to MCP servers.\n\n    See <https://modelcontextprotocol.io> for more information.\n    \"\"\"\n\n    tool_prefix: str | None\n    \"\"\"A prefix to add to all tools that are registered with the server.\n\n    If not empty, will include a trailing underscore(`_`).\n\n    e.g. if `tool_prefix='foo'`, then a tool named `bar` will be registered as `foo_bar`\n    \"\"\"\n\n    log_level: mcp_types.LoggingLevel | None\n    \"\"\"The log level to set when connecting to the server, if any.\n\n    See <https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#logging> for more details.\n\n    If `None`, no log level will be set.\n    \"\"\"\n\n    log_handler: LoggingFnT | None\n    \"\"\"A handler for logging messages from the server.\"\"\"\n\n    timeout: float\n    \"\"\"The timeout in seconds to wait for the client to initialize.\"\"\"\n\n    read_timeout: float\n    \"\"\"Maximum time in seconds to wait for new messages before timing out.\n\n    This timeout applies to the long-lived connection after it's established.\n    If no new messages are received within this time, the connection will be considered stale\n    and may be closed. Defaults to 5 minutes (300 seconds).\n    \"\"\"\n\n    process_tool_call: ProcessToolCallback | None\n    \"\"\"Hook to customize tool calling and optionally pass extra metadata.\"\"\"\n\n    allow_sampling: bool\n    \"\"\"Whether to allow MCP sampling through this client.\"\"\"\n\n    sampling_model: models.Model | None\n    \"\"\"The model to use for sampling.\"\"\"\n\n    max_retries: int\n    \"\"\"The maximum number of times to retry a tool call.\"\"\"\n\n    elicitation_callback: ElicitationFnT | None = None\n    \"\"\"Callback function to handle elicitation requests from the server.\"\"\"\n\n    _id: str | None\n\n    _enter_lock: Lock = field(compare=False)\n    _running_count: int\n    _exit_stack: AsyncExitStack | None\n\n    _client: ClientSession\n    _read_stream: MemoryObjectReceiveStream[SessionMessage | Exception]\n    _write_stream: MemoryObjectSendStream[SessionMessage]\n    _server_info: mcp_types.Implementation\n    _server_capabilities: ServerCapabilities\n    _instructions: str | None\n\n    def __init__(\n        self,\n        tool_prefix: str | None = None,\n        log_level: mcp_types.LoggingLevel | None = None,\n        log_handler: LoggingFnT | None = None,\n        timeout: float = 5,\n        read_timeout: float = 5 * 60,\n        process_tool_call: ProcessToolCallback | None = None,\n        allow_sampling: bool = True,\n        sampling_model: models.Model | None = None,\n        max_retries: int = 1,\n        elicitation_callback: ElicitationFnT | None = None,\n        *,\n        id: str | None = None,\n    ):\n        self.tool_prefix = tool_prefix\n        self.log_level = log_level\n        self.log_handler = log_handler\n        self.timeout = timeout\n        self.read_timeout = read_timeout\n        self.process_tool_call = process_tool_call\n        self.allow_sampling = allow_sampling\n        self.sampling_model = sampling_model\n        self.max_retries = max_retries\n        self.elicitation_callback = elicitation_callback\n\n        self._id = id or tool_prefix\n\n        self.__post_init__()\n\n    def __post_init__(self):\n        self._enter_lock = Lock()\n        self._running_count = 0\n        self._exit_stack = None\n\n    @abstractmethod\n    @asynccontextmanager\n    async def client_streams(\n        self,\n    ) -> AsyncIterator[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n        ]\n    ]:\n        \"\"\"Create the streams for the MCP server.\"\"\"\n        raise NotImplementedError('MCP Server subclasses must implement this method.')\n        yield\n\n    @property\n    def id(self) -> str | None:\n        return self._id\n\n    @id.setter\n    def id(self, value: str | None):\n        self._id = value\n\n    @property\n    def label(self) -> str:\n        if self.id:\n            return super().label  # pragma: no cover\n        else:\n            return repr(self)\n\n    @property\n    def tool_name_conflict_hint(self) -> str:\n        return 'Set the `tool_prefix` attribute to avoid name conflicts.'\n\n    @property\n    def server_info(self) -> mcp_types.Implementation:\n        \"\"\"Access the information send by the MCP server during initialization.\"\"\"\n        if getattr(self, '_server_info', None) is None:\n            raise AttributeError(\n                f'The `{self.__class__.__name__}.server_info` is only instantiated after initialization.'\n            )\n        return self._server_info\n\n    @property\n    def capabilities(self) -> ServerCapabilities:\n        \"\"\"Access the capabilities advertised by the MCP server during initialization.\"\"\"\n        if getattr(self, '_server_capabilities', None) is None:\n            raise AttributeError(\n                f'The `{self.__class__.__name__}.capabilities` is only instantiated after initialization.'\n            )\n        return self._server_capabilities\n\n    @property\n    def instructions(self) -> str | None:\n        \"\"\"Access the instructions sent by the MCP server during initialization.\"\"\"\n        if not hasattr(self, '_instructions'):\n            raise AttributeError(\n                f'The `{self.__class__.__name__}.instructions` is only available after initialization.'\n            )\n        return self._instructions\n\n    async def list_tools(self) -> list[mcp_types.Tool]:\n        \"\"\"Retrieve tools that are currently active on the server.\n\n        Note:\n        - We don't cache tools as they might change.\n        - We also don't subscribe to the server to avoid complexity.\n        \"\"\"\n        async with self:  # Ensure server is running\n            result = await self._client.list_tools()\n        return result.tools\n\n    async def direct_call_tool(\n        self,\n        name: str,\n        args: dict[str, Any],\n        metadata: dict[str, Any] | None = None,\n    ) -> ToolResult:\n        \"\"\"Call a tool on the server.\n\n        Args:\n            name: The name of the tool to call.\n            args: The arguments to pass to the tool.\n            metadata: Request-level metadata (optional)\n\n        Returns:\n            The result of the tool call.\n\n        Raises:\n            ModelRetry: If the tool call fails.\n        \"\"\"\n        async with self:  # Ensure server is running\n            try:\n                result = await self._client.send_request(\n                    mcp_types.ClientRequest(\n                        mcp_types.CallToolRequest(\n                            method='tools/call',\n                            params=mcp_types.CallToolRequestParams(\n                                name=name,\n                                arguments=args,\n                                _meta=mcp_types.RequestParams.Meta(**metadata) if metadata else None,\n                            ),\n                        )\n                    ),\n                    mcp_types.CallToolResult,\n                )\n            except mcp_exceptions.McpError as e:\n                raise exceptions.ModelRetry(e.error.message)\n\n        if result.isError:\n            message: str | None = None\n            if result.content:  # pragma: no branch\n                text_parts = [part.text for part in result.content if isinstance(part, mcp_types.TextContent)]\n                message = '\\n'.join(text_parts)\n\n            raise exceptions.ModelRetry(message or 'MCP tool call failed')\n\n        # Prefer structured content if there are only text parts, which per the docs would contain the JSON-encoded structured content for backward compatibility.\n        # See https://github.com/modelcontextprotocol/python-sdk#structured-output\n        if (structured := result.structuredContent) and not any(\n            not isinstance(part, mcp_types.TextContent) for part in result.content\n        ):\n            # The MCP SDK wraps primitives and generic types like list in a `result` key, but we want to use the raw value returned by the tool function.\n            # See https://github.com/modelcontextprotocol/python-sdk#structured-output\n            if isinstance(structured, dict) and len(structured) == 1 and 'result' in structured:\n                return structured['result']\n            return structured\n\n        mapped = [await self._map_tool_result_part(part) for part in result.content]\n        return mapped[0] if len(mapped) == 1 else mapped\n\n    async def call_tool(\n        self,\n        name: str,\n        tool_args: dict[str, Any],\n        ctx: RunContext[Any],\n        tool: ToolsetTool[Any],\n    ) -> ToolResult:\n        if self.tool_prefix:\n            name = name.removeprefix(f'{self.tool_prefix}_')\n            ctx = replace(ctx, tool_name=name)\n\n        if self.process_tool_call is not None:\n            return await self.process_tool_call(ctx, self.direct_call_tool, name, tool_args)\n        else:\n            return await self.direct_call_tool(name, tool_args)\n\n    async def get_tools(self, ctx: RunContext[Any]) -> dict[str, ToolsetTool[Any]]:\n        return {\n            name: self.tool_for_tool_def(\n                ToolDefinition(\n                    name=name,\n                    description=mcp_tool.description,\n                    parameters_json_schema=mcp_tool.inputSchema,\n                    metadata={\n                        'meta': mcp_tool.meta,\n                        'annotations': mcp_tool.annotations.model_dump() if mcp_tool.annotations else None,\n                        'output_schema': mcp_tool.outputSchema or None,\n                    },\n                ),\n            )\n            for mcp_tool in await self.list_tools()\n            if (name := f'{self.tool_prefix}_{mcp_tool.name}' if self.tool_prefix else mcp_tool.name)\n        }\n\n    def tool_for_tool_def(self, tool_def: ToolDefinition) -> ToolsetTool[Any]:\n        return ToolsetTool(\n            toolset=self,\n            tool_def=tool_def,\n            max_retries=self.max_retries,\n            args_validator=TOOL_SCHEMA_VALIDATOR,\n        )\n\n    async def list_resources(self) -> list[Resource]:\n        \"\"\"Retrieve resources that are currently present on the server.\n\n        Note:\n        - We don't cache resources as they might change.\n        - We also don't subscribe to resource changes to avoid complexity.\n\n        Raises:\n            MCPError: If the server returns an error.\n        \"\"\"\n        async with self:  # Ensure server is running\n            if not self.capabilities.resources:\n                return []\n            try:\n                result = await self._client.list_resources()\n            except mcp_exceptions.McpError as e:\n                raise MCPError.from_mcp_sdk(e) from e\n        return [Resource.from_mcp_sdk(r) for r in result.resources]\n\n    async def list_resource_templates(self) -> list[ResourceTemplate]:\n        \"\"\"Retrieve resource templates that are currently present on the server.\n\n        Raises:\n            MCPError: If the server returns an error.\n        \"\"\"\n        async with self:  # Ensure server is running\n            if not self.capabilities.resources:\n                return []\n            try:\n                result = await self._client.list_resource_templates()\n            except mcp_exceptions.McpError as e:\n                raise MCPError.from_mcp_sdk(e) from e\n        return [ResourceTemplate.from_mcp_sdk(t) for t in result.resourceTemplates]\n\n    @overload\n    async def read_resource(self, uri: str) -> str | messages.BinaryContent | list[str | messages.BinaryContent]: ...\n\n    @overload\n    async def read_resource(\n        self, uri: Resource\n    ) -> str | messages.BinaryContent | list[str | messages.BinaryContent]: ...\n\n    async def read_resource(\n        self, uri: str | Resource\n    ) -> str | messages.BinaryContent | list[str | messages.BinaryContent]:\n        \"\"\"Read the contents of a specific resource by URI.\n\n        Args:\n            uri: The URI of the resource to read, or a Resource object.\n\n        Returns:\n            The resource contents. If the resource has a single content item, returns that item directly.\n            If the resource has multiple content items, returns a list of items.\n\n        Raises:\n            MCPError: If the server returns an error.\n        \"\"\"\n        resource_uri = uri if isinstance(uri, str) else uri.uri\n        async with self:  # Ensure server is running\n            try:\n                result = await self._client.read_resource(AnyUrl(resource_uri))\n            except mcp_exceptions.McpError as e:\n                raise MCPError.from_mcp_sdk(e) from e\n\n        return (\n            self._get_content(result.contents[0])\n            if len(result.contents) == 1\n            else [self._get_content(resource) for resource in result.contents]\n        )\n\n    async def __aenter__(self) -> Self:\n        \"\"\"Enter the MCP server context.\n\n        This will initialize the connection to the server.\n        If this server is an [`MCPServerStdio`][pydantic_ai.mcp.MCPServerStdio], the server will first be started as a subprocess.\n\n        This is a no-op if the MCP server has already been entered.\n        \"\"\"\n        async with self._enter_lock:\n            if self._running_count == 0:\n                async with AsyncExitStack() as exit_stack:\n                    self._read_stream, self._write_stream = await exit_stack.enter_async_context(self.client_streams())\n                    client = ClientSession(\n                        read_stream=self._read_stream,\n                        write_stream=self._write_stream,\n                        sampling_callback=self._sampling_callback if self.allow_sampling else None,\n                        elicitation_callback=self.elicitation_callback,\n                        logging_callback=self.log_handler,\n                        read_timeout_seconds=timedelta(seconds=self.read_timeout),\n                    )\n                    self._client = await exit_stack.enter_async_context(client)\n\n                    with anyio.fail_after(self.timeout):\n                        result = await self._client.initialize()\n                        self._server_info = result.serverInfo\n                        self._server_capabilities = ServerCapabilities.from_mcp_sdk(result.capabilities)\n                        self._instructions = result.instructions\n                        if log_level := self.log_level:\n                            await self._client.set_logging_level(log_level)\n\n                    self._exit_stack = exit_stack.pop_all()\n            self._running_count += 1\n        return self\n\n    async def __aexit__(self, *args: Any) -> bool | None:\n        if self._running_count == 0:\n            raise ValueError('MCPServer.__aexit__ called more times than __aenter__')\n        async with self._enter_lock:\n            self._running_count -= 1\n            if self._running_count == 0 and self._exit_stack is not None:\n                await self._exit_stack.aclose()\n                self._exit_stack = None\n\n    @property\n    def is_running(self) -> bool:\n        \"\"\"Check if the MCP server is running.\"\"\"\n        return bool(self._running_count)\n\n    async def _sampling_callback(\n        self, context: RequestContext[ClientSession, Any], params: mcp_types.CreateMessageRequestParams\n    ) -> mcp_types.CreateMessageResult | mcp_types.ErrorData:\n        \"\"\"MCP sampling callback.\"\"\"\n        if self.sampling_model is None:\n            raise ValueError('Sampling model is not set')  # pragma: no cover\n\n        pai_messages = _mcp.map_from_mcp_params(params)\n        model_settings = models.ModelSettings()\n        if max_tokens := params.maxTokens:  # pragma: no branch\n            model_settings['max_tokens'] = max_tokens\n        if temperature := params.temperature:  # pragma: no branch\n            model_settings['temperature'] = temperature\n        if stop_sequences := params.stopSequences:  # pragma: no branch\n            model_settings['stop_sequences'] = stop_sequences\n\n        model_response = await model_request(self.sampling_model, pai_messages, model_settings=model_settings)\n        return mcp_types.CreateMessageResult(\n            role='assistant',\n            content=_mcp.map_from_model_response(model_response),\n            model=self.sampling_model.model_name,\n        )\n\n    async def _map_tool_result_part(\n        self, part: mcp_types.ContentBlock\n    ) -> str | messages.BinaryContent | dict[str, Any] | list[Any]:\n        # See https://github.com/jlowin/fastmcp/blob/main/docs/servers/tools.mdx#return-values\n\n        if isinstance(part, mcp_types.TextContent):\n            text = part.text\n            if text.startswith(('[', '{')):\n                try:\n                    return pydantic_core.from_json(text)\n                except ValueError:\n                    pass\n            return text\n        elif isinstance(part, mcp_types.ImageContent):\n            return messages.BinaryContent(data=base64.b64decode(part.data), media_type=part.mimeType)\n        elif isinstance(part, mcp_types.AudioContent):\n            # NOTE: The FastMCP server doesn't support audio content.\n            # See <https://github.com/modelcontextprotocol/python-sdk/issues/952> for more details.\n            return messages.BinaryContent(\n                data=base64.b64decode(part.data), media_type=part.mimeType\n            )  # pragma: no cover\n        elif isinstance(part, mcp_types.EmbeddedResource):\n            resource = part.resource\n            return self._get_content(resource)\n        elif isinstance(part, mcp_types.ResourceLink):\n            return await self.read_resource(str(part.uri))\n        else:\n            assert_never(part)\n\n    def _get_content(\n        self, resource: mcp_types.TextResourceContents | mcp_types.BlobResourceContents\n    ) -> str | messages.BinaryContent:\n        if isinstance(resource, mcp_types.TextResourceContents):\n            return resource.text\n        elif isinstance(resource, mcp_types.BlobResourceContents):\n            return messages.BinaryContent(\n                data=base64.b64decode(resource.blob), media_type=resource.mimeType or 'application/octet-stream'\n            )\n        else:\n            assert_never(resource)\n\n    def __eq__(self, value: object, /) -> bool:\n        return isinstance(value, MCPServer) and self.id == value.id and self.tool_prefix == value.tool_prefix\n\n```\n\n#### tool_prefix\n\n```python\ntool_prefix: str | None = tool_prefix\n\n```\n\nA prefix to add to all tools that are registered with the server.\n\nIf not empty, will include a trailing underscore(`_`).\n\ne.g. if `tool_prefix='foo'`, then a tool named `bar` will be registered as `foo_bar`\n\n#### log_level\n\n```python\nlog_level: LoggingLevel | None = log_level\n\n```\n\nThe log level to set when connecting to the server, if any.\n\nSee <https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#logging> for more details.\n\nIf `None`, no log level will be set.\n\n#### log_handler\n\n```python\nlog_handler: LoggingFnT | None = log_handler\n\n```\n\nA handler for logging messages from the server.\n\n#### timeout\n\n```python\ntimeout: float = timeout\n\n```\n\nThe timeout in seconds to wait for the client to initialize.\n\n#### read_timeout\n\n```python\nread_timeout: float = read_timeout\n\n```\n\nMaximum time in seconds to wait for new messages before timing out.\n\nThis timeout applies to the long-lived connection after it's established. If no new messages are received within this time, the connection will be considered stale and may be closed. Defaults to 5 minutes (300 seconds).\n\n#### process_tool_call\n\n```python\nprocess_tool_call: ProcessToolCallback | None = (\n    process_tool_call\n)\n\n```\n\nHook to customize tool calling and optionally pass extra metadata.\n\n#### allow_sampling\n\n```python\nallow_sampling: bool = allow_sampling\n\n```\n\nWhether to allow MCP sampling through this client.\n\n#### sampling_model\n\n```python\nsampling_model: Model | None = sampling_model\n\n```\n\nThe model to use for sampling.\n\n#### max_retries\n\n```python\nmax_retries: int = max_retries\n\n```\n\nThe maximum number of times to retry a tool call.\n\n#### elicitation_callback\n\n```python\nelicitation_callback: ElicitationFnT | None = (\n    elicitation_callback\n)\n\n```\n\nCallback function to handle elicitation requests from the server.\n\n#### client_streams\n\n```python\nclient_streams() -> AsyncIterator[\n    tuple[\n        MemoryObjectReceiveStream[\n            SessionMessage | Exception\n        ],\n        MemoryObjectSendStream[SessionMessage],\n    ]\n]\n\n```\n\nCreate the streams for the MCP server.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\n@abstractmethod\n@asynccontextmanager\nasync def client_streams(\n    self,\n) -> AsyncIterator[\n    tuple[\n        MemoryObjectReceiveStream[SessionMessage | Exception],\n        MemoryObjectSendStream[SessionMessage],\n    ]\n]:\n    \"\"\"Create the streams for the MCP server.\"\"\"\n    raise NotImplementedError('MCP Server subclasses must implement this method.')\n    yield\n\n```\n\n#### server_info\n\n```python\nserver_info: Implementation\n\n```\n\nAccess the information send by the MCP server during initialization.\n\n#### capabilities\n\n```python\ncapabilities: ServerCapabilities\n\n```\n\nAccess the capabilities advertised by the MCP server during initialization.\n\n#### instructions\n\n```python\ninstructions: str | None\n\n```\n\nAccess the instructions sent by the MCP server during initialization.\n\n#### list_tools\n\n```python\nlist_tools() -> list[Tool]\n\n```\n\nRetrieve tools that are currently active on the server.\n\nNote:\n\n- We don't cache tools as they might change.\n- We also don't subscribe to the server to avoid complexity.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\nasync def list_tools(self) -> list[mcp_types.Tool]:\n    \"\"\"Retrieve tools that are currently active on the server.\n\n    Note:\n    - We don't cache tools as they might change.\n    - We also don't subscribe to the server to avoid complexity.\n    \"\"\"\n    async with self:  # Ensure server is running\n        result = await self._client.list_tools()\n    return result.tools\n\n```\n\n#### direct_call_tool\n\n```python\ndirect_call_tool(\n    name: str,\n    args: dict[str, Any],\n    metadata: dict[str, Any] | None = None,\n) -> ToolResult\n\n```\n\nCall a tool on the server.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str` | The name of the tool to call. | *required* | | `args` | `dict[str, Any]` | The arguments to pass to the tool. | *required* | | `metadata` | `dict[str, Any] | None` | Request-level metadata (optional) | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `ToolResult` | The result of the tool call. |\n\nRaises:\n\n| Type | Description | | --- | --- | | `ModelRetry` | If the tool call fails. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\nasync def direct_call_tool(\n    self,\n    name: str,\n    args: dict[str, Any],\n    metadata: dict[str, Any] | None = None,\n) -> ToolResult:\n    \"\"\"Call a tool on the server.\n\n    Args:\n        name: The name of the tool to call.\n        args: The arguments to pass to the tool.\n        metadata: Request-level metadata (optional)\n\n    Returns:\n        The result of the tool call.\n\n    Raises:\n        ModelRetry: If the tool call fails.\n    \"\"\"\n    async with self:  # Ensure server is running\n        try:\n            result = await self._client.send_request(\n                mcp_types.ClientRequest(\n                    mcp_types.CallToolRequest(\n                        method='tools/call',\n                        params=mcp_types.CallToolRequestParams(\n                            name=name,\n                            arguments=args,\n                            _meta=mcp_types.RequestParams.Meta(**metadata) if metadata else None,\n                        ),\n                    )\n                ),\n                mcp_types.CallToolResult,\n            )\n        except mcp_exceptions.McpError as e:\n            raise exceptions.ModelRetry(e.error.message)\n\n    if result.isError:\n        message: str | None = None\n        if result.content:  # pragma: no branch\n            text_parts = [part.text for part in result.content if isinstance(part, mcp_types.TextContent)]\n            message = '\\n'.join(text_parts)\n\n        raise exceptions.ModelRetry(message or 'MCP tool call failed')\n\n    # Prefer structured content if there are only text parts, which per the docs would contain the JSON-encoded structured content for backward compatibility.\n    # See https://github.com/modelcontextprotocol/python-sdk#structured-output\n    if (structured := result.structuredContent) and not any(\n        not isinstance(part, mcp_types.TextContent) for part in result.content\n    ):\n        # The MCP SDK wraps primitives and generic types like list in a `result` key, but we want to use the raw value returned by the tool function.\n        # See https://github.com/modelcontextprotocol/python-sdk#structured-output\n        if isinstance(structured, dict) and len(structured) == 1 and 'result' in structured:\n            return structured['result']\n        return structured\n\n    mapped = [await self._map_tool_result_part(part) for part in result.content]\n    return mapped[0] if len(mapped) == 1 else mapped\n\n```\n\n#### list_resources\n\n```python\nlist_resources() -> list[Resource]\n\n```\n\nRetrieve resources that are currently present on the server.\n\nNote:\n\n- We don't cache resources as they might change.\n- We also don't subscribe to resource changes to avoid complexity.\n\nRaises:\n\n| Type | Description | | --- | --- | | `MCPError` | If the server returns an error. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\nasync def list_resources(self) -> list[Resource]:\n    \"\"\"Retrieve resources that are currently present on the server.\n\n    Note:\n    - We don't cache resources as they might change.\n    - We also don't subscribe to resource changes to avoid complexity.\n\n    Raises:\n        MCPError: If the server returns an error.\n    \"\"\"\n    async with self:  # Ensure server is running\n        if not self.capabilities.resources:\n            return []\n        try:\n            result = await self._client.list_resources()\n        except mcp_exceptions.McpError as e:\n            raise MCPError.from_mcp_sdk(e) from e\n    return [Resource.from_mcp_sdk(r) for r in result.resources]\n\n```\n\n#### list_resource_templates\n\n```python\nlist_resource_templates() -> list[ResourceTemplate]\n\n```\n\nRetrieve resource templates that are currently present on the server.\n\nRaises:\n\n| Type | Description | | --- | --- | | `MCPError` | If the server returns an error. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\nasync def list_resource_templates(self) -> list[ResourceTemplate]:\n    \"\"\"Retrieve resource templates that are currently present on the server.\n\n    Raises:\n        MCPError: If the server returns an error.\n    \"\"\"\n    async with self:  # Ensure server is running\n        if not self.capabilities.resources:\n            return []\n        try:\n            result = await self._client.list_resource_templates()\n        except mcp_exceptions.McpError as e:\n            raise MCPError.from_mcp_sdk(e) from e\n    return [ResourceTemplate.from_mcp_sdk(t) for t in result.resourceTemplates]\n\n```\n\n#### read_resource\n\n```python\nread_resource(\n    uri: str,\n) -> str | BinaryContent | list[str | BinaryContent]\n\n```\n\n```python\nread_resource(\n    uri: Resource,\n) -> str | BinaryContent | list[str | BinaryContent]\n\n```\n\n```python\nread_resource(\n    uri: str | Resource,\n) -> str | BinaryContent | list[str | BinaryContent]\n\n```\n\nRead the contents of a specific resource by URI.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `uri` | `str | Resource` | The URI of the resource to read, or a Resource object. | *required* |\n\nReturns:\n\n| Type | Description | | --- | --- | | `str | BinaryContent | list[str | BinaryContent]` | The resource contents. If the resource has a single content item, returns that item directly. | | `str | BinaryContent | list[str | BinaryContent]` | If the resource has multiple content items, returns a list of items. |\n\nRaises:\n\n| Type | Description | | --- | --- | | `MCPError` | If the server returns an error. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\nasync def read_resource(\n    self, uri: str | Resource\n) -> str | messages.BinaryContent | list[str | messages.BinaryContent]:\n    \"\"\"Read the contents of a specific resource by URI.\n\n    Args:\n        uri: The URI of the resource to read, or a Resource object.\n\n    Returns:\n        The resource contents. If the resource has a single content item, returns that item directly.\n        If the resource has multiple content items, returns a list of items.\n\n    Raises:\n        MCPError: If the server returns an error.\n    \"\"\"\n    resource_uri = uri if isinstance(uri, str) else uri.uri\n    async with self:  # Ensure server is running\n        try:\n            result = await self._client.read_resource(AnyUrl(resource_uri))\n        except mcp_exceptions.McpError as e:\n            raise MCPError.from_mcp_sdk(e) from e\n\n    return (\n        self._get_content(result.contents[0])\n        if len(result.contents) == 1\n        else [self._get_content(resource) for resource in result.contents]\n    )\n\n```\n\n#### __aenter__\n\n```python\n__aenter__() -> Self\n\n```\n\nEnter the MCP server context.\n\nThis will initialize the connection to the server. If this server is an MCPServerStdio, the server will first be started as a subprocess.\n\nThis is a no-op if the MCP server has already been entered.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\nasync def __aenter__(self) -> Self:\n    \"\"\"Enter the MCP server context.\n\n    This will initialize the connection to the server.\n    If this server is an [`MCPServerStdio`][pydantic_ai.mcp.MCPServerStdio], the server will first be started as a subprocess.\n\n    This is a no-op if the MCP server has already been entered.\n    \"\"\"\n    async with self._enter_lock:\n        if self._running_count == 0:\n            async with AsyncExitStack() as exit_stack:\n                self._read_stream, self._write_stream = await exit_stack.enter_async_context(self.client_streams())\n                client = ClientSession(\n                    read_stream=self._read_stream,\n                    write_stream=self._write_stream,\n                    sampling_callback=self._sampling_callback if self.allow_sampling else None,\n                    elicitation_callback=self.elicitation_callback,\n                    logging_callback=self.log_handler,\n                    read_timeout_seconds=timedelta(seconds=self.read_timeout),\n                )\n                self._client = await exit_stack.enter_async_context(client)\n\n                with anyio.fail_after(self.timeout):\n                    result = await self._client.initialize()\n                    self._server_info = result.serverInfo\n                    self._server_capabilities = ServerCapabilities.from_mcp_sdk(result.capabilities)\n                    self._instructions = result.instructions\n                    if log_level := self.log_level:\n                        await self._client.set_logging_level(log_level)\n\n                self._exit_stack = exit_stack.pop_all()\n        self._running_count += 1\n    return self\n\n```\n\n#### is_running\n\n```python\nis_running: bool\n\n```\n\nCheck if the MCP server is running.\n\n### MCPServerStdio\n\nBases: `MCPServer`\n\nRuns an MCP server in a subprocess and communicates with it over stdin/stdout.\n\nThis class implements the stdio transport from the MCP specification. See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio> for more information.\n\nNote\n\nUsing this class as an async context manager will start the server as a subprocess when entering the context, and stop it when exiting the context.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(  # (1)!\n    'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10\n)\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n```\n\n1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n````python\nclass MCPServerStdio(MCPServer):\n    \"\"\"Runs an MCP server in a subprocess and communicates with it over stdin/stdout.\n\n    This class implements the stdio transport from the MCP specification.\n    See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio> for more information.\n\n    !!! note\n        Using this class as an async context manager will start the server as a subprocess when entering the context,\n        and stop it when exiting the context.\n\n    Example:\n    ```python {py=\"3.10\"}\n    from pydantic_ai import Agent\n    from pydantic_ai.mcp import MCPServerStdio\n\n    server = MCPServerStdio(  # (1)!\n        'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10\n    )\n    agent = Agent('openai:gpt-4o', toolsets=[server])\n    ```\n\n    1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.\n    \"\"\"\n\n    command: str\n    \"\"\"The command to run.\"\"\"\n\n    args: Sequence[str]\n    \"\"\"The arguments to pass to the command.\"\"\"\n\n    env: dict[str, str] | None\n    \"\"\"The environment variables the CLI server will have access to.\n\n    By default the subprocess will not inherit any environment variables from the parent process.\n    If you want to inherit the environment variables from the parent process, use `env=os.environ`.\n    \"\"\"\n\n    cwd: str | Path | None\n    \"\"\"The working directory to use when spawning the process.\"\"\"\n\n    # last fields are re-defined from the parent class so they appear as fields\n    tool_prefix: str | None\n    log_level: mcp_types.LoggingLevel | None\n    log_handler: LoggingFnT | None\n    timeout: float\n    read_timeout: float\n    process_tool_call: ProcessToolCallback | None\n    allow_sampling: bool\n    sampling_model: models.Model | None\n    max_retries: int\n    elicitation_callback: ElicitationFnT | None = None\n\n    def __init__(\n        self,\n        command: str,\n        args: Sequence[str],\n        *,\n        env: dict[str, str] | None = None,\n        cwd: str | Path | None = None,\n        tool_prefix: str | None = None,\n        log_level: mcp_types.LoggingLevel | None = None,\n        log_handler: LoggingFnT | None = None,\n        timeout: float = 5,\n        read_timeout: float = 5 * 60,\n        process_tool_call: ProcessToolCallback | None = None,\n        allow_sampling: bool = True,\n        sampling_model: models.Model | None = None,\n        max_retries: int = 1,\n        elicitation_callback: ElicitationFnT | None = None,\n        id: str | None = None,\n    ):\n        \"\"\"Build a new MCP server.\n\n        Args:\n            command: The command to run.\n            args: The arguments to pass to the command.\n            env: The environment variables to set in the subprocess.\n            cwd: The working directory to use when spawning the process.\n            tool_prefix: A prefix to add to all tools that are registered with the server.\n            log_level: The log level to set when connecting to the server, if any.\n            log_handler: A handler for logging messages from the server.\n            timeout: The timeout in seconds to wait for the client to initialize.\n            read_timeout: Maximum time in seconds to wait for new messages before timing out.\n            process_tool_call: Hook to customize tool calling and optionally pass extra metadata.\n            allow_sampling: Whether to allow MCP sampling through this client.\n            sampling_model: The model to use for sampling.\n            max_retries: The maximum number of times to retry a tool call.\n            elicitation_callback: Callback function to handle elicitation requests from the server.\n            id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.\n        \"\"\"\n        self.command = command\n        self.args = args\n        self.env = env\n        self.cwd = cwd\n\n        super().__init__(\n            tool_prefix,\n            log_level,\n            log_handler,\n            timeout,\n            read_timeout,\n            process_tool_call,\n            allow_sampling,\n            sampling_model,\n            max_retries,\n            elicitation_callback,\n            id=id,\n        )\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:\n        return core_schema.no_info_after_validator_function(\n            lambda dct: MCPServerStdio(**dct),\n            core_schema.typed_dict_schema(\n                {\n                    'command': core_schema.typed_dict_field(core_schema.str_schema()),\n                    'args': core_schema.typed_dict_field(core_schema.list_schema(core_schema.str_schema())),\n                    'env': core_schema.typed_dict_field(\n                        core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()),\n                        required=False,\n                    ),\n                }\n            ),\n        )\n\n    @asynccontextmanager\n    async def client_streams(\n        self,\n    ) -> AsyncIterator[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n        ]\n    ]:\n        server = StdioServerParameters(command=self.command, args=list(self.args), env=self.env, cwd=self.cwd)\n        async with stdio_client(server=server) as (read_stream, write_stream):\n            yield read_stream, write_stream\n\n    def __repr__(self) -> str:\n        repr_args = [\n            f'command={self.command!r}',\n            f'args={self.args!r}',\n        ]\n        if self.id:\n            repr_args.append(f'id={self.id!r}')  # pragma: lax no cover\n        return f'{self.__class__.__name__}({\", \".join(repr_args)})'\n\n    def __eq__(self, value: object, /) -> bool:\n        return (\n            super().__eq__(value)\n            and isinstance(value, MCPServerStdio)\n            and self.command == value.command\n            and self.args == value.args\n            and self.env == value.env\n            and self.cwd == value.cwd\n        )\n\n````\n\n#### __init__\n\n```python\n__init__(\n    command: str,\n    args: Sequence[str],\n    *,\n    env: dict[str, str] | None = None,\n    cwd: str | Path | None = None,\n    tool_prefix: str | None = None,\n    log_level: LoggingLevel | None = None,\n    log_handler: LoggingFnT | None = None,\n    timeout: float = 5,\n    read_timeout: float = 5 * 60,\n    process_tool_call: ProcessToolCallback | None = None,\n    allow_sampling: bool = True,\n    sampling_model: Model | None = None,\n    max_retries: int = 1,\n    elicitation_callback: ElicitationFnT | None = None,\n    id: str | None = None\n)\n\n```\n\nBuild a new MCP server.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `command` | `str` | The command to run. | *required* | | `args` | `Sequence[str]` | The arguments to pass to the command. | *required* | | `env` | `dict[str, str] | None` | The environment variables to set in the subprocess. | `None` | | `cwd` | `str | Path | None` | The working directory to use when spawning the process. | `None` | | `tool_prefix` | `str | None` | A prefix to add to all tools that are registered with the server. | `None` | | `log_level` | `LoggingLevel | None` | The log level to set when connecting to the server, if any. | `None` | | `log_handler` | `LoggingFnT | None` | A handler for logging messages from the server. | `None` | | `timeout` | `float` | The timeout in seconds to wait for the client to initialize. | `5` | | `read_timeout` | `float` | Maximum time in seconds to wait for new messages before timing out. | `5 * 60` | | `process_tool_call` | `ProcessToolCallback | None` | Hook to customize tool calling and optionally pass extra metadata. | `None` | | `allow_sampling` | `bool` | Whether to allow MCP sampling through this client. | `True` | | `sampling_model` | `Model | None` | The model to use for sampling. | `None` | | `max_retries` | `int` | The maximum number of times to retry a tool call. | `1` | | `elicitation_callback` | `ElicitationFnT | None` | Callback function to handle elicitation requests from the server. | `None` | | `id` | `str | None` | An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\ndef __init__(\n    self,\n    command: str,\n    args: Sequence[str],\n    *,\n    env: dict[str, str] | None = None,\n    cwd: str | Path | None = None,\n    tool_prefix: str | None = None,\n    log_level: mcp_types.LoggingLevel | None = None,\n    log_handler: LoggingFnT | None = None,\n    timeout: float = 5,\n    read_timeout: float = 5 * 60,\n    process_tool_call: ProcessToolCallback | None = None,\n    allow_sampling: bool = True,\n    sampling_model: models.Model | None = None,\n    max_retries: int = 1,\n    elicitation_callback: ElicitationFnT | None = None,\n    id: str | None = None,\n):\n    \"\"\"Build a new MCP server.\n\n    Args:\n        command: The command to run.\n        args: The arguments to pass to the command.\n        env: The environment variables to set in the subprocess.\n        cwd: The working directory to use when spawning the process.\n        tool_prefix: A prefix to add to all tools that are registered with the server.\n        log_level: The log level to set when connecting to the server, if any.\n        log_handler: A handler for logging messages from the server.\n        timeout: The timeout in seconds to wait for the client to initialize.\n        read_timeout: Maximum time in seconds to wait for new messages before timing out.\n        process_tool_call: Hook to customize tool calling and optionally pass extra metadata.\n        allow_sampling: Whether to allow MCP sampling through this client.\n        sampling_model: The model to use for sampling.\n        max_retries: The maximum number of times to retry a tool call.\n        elicitation_callback: Callback function to handle elicitation requests from the server.\n        id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.\n    \"\"\"\n    self.command = command\n    self.args = args\n    self.env = env\n    self.cwd = cwd\n\n    super().__init__(\n        tool_prefix,\n        log_level,\n        log_handler,\n        timeout,\n        read_timeout,\n        process_tool_call,\n        allow_sampling,\n        sampling_model,\n        max_retries,\n        elicitation_callback,\n        id=id,\n    )\n\n```\n\n#### command\n\n```python\ncommand: str = command\n\n```\n\nThe command to run.\n\n#### args\n\n```python\nargs: Sequence[str] = args\n\n```\n\nThe arguments to pass to the command.\n\n#### env\n\n```python\nenv: dict[str, str] | None = env\n\n```\n\nThe environment variables the CLI server will have access to.\n\nBy default the subprocess will not inherit any environment variables from the parent process. If you want to inherit the environment variables from the parent process, use `env=os.environ`.\n\n#### cwd\n\n```python\ncwd: str | Path | None = cwd\n\n```\n\nThe working directory to use when spawning the process.\n\n### MCPServerSSE\n\nBases: `_MCPServerHTTP`\n\nAn MCP server that connects over streamable HTTP connections.\n\nThis class implements the SSE transport from the MCP specification. See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\nNote\n\nUsing this class as an async context manager will create a new pool of HTTP connections to connect to a server which should already be running.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\nserver = MCPServerSSE('http://localhost:3001/sse')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n````python\nclass MCPServerSSE(_MCPServerHTTP):\n    \"\"\"An MCP server that connects over streamable HTTP connections.\n\n    This class implements the SSE transport from the MCP specification.\n    See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\n    !!! note\n        Using this class as an async context manager will create a new pool of HTTP connections to connect\n        to a server which should already be running.\n\n    Example:\n    ```python {py=\"3.10\"}\n    from pydantic_ai import Agent\n    from pydantic_ai.mcp import MCPServerSSE\n\n    server = MCPServerSSE('http://localhost:3001/sse')\n    agent = Agent('openai:gpt-4o', toolsets=[server])\n    ```\n    \"\"\"\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:\n        return core_schema.no_info_after_validator_function(\n            lambda dct: MCPServerSSE(**dct),\n            core_schema.typed_dict_schema(\n                {\n                    'url': core_schema.typed_dict_field(core_schema.str_schema()),\n                    'headers': core_schema.typed_dict_field(\n                        core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()), required=False\n                    ),\n                }\n            ),\n        )\n\n    @property\n    def _transport_client(self):\n        return sse_client  # pragma: no cover\n\n    def __eq__(self, value: object, /) -> bool:\n        return super().__eq__(value) and isinstance(value, MCPServerSSE) and self.url == value.url\n\n````\n\n### MCPServerHTTP\n\nBases: `MCPServerSSE`\n\nDeprecated\n\nThe `MCPServerHTTP` class is deprecated, use `MCPServerSSE` instead.\n\nAn MCP server that connects over HTTP using the old SSE transport.\n\nThis class implements the SSE transport from the MCP specification. See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\nNote\n\nUsing this class as an async context manager will create a new pool of HTTP connections to connect to a server which should already be running.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerHTTP\n\nserver = MCPServerHTTP('http://localhost:3001/sse')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n````python\n@deprecated('The `MCPServerHTTP` class is deprecated, use `MCPServerSSE` instead.')\nclass MCPServerHTTP(MCPServerSSE):\n    \"\"\"An MCP server that connects over HTTP using the old SSE transport.\n\n    This class implements the SSE transport from the MCP specification.\n    See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\n    !!! note\n        Using this class as an async context manager will create a new pool of HTTP connections to connect\n        to a server which should already be running.\n\n    Example:\n    ```python {py=\"3.10\" test=\"skip\"}\n    from pydantic_ai import Agent\n    from pydantic_ai.mcp import MCPServerHTTP\n\n    server = MCPServerHTTP('http://localhost:3001/sse')\n    agent = Agent('openai:gpt-4o', toolsets=[server])\n    ```\n    \"\"\"\n\n````\n\n### MCPServerStreamableHTTP\n\nBases: `_MCPServerHTTP`\n\nAn MCP server that connects over HTTP using the Streamable HTTP transport.\n\nThis class implements the Streamable HTTP transport from the MCP specification. See <https://modelcontextprotocol.io/introduction#streamable-http> for more information.\n\nNote\n\nUsing this class as an async context manager will create a new pool of HTTP connections to connect to a server which should already be running.\n\nExample:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n````python\nclass MCPServerStreamableHTTP(_MCPServerHTTP):\n    \"\"\"An MCP server that connects over HTTP using the Streamable HTTP transport.\n\n    This class implements the Streamable HTTP transport from the MCP specification.\n    See <https://modelcontextprotocol.io/introduction#streamable-http> for more information.\n\n    !!! note\n        Using this class as an async context manager will create a new pool of HTTP connections to connect\n        to a server which should already be running.\n\n    Example:\n    ```python {py=\"3.10\"}\n    from pydantic_ai import Agent\n    from pydantic_ai.mcp import MCPServerStreamableHTTP\n\n    server = MCPServerStreamableHTTP('http://localhost:8000/mcp')\n    agent = Agent('openai:gpt-4o', toolsets=[server])\n    ```\n    \"\"\"\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:\n        return core_schema.no_info_after_validator_function(\n            lambda dct: MCPServerStreamableHTTP(**dct),\n            core_schema.typed_dict_schema(\n                {\n                    'url': core_schema.typed_dict_field(core_schema.str_schema()),\n                    'headers': core_schema.typed_dict_field(\n                        core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()), required=False\n                    ),\n                }\n            ),\n        )\n\n    @property\n    def _transport_client(self):\n        return streamablehttp_client\n\n    def __eq__(self, value: object, /) -> bool:\n        return super().__eq__(value) and isinstance(value, MCPServerStreamableHTTP) and self.url == value.url\n\n````\n\n### load_mcp_servers\n\n```python\nload_mcp_servers(\n    config_path: str | Path,\n) -> list[\n    MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE\n]\n\n```\n\nLoad MCP servers from a configuration file.\n\nEnvironment variables can be referenced in the configuration file using:\n\n- `${VAR_NAME}` syntax - expands to the value of VAR_NAME, raises error if not defined\n- `${VAR_NAME:-default}` syntax - expands to VAR_NAME if set, otherwise uses the default value\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `config_path` | `str | Path` | The path to the configuration file. | *required* |\n\nReturns:\n\n| Type | Description | | --- | --- | | `list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE]` | A list of MCP servers. |\n\nRaises:\n\n| Type | Description | | --- | --- | | `FileNotFoundError` | If the configuration file does not exist. | | `ValidationError` | If the configuration file does not match the schema. | | `ValueError` | If an environment variable referenced in the configuration is not defined and no default value is provided. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n```python\ndef load_mcp_servers(config_path: str | Path) -> list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE]:\n    \"\"\"Load MCP servers from a configuration file.\n\n    Environment variables can be referenced in the configuration file using:\n    - `${VAR_NAME}` syntax - expands to the value of VAR_NAME, raises error if not defined\n    - `${VAR_NAME:-default}` syntax - expands to VAR_NAME if set, otherwise uses the default value\n\n    Args:\n        config_path: The path to the configuration file.\n\n    Returns:\n        A list of MCP servers.\n\n    Raises:\n        FileNotFoundError: If the configuration file does not exist.\n        ValidationError: If the configuration file does not match the schema.\n        ValueError: If an environment variable referenced in the configuration is not defined and no default value is provided.\n    \"\"\"\n    config_path = Path(config_path)\n\n    if not config_path.exists():\n        raise FileNotFoundError(f'Config file {config_path} not found')\n\n    config_data = pydantic_core.from_json(config_path.read_bytes())\n    expanded_config_data = _expand_env_vars(config_data)\n    config = MCPServerConfig.model_validate(expanded_config_data)\n\n    servers: list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE] = []\n    for name, server in config.mcp_servers.items():\n        server.id = name\n        server.tool_prefix = name\n        servers.append(server)\n\n    return servers\n\n```",
  "content_length": 65933
}