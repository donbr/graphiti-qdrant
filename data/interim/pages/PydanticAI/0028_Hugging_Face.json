{
  "title": "Hugging Face",
  "source_url": null,
  "content": "[Hugging Face](https://huggingface.co/) is an AI platform with all major open source models, datasets, MCPs, and demos. You can use [Inference Providers](https://huggingface.co/docs/inference-providers) to run open source models like DeepSeek R1 on scalable serverless infrastructure.\n\n## Install\n\nTo use `HuggingFaceModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `huggingface` optional group:\n\n```bash\npip install \"pydantic-ai-slim[huggingface]\"\n\n```\n\n```bash\nuv add \"pydantic-ai-slim[huggingface]\"\n\n```\n\n## Configuration\n\nTo use [Hugging Face](https://huggingface.co/) inference, you'll need to set up an account which will give you [free tier](https://huggingface.co/docs/inference-providers/pricing) allowance on [Inference Providers](https://huggingface.co/docs/inference-providers). To setup inference, follow these steps:\n\n1. Go to [Hugging Face](https://huggingface.co/join) and sign up for an account.\n1. Create a new access token in [Hugging Face](https://huggingface.co/settings/tokens).\n1. Set the `HF_TOKEN` environment variable to the token you just created.\n\nOnce you have a Hugging Face access token, you can set it as an environment variable:\n\n```bash\nexport HF_TOKEN='hf_token'\n\n```\n\n## Usage\n\nYou can then use HuggingFaceModel by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('huggingface:Qwen/Qwen3-235B-A22B')\n...\n\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B')\nagent = Agent(model)\n...\n\n```\n\nBy default, the HuggingFaceModel uses the HuggingFaceProvider that will select automatically the first of the inference providers (Cerebras, Together AI, Cohere..etc) available for the model, sorted by your preferred order in https://hf.co/settings/inference-providers.\n\n## Configure the provider\n\nIf you want to pass parameters in code to the provider, you can programmatically instantiate the HuggingFaceProvider and pass it to the model:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B', provider=HuggingFaceProvider(api_key='hf_token', provider_name='nebius'))\nagent = Agent(model)\n...\n\n```\n\n## Custom Hugging Face client\n\nHuggingFaceProvider also accepts a custom [`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient) client via the `hf_client` parameter, so you can customise the `headers`, `bill_to` (billing to an HF organization you're a member of), `base_url` etc. as defined in the [Hugging Face Hub python library docs](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client).\n\n```python\nfrom huggingface_hub import AsyncInferenceClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nclient = AsyncInferenceClient(\n    bill_to='openai',\n    api_key='hf_token',\n    provider='fireworks-ai',\n)\n\nmodel = HuggingFaceModel(\n    'Qwen/Qwen3-235B-A22B',\n    provider=HuggingFaceProvider(hf_client=client),\n)\nagent = Agent(model)\n...\n\n```",
  "content_length": 3383
}