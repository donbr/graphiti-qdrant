{
  "title": "`pydantic_ai.models.bedrock`",
  "source_url": null,
  "content": "## Setup\n\nFor details on how to set up authentication with this model, see [model configuration for Bedrock](../../../models/bedrock/).\n\n### LatestBedrockModelNames\n\n```python\nLatestBedrockModelNames = Literal[\n    \"amazon.titan-tg1-large\",\n    \"amazon.titan-text-lite-v1\",\n    \"amazon.titan-text-express-v1\",\n    \"us.amazon.nova-pro-v1:0\",\n    \"us.amazon.nova-lite-v1:0\",\n    \"us.amazon.nova-micro-v1:0\",\n    \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    \"anthropic.claude-3-5-haiku-20241022-v1:0\",\n    \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n    \"anthropic.claude-instant-v1\",\n    \"anthropic.claude-v2:1\",\n    \"anthropic.claude-v2\",\n    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n    \"us.anthropic.claude-3-sonnet-20240229-v1:0\",\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    \"us.anthropic.claude-3-haiku-20240307-v1:0\",\n    \"anthropic.claude-3-opus-20240229-v1:0\",\n    \"us.anthropic.claude-3-opus-20240229-v1:0\",\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    \"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    \"anthropic.claude-opus-4-20250514-v1:0\",\n    \"us.anthropic.claude-opus-4-20250514-v1:0\",\n    \"global.anthropic.claude-opus-4-5-20251101-v1:0\",\n    \"anthropic.claude-sonnet-4-20250514-v1:0\",\n    \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n    \"eu.anthropic.claude-sonnet-4-20250514-v1:0\",\n    \"anthropic.claude-sonnet-4-5-20250929-v1:0\",\n    \"us.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n    \"eu.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n    \"anthropic.claude-haiku-4-5-20251001-v1:0\",\n    \"us.anthropic.claude-haiku-4-5-20251001-v1:0\",\n    \"eu.anthropic.claude-haiku-4-5-20251001-v1:0\",\n    \"cohere.command-text-v14\",\n    \"cohere.command-r-v1:0\",\n    \"cohere.command-r-plus-v1:0\",\n    \"cohere.command-light-text-v14\",\n    \"meta.llama3-8b-instruct-v1:0\",\n    \"meta.llama3-70b-instruct-v1:0\",\n    \"meta.llama3-1-8b-instruct-v1:0\",\n    \"us.meta.llama3-1-8b-instruct-v1:0\",\n    \"meta.llama3-1-70b-instruct-v1:0\",\n    \"us.meta.llama3-1-70b-instruct-v1:0\",\n    \"meta.llama3-1-405b-instruct-v1:0\",\n    \"us.meta.llama3-2-11b-instruct-v1:0\",\n    \"us.meta.llama3-2-90b-instruct-v1:0\",\n    \"us.meta.llama3-2-1b-instruct-v1:0\",\n    \"us.meta.llama3-2-3b-instruct-v1:0\",\n    \"us.meta.llama3-3-70b-instruct-v1:0\",\n    \"mistral.mistral-7b-instruct-v0:2\",\n    \"mistral.mixtral-8x7b-instruct-v0:1\",\n    \"mistral.mistral-large-2402-v1:0\",\n    \"mistral.mistral-large-2407-v1:0\",\n]\n\n```\n\nLatest Bedrock models.\n\n### BedrockModelName\n\n```python\nBedrockModelName = str | LatestBedrockModelNames\n\n```\n\nPossible Bedrock model names.\n\nSince Bedrock supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Bedrock docs](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) for a full list.\n\n### BedrockModelSettings\n\nBases: `ModelSettings`\n\nSettings for Bedrock models.\n\nSee [the Bedrock Converse API docs](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax) for a full list. See [the boto3 implementation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) of the Bedrock Converse API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/bedrock.py`\n\n```python\nclass BedrockModelSettings(ModelSettings, total=False):\n    \"\"\"Settings for Bedrock models.\n\n    See [the Bedrock Converse API docs](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax) for a full list.\n    See [the boto3 implementation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) of the Bedrock Converse API.\n    \"\"\"\n\n    # ALL FIELDS MUST BE `bedrock_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\n    bedrock_guardrail_config: GuardrailConfigurationTypeDef\n    \"\"\"Content moderation and safety settings for Bedrock API requests.\n\n    See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_GuardrailConfiguration.html>.\n    \"\"\"\n\n    bedrock_performance_configuration: PerformanceConfigurationTypeDef\n    \"\"\"Performance optimization settings for model inference.\n\n    See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PerformanceConfiguration.html>.\n    \"\"\"\n\n    bedrock_request_metadata: dict[str, str]\n    \"\"\"Additional metadata to attach to Bedrock API requests.\n\n    See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax>.\n    \"\"\"\n\n    bedrock_additional_model_response_fields_paths: list[str]\n    \"\"\"JSON paths to extract additional fields from model responses.\n\n    See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>.\n    \"\"\"\n\n    bedrock_prompt_variables: Mapping[str, PromptVariableValuesTypeDef]\n    \"\"\"Variables for substitution into prompt templates.\n\n    See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PromptVariableValues.html>.\n    \"\"\"\n\n    bedrock_additional_model_requests_fields: Mapping[str, Any]\n    \"\"\"Additional model-specific parameters to include in requests.\n\n    See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>.\n    \"\"\"\n\n```\n\n#### bedrock_guardrail_config\n\n```python\nbedrock_guardrail_config: GuardrailConfigurationTypeDef\n\n```\n\nContent moderation and safety settings for Bedrock API requests.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_GuardrailConfiguration.html>.\n\n#### bedrock_performance_configuration\n\n```python\nbedrock_performance_configuration: (\n    PerformanceConfigurationTypeDef\n)\n\n```\n\nPerformance optimization settings for model inference.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PerformanceConfiguration.html>.\n\n#### bedrock_request_metadata\n\n```python\nbedrock_request_metadata: dict[str, str]\n\n```\n\nAdditional metadata to attach to Bedrock API requests.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax>.\n\n#### bedrock_additional_model_response_fields_paths\n\n```python\nbedrock_additional_model_response_fields_paths: list[str]\n\n```\n\nJSON paths to extract additional fields from model responses.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>.\n\n#### bedrock_prompt_variables\n\n```python\nbedrock_prompt_variables: Mapping[\n    str, PromptVariableValuesTypeDef\n]\n\n```\n\nVariables for substitution into prompt templates.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PromptVariableValues.html>.\n\n#### bedrock_additional_model_requests_fields\n\n```python\nbedrock_additional_model_requests_fields: Mapping[str, Any]\n\n```\n\nAdditional model-specific parameters to include in requests.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>.\n\n### BedrockConverseModel\n\nBases: `Model`\n\nA model that uses the Bedrock Converse API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/bedrock.py`\n\n```python\n@dataclass(init=False)\nclass BedrockConverseModel(Model):\n    \"\"\"A model that uses the Bedrock Converse API.\"\"\"\n\n    client: BedrockRuntimeClient\n\n    _model_name: BedrockModelName = field(repr=False)\n    _provider: Provider[BaseClient] = field(repr=False)\n\n    def __init__(\n        self,\n        model_name: BedrockModelName,\n        *,\n        provider: Literal['bedrock', 'gateway'] | Provider[BaseClient] = 'bedrock',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize a Bedrock model.\n\n        Args:\n            model_name: The name of the model to use.\n            model_name: The name of the Bedrock model to use. List of model names available\n                [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).\n            provider: The provider to use for authentication and API access. Can be either the string\n                'bedrock' or an instance of `Provider[BaseClient]`. If not provided, a new provider will be\n                created using the other parameters.\n            profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n            settings: Model-specific settings that will be used as defaults for this model.\n        \"\"\"\n        self._model_name = model_name\n\n        if isinstance(provider, str):\n            provider = infer_provider('gateway/bedrock' if provider == 'gateway' else provider)\n        self._provider = provider\n        self.client = cast('BedrockRuntimeClient', provider.client)\n\n        super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.meta.endpoint_url)\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The model provider.\"\"\"\n        return self._provider.name\n\n    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ToolTypeDef]:\n        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]\n\n    @staticmethod\n    def _map_tool_definition(f: ToolDefinition) -> ToolTypeDef:\n        tool_spec: ToolSpecificationTypeDef = {'name': f.name, 'inputSchema': {'json': f.parameters_json_schema}}\n\n        if f.description:  # pragma: no branch\n            tool_spec['description'] = f.description\n\n        return {'toolSpec': tool_spec}\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        settings = cast(BedrockModelSettings, model_settings or {})\n        response = await self._messages_create(messages, False, settings, model_request_parameters)\n        model_response = await self._process_response(response)\n        return model_response\n\n    async def count_tokens(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> usage.RequestUsage:\n        \"\"\"Count the number of tokens, works with limited models.\n\n        Check the actual supported models on <https://docs.aws.amazon.com/bedrock/latest/userguide/count-tokens.html>\n        \"\"\"\n        model_settings, model_request_parameters = self.prepare_request(model_settings, model_request_parameters)\n        system_prompt, bedrock_messages = await self._map_messages(messages, model_request_parameters)\n        params: CountTokensRequestTypeDef = {\n            'modelId': self._remove_inference_geo_prefix(self.model_name),\n            'input': {\n                'converse': {\n                    'messages': bedrock_messages,\n                    'system': system_prompt,\n                },\n            },\n        }\n        try:\n            response = await anyio.to_thread.run_sync(functools.partial(self.client.count_tokens, **params))\n        except ClientError as e:\n            status_code = e.response.get('ResponseMetadata', {}).get('HTTPStatusCode')\n            if isinstance(status_code, int):\n                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.response) from e\n            raise ModelAPIError(model_name=self.model_name, message=str(e)) from e\n        return usage.RequestUsage(input_tokens=response['inputTokens'])\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        settings = cast(BedrockModelSettings, model_settings or {})\n        response = await self._messages_create(messages, True, settings, model_request_parameters)\n        yield BedrockStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _model_name=self.model_name,\n            _event_stream=response['stream'],\n            _provider_name=self._provider.name,\n            _provider_response_id=response.get('ResponseMetadata', {}).get('RequestId', None),\n        )\n\n    async def _process_response(self, response: ConverseResponseTypeDef) -> ModelResponse:\n        items: list[ModelResponsePart] = []\n        if message := response['output'].get('message'):  # pragma: no branch\n            for item in message['content']:\n                if reasoning_content := item.get('reasoningContent'):\n                    if redacted_content := reasoning_content.get('redactedContent'):\n                        items.append(\n                            ThinkingPart(\n                                id='redacted_content',\n                                content='',\n                                signature=redacted_content.decode('utf-8'),\n                                provider_name=self.system,\n                            )\n                        )\n                    elif reasoning_text := reasoning_content.get('reasoningText'):  # pragma: no branch\n                        signature = reasoning_text.get('signature')\n                        items.append(\n                            ThinkingPart(\n                                content=reasoning_text['text'],\n                                signature=signature,\n                                provider_name=self.system if signature else None,\n                            )\n                        )\n                if text := item.get('text'):\n                    items.append(TextPart(content=text))\n                elif tool_use := item.get('toolUse'):\n                    items.append(\n                        ToolCallPart(\n                            tool_name=tool_use['name'],\n                            args=tool_use['input'],\n                            tool_call_id=tool_use['toolUseId'],\n                        ),\n                    )\n        u = usage.RequestUsage(\n            input_tokens=response['usage']['inputTokens'],\n            output_tokens=response['usage']['outputTokens'],\n        )\n        response_id = response.get('ResponseMetadata', {}).get('RequestId', None)\n        raw_finish_reason = response['stopReason']\n        provider_details = {'finish_reason': raw_finish_reason}\n        finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n\n        return ModelResponse(\n            parts=items,\n            usage=u,\n            model_name=self.model_name,\n            provider_response_id=response_id,\n            provider_name=self._provider.name,\n            finish_reason=finish_reason,\n            provider_details=provider_details,\n        )\n\n    @overload\n    async def _messages_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[True],\n        model_settings: BedrockModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ConverseStreamResponseTypeDef:\n        pass\n\n    @overload\n    async def _messages_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[False],\n        model_settings: BedrockModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ConverseResponseTypeDef:\n        pass\n\n    async def _messages_create(\n        self,\n        messages: list[ModelMessage],\n        stream: bool,\n        model_settings: BedrockModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ConverseResponseTypeDef | ConverseStreamResponseTypeDef:\n        system_prompt, bedrock_messages = await self._map_messages(messages, model_request_parameters)\n        inference_config = self._map_inference_config(model_settings)\n\n        params: ConverseRequestTypeDef = {\n            'modelId': self.model_name,\n            'messages': bedrock_messages,\n            'system': system_prompt,\n            'inferenceConfig': inference_config,\n        }\n\n        tool_config = self._map_tool_config(model_request_parameters)\n        if tool_config:\n            params['toolConfig'] = tool_config\n\n        if model_request_parameters.builtin_tools:\n            raise UserError('Bedrock does not support built-in tools')\n\n        # Bedrock supports a set of specific extra parameters\n        if model_settings:\n            if guardrail_config := model_settings.get('bedrock_guardrail_config', None):\n                params['guardrailConfig'] = guardrail_config\n            if performance_configuration := model_settings.get('bedrock_performance_configuration', None):\n                params['performanceConfig'] = performance_configuration\n            if request_metadata := model_settings.get('bedrock_request_metadata', None):\n                params['requestMetadata'] = request_metadata\n            if additional_model_response_fields_paths := model_settings.get(\n                'bedrock_additional_model_response_fields_paths', None\n            ):\n                params['additionalModelResponseFieldPaths'] = additional_model_response_fields_paths\n            if additional_model_requests_fields := model_settings.get('bedrock_additional_model_requests_fields', None):\n                params['additionalModelRequestFields'] = additional_model_requests_fields\n            if prompt_variables := model_settings.get('bedrock_prompt_variables', None):\n                params['promptVariables'] = prompt_variables\n\n        try:\n            if stream:\n                model_response = await anyio.to_thread.run_sync(\n                    functools.partial(self.client.converse_stream, **params)\n                )\n            else:\n                model_response = await anyio.to_thread.run_sync(functools.partial(self.client.converse, **params))\n        except ClientError as e:\n            status_code = e.response.get('ResponseMetadata', {}).get('HTTPStatusCode')\n            if isinstance(status_code, int):\n                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.response) from e\n            raise ModelAPIError(model_name=self.model_name, message=str(e)) from e\n        return model_response\n\n    @staticmethod\n    def _map_inference_config(\n        model_settings: ModelSettings | None,\n    ) -> InferenceConfigurationTypeDef:\n        model_settings = model_settings or {}\n        inference_config: InferenceConfigurationTypeDef = {}\n\n        if max_tokens := model_settings.get('max_tokens'):\n            inference_config['maxTokens'] = max_tokens\n        if (temperature := model_settings.get('temperature')) is not None:\n            inference_config['temperature'] = temperature\n        if top_p := model_settings.get('top_p'):\n            inference_config['topP'] = top_p\n        if stop_sequences := model_settings.get('stop_sequences'):\n            inference_config['stopSequences'] = stop_sequences\n\n        return inference_config\n\n    def _map_tool_config(self, model_request_parameters: ModelRequestParameters) -> ToolConfigurationTypeDef | None:\n        tools = self._get_tools(model_request_parameters)\n        if not tools:\n            return None\n\n        tool_choice: ToolChoiceTypeDef\n        if not model_request_parameters.allow_text_output:\n            tool_choice = {'any': {}}\n        else:\n            tool_choice = {'auto': {}}\n\n        tool_config: ToolConfigurationTypeDef = {'tools': tools}\n        if tool_choice and BedrockModelProfile.from_profile(self.profile).bedrock_supports_tool_choice:\n            tool_config['toolChoice'] = tool_choice\n\n        return tool_config\n\n    async def _map_messages(  # noqa: C901\n        self, messages: list[ModelMessage], model_request_parameters: ModelRequestParameters\n    ) -> tuple[list[SystemContentBlockTypeDef], list[MessageUnionTypeDef]]:\n        \"\"\"Maps a `pydantic_ai.Message` to the Bedrock `MessageUnionTypeDef`.\n\n        Groups consecutive ToolReturnPart objects into a single user message as required by Bedrock Claude/Nova models.\n        \"\"\"\n        profile = BedrockModelProfile.from_profile(self.profile)\n        system_prompt: list[SystemContentBlockTypeDef] = []\n        bedrock_messages: list[MessageUnionTypeDef] = []\n        document_count: Iterator[int] = count(1)\n        for message in messages:\n            if isinstance(message, ModelRequest):\n                for part in message.parts:\n                    if isinstance(part, SystemPromptPart) and part.content:\n                        system_prompt.append({'text': part.content})\n                    elif isinstance(part, UserPromptPart):\n                        bedrock_messages.extend(await self._map_user_prompt(part, document_count))\n                    elif isinstance(part, ToolReturnPart):\n                        assert part.tool_call_id is not None\n                        bedrock_messages.append(\n                            {\n                                'role': 'user',\n                                'content': [\n                                    {\n                                        'toolResult': {\n                                            'toolUseId': part.tool_call_id,\n                                            'content': [\n                                                {'text': part.model_response_str()}\n                                                if profile.bedrock_tool_result_format == 'text'\n                                                else {'json': part.model_response_object()}\n                                            ],\n                                            'status': 'success',\n                                        }\n                                    }\n                                ],\n                            }\n                        )\n                    elif isinstance(part, RetryPromptPart):\n                        # TODO(Marcelo): We need to add a test here.\n                        if part.tool_name is None:  # pragma: no cover\n                            bedrock_messages.append({'role': 'user', 'content': [{'text': part.model_response()}]})\n                        else:\n                            assert part.tool_call_id is not None\n                            bedrock_messages.append(\n                                {\n                                    'role': 'user',\n                                    'content': [\n                                        {\n                                            'toolResult': {\n                                                'toolUseId': part.tool_call_id,\n                                                'content': [{'text': part.model_response()}],\n                                                'status': 'error',\n                                            }\n                                        }\n                                    ],\n                                }\n                            )\n            elif isinstance(message, ModelResponse):\n                content: list[ContentBlockOutputTypeDef] = []\n                for item in message.parts:\n                    if isinstance(item, TextPart):\n                        content.append({'text': item.content})\n                    elif isinstance(item, ThinkingPart):\n                        if (\n                            item.provider_name == self.system\n                            and item.signature\n                            and BedrockModelProfile.from_profile(self.profile).bedrock_send_back_thinking_parts\n                        ):\n                            if item.id == 'redacted_content':\n                                reasoning_content: ReasoningContentBlockOutputTypeDef = {\n                                    'redactedContent': item.signature.encode('utf-8'),\n                                }\n                            else:\n                                reasoning_content: ReasoningContentBlockOutputTypeDef = {\n                                    'reasoningText': {\n                                        'text': item.content,\n                                        'signature': item.signature,\n                                    }\n                                }\n                            content.append({'reasoningContent': reasoning_content})\n                        else:\n                            start_tag, end_tag = self.profile.thinking_tags\n                            content.append({'text': '\\n'.join([start_tag, item.content, end_tag])})\n                    elif isinstance(item, BuiltinToolCallPart | BuiltinToolReturnPart):\n                        pass\n                    else:\n                        assert isinstance(item, ToolCallPart)\n                        content.append(self._map_tool_call(item))\n                bedrock_messages.append({'role': 'assistant', 'content': content})\n            else:\n                assert_never(message)\n\n        # Merge together sequential user messages.\n        processed_messages: list[MessageUnionTypeDef] = []\n        last_message: dict[str, Any] | None = None\n        for current_message in bedrock_messages:\n            if (\n                last_message is not None\n                and current_message['role'] == last_message['role']\n                and current_message['role'] == 'user'\n            ):\n                # Add the new user content onto the existing user message.\n                last_content = list(last_message['content'])\n                last_content.extend(current_message['content'])\n                last_message['content'] = last_content\n                continue\n\n            # Add the entire message to the list of messages.\n            processed_messages.append(current_message)\n            last_message = cast(dict[str, Any], current_message)\n\n        if instructions := self._get_instructions(messages, model_request_parameters):\n            system_prompt.insert(0, {'text': instructions})\n\n        return system_prompt, processed_messages\n\n    @staticmethod\n    async def _map_user_prompt(part: UserPromptPart, document_count: Iterator[int]) -> list[MessageUnionTypeDef]:\n        content: list[ContentBlockUnionTypeDef] = []\n        if isinstance(part.content, str):\n            content.append({'text': part.content})\n        else:\n            for item in part.content:\n                if isinstance(item, str):\n                    content.append({'text': item})\n                elif isinstance(item, BinaryContent):\n                    format = item.format\n                    if item.is_document:\n                        name = f'Document {next(document_count)}'\n                        assert format in ('pdf', 'txt', 'csv', 'doc', 'docx', 'xls', 'xlsx', 'html', 'md')\n                        content.append({'document': {'name': name, 'format': format, 'source': {'bytes': item.data}}})\n                    elif item.is_image:\n                        assert format in ('jpeg', 'png', 'gif', 'webp')\n                        content.append({'image': {'format': format, 'source': {'bytes': item.data}}})\n                    elif item.is_video:\n                        assert format in ('mkv', 'mov', 'mp4', 'webm', 'flv', 'mpeg', 'mpg', 'wmv', 'three_gp')\n                        content.append({'video': {'format': format, 'source': {'bytes': item.data}}})\n                    else:\n                        raise NotImplementedError('Binary content is not supported yet.')\n                elif isinstance(item, ImageUrl | DocumentUrl | VideoUrl):\n                    downloaded_item = await download_item(item, data_format='bytes', type_format='extension')\n                    format = downloaded_item['data_type']\n                    if item.kind == 'image-url':\n                        format = item.media_type.split('/')[1]\n                        assert format in ('jpeg', 'png', 'gif', 'webp'), f'Unsupported image format: {format}'\n                        image: ImageBlockTypeDef = {'format': format, 'source': {'bytes': downloaded_item['data']}}\n                        content.append({'image': image})\n\n                    elif item.kind == 'document-url':\n                        name = f'Document {next(document_count)}'\n                        document: DocumentBlockTypeDef = {\n                            'name': name,\n                            'format': item.format,\n                            'source': {'bytes': downloaded_item['data']},\n                        }\n                        content.append({'document': document})\n\n                    elif item.kind == 'video-url':  # pragma: no branch\n                        format = item.media_type.split('/')[1]\n                        assert format in (\n                            'mkv',\n                            'mov',\n                            'mp4',\n                            'webm',\n                            'flv',\n                            'mpeg',\n                            'mpg',\n                            'wmv',\n                            'three_gp',\n                        ), f'Unsupported video format: {format}'\n                        video: VideoBlockTypeDef = {'format': format, 'source': {'bytes': downloaded_item['data']}}\n                        content.append({'video': video})\n                elif isinstance(item, AudioUrl):  # pragma: no cover\n                    raise NotImplementedError('Audio is not supported yet.')\n                elif isinstance(item, CachePoint):\n                    # Bedrock support has not been implemented yet: https://github.com/pydantic/pydantic-ai/issues/3418\n                    pass\n                else:\n                    assert_never(item)\n        return [{'role': 'user', 'content': content}]\n\n    @staticmethod\n    def _map_tool_call(t: ToolCallPart) -> ContentBlockOutputTypeDef:\n        return {\n            'toolUse': {'toolUseId': _utils.guard_tool_call_id(t=t), 'name': t.tool_name, 'input': t.args_as_dict()}\n        }\n\n    @staticmethod\n    def _remove_inference_geo_prefix(model_name: BedrockModelName) -> BedrockModelName:\n        \"\"\"Remove inference geographic prefix from model ID if present.\"\"\"\n        for prefix in _AWS_BEDROCK_INFERENCE_GEO_PREFIXES:\n            if model_name.startswith(prefix):\n                return model_name.removeprefix(prefix)\n        return model_name\n\n```\n\n#### __init__\n\n```python\n__init__(\n    model_name: BedrockModelName,\n    *,\n    provider: (\n        Literal[\"bedrock\", \"gateway\"] | Provider[BaseClient]\n    ) = \"bedrock\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize a Bedrock model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model_name` | `BedrockModelName` | The name of the model to use. | *required* | | `model_name` | `BedrockModelName` | The name of the Bedrock model to use. List of model names available here. | *required* | | `provider` | `Literal['bedrock', 'gateway'] | Provider[BaseClient]` | The provider to use for authentication and API access. Can be either the string 'bedrock' or an instance of Provider[BaseClient]. If not provided, a new provider will be created using the other parameters. | `'bedrock'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` | | `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/bedrock.py`\n\n```python\ndef __init__(\n    self,\n    model_name: BedrockModelName,\n    *,\n    provider: Literal['bedrock', 'gateway'] | Provider[BaseClient] = 'bedrock',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize a Bedrock model.\n\n    Args:\n        model_name: The name of the model to use.\n        model_name: The name of the Bedrock model to use. List of model names available\n            [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).\n        provider: The provider to use for authentication and API access. Can be either the string\n            'bedrock' or an instance of `Provider[BaseClient]`. If not provided, a new provider will be\n            created using the other parameters.\n        profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n        settings: Model-specific settings that will be used as defaults for this model.\n    \"\"\"\n    self._model_name = model_name\n\n    if isinstance(provider, str):\n        provider = infer_provider('gateway/bedrock' if provider == 'gateway' else provider)\n    self._provider = provider\n    self.client = cast('BedrockRuntimeClient', provider.client)\n\n    super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n```\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe model provider.\n\n#### count_tokens\n\n```python\ncount_tokens(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> RequestUsage\n\n```\n\nCount the number of tokens, works with limited models.\n\nCheck the actual supported models on <https://docs.aws.amazon.com/bedrock/latest/userguide/count-tokens.html>\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/bedrock.py`\n\n```python\nasync def count_tokens(\n    self,\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> usage.RequestUsage:\n    \"\"\"Count the number of tokens, works with limited models.\n\n    Check the actual supported models on <https://docs.aws.amazon.com/bedrock/latest/userguide/count-tokens.html>\n    \"\"\"\n    model_settings, model_request_parameters = self.prepare_request(model_settings, model_request_parameters)\n    system_prompt, bedrock_messages = await self._map_messages(messages, model_request_parameters)\n    params: CountTokensRequestTypeDef = {\n        'modelId': self._remove_inference_geo_prefix(self.model_name),\n        'input': {\n            'converse': {\n                'messages': bedrock_messages,\n                'system': system_prompt,\n            },\n        },\n    }\n    try:\n        response = await anyio.to_thread.run_sync(functools.partial(self.client.count_tokens, **params))\n    except ClientError as e:\n        status_code = e.response.get('ResponseMetadata', {}).get('HTTPStatusCode')\n        if isinstance(status_code, int):\n            raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.response) from e\n        raise ModelAPIError(model_name=self.model_name, message=str(e)) from e\n    return usage.RequestUsage(input_tokens=response['inputTokens'])\n\n```\n\n### BedrockStreamedResponse\n\nBases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for Bedrock models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/bedrock.py`\n\n```python\n@dataclass\nclass BedrockStreamedResponse(StreamedResponse):\n    \"\"\"Implementation of `StreamedResponse` for Bedrock models.\"\"\"\n\n    _model_name: BedrockModelName\n    _event_stream: EventStream[ConverseStreamOutputTypeDef]\n    _provider_name: str\n    _timestamp: datetime = field(default_factory=_utils.now_utc)\n    _provider_response_id: str | None = None\n\n    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:  # noqa: C901\n        \"\"\"Return an async iterator of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\n\n        This method should be implemented by subclasses to translate the vendor-specific stream of events into\n        pydantic_ai-format events.\n        \"\"\"\n        if self._provider_response_id is not None:  # pragma: no cover\n            self.provider_response_id = self._provider_response_id\n\n        chunk: ConverseStreamOutputTypeDef\n        tool_id: str | None = None\n        async for chunk in _AsyncIteratorWrapper(self._event_stream):\n            match chunk:\n                case {'messageStart': _}:\n                    continue\n                case {'messageStop': message_stop}:\n                    raw_finish_reason = message_stop['stopReason']\n                    self.provider_details = {'finish_reason': raw_finish_reason}\n                    self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n                case {'metadata': metadata}:\n                    if 'usage' in metadata:  # pragma: no branch\n                        self._usage += self._map_usage(metadata)\n                case {'contentBlockStart': content_block_start}:\n                    index = content_block_start['contentBlockIndex']\n                    start = content_block_start['start']\n                    if 'toolUse' in start:  # pragma: no branch\n                        tool_use_start = start['toolUse']\n                        tool_id = tool_use_start['toolUseId']\n                        tool_name = tool_use_start['name']\n                        maybe_event = self._parts_manager.handle_tool_call_delta(\n                            vendor_part_id=index,\n                            tool_name=tool_name,\n                            args=None,\n                            tool_call_id=tool_id,\n                        )\n                        if maybe_event:  # pragma: no branch\n                            yield maybe_event\n                case {'contentBlockDelta': content_block_delta}:\n                    index = content_block_delta['contentBlockIndex']\n                    delta = content_block_delta['delta']\n                    if 'reasoningContent' in delta:\n                        if redacted_content := delta['reasoningContent'].get('redactedContent'):\n                            yield self._parts_manager.handle_thinking_delta(\n                                vendor_part_id=index,\n                                id='redacted_content',\n                                signature=redacted_content.decode('utf-8'),\n                                provider_name=self.provider_name,\n                            )\n                        else:\n                            signature = delta['reasoningContent'].get('signature')\n                            yield self._parts_manager.handle_thinking_delta(\n                                vendor_part_id=index,\n                                content=delta['reasoningContent'].get('text'),\n                                signature=signature,\n                                provider_name=self.provider_name if signature else None,\n                            )\n                    if text := delta.get('text'):\n                        maybe_event = self._parts_manager.handle_text_delta(vendor_part_id=index, content=text)\n                        if maybe_event is not None:  # pragma: no branch\n                            yield maybe_event\n                    if 'toolUse' in delta:\n                        tool_use = delta['toolUse']\n                        maybe_event = self._parts_manager.handle_tool_call_delta(\n                            vendor_part_id=index,\n                            tool_name=tool_use.get('name'),\n                            args=tool_use.get('input'),\n                            tool_call_id=tool_id,\n                        )\n                        if maybe_event:  # pragma: no branch\n                            yield maybe_event\n                case _:\n                    pass  # pyright wants match statements to be exhaustive\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"Get the model name of the response.\"\"\"\n        return self._model_name\n\n    @property\n    def provider_name(self) -> str:\n        \"\"\"Get the provider name.\"\"\"\n        return self._provider_name\n\n    @property\n    def timestamp(self) -> datetime:\n        return self._timestamp\n\n    def _map_usage(self, metadata: ConverseStreamMetadataEventTypeDef) -> usage.RequestUsage:\n        return usage.RequestUsage(\n            input_tokens=metadata['usage']['inputTokens'],\n            output_tokens=metadata['usage']['outputTokens'],\n        )\n\n```\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nGet the model name of the response.\n\n#### provider_name\n\n```python\nprovider_name: str\n\n```\n\nGet the provider name.",
  "content_length": 40983
}