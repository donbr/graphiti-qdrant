{
  "title": "Model Providers",
  "source_url": null,
  "content": "Pydantic AI is model-agnostic and has built-in support for multiple model providers:\n\n- [OpenAI](../openai/)\n- [Anthropic](../anthropic/)\n- [Gemini](../google/) (via two different APIs: Generative Language API and VertexAI API)\n- [Bedrock](../bedrock/)\n- [Cohere](../cohere/)\n- [Groq](../groq/)\n- [Hugging Face](../huggingface/)\n- [Mistral](../mistral/)\n- [OpenRouter](../openrouter/)\n- [Outlines](../outlines/)\n\n## OpenAI-compatible Providers\n\nIn addition, many providers are compatible with the OpenAI API, and can be used with `OpenAIChatModel` in Pydantic AI:\n\n- [Azure AI Foundry](../openai/#azure-ai-foundry)\n- [Cerebras](../openai/#cerebras)\n- [DeepSeek](../openai/#deepseek)\n- [Fireworks AI](../openai/#fireworks-ai)\n- [GitHub Models](../openai/#github-models)\n- [Grok (xAI)](../openai/#grok-xai)\n- [Heroku](../openai/#heroku-ai)\n- [LiteLLM](../openai/#litellm)\n- [Nebius AI Studio](../openai/#nebius-ai-studio)\n- [Ollama](../openai/#ollama)\n- [OVHcloud AI Endpoints](../openai/#ovhcloud-ai-endpoints)\n- [Perplexity](../openai/#perplexity)\n- [Together AI](../openai/#together-ai)\n- [Vercel AI Gateway](../openai/#vercel-ai-gateway)\n\nPydantic AI also comes with [`TestModel`](../../api/models/test/) and [`FunctionModel`](../../api/models/function/) for testing and development.\n\nTo use each model provider, you need to configure your local environment and make sure you have the right packages installed. If you try to use the model without having done so, you'll be told what to install.\n\n## Models and Providers\n\nPydantic AI uses a few key terms to describe how it interacts with different LLMs:\n\n- **Model**: This refers to the Pydantic AI class used to make requests following a specific LLM API (generally by wrapping a vendor-provided SDK, like the `openai` python SDK). These classes implement a vendor-SDK-agnostic API, ensuring a single Pydantic AI agent is portable to different LLM vendors without any other code changes just by swapping out the Model it uses. Model classes are named roughly in the format `<VendorSdk>Model`, for example, we have `OpenAIChatModel`, `AnthropicModel`, `GoogleModel`, etc. When using a Model class, you specify the actual LLM model name (e.g., `gpt-5`, `claude-sonnet-4-5`, `gemini-2.5-flash`) as a parameter.\n- **Provider**: This refers to provider-specific classes which handle the authentication and connections to an LLM vendor. Passing a non-default *Provider* as a parameter to a Model is how you can ensure that your agent will make requests to a specific endpoint, or make use of a specific approach to authentication (e.g., you can use Azure auth with the `OpenAIChatModel` by way of the `AzureProvider`). In particular, this is how you can make use of an AI gateway, or an LLM vendor that offers API compatibility with the vendor SDK used by an existing Model (such as `OpenAIChatModel`).\n- **Profile**: This refers to a description of how requests to a specific model or family of models need to be constructed to get the best results, independent of the model and provider classes used. For example, different models have different restrictions on the JSON schemas that can be used for tools, and the same schema transformer needs to be used for Gemini models whether you're using `GoogleModel` with model name `gemini-2.5-pro-preview`, or `OpenAIChatModel` with `OpenRouterProvider` and model name `google/gemini-2.5-pro-preview`.\n\nWhen you instantiate an Agent with just a name formatted as `<provider>:<model>`, e.g. `openai:gpt-5` or `openrouter:google/gemini-2.5-pro-preview`, Pydantic AI will automatically select the appropriate model class, provider, and profile. If you want to use a different provider or profile, you can instantiate a model class directly and pass in `provider` and/or `profile` arguments.\n\n## Custom Models\n\nNote\n\nIf a model API is compatible with the OpenAI API, you do not need a custom model class and can provide your own [custom provider](../openai/#openai-compatible-models) instead.\n\nTo implement support for a model API that's not already supported, you will need to subclass the Model abstract base class. For streaming, you'll also need to implement the StreamedResponse abstract base class.\n\nThe best place to start is to review the source code for existing implementations, e.g. [`OpenAIChatModel`](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py).\n\nFor details on when we'll accept contributions adding new models to Pydantic AI, see the [contributing guidelines](../../contributing/#new-model-rules).\n\n## Fallback Model\n\nYou can use FallbackModel to attempt multiple models in sequence until one successfully returns a result. Under the hood, Pydantic AI automatically switches from one model to the next if the current model returns a 4xx or 5xx status code.\n\nNote\n\nThe provider SDKs on which Models are based (like OpenAI, Anthropic, etc.) often have built-in retry logic that can delay the `FallbackModel` from activating.\n\n```text\nWhen using `FallbackModel`, it's recommended to disable provider SDK retries to ensure immediate fallback, for example by setting `max_retries=0` on a [custom OpenAI client](openai.md#custom-openai-client).\n\n```\n\nIn the following example, the agent first makes a request to the OpenAI model (which fails due to an invalid API key), and then falls back to the Anthropic model.\n\nfallback_model.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\nopenai_model = OpenAIChatModel('gpt-5')\nanthropic_model = AnthropicModel('claude-sonnet-4-5')\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\nresponse = agent.run_sync('What is the capital of France?')\nprint(response.data)\n#> Paris\n\nprint(response.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='What is the capital of France?',\n                timestamp=datetime.datetime(...),\n                part_kind='user-prompt',\n            )\n        ],\n        kind='request',\n    ),\n    ModelResponse(\n        parts=[TextPart(content='Paris', part_kind='text')],\n        model_name='claude-sonnet-4-5',\n        timestamp=datetime.datetime(...),\n        kind='response',\n        provider_response_id=None,\n    ),\n]\n\"\"\"\n\n```\n\nThe `ModelResponse` message above indicates in the `model_name` field that the output was returned by the Anthropic model, which is the second model specified in the `FallbackModel`.\n\nNote\n\nEach model's options should be configured individually. For example, `base_url`, `api_key`, and custom clients should be set on each model itself, not on the `FallbackModel`.\n\n### Per-Model Settings\n\nYou can configure different ModelSettings for each model in a fallback chain by passing the `settings` parameter when creating each model. This is particularly useful when different providers have different optimal configurations:\n\nfallback_model_per_settings.py\n\n```python\nfrom pydantic_ai import Agent, ModelSettings\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\n### Configure each model with provider-specific optimal settings\nopenai_model = OpenAIChatModel(\n    'gpt-5',\n    settings=ModelSettings(temperature=0.7, max_tokens=1000)  # Higher creativity for OpenAI\n)\nanthropic_model = AnthropicModel(\n    'claude-sonnet-4-5',\n    settings=ModelSettings(temperature=0.2, max_tokens=1000)  # Lower temperature for consistency\n)\n\nfallback_model = FallbackModel(openai_model, anthropic_model)\nagent = Agent(fallback_model)\n\nresult = agent.run_sync('Write a creative story about space exploration')\nprint(result.output)\n\"\"\"\nIn the year 2157, Captain Maya Chen piloted her spacecraft through the vast expanse of the Andromeda Galaxy. As she discovered a planet with crystalline mountains that sang in harmony with the cosmic winds, she realized that space exploration was not just about finding new worlds, but about finding new ways to understand the universe and our place within it.\n\"\"\"\n\n```\n\nIn this example, if the OpenAI model fails, the agent will automatically fall back to the Anthropic model with its own configured settings. The `FallbackModel` itself doesn't have settings - it uses the individual settings of whichever model successfully handles the request.\n\nIn this next example, we demonstrate the exception-handling capabilities of `FallbackModel`. If all models fail, a FallbackExceptionGroup is raised, which contains all the exceptions encountered during the `run` execution.\n\nfallback_model_failure.py\n\n```python\nfrom pydantic_ai import Agent, ModelAPIError\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\nopenai_model = OpenAIChatModel('gpt-5')\nanthropic_model = AnthropicModel('claude-sonnet-4-5')\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\ntry:\n    response = agent.run_sync('What is the capital of France?')\nexcept* ModelAPIError as exc_group:\n    for exc in exc_group.exceptions:\n        print(exc)\n\n```\n\nSince [`except*`](https://docs.python.org/3/reference/compound_stmts.html#except-star) is only supported in Python 3.11+, we use the [`exceptiongroup`](https://github.com/agronholm/exceptiongroup) backport package for earlier Python versions:\n\nfallback_model_failure.py\n\n```python\nfrom exceptiongroup import catch\n\nfrom pydantic_ai import Agent, ModelAPIError\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\n\ndef model_status_error_handler(exc_group: BaseExceptionGroup) -> None:\n    for exc in exc_group.exceptions:\n        print(exc)\n\n\nopenai_model = OpenAIChatModel('gpt-5')\nanthropic_model = AnthropicModel('claude-sonnet-4-5')\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\nwith catch({ModelAPIError: model_status_error_handler}):\n    response = agent.run_sync('What is the capital of France?')\n\n```\n\nBy default, the `FallbackModel` only moves on to the next model if the current model raises a ModelAPIError, which includes ModelHTTPError. You can customize this behavior by passing a custom `fallback_on` argument to the `FallbackModel` constructor.",
  "content_length": 10536
}