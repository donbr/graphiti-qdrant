{
  "title": "Agent2Agent (A2A) Protocol",
  "source_url": null,
  "content": "The [Agent2Agent (A2A) Protocol](https://google.github.io/A2A/) is an open standard introduced by Google that enables communication and interoperability between AI agents, regardless of the framework or vendor they are built on.\n\nAt Pydantic, we built the [FastA2A](#fasta2a) library to make it easier to implement the A2A protocol in Python.\n\nWe also built a convenience method that expose Pydantic AI agents as A2A servers - let's have a quick look at how to use it:\n\n[Learn about Gateway](../gateway) agent_to_a2a.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5', instructions='Be fun!')\napp = agent.to_a2a()\n\n```\n\nagent_to_a2a.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='Be fun!')\napp = agent.to_a2a()\n\n```\n\n*You can run the example with `uvicorn agent_to_a2a:app --host 0.0.0.0 --port 8000`*\n\nThis will expose the agent as an A2A server, and you can start sending requests to it.\n\nSee more about [exposing Pydantic AI agents as A2A servers](#pydantic-ai-agent-to-a2a-server).\n\n## FastA2A\n\n**FastA2A** is an agentic framework agnostic implementation of the A2A protocol in Python. The library is designed to be used with any agentic framework, and is **not exclusive to Pydantic AI**.\n\n### Design\n\n**FastA2A** is built on top of [Starlette](https://www.starlette.io), which means it's fully compatible with any ASGI server.\n\nGiven the nature of the A2A protocol, it's important to understand the design before using it, as a developer you'll need to provide some components:\n\n- Storage: to save and load tasks, as well as store context for conversations\n- Broker: to schedule tasks\n- Worker: to execute tasks\n\nLet's have a look at how those components fit together:\n\n```\nflowchart TB\n    Server[\"HTTP Server\"] <--> |Sends Requests/<br>Receives Results| TM\n\n    subgraph CC[Core Components]\n        direction RL\n        TM[\"TaskManager<br>(coordinates)\"] --> |Schedules Tasks| Broker\n        TM <--> Storage\n        Broker[\"Broker<br>(queues & schedules)\"] <--> Storage[\"Storage<br>(persistence)\"]\n        Broker --> |Delegates Execution| Worker\n    end\n\n    Worker[\"Worker<br>(implementation)\"]\n```\n\nFastA2A allows you to bring your own Storage, Broker and Worker.\n\n#### Understanding Tasks and Context\n\nIn the A2A protocol:\n\n- **Task**: Represents one complete execution of an agent. When a client sends a message to the agent, a new task is created. The agent runs until completion (or failure), and this entire execution is considered one task. The final output is stored as a task artifact.\n- **Context**: Represents a conversation thread that can span multiple tasks. The A2A protocol uses a `context_id` to maintain conversation continuity:\n- When a new message is sent without a `context_id`, the server generates a new one\n- Subsequent messages can include the same `context_id` to continue the conversation\n- All tasks sharing the same `context_id` have access to the complete message history\n\n#### Storage Architecture\n\nThe Storage component serves two purposes:\n\n1. **Task Storage**: Stores tasks in A2A protocol format, including their status, artifacts, and message history\n1. **Context Storage**: Stores conversation context in a format optimized for the specific agent implementation\n\nThis design allows for agents to store rich internal state (e.g., tool calls, reasoning traces) as well as store task-specific A2A-formatted messages and artifacts.\n\nFor example, a Pydantic AI agent might store its complete internal message format (including tool calls and responses) in the context storage, while storing only the A2A-compliant messages in the task history.\n\n### Installation\n\nFastA2A is available on PyPI as [`fasta2a`](https://pypi.org/project/fasta2a/) so installation is as simple as:\n\n```bash\npip install fasta2a\n\n```\n\n```bash\nuv add fasta2a\n\n```\n\nThe only dependencies are:\n\n- [starlette](https://www.starlette.io): to expose the A2A server as an [ASGI application](https://asgi.readthedocs.io/en/latest/)\n- [pydantic](https://pydantic.dev): to validate the request/response messages\n- [opentelemetry-api](https://opentelemetry-python.readthedocs.io/en/latest): to provide tracing capabilities\n\nYou can install Pydantic AI with the `a2a` extra to include **FastA2A**:\n\n```bash\npip install 'pydantic-ai-slim[a2a]'\n\n```\n\n```bash\nuv add 'pydantic-ai-slim[a2a]'\n\n```\n\n### Pydantic AI Agent to A2A Server\n\nTo expose a Pydantic AI agent as an A2A server, you can use the `to_a2a` method:\n\n[Learn about Gateway](../gateway) agent_to_a2a.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5', instructions='Be fun!')\napp = agent.to_a2a()\n\n```\n\nagent_to_a2a.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='Be fun!')\napp = agent.to_a2a()\n\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server.\n\n```bash\nuvicorn agent_to_a2a:app --host 0.0.0.0 --port 8000\n\n```\n\nSince the goal of `to_a2a` is to be a convenience method, it accepts the same arguments as the FastA2A constructor.\n\nWhen using `to_a2a()`, Pydantic AI automatically:\n\n- Stores the complete conversation history (including tool calls and responses) in the context storage\n- Ensures that subsequent messages with the same `context_id` have access to the full conversation history\n- Persists agent results as A2A artifacts:\n- String results become `TextPart` artifacts and also appear in the message history\n- Structured data (Pydantic models, dataclasses, tuples, etc.) become `DataPart` artifacts with the data wrapped as `{\"result\": <your_data>}`\n- Artifacts include metadata with type information and JSON schema when available\n\n## Introduction\n\nAgents are Pydantic AI's primary interface for interacting with LLMs.\n\nIn some use cases a single Agent will control an entire application or component, but multiple agents can also interact to embody more complex workflows.\n\nThe Agent class has full API documentation, but conceptually you can think of an agent as a container for:\n\n| **Component** | **Description** | | --- | --- | | [Instructions](#instructions) | A set of instructions for the LLM written by the developer. | | [Function tool(s)](../tools/) and [toolsets](../toolsets/) | Functions that the LLM may call to get information while generating a response. | | [Structured output type](../output/) | The structured datatype the LLM must return at the end of a run, if specified. | | [Dependency type constraint](../dependencies/) | Dynamic instructions functions, tools, and output functions may all use dependencies when they're run. | | [LLM model](../api/models/base/) | Optional default LLM model associated with the agent. Can also be specified when running the agent. | | [Model Settings](#additional-configuration) | Optional default model settings to help fine tune requests. Can also be specified when running the agent. |\n\nIn typing terms, agents are generic in their dependency and output types, e.g., an agent which required dependencies of type `Foobar` and produced outputs of type `list[str]` would have type `Agent[Foobar, list[str]]`. In practice, you shouldn't need to care about this, it should just mean your IDE can tell you when you have the right type, and if you choose to use [static type checking](#static-type-checking) it should work well with Pydantic AI.\n\nHere's a toy example of an agent that simulates a roulette wheel:\n\n[Learn about Gateway](../gateway) roulette_wheel.py\n\n```python\nfrom pydantic_ai import Agent, RunContext\n\nroulette_agent = Agent(  # (1)!\n    'gateway/openai:gpt-5',\n    deps_type=int,\n    output_type=bool,\n    system_prompt=(\n        'Use the `roulette_wheel` function to see if the '\n        'customer has won based on the number they provide.'\n    ),\n)\n\n\n@roulette_agent.tool\nasync def roulette_wheel(ctx: RunContext[int], square: int) -> str:  # (2)!\n    \"\"\"check if the square is a winner\"\"\"\n    return 'winner' if square == ctx.deps else 'loser'\n\n\n### Run the agent\nsuccess_number = 18  # (3)!\nresult = roulette_agent.run_sync('Put my money on square eighteen', deps=success_number)\nprint(result.output)  # (4)!\n#> True\n\nresult = roulette_agent.run_sync('I bet five is the winner', deps=success_number)\nprint(result.output)\n#> False\n\n```\n\n1. Create an agent, which expects an integer dependency and produces a boolean output. This agent will have type `Agent[int, bool]`.\n1. Define a tool that checks if the square is a winner. Here RunContext is parameterized with the dependency type `int`; if you got the dependency type wrong you'd get a typing error.\n1. In reality, you might want to use a random number here e.g. `random.randint(0, 36)`.\n1. `result.output` will be a boolean indicating if the square is a winner. Pydantic performs the output validation, and it'll be typed as a `bool` since its type is derived from the `output_type` generic parameter of the agent.\n\nroulette_wheel.py\n\n```python\nfrom pydantic_ai import Agent, RunContext\n\nroulette_agent = Agent(  # (1)!\n    'openai:gpt-5',\n    deps_type=int,\n    output_type=bool,\n    system_prompt=(\n        'Use the `roulette_wheel` function to see if the '\n        'customer has won based on the number they provide.'\n    ),\n)\n\n\n@roulette_agent.tool\nasync def roulette_wheel(ctx: RunContext[int], square: int) -> str:  # (2)!\n    \"\"\"check if the square is a winner\"\"\"\n    return 'winner' if square == ctx.deps else 'loser'\n\n\n### Run the agent\nsuccess_number = 18  # (3)!\nresult = roulette_agent.run_sync('Put my money on square eighteen', deps=success_number)\nprint(result.output)  # (4)!\n#> True\n\nresult = roulette_agent.run_sync('I bet five is the winner', deps=success_number)\nprint(result.output)\n#> False\n\n```\n\n1. Create an agent, which expects an integer dependency and produces a boolean output. This agent will have type `Agent[int, bool]`.\n1. Define a tool that checks if the square is a winner. Here RunContext is parameterized with the dependency type `int`; if you got the dependency type wrong you'd get a typing error.\n1. In reality, you might want to use a random number here e.g. `random.randint(0, 36)`.\n1. `result.output` will be a boolean indicating if the square is a winner. Pydantic performs the output validation, and it'll be typed as a `bool` since its type is derived from the `output_type` generic parameter of the agent.\n\nAgents are designed for reuse, like FastAPI Apps\n\nAgents are intended to be instantiated once (frequently as module globals) and reused throughout your application, similar to a small FastAPI app or an APIRouter.\n\n## Running Agents\n\nThere are five ways to run an agent:\n\n1. agent.run() — an async function which returns a RunResult containing a completed response.\n1. agent.run_sync() — a plain, synchronous function which returns a RunResult containing a completed response (internally, this just calls `loop.run_until_complete(self.run())`).\n1. agent.run_stream() — an async context manager which returns a StreamedRunResult, which contains methods to stream text and structured output as an async iterable. agent.run_stream_sync() is a synchronous variation that returns a StreamedRunResultSync with synchronous versions of the same methods.\n1. agent.run_stream_events() — a function which returns an async iterable of AgentStreamEvents and a AgentRunResultEvent containing the final run result.\n1. agent.iter() — a context manager which returns an AgentRun, an async iterable over the nodes of the agent's underlying Graph.\n\nHere's a simple example demonstrating the first four:\n\n[Learn about Gateway](../gateway) run_agent.py\n\n```python\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('gateway/openai:gpt-5')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n\n\nasync def main():\n    result = await agent.run('What is the capital of France?')\n    print(result.output)\n    #> The capital of France is Paris.\n\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        async for text in response.stream_text():\n            print(text)\n            #> The capital of\n            #> The capital of the UK is\n            #> The capital of the UK is London.\n\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of Mexico?'):\n        events.append(event)\n    print(events)\n    \"\"\"\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='Mexico is Mexico ')),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='City.')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of Mexico is Mexico City.')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of Mexico is Mexico City.')\n        ),\n    ]\n    \"\"\"\n\n```\n\nrun_agent.py\n\n```python\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-5')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n\n\nasync def main():\n    result = await agent.run('What is the capital of France?')\n    print(result.output)\n    #> The capital of France is Paris.\n\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        async for text in response.stream_text():\n            print(text)\n            #> The capital of\n            #> The capital of the UK is\n            #> The capital of the UK is London.\n\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of Mexico?'):\n        events.append(event)\n    print(events)\n    \"\"\"\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='Mexico is Mexico ')),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='City.')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of Mexico is Mexico City.')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of Mexico is Mexico City.')\n        ),\n    ]\n    \"\"\"\n\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nYou can also pass messages from previous runs to continue a conversation or provide context, as described in [Messages and Chat History](../message-history/).\n\n### Streaming Events and Final Output\n\nAs shown in the example above, run_stream() makes it easy to stream the agent's final output as it comes in. It also takes an optional `event_stream_handler` argument that you can use to gain insight into what is happening during the run before the final output is produced.\n\nThe example below shows how to stream events and text output. You can also [stream structured output](../output/#streaming-structured-output).\n\nNote\n\nAs the `run_stream()` method will consider the first output matching the [output type](../output/#structured-output) to be the final output, it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.\n\nIf you want to always run the agent graph to completion and stream all events from the model's streaming response and the agent's execution of tools, use agent.run_stream_events() or agent.iter() instead, as described in the following sections.\n\n[Learn about Gateway](../gateway) run_stream_event_stream_handler.py\n\n```python\nimport asyncio\nfrom collections.abc import AsyncIterable\nfrom datetime import date\n\nfrom pydantic_ai import (\n    Agent,\n    AgentStreamEvent,\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    RunContext,\n    TextPartDelta,\n    ThinkingPartDelta,\n    ToolCallPartDelta,\n)\n\nweather_agent = Agent(\n    'gateway/openai:gpt-5',\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\nasync def weather_forecast(\n    ctx: RunContext,\n    location: str,\n    forecast_date: date,\n) -> str:\n    return f'The forecast in {location} on {forecast_date} is 24°C and sunny.'\n\n\noutput_messages: list[str] = []\n\nasync def handle_event(event: AgentStreamEvent):\n    if isinstance(event, PartStartEvent):\n        output_messages.append(f'[Request] Starting part {event.index}: {event.part!r}')\n    elif isinstance(event, PartDeltaEvent):\n        if isinstance(event.delta, TextPartDelta):\n            output_messages.append(f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}')\n        elif isinstance(event.delta, ThinkingPartDelta):\n            output_messages.append(f'[Request] Part {event.index} thinking delta: {event.delta.content_delta!r}')\n        elif isinstance(event.delta, ToolCallPartDelta):\n            output_messages.append(f'[Request] Part {event.index} args delta: {event.delta.args_delta}')\n    elif isinstance(event, FunctionToolCallEvent):\n        output_messages.append(\n            f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n        )\n    elif isinstance(event, FunctionToolResultEvent):\n        output_messages.append(f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}')\n    elif isinstance(event, FinalResultEvent):\n        output_messages.append(f'[Result] The model starting producing a final result (tool_name={event.tool_name})')\n\n\nasync def event_stream_handler(\n    ctx: RunContext,\n    event_stream: AsyncIterable[AgentStreamEvent],\n):\n    async for event in event_stream:\n        await handle_event(event)\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    async with weather_agent.run_stream(user_prompt, event_stream_handler=event_stream_handler) as run:\n        async for output in run.stream_text():\n            output_messages.append(f'[Output] {output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24°C and sunny.\",\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model starting producing a final result (tool_name=None)',\n        '[Output] It will be ',\n        '[Output] It will be warm and sunny ',\n        '[Output] It will be warm and sunny in Paris on ',\n        '[Output] It will be warm and sunny in Paris on Tuesday.',\n    ]\n    \"\"\"\n\n```\n\nrun_stream_event_stream_handler.py\n\n```python\nimport asyncio\nfrom collections.abc import AsyncIterable\nfrom datetime import date\n\nfrom pydantic_ai import (\n    Agent,\n    AgentStreamEvent,\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    RunContext,\n    TextPartDelta,\n    ThinkingPartDelta,\n    ToolCallPartDelta,\n)\n\nweather_agent = Agent(\n    'openai:gpt-5',\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\nasync def weather_forecast(\n    ctx: RunContext,\n    location: str,\n    forecast_date: date,\n) -> str:\n    return f'The forecast in {location} on {forecast_date} is 24°C and sunny.'\n\n\noutput_messages: list[str] = []\n\nasync def handle_event(event: AgentStreamEvent):\n    if isinstance(event, PartStartEvent):\n        output_messages.append(f'[Request] Starting part {event.index}: {event.part!r}')\n    elif isinstance(event, PartDeltaEvent):\n        if isinstance(event.delta, TextPartDelta):\n            output_messages.append(f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}')\n        elif isinstance(event.delta, ThinkingPartDelta):\n            output_messages.append(f'[Request] Part {event.index} thinking delta: {event.delta.content_delta!r}')\n        elif isinstance(event.delta, ToolCallPartDelta):\n            output_messages.append(f'[Request] Part {event.index} args delta: {event.delta.args_delta}')\n    elif isinstance(event, FunctionToolCallEvent):\n        output_messages.append(\n            f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n        )\n    elif isinstance(event, FunctionToolResultEvent):\n        output_messages.append(f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}')\n    elif isinstance(event, FinalResultEvent):\n        output_messages.append(f'[Result] The model starting producing a final result (tool_name={event.tool_name})')\n\n\nasync def event_stream_handler(\n    ctx: RunContext,\n    event_stream: AsyncIterable[AgentStreamEvent],\n):\n    async for event in event_stream:\n        await handle_event(event)\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    async with weather_agent.run_stream(user_prompt, event_stream_handler=event_stream_handler) as run:\n        async for output in run.stream_text():\n            output_messages.append(f'[Output] {output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24°C and sunny.\",\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model starting producing a final result (tool_name=None)',\n        '[Output] It will be ',\n        '[Output] It will be warm and sunny ',\n        '[Output] It will be warm and sunny in Paris on ',\n        '[Output] It will be warm and sunny in Paris on Tuesday.',\n    ]\n    \"\"\"\n\n```\n\n### Streaming All Events\n\nLike `agent.run_stream()`, agent.run() takes an optional `event_stream_handler` argument that lets you stream all events from the model's streaming response and the agent's execution of tools. Unlike `run_stream()`, it always runs the agent graph to completion even if text was received ahead of tool calls that looked like it could've been the final result.\n\nFor convenience, a agent.run_stream_events() method is also available as a wrapper around `run(event_stream_handler=...)`, which returns an async iterable of AgentStreamEvents and a AgentRunResultEvent containing the final run result.\n\nNote\n\nAs they return raw events as they come in, the `run_stream_events()` and `run(event_stream_handler=...)` methods require you to piece together the streamed text and structured output yourself from the `PartStartEvent` and subsequent `PartDeltaEvent`s.\n\nTo get the best of both worlds, at the expense of some additional complexity, you can use agent.iter() as described in the next section, which lets you [iterate over the agent graph](#iterating-over-an-agents-graph) and [stream both events and output](#streaming-all-events-and-output) at every step.\n\nrun_events.py\n\n```python\nimport asyncio\n\nfrom pydantic_ai import AgentRunResultEvent\n\nfrom run_stream_event_stream_handler import handle_event, output_messages, weather_agent\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    async for event in weather_agent.run_stream_events(user_prompt):\n        if isinstance(event, AgentRunResultEvent):\n            output_messages.append(f'[Final Output] {event.result.output}')\n        else:\n            await handle_event(event)\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24°C and sunny.\",\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model starting producing a final result (tool_name=None)',\n        \"[Request] Part 0 text delta: 'warm and sunny '\",\n        \"[Request] Part 0 text delta: 'in Paris on '\",\n        \"[Request] Part 0 text delta: 'Tuesday.'\",\n        '[Final Output] It will be warm and sunny in Paris on Tuesday.',\n    ]\n    \"\"\"\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\n### Iterating Over an Agent's Graph\n\nUnder the hood, each `Agent` in Pydantic AI uses **pydantic-graph** to manage its execution flow. **pydantic-graph** is a generic, type-centric library for building and running finite state machines in Python. It doesn't actually depend on Pydantic AI — you can use it standalone for workflows that have nothing to do with GenAI — but Pydantic AI makes use of it to orchestrate the handling of model requests and model responses in an agent's run.\n\nIn many scenarios, you don't need to worry about pydantic-graph at all; calling `agent.run(...)` simply traverses the underlying graph from start to finish. However, if you need deeper insight or control — for example to inject your own logic at specific stages — Pydantic AI exposes the lower-level iteration process via Agent.iter. This method returns an AgentRun, which you can async-iterate over, or manually drive node-by-node via the next method. Once the agent's graph returns an End, you have the final result along with a detailed history of all steps.\n\n#### `async for` iteration\n\nHere's an example of using `async for` with `iter` to record each node the agent executes:\n\n[Learn about Gateway](../gateway) agent_iter_async_for.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5')\n\n\nasync def main():\n    nodes = []\n    # Begin an AgentRun, which is an async-iterable over the nodes of the agent's graph\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            # Each node represents a step in the agent's execution\n            nodes.append(node)\n    print(nodes)\n    \"\"\"\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    \"\"\"\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n\n```\n\nagent_iter_async_for.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5')\n\n\nasync def main():\n    nodes = []\n    # Begin an AgentRun, which is an async-iterable over the nodes of the agent's graph\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            # Each node represents a step in the agent's execution\n            nodes.append(node)\n    print(nodes)\n    \"\"\"\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    \"\"\"\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\n- The `AgentRun` is an async iterator that yields each node (`BaseNode` or `End`) in the flow.\n- The run ends when an `End` node is returned.\n\n#### Using `.next(...)` manually\n\nYou can also drive the iteration manually by passing the node you want to run next to the `AgentRun.next(...)` method. This allows you to inspect or modify the node before it executes or skip nodes based on your own logic, and to catch errors in `next()` more easily:\n\n[Learn about Gateway](../gateway) agent_iter_next.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_graph import End\n\nagent = Agent('gateway/openai:gpt-5')\n\n\nasync def main():\n    async with agent.iter('What is the capital of France?') as agent_run:\n        node = agent_run.next_node  # (1)!\n\n        all_nodes = [node]\n\n        # Drive the iteration manually:\n        while not isinstance(node, End):  # (2)!\n            node = await agent_run.next(node)  # (3)!\n            all_nodes.append(node)  # (4)!\n\n        print(all_nodes)\n        \"\"\"\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ],\n                    run_id='...',\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-5',\n                    timestamp=datetime.datetime(...),\n                    run_id='...',\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        \"\"\"\n\n```\n\n1. We start by grabbing the first node that will be run in the agent's graph.\n1. The agent run is finished once an `End` node has been produced; instances of `End` cannot be passed to `next`.\n1. When you call `await agent_run.next(node)`, it executes that node in the agent's graph, updates the run's history, and returns the *next* node to run.\n1. You could also inspect or mutate the new `node` here as needed.\n\nagent_iter_next.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_graph import End\n\nagent = Agent('openai:gpt-5')\n\n\nasync def main():\n    async with agent.iter('What is the capital of France?') as agent_run:\n        node = agent_run.next_node  # (1)!\n\n        all_nodes = [node]\n\n        # Drive the iteration manually:\n        while not isinstance(node, End):  # (2)!\n            node = await agent_run.next(node)  # (3)!\n            all_nodes.append(node)  # (4)!\n\n        print(all_nodes)\n        \"\"\"\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ],\n                    run_id='...',\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-5',\n                    timestamp=datetime.datetime(...),\n                    run_id='...',\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        \"\"\"\n\n```\n\n1. We start by grabbing the first node that will be run in the agent's graph.\n1. The agent run is finished once an `End` node has been produced; instances of `End` cannot be passed to `next`.\n1. When you call `await agent_run.next(node)`, it executes that node in the agent's graph, updates the run's history, and returns the *next* node to run.\n1. You could also inspect or mutate the new `node` here as needed.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\n#### Accessing usage and final output\n\nYou can retrieve usage statistics (tokens, requests, etc.) at any time from the AgentRun object via `agent_run.usage()`. This method returns a RunUsage object containing the usage data.\n\nOnce the run finishes, `agent_run.result` becomes a AgentRunResult object containing the final output (and related metadata).\n\n#### Streaming All Events and Output\n\nHere is an example of streaming an agent run in combination with `async for` iteration:\n\nstreaming_iter.py\n\n```python\nimport asyncio\nfrom dataclasses import dataclass\nfrom datetime import date\n\nfrom pydantic_ai import (\n    Agent,\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    RunContext,\n    TextPartDelta,\n    ThinkingPartDelta,\n    ToolCallPartDelta,\n)\n\n\n@dataclass\nclass WeatherService:\n    async def get_forecast(self, location: str, forecast_date: date) -> str:\n        # In real code: call weather API, DB queries, etc.\n        return f'The forecast in {location} on {forecast_date} is 24°C and sunny.'\n\n    async def get_historic_weather(self, location: str, forecast_date: date) -> str:\n        # In real code: call a historical weather API or DB\n        return f'The weather in {location} on {forecast_date} was 18°C and partly cloudy.'\n\n\nweather_agent = Agent[WeatherService, str](\n    'openai:gpt-5',\n    deps_type=WeatherService,\n    output_type=str,  # We'll produce a final answer as plain text\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\nasync def weather_forecast(\n    ctx: RunContext[WeatherService],\n    location: str,\n    forecast_date: date,\n) -> str:\n    if forecast_date >= date.today():\n        return await ctx.deps.get_forecast(location, forecast_date)\n    else:\n        return await ctx.deps.get_historic_weather(location, forecast_date)\n\n\noutput_messages: list[str] = []\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    # Begin a node-by-node, streaming iteration\n    async with weather_agent.iter(user_prompt, deps=WeatherService()) as run:\n        async for node in run:\n            if Agent.is_user_prompt_node(node):\n                # A user prompt node => The user has provided input\n                output_messages.append(f'=== UserPromptNode: {node.user_prompt} ===')\n            elif Agent.is_model_request_node(node):\n                # A model request node => We can stream tokens from the model's request\n                output_messages.append('=== ModelRequestNode: streaming partial request tokens ===')\n                async with node.stream(run.ctx) as request_stream:\n                    final_result_found = False\n                    async for event in request_stream:\n                        if isinstance(event, PartStartEvent):\n                            output_messages.append(f'[Request] Starting part {event.index}: {event.part!r}')\n                        elif isinstance(event, PartDeltaEvent):\n                            if isinstance(event.delta, TextPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}'\n                                )\n                            elif isinstance(event.delta, ThinkingPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} thinking delta: {event.delta.content_delta!r}'\n                                )\n                            elif isinstance(event.delta, ToolCallPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} args delta: {event.delta.args_delta}'\n                                )\n                        elif isinstance(event, FinalResultEvent):\n                            output_messages.append(\n                                f'[Result] The model started producing a final result (tool_name={event.tool_name})'\n                            )\n                            final_result_found = True\n                            break\n\n                    if final_result_found:\n                        # Once the final result is found, we can call `AgentStream.stream_text()` to stream the text.\n                        # A similar `AgentStream.stream_output()` method is available to stream structured output.\n                        async for output in request_stream.stream_text():\n                            output_messages.append(f'[Output] {output}')\n            elif Agent.is_call_tools_node(node):\n                # A handle-response node => The model returned some data, potentially calls a tool\n                output_messages.append('=== CallToolsNode: streaming partial response & tool usage ===')\n                async with node.stream(run.ctx) as handle_stream:\n                    async for event in handle_stream:\n                        if isinstance(event, FunctionToolCallEvent):\n                            output_messages.append(\n                                f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n                            )\n                        elif isinstance(event, FunctionToolResultEvent):\n                            output_messages.append(\n                                f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}'\n                            )\n            elif Agent.is_end_node(node):\n                # Once an End node is reached, the agent run is complete\n                assert run.result is not None\n                assert run.result.output == node.data.output\n                output_messages.append(f'=== Final Agent Output: {run.result.output} ===')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        '=== UserPromptNode: What will the weather be like in Paris on Tuesday? ===',\n        '=== ModelRequestNode: streaming partial request tokens ===',\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '=== CallToolsNode: streaming partial response & tool usage ===',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24°C and sunny.\",\n        '=== ModelRequestNode: streaming partial request tokens ===',\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model started producing a final result (tool_name=None)',\n        '[Output] It will be ',\n        '[Output] It will be warm and sunny ',\n        '[Output] It will be warm and sunny in Paris on ',\n        '[Output] It will be warm and sunny in Paris on Tuesday.',\n        '=== CallToolsNode: streaming partial response & tool usage ===',\n        '=== Final Agent Output: It will be warm and sunny in Paris on Tuesday. ===',\n    ]\n    \"\"\"\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\n### Additional Configuration\n\n#### Usage Limits\n\nPydantic AI offers a UsageLimits structure to help you limit your usage (tokens, requests, and tool calls) on model runs.\n\nYou can apply these settings by passing the `usage_limits` argument to the `run{_sync,_stream}` functions.\n\nConsider the following example, where we limit the number of response tokens:\n\n[Learn about Gateway](../gateway)\n\n```python\nfrom pydantic_ai import Agent, UsageLimitExceeded, UsageLimits\n\nagent = Agent('gateway/anthropic:claude-sonnet-4-5')\n\nresult_sync = agent.run_sync(\n    'What is the capital of Italy? Answer with just the city.',\n    usage_limits=UsageLimits(response_tokens_limit=10),\n)\nprint(result_sync.output)\n#> Rome\nprint(result_sync.usage())\n#> RunUsage(input_tokens=62, output_tokens=1, requests=1)\n\ntry:\n    result_sync = agent.run_sync(\n        'What is the capital of Italy? Answer with a paragraph.',\n        usage_limits=UsageLimits(response_tokens_limit=10),\n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> Exceeded the output_tokens_limit of 10 (output_tokens=32)\n\n```\n\n```python\nfrom pydantic_ai import Agent, UsageLimitExceeded, UsageLimits\n\nagent = Agent('anthropic:claude-sonnet-4-5')\n\nresult_sync = agent.run_sync(\n    'What is the capital of Italy? Answer with just the city.',\n    usage_limits=UsageLimits(response_tokens_limit=10),\n)\nprint(result_sync.output)\n#> Rome\nprint(result_sync.usage())\n#> RunUsage(input_tokens=62, output_tokens=1, requests=1)\n\ntry:\n    result_sync = agent.run_sync(\n        'What is the capital of Italy? Answer with a paragraph.',\n        usage_limits=UsageLimits(response_tokens_limit=10),\n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> Exceeded the output_tokens_limit of 10 (output_tokens=32)\n\n```\n\nRestricting the number of requests can be useful in preventing infinite loops or excessive tool calling:\n\n[Learn about Gateway](../gateway)\n\n```python\nfrom typing_extensions import TypedDict\n\nfrom pydantic_ai import Agent, ModelRetry, UsageLimitExceeded, UsageLimits\n\n\nclass NeverOutputType(TypedDict):\n    \"\"\"\n    Never ever coerce data to this type.\n    \"\"\"\n\n    never_use_this: str\n\n\nagent = Agent(\n    'gateway/anthropic:claude-sonnet-4-5',\n    retries=3,\n    output_type=NeverOutputType,\n    system_prompt='Any time you get a response, call the `infinite_retry_tool` to produce another response.',\n)\n\n\n@agent.tool_plain(retries=5)  # (1)!\ndef infinite_retry_tool() -> int:\n    raise ModelRetry('Please try again.')\n\n\ntry:\n    result_sync = agent.run_sync(\n        'Begin infinite retry loop!', usage_limits=UsageLimits(request_limit=3)  # (2)!\n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> The next request would exceed the request_limit of 3\n\n```\n\n1. This tool has the ability to retry 5 times before erroring, simulating a tool that might get stuck in a loop.\n1. This run will error after 3 requests, preventing the infinite tool calling.\n\n```python\nfrom typing_extensions import TypedDict\n\nfrom pydantic_ai import Agent, ModelRetry, UsageLimitExceeded, UsageLimits\n\n\nclass NeverOutputType(TypedDict):\n    \"\"\"\n    Never ever coerce data to this type.\n    \"\"\"\n\n    never_use_this: str\n\n\nagent = Agent(\n    'anthropic:claude-sonnet-4-5',\n    retries=3,\n    output_type=NeverOutputType,\n    system_prompt='Any time you get a response, call the `infinite_retry_tool` to produce another response.',\n)\n\n\n@agent.tool_plain(retries=5)  # (1)!\ndef infinite_retry_tool() -> int:\n    raise ModelRetry('Please try again.')\n\n\ntry:\n    result_sync = agent.run_sync(\n        'Begin infinite retry loop!', usage_limits=UsageLimits(request_limit=3)  # (2)!\n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> The next request would exceed the request_limit of 3\n\n```\n\n1. This tool has the ability to retry 5 times before erroring, simulating a tool that might get stuck in a loop.\n1. This run will error after 3 requests, preventing the infinite tool calling.\n\n##### Capping tool calls\n\nIf you need a limit on the number of successful tool invocations within a single run, use `tool_calls_limit`:\n\n[Learn about Gateway](../gateway)\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.exceptions import UsageLimitExceeded\nfrom pydantic_ai.usage import UsageLimits\n\nagent = Agent('gateway/anthropic:claude-sonnet-4-5')\n\n@agent.tool_plain\ndef do_work() -> str:\n    return 'ok'\n\ntry:\n    # Allow at most one executed tool call in this run\n    agent.run_sync('Please call the tool twice', usage_limits=UsageLimits(tool_calls_limit=1))\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> The next tool call(s) would exceed the tool_calls_limit of 1 (tool_calls=2).\n\n```\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.exceptions import UsageLimitExceeded\nfrom pydantic_ai.usage import UsageLimits\n\nagent = Agent('anthropic:claude-sonnet-4-5')\n\n@agent.tool_plain\ndef do_work() -> str:\n    return 'ok'\n\ntry:\n    # Allow at most one executed tool call in this run\n    agent.run_sync('Please call the tool twice', usage_limits=UsageLimits(tool_calls_limit=1))\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> The next tool call(s) would exceed the tool_calls_limit of 1 (tool_calls=2).\n\n```\n\nNote\n\n- Usage limits are especially relevant if you've registered many tools. Use `request_limit` to bound the number of model turns, and `tool_calls_limit` to cap the number of successful tool executions within a run.\n- The `tool_calls_limit` is checked before executing tool calls. If the model returns parallel tool calls that would exceed the limit, no tools will be executed.\n\n#### Model (Run) Settings\n\nPydantic AI offers a settings.ModelSettings structure to help you fine tune your requests. This structure allows you to configure common parameters that influence the model's behavior, such as `temperature`, `max_tokens`, `timeout`, and more.\n\nThere are three ways to apply these settings, with a clear precedence order:\n\n1. **Model-level defaults** - Set when creating a model instance via the `settings` parameter. These serve as the base defaults for that model.\n1. **Agent-level defaults** - Set during Agent initialization via the `model_settings` argument. These are merged with model defaults, with agent settings taking precedence.\n1. **Run-time overrides** - Passed to `run{_sync,_stream}` functions via the `model_settings` argument. These have the highest priority and are merged with the combined agent and model defaults.\n\nFor example, if you'd like to set the `temperature` setting to `0.0` to ensure less random behavior, you can do the following:\n\n```python\nfrom pydantic_ai import Agent, ModelSettings\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\n### 1. Model-level defaults\nmodel = OpenAIChatModel(\n    'gpt-5',\n    settings=ModelSettings(temperature=0.8, max_tokens=500)  # Base defaults\n)\n\n### 2. Agent-level defaults (overrides model defaults by merging)\nagent = Agent(model, model_settings=ModelSettings(temperature=0.5))\n\n### 3. Run-time overrides (highest priority)\nresult_sync = agent.run_sync(\n    'What is the capital of Italy?',\n    model_settings=ModelSettings(temperature=0.0)  # Final temperature: 0.0\n)\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n\n```\n\nThe final request uses `temperature=0.0` (run-time), `max_tokens=500` (from model), demonstrating how settings merge with run-time taking precedence.\n\nModel Settings Support\n\nModel-level settings are supported by all concrete model implementations (OpenAI, Anthropic, Google, etc.). Wrapper models like `FallbackModel`, `WrapperModel`, and `InstrumentedModel` don't have their own settings - they use the settings of their underlying models.\n\n### Model specific settings\n\nIf you wish to further customize model behavior, you can use a subclass of ModelSettings, like GoogleModelSettings, associated with your model of choice.\n\nFor example:\n\n```python\nfrom pydantic_ai import Agent, UnexpectedModelBehavior\nfrom pydantic_ai.models.google import GoogleModelSettings\n\nagent = Agent('google-gla:gemini-2.5-flash')\n\ntry:\n    result = agent.run_sync(\n        'Write a list of 5 very rude things that I might say to the universe after stubbing my toe in the dark:',\n        model_settings=GoogleModelSettings(\n            temperature=0.0,  # general model settings can also be specified\n            gemini_safety_settings=[\n                {\n                    'category': 'HARM_CATEGORY_HARASSMENT',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n                {\n                    'category': 'HARM_CATEGORY_HATE_SPEECH',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n            ],\n        ),\n    )\nexcept UnexpectedModelBehavior as e:\n    print(e)  # (1)!\n    \"\"\"\n    Content filter 'SAFETY' triggered, body:\n    <safety settings details>\n    \"\"\"\n\n```\n\n1. This error is raised because the safety thresholds were exceeded.\n\n## Runs vs. Conversations\n\nAn agent **run** might represent an entire conversation — there's no limit to how many messages can be exchanged in a single run. However, a **conversation** might also be composed of multiple runs, especially if you need to maintain state between separate interactions or API calls.\n\nHere's an example of a conversation comprised of multiple runs:\n\n[Learn about Gateway](../gateway) conversation_example.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5')\n\n### First run\nresult1 = agent.run_sync('Who was Albert Einstein?')\nprint(result1.output)\n#> Albert Einstein was a German-born theoretical physicist.\n\n### Second run, passing previous messages\nresult2 = agent.run_sync(\n    'What was his most famous equation?',\n    message_history=result1.new_messages(),  # (1)!\n)\nprint(result2.output)\n#> Albert Einstein's most famous equation is (E = mc^2).\n\n```\n\n1. Continue the conversation; without `message_history` the model would not know who \"his\" was referring to.\n\nconversation_example.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5')\n\n### First run\nresult1 = agent.run_sync('Who was Albert Einstein?')\nprint(result1.output)\n#> Albert Einstein was a German-born theoretical physicist.\n\n### Second run, passing previous messages\nresult2 = agent.run_sync(\n    'What was his most famous equation?',\n    message_history=result1.new_messages(),  # (1)!\n)\nprint(result2.output)\n#> Albert Einstein's most famous equation is (E = mc^2).\n\n```\n\n1. Continue the conversation; without `message_history` the model would not know who \"his\" was referring to.\n\n*(This example is complete, it can be run \"as is\")*\n\n## Type safe by design\n\nPydantic AI is designed to work well with static type checkers, like mypy and pyright.\n\nTyping is (somewhat) optional\n\nPydantic AI is designed to make type checking as useful as possible for you if you choose to use it, but you don't have to use types everywhere all the time.\n\nThat said, because Pydantic AI uses Pydantic, and Pydantic uses type hints as the definition for schema and validation, some types (specifically type hints on parameters to tools, and the `output_type` arguments to Agent) are used at runtime.\n\nWe (the library developers) have messed up if type hints are confusing you more than helping you, if you find this, please create an [issue](https://github.com/pydantic/pydantic-ai/issues) explaining what's annoying you!\n\nIn particular, agents are generic in both the type of their dependencies and the type of the outputs they return, so you can use the type hints to ensure you're using the right types.\n\nConsider the following script with type mistakes:\n\ntype_mistakes.py\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass User:\n    name: str\n\n\nagent = Agent(\n    'test',\n    deps_type=User,  # (1)!\n    output_type=bool,\n)\n\n\n@agent.system_prompt\ndef add_user_name(ctx: RunContext[str]) -> str:  # (2)!\n    return f\"The user's name is {ctx.deps}.\"\n\n\ndef foobar(x: bytes) -> None:\n    pass\n\n\nresult = agent.run_sync('Does their name start with \"A\"?', deps=User('Anne'))\nfoobar(result.output)  # (3)!\n\n```\n\n1. The agent is defined as expecting an instance of `User` as `deps`.\n1. But here `add_user_name` is defined as taking a `str` as the dependency, not a `User`.\n1. Since the agent is defined as returning a `bool`, this will raise a type error since `foobar` expects `bytes`.\n\nRunning `mypy` on this will give the following output:\n\n```bash\n➤ uv run mypy type_mistakes.py\ntype_mistakes.py:18: error: Argument 1 to \"system_prompt\" of \"Agent\" has incompatible type \"Callable[[RunContext[str]], str]\"; expected \"Callable[[RunContext[User]], str]\"  [arg-type]\ntype_mistakes.py:28: error: Argument 1 to \"foobar\" has incompatible type \"bool\"; expected \"bytes\"  [arg-type]\nFound 2 errors in 1 file (checked 1 source file)\n\n```\n\nRunning `pyright` would identify the same issues.\n\n## System Prompts\n\nSystem prompts might seem simple at first glance since they're just strings (or sequences of strings that are concatenated), but crafting the right system prompt is key to getting the model to behave as you want.\n\nTip\n\nFor most use cases, you should use `instructions` instead of \"system prompts\".\n\nIf you know what you are doing though and want to preserve system prompt messages in the message history sent to the LLM in subsequent completions requests, you can achieve this using the `system_prompt` argument/decorator.\n\nSee the section below on [Instructions](#instructions) for more information.\n\nGenerally, system prompts fall into two categories:\n\n1. **Static system prompts**: These are known when writing the code and can be defined via the `system_prompt` parameter of the Agent constructor.\n1. **Dynamic system prompts**: These depend in some way on context that isn't known until runtime, and should be defined via functions decorated with @agent.system_prompt.\n\nYou can add both to a single agent; they're appended in the order they're defined at runtime.\n\nHere's an example using both types of system prompts:\n\n[Learn about Gateway](../gateway) system_prompts.py\n\n```python\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'gateway/openai:gpt-5',\n    deps_type=str,  # (1)!\n    system_prompt=\"Use the customer's name while replying to them.\",  # (2)!\n)\n\n\n@agent.system_prompt  # (3)!\ndef add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.system_prompt\ndef add_the_date() -> str:  # (4)!\n    return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02.\n\n```\n\n1. The agent expects a string dependency.\n1. Static system prompt defined at agent creation time.\n1. Dynamic system prompt defined via a decorator with RunContext, this is called just after `run_sync`, not when the agent is created, so can benefit from runtime information like the dependencies used on that run.\n1. Another dynamic system prompt, system prompts don't have to have the `RunContext` parameter.\n\nsystem_prompts.py\n\n```python\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=str,  # (1)!\n    system_prompt=\"Use the customer's name while replying to them.\",  # (2)!\n)\n\n\n@agent.system_prompt  # (3)!\ndef add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.system_prompt\ndef add_the_date() -> str:  # (4)!\n    return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02.\n\n```\n\n1. The agent expects a string dependency.\n1. Static system prompt defined at agent creation time.\n1. Dynamic system prompt defined via a decorator with RunContext, this is called just after `run_sync`, not when the agent is created, so can benefit from runtime information like the dependencies used on that run.\n1. Another dynamic system prompt, system prompts don't have to have the `RunContext` parameter.\n\n*(This example is complete, it can be run \"as is\")*\n\n## Instructions\n\nInstructions are similar to system prompts. The main difference is that when an explicit `message_history` is provided in a call to `Agent.run` and similar methods, *instructions* from any existing messages in the history are not included in the request to the model — only the instructions of the *current* agent are included.\n\nYou should use:\n\n- `instructions` when you want your request to the model to only include system prompts for the *current* agent\n- `system_prompt` when you want your request to the model to *retain* the system prompts used in previous requests (possibly made using other agents)\n\nIn general, we recommend using `instructions` instead of `system_prompt` unless you have a specific reason to use `system_prompt`.\n\nInstructions, like system prompts, can be specified at different times:\n\n1. **Static instructions**: These are known when writing the code and can be defined via the `instructions` parameter of the Agent constructor.\n1. **Dynamic instructions**: These rely on context that is only available at runtime and should be defined using functions decorated with @agent.instructions. Unlike dynamic system prompts, which may be reused when `message_history` is present, dynamic instructions are always reevaluated.\n1. \\**Runtime instructions*: These are additional instructions for a specific run that can be passed to one of the [run methods](#running-agents) using the `instructions` argument.\n\nAll three types of instructions can be added to a single agent, and they are appended in the order they are defined at runtime.\n\nHere's an example using a static instruction as well as dynamic instructions:\n\n[Learn about Gateway](../gateway) instructions.py\n\n```python\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'gateway/openai:gpt-5',\n    deps_type=str,  # (1)!\n    instructions=\"Use the customer's name while replying to them.\",  # (2)!\n)\n\n\n@agent.instructions  # (3)!\ndef add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.instructions\ndef add_the_date() -> str:  # (4)!\n    return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02.\n\n```\n\n1. The agent expects a string dependency.\n1. Static instructions defined at agent creation time.\n1. Dynamic instructions defined via a decorator with RunContext, this is called just after `run_sync`, not when the agent is created, so can benefit from runtime information like the dependencies used on that run.\n1. Another dynamic instruction, instructions don't have to have the `RunContext` parameter.\n\ninstructions.py\n\n```python\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=str,  # (1)!\n    instructions=\"Use the customer's name while replying to them.\",  # (2)!\n)\n\n\n@agent.instructions  # (3)!\ndef add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.instructions\ndef add_the_date() -> str:  # (4)!\n    return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02.\n\n```\n\n1. The agent expects a string dependency.\n1. Static instructions defined at agent creation time.\n1. Dynamic instructions defined via a decorator with RunContext, this is called just after `run_sync`, not when the agent is created, so can benefit from runtime information like the dependencies used on that run.\n1. Another dynamic instruction, instructions don't have to have the `RunContext` parameter.\n\n*(This example is complete, it can be run \"as is\")*\n\nNote that returning an empty string will result in no instruction message added.\n\n## Reflection and self-correction\n\nValidation errors from both function tool parameter validation and [structured output validation](../output/#structured-output) can be passed back to the model with a request to retry.\n\nYou can also raise ModelRetry from within a [tool](../tools/) or [output function](../output/#output-functions) to tell the model it should retry generating a response.\n\n- The default retry count is **1** but can be altered for the entire agent, a specific tool, or outputs.\n- You can access the current retry count from within a tool or output function via ctx.retry.\n\nHere's an example:\n\n[Learn about Gateway](../gateway) tool_retry.py\n\n```python\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\nfrom fake_database import DatabaseConn\n\n\nclass ChatResult(BaseModel):\n    user_id: int\n    message: str\n\n\nagent = Agent(\n    'gateway/openai:gpt-5',\n    deps_type=DatabaseConn,\n    output_type=ChatResult,\n)\n\n\n@agent.tool(retries=2)\ndef get_user_by_name(ctx: RunContext[DatabaseConn], name: str) -> int:\n    \"\"\"Get a user's ID from their full name.\"\"\"\n    print(name)\n    #> John\n    #> John Doe\n    user_id = ctx.deps.users.get(name=name)\n    if user_id is None:\n        raise ModelRetry(\n            f'No user found with name {name!r}, remember to provide their full name'\n        )\n    return user_id\n\n\nresult = agent.run_sync(\n    'Send a message to John Doe asking for coffee next week', deps=DatabaseConn()\n)\nprint(result.output)\n\"\"\"\nuser_id=123 message='Hello John, would you be free for coffee sometime next week? Let me know what works for you!'\n\"\"\"\n\n```\n\ntool_retry.py\n\n```python\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\nfrom fake_database import DatabaseConn\n\n\nclass ChatResult(BaseModel):\n    user_id: int\n    message: str\n\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=DatabaseConn,\n    output_type=ChatResult,\n)\n\n\n@agent.tool(retries=2)\ndef get_user_by_name(ctx: RunContext[DatabaseConn], name: str) -> int:\n    \"\"\"Get a user's ID from their full name.\"\"\"\n    print(name)\n    #> John\n    #> John Doe\n    user_id = ctx.deps.users.get(name=name)\n    if user_id is None:\n        raise ModelRetry(\n            f'No user found with name {name!r}, remember to provide their full name'\n        )\n    return user_id\n\n\nresult = agent.run_sync(\n    'Send a message to John Doe asking for coffee next week', deps=DatabaseConn()\n)\nprint(result.output)\n\"\"\"\nuser_id=123 message='Hello John, would you be free for coffee sometime next week? Let me know what works for you!'\n\"\"\"\n\n```\n\n## Model errors\n\nIf models behave unexpectedly (e.g., the retry limit is exceeded, or their API returns `503`), agent runs will raise UnexpectedModelBehavior.\n\nIn these cases, capture_run_messages can be used to access the messages exchanged during the run to help diagnose the issue.\n\n[Learn about Gateway](../gateway) agent_model_errors.py\n\n```python\nfrom pydantic_ai import Agent, ModelRetry, UnexpectedModelBehavior, capture_run_messages\n\nagent = Agent('gateway/openai:gpt-5')\n\n\n@agent.tool_plain\ndef calc_volume(size: int) -> int:  # (1)!\n    if size == 42:\n        return size**3\n    else:\n        raise ModelRetry('Please try again.')\n\n\nwith capture_run_messages() as messages:  # (2)!\n    try:\n        result = agent.run_sync('Please get me the volume of a box with size 6.')\n    except UnexpectedModelBehavior as e:\n        print('An error occurred:', e)\n        #> An error occurred: Tool 'calc_volume' exceeded max retries count of 1\n        print('cause:', repr(e.__cause__))\n        #> cause: ModelRetry('Please try again.')\n        print('messages:', messages)\n        \"\"\"\n        messages:\n        [\n            ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='Please get me the volume of a box with size 6.',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=RequestUsage(input_tokens=62, output_tokens=4),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            ),\n            ModelRequest(\n                parts=[\n                    RetryPromptPart(\n                        content='Please try again.',\n                        tool_name='calc_volume',\n                        tool_call_id='pyd_ai_tool_call_id',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=RequestUsage(input_tokens=72, output_tokens=8),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            ),\n        ]\n        \"\"\"\n    else:\n        print(result.output)\n\n```\n\n1. Define a tool that will raise `ModelRetry` repeatedly in this case.\n1. capture_run_messages is used to capture the messages exchanged during the run.\n\nagent_model_errors.py\n\n```python\nfrom pydantic_ai import Agent, ModelRetry, UnexpectedModelBehavior, capture_run_messages\n\nagent = Agent('openai:gpt-5')\n\n\n@agent.tool_plain\ndef calc_volume(size: int) -> int:  # (1)!\n    if size == 42:\n        return size**3\n    else:\n        raise ModelRetry('Please try again.')\n\n\nwith capture_run_messages() as messages:  # (2)!\n    try:\n        result = agent.run_sync('Please get me the volume of a box with size 6.')\n    except UnexpectedModelBehavior as e:\n        print('An error occurred:', e)\n        #> An error occurred: Tool 'calc_volume' exceeded max retries count of 1\n        print('cause:', repr(e.__cause__))\n        #> cause: ModelRetry('Please try again.')\n        print('messages:', messages)\n        \"\"\"\n        messages:\n        [\n            ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='Please get me the volume of a box with size 6.',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=RequestUsage(input_tokens=62, output_tokens=4),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            ),\n            ModelRequest(\n                parts=[\n                    RetryPromptPart(\n                        content='Please try again.',\n                        tool_name='calc_volume',\n                        tool_call_id='pyd_ai_tool_call_id',\n                        timestamp=datetime.datetime(...),\n                    )\n                ],\n                run_id='...',\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=RequestUsage(input_tokens=72, output_tokens=8),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n                run_id='...',\n            ),\n        ]\n        \"\"\"\n    else:\n        print(result.output)\n\n```\n\n1. Define a tool that will raise `ModelRetry` repeatedly in this case.\n1. capture_run_messages is used to capture the messages exchanged during the run.\n\n*(This example is complete, it can be run \"as is\")*\n\nNote\n\nIf you call run, run_sync, or run_stream more than once within a single `capture_run_messages` context, `messages` will represent the messages exchanged during the first call only.",
  "content_length": 70723
}