{
  "title": "OpenAI",
  "source_url": null,
  "content": "## Install\n\nTo use OpenAI models or OpenAI-compatible APIs, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `openai` optional group:\n\n```bash\npip install \"pydantic-ai-slim[openai]\"\n\n```\n\n```bash\nuv add \"pydantic-ai-slim[openai]\"\n\n```\n\n## Configuration\n\nTo use `OpenAIChatModel` with the OpenAI API, go to [platform.openai.com](https://platform.openai.com/) and follow your nose until you find the place to generate an API key.\n\n## Environment variable\n\nOnce you have the API key, you can set it as an environment variable:\n\n```bash\nexport OPENAI_API_KEY='your-api-key'\n\n```\n\nYou can then use `OpenAIChatModel` by name:\n\n[Learn about Gateway](../../gateway)\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5')\n...\n\n```\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5')\n...\n\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\nmodel = OpenAIChatModel('gpt-5')\nagent = Agent(model)\n...\n\n```\n\nBy default, the `OpenAIChatModel` uses the `OpenAIProvider` with the `base_url` set to `https://api.openai.com/v1`.\n\n## Configure the provider\n\nIf you want to pass parameters in code to the provider, you can programmatically instantiate the OpenAIProvider and pass it to the model:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(api_key='your-api-key'))\nagent = Agent(model)\n...\n\n```\n\n## Custom OpenAI Client\n\n`OpenAIProvider` also accepts a custom `AsyncOpenAI` client via the `openai_client` parameter, so you can customise the `organization`, `project`, `base_url` etc. as defined in the [OpenAI API docs](https://platform.openai.com/docs/api-reference).\n\ncustom_openai_client.py\n\n```python\nfrom openai import AsyncOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncOpenAI(max_retries=3)\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(openai_client=client))\nagent = Agent(model)\n...\n\n```\n\nYou could also use the [`AsyncAzureOpenAI`](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints) client to use the Azure OpenAI API. Note that the `AsyncAzureOpenAI` is a subclass of `AsyncOpenAI`.\n\n```python\nfrom openai import AsyncAzureOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncAzureOpenAI(\n    azure_endpoint='...',\n    api_version='2024-07-01-preview',\n    api_key='your-api-key',\n)\n\nmodel = OpenAIChatModel(\n    'gpt-5',\n    provider=OpenAIProvider(openai_client=client),\n)\nagent = Agent(model)\n...\n\n```\n\n## OpenAI Responses API\n\nPydantic AI also supports OpenAI's [Responses API](https://platform.openai.com/docs/api-reference/responses) through the\n\nYou can use OpenAIResponsesModel by name:\n\n[Learn about Gateway](../../gateway)\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai-responses:gpt-5')\n...\n\n```\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai-responses:gpt-5')\n...\n\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel\n\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model)\n...\n\n```\n\nYou can learn more about the differences between the Responses API and Chat Completions API in the [OpenAI API docs](https://platform.openai.com/docs/guides/migrate-to-responses).\n\n### Built-in tools\n\nThe Responses API has built-in tools that you can use instead of building your own:\n\n- [Web search](https://platform.openai.com/docs/guides/tools-web-search): allow models to search the web for the latest information before generating a response.\n- [Code interpreter](https://platform.openai.com/docs/guides/tools-code-interpreter): allow models to write and run Python code in a sandboxed environment before generating a response.\n- [Image generation](https://platform.openai.com/docs/guides/tools-image-generation): allow models to generate images based on a text prompt.\n- [File search](https://platform.openai.com/docs/guides/tools-file-search): allow models to search your files for relevant information before generating a response.\n- [Computer use](https://platform.openai.com/docs/guides/tools-computer-use): allow models to use a computer to perform tasks on your behalf.\n\nWeb search, Code interpreter, and Image generation are natively supported through the [Built-in tools](../../builtin-tools/) feature.\n\nFile search and Computer use can be enabled by passing an [`openai.types.responses.FileSearchToolParam`](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/file_search_tool_param.py) or [`openai.types.responses.ComputerToolParam`](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/computer_tool_param.py) in the `openai_builtin_tools` setting on OpenAIResponsesModelSettings. They don't currently generate BuiltinToolCallPart or BuiltinToolReturnPart parts in the message history, or streamed events; please submit an issue if you need native support for these built-in tools.\n\nfile_search_tool.py\n\n```python\nfrom openai.types.responses import FileSearchToolParam\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel_settings = OpenAIResponsesModelSettings(\n    openai_builtin_tools=[\n        FileSearchToolParam(\n            type='file_search',\n            vector_store_ids=['your-history-book-vector-store-id']\n        )\n    ],\n)\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model=model, model_settings=model_settings)\n\nresult = agent.run_sync('Who was Albert Einstein?')\nprint(result.output)\n#> Albert Einstein was a German-born theoretical physicist.\n\n```\n\n#### Referencing earlier responses\n\nThe Responses API supports referencing earlier model responses in a new request using a `previous_response_id` parameter, to ensure the full [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#passing-context-from-the-previous-response) including [reasoning items](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) are kept in context. This is available through the `openai_previous_response_id` field in OpenAIResponsesModelSettings.\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model=model)\n\nresult = agent.run_sync('The secret is 1234')\nmodel_settings = OpenAIResponsesModelSettings(\n    openai_previous_response_id=result.all_messages()[-1].provider_response_id\n)\nresult = agent.run_sync('What is the secret code?', model_settings=model_settings)\nprint(result.output)\n#> 1234\n\n```\n\nBy passing the `provider_response_id` from an earlier run, you can allow the model to build on its own prior reasoning without needing to resend the full message history.\n\n##### Automatically referencing earlier responses\n\nWhen the `openai_previous_response_id` field is set to `'auto'`, Pydantic AI will automatically select the most recent `provider_response_id` from message history and omit messages that came before it, letting the OpenAI API leverage server-side history instead for improved efficiency.\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model=model)\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\n### When set to 'auto', the most recent provider_response_id\n### and messages after it are sent as request.\nmodel_settings = OpenAIResponsesModelSettings(openai_previous_response_id='auto')\nresult2 = agent.run_sync(\n    'Explain?',\n    message_history=result1.new_messages(),\n    model_settings=model_settings\n)\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\n```\n\n## OpenAI-compatible Models\n\nMany providers and models are compatible with the OpenAI API, and can be used with `OpenAIChatModel` in Pydantic AI. Before getting started, check the [installation and configuration](#install) instructions above.\n\nTo use another OpenAI-compatible API, you can make use of the `base_url` and `api_key` arguments from `OpenAIProvider`:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>', api_key='your-api-key'\n    ),\n)\nagent = Agent(model)\n...\n\n```\n\nVarious providers also have their own provider classes so that you don't need to specify the base URL yourself and you can use the standard `<PROVIDER>_API_KEY` environment variable to set the API key. When a provider has its own provider class, you can use the `Agent(\"<provider>:<model>\")` shorthand, e.g. `Agent(\"deepseek:deepseek-chat\")` or `Agent(\"moonshotai:kimi-k2-0711-preview\")`, instead of building the `OpenAIChatModel` explicitly. Similarly, you can pass the provider name as a string to the `provider` argument on `OpenAIChatModel` instead of building instantiating the provider class explicitly.\n\n#### Model Profile\n\nSometimes, the provider or model you're using will have slightly different requirements than OpenAI's API or models, like having different restrictions on JSON schemas for tool definitions, or not supporting tool definitions to be marked as strict.\n\nWhen using an alternative provider class provided by Pydantic AI, an appropriate model profile is typically selected automatically based on the model name. If the model you're using is not working correctly out of the box, you can tweak various aspects of how model requests are constructed by providing your own ModelProfile (for behaviors shared among all model classes) or OpenAIModelProfile (for behaviors specific to `OpenAIChatModel`):\n\n```python\nfrom pydantic_ai import Agent, InlineDefsJsonSchemaTransformer\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.profiles.openai import OpenAIModelProfile\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'\n    ),\n    profile=OpenAIModelProfile(\n        json_schema_transformer=InlineDefsJsonSchemaTransformer,  # Supported by any model class on a plain ModelProfile\n        openai_supports_strict_tool_definition=False  # Supported by OpenAIModel only, requires OpenAIModelProfile\n    )\n)\nagent = Agent(model)\n\n```\n\n### DeepSeek\n\nTo use the [DeepSeek](https://deepseek.com) provider, first create an API key by following the [Quick Start guide](https://api-docs.deepseek.com/).\n\nYou can then set the `DEEPSEEK_API_KEY` environment variable and use DeepSeekProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('deepseek:deepseek-chat')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\nmodel = OpenAIChatModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(api_key='your-deepseek-api-key'),\n)\nagent = Agent(model)\n...\n\n```\n\nYou can also customize any provider with a custom `http_client`:\n\n```python\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = OpenAIChatModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(\n        api_key='your-deepseek-api-key', http_client=custom_http_client\n    ),\n)\nagent = Agent(model)\n...\n\n```\n\n### Ollama\n\nPydantic AI supports both self-hosted [Ollama](https://ollama.com/) servers (running locally or remotely) and [Ollama Cloud](https://ollama.com/cloud).\n\nFor servers running locally, use the `http://localhost:11434/v1` base URL. For Ollama Cloud, use `https://ollama.com/v1` and ensure an API key is set.\n\nYou can set the `OLLAMA_BASE_URL` and (optionally) `OLLAMA_API_KEY` environment variables and use OllamaProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('ollama:gpt-oss:20b')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nollama_model = OpenAIChatModel(\n    model_name='gpt-oss:20b',\n    provider=OllamaProvider(base_url='http://localhost:11434/v1'),  # (1)!\n)\nagent = Agent(ollama_model, output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n#> city='London' country='United Kingdom'\nprint(result.usage())\n#> RunUsage(input_tokens=57, output_tokens=8, requests=1)\n\n```\n\n1. For Ollama Cloud, use the `base_url='https://ollama.com/v1'` and set the `OLLAMA_API_KEY` environment variable.\n\n### Azure AI Foundry\n\nTo use [Azure AI Foundry](https://ai.azure.com/) as your provider, you can set the `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, and `OPENAI_API_VERSION` environment variables and use AzureProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('azure:gpt-5')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.azure import AzureProvider\n\nmodel = OpenAIChatModel(\n    'gpt-5',\n    provider=AzureProvider(\n        azure_endpoint='your-azure-endpoint',\n        api_version='your-api-version',\n        api_key='your-api-key',\n    ),\n)\nagent = Agent(model)\n...\n\n```\n\n### Vercel AI Gateway\n\nTo use [Vercel's AI Gateway](https://vercel.com/docs/ai-gateway), first follow the [documentation](https://vercel.com/docs/ai-gateway) instructions on obtaining an API key or OIDC token.\n\nYou can set the `VERCEL_AI_GATEWAY_API_KEY` and `VERCEL_OIDC_TOKEN` environment variables and use VercelProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('vercel:anthropic/claude-4-sonnet')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.vercel import VercelProvider\n\nmodel = OpenAIChatModel(\n    'anthropic/claude-4-sonnet',\n    provider=VercelProvider(api_key='your-vercel-ai-gateway-api-key'),\n)\nagent = Agent(model)\n...\n\n```\n\n### Grok (xAI)\n\nGo to [xAI API Console](https://console.x.ai/) and create an API key.\n\nYou can set the `GROK_API_KEY` environment variable and use GrokProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('grok:grok-2-1212')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.grok import GrokProvider\n\nmodel = OpenAIChatModel(\n    'grok-2-1212',\n    provider=GrokProvider(api_key='your-xai-api-key'),\n)\nagent = Agent(model)\n...\n\n```\n\n### MoonshotAI\n\nCreate an API key in the [Moonshot Console](https://platform.moonshot.ai/console).\n\nYou can set the `MOONSHOTAI_API_KEY` environment variable and use MoonshotAIProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('moonshotai:kimi-k2-0711-preview')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.moonshotai import MoonshotAIProvider\n\nmodel = OpenAIChatModel(\n    'kimi-k2-0711-preview',\n    provider=MoonshotAIProvider(api_key='your-moonshot-api-key'),\n)\nagent = Agent(model)\n...\n\n```\n\n### GitHub Models\n\nTo use [GitHub Models](https://docs.github.com/en/github-models), you'll need a GitHub personal access token with the `models: read` permission.\n\nYou can set the `GITHUB_API_KEY` environment variable and use GitHubProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('github:xai/grok-3-mini')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.github import GitHubProvider\n\nmodel = OpenAIChatModel(\n    'xai/grok-3-mini',  # GitHub Models uses prefixed model names\n    provider=GitHubProvider(api_key='your-github-token'),\n)\nagent = Agent(model)\n...\n\n```\n\nGitHub Models supports various model families with different prefixes. You can see the full list on the [GitHub Marketplace](https://github.com/marketplace?type=models) or the public [catalog endpoint](https://models.github.ai/catalog/models).\n\n### Perplexity\n\nFollow the Perplexity [getting started](https://docs.perplexity.ai/guides/getting-started) guide to create an API key.\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'sonar-pro',\n    provider=OpenAIProvider(\n        base_url='https://api.perplexity.ai',\n        api_key='your-perplexity-api-key',\n    ),\n)\nagent = Agent(model)\n...\n\n```\n\n### Fireworks AI\n\nGo to [Fireworks.AI](https://fireworks.ai/) and create an API key in your account settings.\n\nYou can set the `FIREWORKS_API_KEY` environment variable and use FireworksProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('fireworks:accounts/fireworks/models/qwq-32b')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.fireworks import FireworksProvider\n\nmodel = OpenAIChatModel(\n    'accounts/fireworks/models/qwq-32b',  # model library available at https://fireworks.ai/models\n    provider=FireworksProvider(api_key='your-fireworks-api-key'),\n)\nagent = Agent(model)\n...\n\n```\n\n### Together AI\n\nGo to [Together.ai](https://www.together.ai/) and create an API key in your account settings.\n\nYou can set the `TOGETHER_API_KEY` environment variable and use TogetherProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('together:meta-llama/Llama-3.3-70B-Instruct-Turbo-Free')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.together import TogetherProvider\n\nmodel = OpenAIChatModel(\n    'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free',  # model library available at https://www.together.ai/models\n    provider=TogetherProvider(api_key='your-together-api-key'),\n)\nagent = Agent(model)\n...\n\n```\n\n### Heroku AI\n\nTo use [Heroku AI](https://www.heroku.com/ai), first create an API key.\n\nYou can set the `HEROKU_INFERENCE_KEY` and (optionally )`HEROKU_INFERENCE_URL` environment variables and use HerokuProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('heroku:claude-sonnet-4-5')\n...\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.heroku import HerokuProvider\n\nmodel = OpenAIChatModel(\n    'claude-sonnet-4-5',\n    provider=HerokuProvider(api_key='your-heroku-inference-key'),\n)\nagent = Agent(model)\n...\n\n```\n\n### Cerebras\n\nTo use [Cerebras](https://cerebras.ai/), you need to create an API key in the [Cerebras Console](https://cloud.cerebras.ai/).\n\nYou can set the `CEREBRAS_API_KEY` environment variable and use CerebrasProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('cerebras:llama3.3-70b')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.cerebras import CerebrasProvider\n\nmodel = OpenAIChatModel(\n    'llama3.3-70b',\n    provider=CerebrasProvider(api_key='your-cerebras-api-key'),\n)\nagent = Agent(model)\n\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n\n```\n\n### LiteLLM\n\nTo use [LiteLLM](https://www.litellm.ai/), set the configs as outlined in the [doc](https://docs.litellm.ai/docs/set_keys). In `LiteLLMProvider`, you can pass `api_base` and `api_key`. The value of these configs will depend on your setup. For example, if you are using OpenAI models, then you need to pass `https://api.openai.com/v1` as the `api_base` and your OpenAI API key as the `api_key`. If you are using a LiteLLM proxy server running on your local machine, then you need to pass `http://localhost:<port>` as the `api_base` and your LiteLLM API key (or a placeholder) as the `api_key`.\n\nTo use custom LLMs, use `custom/` prefix in the model name.\n\nOnce you have the configs, use the LiteLLMProvider as follows:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.litellm import LiteLLMProvider\n\nmodel = OpenAIChatModel(\n    'openai/gpt-5',\n    provider=LiteLLMProvider(\n        api_base='<api-base-url>',\n        api_key='<api-key>'\n    )\n)\nagent = Agent(model)\n\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n...\n\n```\n\n### Nebius AI Studio\n\nGo to [Nebius AI Studio](https://studio.nebius.com/) and create an API key.\n\nYou can set the `NEBIUS_API_KEY` environment variable and use NebiusProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('nebius:Qwen/Qwen3-32B-fast')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.nebius import NebiusProvider\n\nmodel = OpenAIChatModel(\n    'Qwen/Qwen3-32B-fast',\n    provider=NebiusProvider(api_key='your-nebius-api-key'),\n)\nagent = Agent(model)\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n\n```\n\n### OVHcloud AI Endpoints\n\nTo use OVHcloud AI Endpoints, you need to create a new API key. To do so, go to the [OVHcloud manager](https://ovh.com/manager), then in Public Cloud > AI Endpoints > API keys. Click on `Create a new API key` and copy your new key.\n\nYou can explore the [catalog](https://endpoints.ai.cloud.ovh.net/catalog) to find which models are available.\n\nYou can set the `OVHCLOUD_API_KEY` environment variable and use OVHcloudProvider by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('ovhcloud:gpt-oss-120b')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n\n```\n\nIf you need to configure the provider, you can use the OVHcloudProvider class:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ovhcloud import OVHcloudProvider\n\nmodel = OpenAIChatModel(\n    'gpt-oss-120b',\n    provider=OVHcloudProvider(api_key='your-api-key'),\n)\nagent = Agent(model)\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n\n```",
  "content_length": 24107
}