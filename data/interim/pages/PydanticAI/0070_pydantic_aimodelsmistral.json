{
  "title": "`pydantic_ai.models.mistral`",
  "source_url": null,
  "content": "## Setup\n\nFor details on how to set up authentication with this model, see [model configuration for Mistral](../../../models/mistral/).\n\n### LatestMistralModelNames\n\n```python\nLatestMistralModelNames = Literal[\n    \"mistral-large-latest\",\n    \"mistral-small-latest\",\n    \"codestral-latest\",\n    \"mistral-moderation-latest\",\n]\n\n```\n\nLatest Mistral models.\n\n### MistralModelName\n\n```python\nMistralModelName = str | LatestMistralModelNames\n\n```\n\nPossible Mistral model names.\n\nSince Mistral supports a variety of date-stamped models, we explicitly list the most popular models but allow any name in the type hints. Since [the Mistral docs](https://docs.mistral.ai/getting-started/models/models_overview/) for a full list.\n\n### MistralModelSettings\n\nBases: `ModelSettings`\n\nSettings used for a Mistral model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\n\n```python\nclass MistralModelSettings(ModelSettings, total=False):\n    \"\"\"Settings used for a Mistral model request.\"\"\"\n\n```\n\n### MistralModel\n\nBases: `Model`\n\nA model that uses Mistral.\n\nInternally, this uses the [Mistral Python client](https://github.com/mistralai/client-python) to interact with the API.\n\n[API Documentation](https://docs.mistral.ai/)\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\n\n````python\n@dataclass(init=False)\nclass MistralModel(Model):\n    \"\"\"A model that uses Mistral.\n\n    Internally, this uses the [Mistral Python client](https://github.com/mistralai/client-python) to interact with the API.\n\n    [API Documentation](https://docs.mistral.ai/)\n    \"\"\"\n\n    client: Mistral = field(repr=False)\n    json_mode_schema_prompt: str\n\n    _model_name: MistralModelName = field(repr=False)\n    _provider: Provider[Mistral] = field(repr=False)\n\n    def __init__(\n        self,\n        model_name: MistralModelName,\n        *,\n        provider: Literal['mistral'] | Provider[Mistral] = 'mistral',\n        profile: ModelProfileSpec | None = None,\n        json_mode_schema_prompt: str = \"\"\"Answer in JSON Object, respect the format:\\n```\\n{schema}\\n```\\n\"\"\",\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize a Mistral model.\n\n        Args:\n            model_name: The name of the model to use.\n            provider: The provider to use for authentication and API access. Can be either the string\n                'mistral' or an instance of `Provider[Mistral]`. If not provided, a new provider will be\n                created using the other parameters.\n            profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n            json_mode_schema_prompt: The prompt to show when the model expects a JSON object as input.\n            settings: Model-specific settings that will be used as defaults for this model.\n        \"\"\"\n        self._model_name = model_name\n        self.json_mode_schema_prompt = json_mode_schema_prompt\n\n        if isinstance(provider, str):\n            provider = infer_provider(provider)\n        self._provider = provider\n        self.client = provider.client\n\n        super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n    @property\n    def base_url(self) -> str:\n        return self._provider.base_url\n\n    @property\n    def model_name(self) -> MistralModelName:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The model provider.\"\"\"\n        return self._provider.name\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        \"\"\"Make a non-streaming request to the model from Pydantic AI call.\"\"\"\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._completions_create(\n            messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters\n        )\n        model_response = self._process_response(response)\n        return model_response\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        \"\"\"Make a streaming request to the model from Pydantic AI call.\"\"\"\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._stream_completions_create(\n            messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters\n        )\n        async with response:\n            yield await self._process_streamed_response(response, model_request_parameters)\n\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        model_settings: MistralModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> MistralChatCompletionResponse:\n        \"\"\"Make a non-streaming request to the model.\"\"\"\n        # TODO(Marcelo): We need to replace the current MistralAI client to use the beta client.\n        # See https://docs.mistral.ai/agents/connectors/websearch/ to support web search.\n        if model_request_parameters.builtin_tools:\n            raise UserError('Mistral does not support built-in tools')\n\n        try:\n            response = await self.client.chat.complete_async(\n                model=str(self._model_name),\n                messages=self._map_messages(messages, model_request_parameters),\n                n=1,\n                tools=self._map_function_and_output_tools_definition(model_request_parameters) or UNSET,\n                tool_choice=self._get_tool_choice(model_request_parameters),\n                stream=False,\n                max_tokens=model_settings.get('max_tokens', UNSET),\n                temperature=model_settings.get('temperature', UNSET),\n                top_p=model_settings.get('top_p', 1),\n                timeout_ms=self._get_timeout_ms(model_settings.get('timeout')),\n                random_seed=model_settings.get('seed', UNSET),\n                stop=model_settings.get('stop_sequences', None),\n                http_headers={'User-Agent': get_user_agent()},\n            )\n        except SDKError as e:\n            if (status_code := e.status_code) >= 400:\n                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n            raise ModelAPIError(model_name=self.model_name, message=e.message) from e\n\n        assert response, 'A unexpected empty response from Mistral.'\n        return response\n\n    async def _stream_completions_create(\n        self,\n        messages: list[ModelMessage],\n        model_settings: MistralModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> MistralEventStreamAsync[MistralCompletionEvent]:\n        \"\"\"Create a streaming completion request to the Mistral model.\"\"\"\n        response: MistralEventStreamAsync[MistralCompletionEvent] | None\n        mistral_messages = self._map_messages(messages, model_request_parameters)\n\n        # TODO(Marcelo): We need to replace the current MistralAI client to use the beta client.\n        # See https://docs.mistral.ai/agents/connectors/websearch/ to support web search.\n        if model_request_parameters.builtin_tools:\n            raise UserError('Mistral does not support built-in tools')\n\n        if model_request_parameters.function_tools:\n            # Function Calling\n            response = await self.client.chat.stream_async(\n                model=str(self._model_name),\n                messages=mistral_messages,\n                n=1,\n                tools=self._map_function_and_output_tools_definition(model_request_parameters) or UNSET,\n                tool_choice=self._get_tool_choice(model_request_parameters),\n                temperature=model_settings.get('temperature', UNSET),\n                top_p=model_settings.get('top_p', 1),\n                max_tokens=model_settings.get('max_tokens', UNSET),\n                timeout_ms=self._get_timeout_ms(model_settings.get('timeout')),\n                presence_penalty=model_settings.get('presence_penalty'),\n                frequency_penalty=model_settings.get('frequency_penalty'),\n                stop=model_settings.get('stop_sequences', None),\n                http_headers={'User-Agent': get_user_agent()},\n            )\n\n        elif model_request_parameters.output_tools:\n            # TODO: Port to native \"manual JSON\" mode\n            # Json Mode\n            parameters_json_schemas = [tool.parameters_json_schema for tool in model_request_parameters.output_tools]\n            user_output_format_message = self._generate_user_output_format(parameters_json_schemas)\n            mistral_messages.append(user_output_format_message)\n\n            response = await self.client.chat.stream_async(\n                model=str(self._model_name),\n                messages=mistral_messages,\n                response_format={\n                    'type': 'json_object'\n                },  # TODO: Should be able to use json_schema now: https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/, https://github.com/mistralai/client-python/blob/bc4adf335968c8a272e1ab7da8461c9943d8e701/src/mistralai/extra/utils/response_format.py#L9\n                stream=True,\n                http_headers={'User-Agent': get_user_agent()},\n            )\n\n        else:\n            # Stream Mode\n            response = await self.client.chat.stream_async(\n                model=str(self._model_name),\n                messages=mistral_messages,\n                stream=True,\n                http_headers={'User-Agent': get_user_agent()},\n            )\n        assert response, 'A unexpected empty response from Mistral.'\n        return response\n\n    def _get_tool_choice(self, model_request_parameters: ModelRequestParameters) -> MistralToolChoiceEnum | None:\n        \"\"\"Get tool choice for the model.\n\n        - \"auto\": Default mode. Model decides if it uses the tool or not.\n        - \"any\": Select any tool.\n        - \"none\": Prevents tool use.\n        - \"required\": Forces tool use.\n        \"\"\"\n        if not model_request_parameters.function_tools and not model_request_parameters.output_tools:\n            return None\n        elif not model_request_parameters.allow_text_output:\n            return 'required'\n        else:\n            return 'auto'\n\n    def _map_function_and_output_tools_definition(\n        self, model_request_parameters: ModelRequestParameters\n    ) -> list[MistralTool] | None:\n        \"\"\"Map function and output tools to MistralTool format.\n\n        Returns None if both function_tools and output_tools are empty.\n        \"\"\"\n        tools = [\n            MistralTool(\n                function=MistralFunction(\n                    name=r.name, parameters=r.parameters_json_schema, description=r.description or ''\n                )\n            )\n            for r in model_request_parameters.tool_defs.values()\n        ]\n        return tools if tools else None\n\n    def _process_response(self, response: MistralChatCompletionResponse) -> ModelResponse:\n        \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\n        assert response.choices, 'Unexpected empty response choice.'\n\n        if response.created:\n            timestamp = number_to_datetime(response.created)\n        else:\n            timestamp = _now_utc()\n\n        choice = response.choices[0]\n        content = choice.message.content\n        tool_calls = choice.message.tool_calls\n\n        parts: list[ModelResponsePart] = []\n        text, thinking = _map_content(content)\n        for thought in thinking:\n            parts.append(ThinkingPart(content=thought))\n        if text:\n            parts.append(TextPart(content=text))\n\n        if isinstance(tool_calls, list):\n            for tool_call in tool_calls:\n                tool = self._map_mistral_to_pydantic_tool_call(tool_call=tool_call)\n                parts.append(tool)\n\n        raw_finish_reason = choice.finish_reason\n        provider_details = {'finish_reason': raw_finish_reason}\n        finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n\n        return ModelResponse(\n            parts=parts,\n            usage=_map_usage(response),\n            model_name=response.model,\n            timestamp=timestamp,\n            provider_response_id=response.id,\n            provider_name=self._provider.name,\n            finish_reason=finish_reason,\n            provider_details=provider_details,\n        )\n\n    async def _process_streamed_response(\n        self,\n        response: MistralEventStreamAsync[MistralCompletionEvent],\n        model_request_parameters: ModelRequestParameters,\n    ) -> StreamedResponse:\n        \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\n        peekable_response = _utils.PeekableAsyncStream(response)\n        first_chunk = await peekable_response.peek()\n        if isinstance(first_chunk, _utils.Unset):\n            raise UnexpectedModelBehavior(  # pragma: no cover\n                'Streamed response ended without content or tool calls'\n            )\n\n        if first_chunk.data.created:\n            timestamp = number_to_datetime(first_chunk.data.created)\n        else:\n            timestamp = _now_utc()\n\n        return MistralStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _response=peekable_response,\n            _model_name=first_chunk.data.model,\n            _timestamp=timestamp,\n            _provider_name=self._provider.name,\n        )\n\n    @staticmethod\n    def _map_mistral_to_pydantic_tool_call(tool_call: MistralToolCall) -> ToolCallPart:\n        \"\"\"Maps a MistralToolCall to a ToolCall.\"\"\"\n        tool_call_id = tool_call.id or _generate_tool_call_id()\n        func_call = tool_call.function\n\n        return ToolCallPart(func_call.name, func_call.arguments, tool_call_id)\n\n    @staticmethod\n    def _map_tool_call(t: ToolCallPart) -> MistralToolCall:\n        \"\"\"Maps a pydantic-ai ToolCall to a MistralToolCall.\"\"\"\n        return MistralToolCall(\n            id=_utils.guard_tool_call_id(t=t),\n            type='function',\n            function=MistralFunctionCall(name=t.tool_name, arguments=t.args or {}),\n        )\n\n    def _generate_user_output_format(self, schemas: list[dict[str, Any]]) -> MistralUserMessage:\n        \"\"\"Get a message with an example of the expected output format.\"\"\"\n        examples: list[dict[str, Any]] = []\n        for schema in schemas:\n            typed_dict_definition: dict[str, Any] = {}\n            for key, value in schema.get('properties', {}).items():\n                typed_dict_definition[key] = self._get_python_type(value)\n            examples.append(typed_dict_definition)\n\n        example_schema = examples[0] if len(examples) == 1 else examples\n        return MistralUserMessage(content=self.json_mode_schema_prompt.format(schema=example_schema))\n\n    @classmethod\n    def _get_python_type(cls, value: dict[str, Any]) -> str:\n        \"\"\"Return a string representation of the Python type for a single JSON schema property.\n\n        This function handles recursion for nested arrays/objects and `anyOf`.\n        \"\"\"\n        # 1) Handle anyOf first, because it's a different schema structure\n        if any_of := value.get('anyOf'):\n            # Simplistic approach: pick the first option in anyOf\n            # (In reality, you'd possibly want to merge or union types)\n            return f'Optional[{cls._get_python_type(any_of[0])}]'\n\n        # 2) If we have a top-level \"type\" field\n        value_type = value.get('type')\n        if not value_type:\n            # No explicit type; fallback\n            return 'Any'\n\n        # 3) Direct simple type mapping (string, integer, float, bool, None)\n        if value_type in SIMPLE_JSON_TYPE_MAPPING and value_type != 'array' and value_type != 'object':\n            return SIMPLE_JSON_TYPE_MAPPING[value_type]\n\n        # 4) Array: Recursively get the item type\n        if value_type == 'array':\n            items = value.get('items', {})\n            return f'list[{cls._get_python_type(items)}]'\n\n        # 5) Object: Check for additionalProperties\n        if value_type == 'object':\n            additional_properties = value.get('additionalProperties', {})\n            if isinstance(additional_properties, bool):\n                return 'bool'  # pragma: lax no cover\n            additional_properties_type = additional_properties.get('type')\n            if (\n                additional_properties_type in SIMPLE_JSON_TYPE_MAPPING\n                and additional_properties_type != 'array'\n                and additional_properties_type != 'object'\n            ):\n                # dict[str, bool/int/float/etc...]\n                return f'dict[str, {SIMPLE_JSON_TYPE_MAPPING[additional_properties_type]}]'\n            elif additional_properties_type == 'array':\n                array_items = additional_properties.get('items', {})\n                return f'dict[str, list[{cls._get_python_type(array_items)}]]'\n            elif additional_properties_type == 'object':\n                # nested dictionary of unknown shape\n                return 'dict[str, dict[str, Any]]'\n            else:\n                # If no additionalProperties type or something else, default to a generic dict\n                return 'dict[str, Any]'\n\n        # 6) Fallback\n        return 'Any'\n\n    @staticmethod\n    def _get_timeout_ms(timeout: Timeout | float | None) -> int | None:\n        \"\"\"Convert a timeout to milliseconds.\"\"\"\n        if timeout is None:\n            return None\n        if isinstance(timeout, float):  # pragma: no cover\n            return int(1000 * timeout)\n        raise NotImplementedError('Timeout object is not yet supported for MistralModel.')\n\n    def _map_user_message(self, message: ModelRequest) -> Iterable[MistralMessages]:\n        for part in message.parts:\n            if isinstance(part, SystemPromptPart):\n                yield MistralSystemMessage(content=part.content)\n            elif isinstance(part, UserPromptPart):\n                yield self._map_user_prompt(part)\n            elif isinstance(part, ToolReturnPart):\n                yield MistralToolMessage(\n                    tool_call_id=part.tool_call_id,\n                    content=part.model_response_str(),\n                )\n            elif isinstance(part, RetryPromptPart):\n                if part.tool_name is None:\n                    yield MistralUserMessage(content=part.model_response())  # pragma: no cover\n                else:\n                    yield MistralToolMessage(\n                        tool_call_id=part.tool_call_id,\n                        content=part.model_response(),\n                    )\n            else:\n                assert_never(part)\n\n    def _map_messages(\n        self, messages: list[ModelMessage], model_request_parameters: ModelRequestParameters\n    ) -> list[MistralMessages]:\n        \"\"\"Just maps a `pydantic_ai.Message` to a `MistralMessage`.\"\"\"\n        mistral_messages: list[MistralMessages] = []\n        for message in messages:\n            if isinstance(message, ModelRequest):\n                mistral_messages.extend(self._map_user_message(message))\n            elif isinstance(message, ModelResponse):\n                content_chunks: list[MistralContentChunk] = []\n                thinking_chunks: list[MistralTextChunk | MistralReferenceChunk] = []\n                tool_calls: list[MistralToolCall] = []\n\n                for part in message.parts:\n                    if isinstance(part, TextPart):\n                        content_chunks.append(MistralTextChunk(text=part.content))\n                    elif isinstance(part, ThinkingPart):\n                        thinking_chunks.append(MistralTextChunk(text=part.content))\n                    elif isinstance(part, ToolCallPart):\n                        tool_calls.append(self._map_tool_call(part))\n                    elif isinstance(part, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover\n                        # This is currently never returned from mistral\n                        pass\n                    elif isinstance(part, FilePart):  # pragma: no cover\n                        # Files generated by models are not sent back to models that don't themselves generate files.\n                        pass\n                    else:\n                        assert_never(part)\n                if thinking_chunks:\n                    content_chunks.insert(0, MistralThinkChunk(thinking=thinking_chunks))\n                mistral_messages.append(MistralAssistantMessage(content=content_chunks, tool_calls=tool_calls))\n            else:\n                assert_never(message)\n        if instructions := self._get_instructions(messages, model_request_parameters):\n            mistral_messages.insert(0, MistralSystemMessage(content=instructions))\n\n        # Post-process messages to insert fake assistant message after tool message if followed by user message\n        # to work around `Unexpected role 'user' after role 'tool'` error.\n        processed_messages: list[MistralMessages] = []\n        for i, current_message in enumerate(mistral_messages):\n            processed_messages.append(current_message)\n\n            if isinstance(current_message, MistralToolMessage) and i + 1 < len(mistral_messages):\n                next_message = mistral_messages[i + 1]\n                if isinstance(next_message, MistralUserMessage):\n                    # Insert a dummy assistant message\n                    processed_messages.append(MistralAssistantMessage(content=[MistralTextChunk(text='OK')]))\n\n        return processed_messages\n\n    def _map_user_prompt(self, part: UserPromptPart) -> MistralUserMessage:\n        content: str | list[MistralContentChunk]\n        if isinstance(part.content, str):\n            content = part.content\n        else:\n            content = []\n            for item in part.content:\n                if isinstance(item, str):\n                    content.append(MistralTextChunk(text=item))\n                elif isinstance(item, ImageUrl):\n                    content.append(MistralImageURLChunk(image_url=MistralImageURL(url=item.url)))\n                elif isinstance(item, BinaryContent):\n                    if item.is_image:\n                        image_url = MistralImageURL(url=item.data_uri)\n                        content.append(MistralImageURLChunk(image_url=image_url, type='image_url'))\n                    elif item.media_type == 'application/pdf':\n                        content.append(MistralDocumentURLChunk(document_url=item.data_uri, type='document_url'))\n                    else:\n                        raise RuntimeError('BinaryContent other than image or PDF is not supported in Mistral.')\n                elif isinstance(item, DocumentUrl):\n                    if item.media_type == 'application/pdf':\n                        content.append(MistralDocumentURLChunk(document_url=item.url, type='document_url'))\n                    else:\n                        raise RuntimeError('DocumentUrl other than PDF is not supported in Mistral.')\n                elif isinstance(item, VideoUrl):\n                    raise RuntimeError('VideoUrl is not supported in Mistral.')\n                else:  # pragma: no cover\n                    raise RuntimeError(f'Unsupported content type: {type(item)}')\n        return MistralUserMessage(content=content)\n\n````\n\n#### __init__\n\n````python\n__init__(\n    model_name: MistralModelName,\n    *,\n    provider: (\n        Literal[\"mistral\"] | Provider[Mistral]\n    ) = \"mistral\",\n    profile: ModelProfileSpec | None = None,\n    json_mode_schema_prompt: str = \"Answer in JSON Object, respect the format:\\n```\\n{schema}\\n```\\n\",\n    settings: ModelSettings | None = None\n)\n\n````\n\nInitialize a Mistral model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model_name` | `MistralModelName` | The name of the model to use. | *required* | | `provider` | `Literal['mistral'] | Provider[Mistral]` | The provider to use for authentication and API access. Can be either the string 'mistral' or an instance of Provider[Mistral]. If not provided, a new provider will be created using the other parameters. | `'mistral'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` | | `json_mode_schema_prompt` | `str` | The prompt to show when the model expects a JSON object as input. | ```` 'Answer in JSON Object, respect the format:\\n```\\n{schema}\\n```\\n' ```` | | `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\n\n````python\ndef __init__(\n    self,\n    model_name: MistralModelName,\n    *,\n    provider: Literal['mistral'] | Provider[Mistral] = 'mistral',\n    profile: ModelProfileSpec | None = None,\n    json_mode_schema_prompt: str = \"\"\"Answer in JSON Object, respect the format:\\n```\\n{schema}\\n```\\n\"\"\",\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize a Mistral model.\n\n    Args:\n        model_name: The name of the model to use.\n        provider: The provider to use for authentication and API access. Can be either the string\n            'mistral' or an instance of `Provider[Mistral]`. If not provided, a new provider will be\n            created using the other parameters.\n        profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n        json_mode_schema_prompt: The prompt to show when the model expects a JSON object as input.\n        settings: Model-specific settings that will be used as defaults for this model.\n    \"\"\"\n    self._model_name = model_name\n    self.json_mode_schema_prompt = json_mode_schema_prompt\n\n    if isinstance(provider, str):\n        provider = infer_provider(provider)\n    self._provider = provider\n    self.client = provider.client\n\n    super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n````\n\n#### model_name\n\n```python\nmodel_name: MistralModelName\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe model provider.\n\n#### request\n\n```python\nrequest(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse\n\n```\n\nMake a non-streaming request to the model from Pydantic AI call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\n\n```python\nasync def request(\n    self,\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse:\n    \"\"\"Make a non-streaming request to the model from Pydantic AI call.\"\"\"\n    check_allow_model_requests()\n    model_settings, model_request_parameters = self.prepare_request(\n        model_settings,\n        model_request_parameters,\n    )\n    response = await self._completions_create(\n        messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters\n    )\n    model_response = self._process_response(response)\n    return model_response\n\n```\n\n#### request_stream\n\n```python\nrequest_stream(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]\n\n```\n\nMake a streaming request to the model from Pydantic AI call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\n\n```python\n@asynccontextmanager\nasync def request_stream(\n    self,\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]:\n    \"\"\"Make a streaming request to the model from Pydantic AI call.\"\"\"\n    check_allow_model_requests()\n    model_settings, model_request_parameters = self.prepare_request(\n        model_settings,\n        model_request_parameters,\n    )\n    response = await self._stream_completions_create(\n        messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters\n    )\n    async with response:\n        yield await self._process_streamed_response(response, model_request_parameters)\n\n```\n\n### MistralStreamedResponse\n\nBases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for Mistral models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\n\n```python\n@dataclass\nclass MistralStreamedResponse(StreamedResponse):\n    \"\"\"Implementation of `StreamedResponse` for Mistral models.\"\"\"\n\n    _model_name: MistralModelName\n    _response: AsyncIterable[MistralCompletionEvent]\n    _timestamp: datetime\n    _provider_name: str\n\n    _delta_content: str = field(default='', init=False)\n\n    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\n        chunk: MistralCompletionEvent\n        async for chunk in self._response:\n            self._usage += _map_usage(chunk.data)\n\n            if chunk.data.id:  # pragma: no branch\n                self.provider_response_id = chunk.data.id\n\n            try:\n                choice = chunk.data.choices[0]\n            except IndexError:\n                continue\n\n            if raw_finish_reason := choice.finish_reason:\n                self.provider_details = {'finish_reason': raw_finish_reason}\n                self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)\n\n            # Handle the text part of the response\n            content = choice.delta.content\n            text, thinking = _map_content(content)\n            for thought in thinking:\n                self._parts_manager.handle_thinking_delta(vendor_part_id='thinking', content=thought)\n            if text:\n                # Attempt to produce an output tool call from the received text\n                output_tools = {c.name: c for c in self.model_request_parameters.output_tools}\n                if output_tools:\n                    self._delta_content += text\n                    # TODO: Port to native \"manual JSON\" mode\n                    maybe_tool_call_part = self._try_get_output_tool_from_text(self._delta_content, output_tools)\n                    if maybe_tool_call_part:\n                        yield self._parts_manager.handle_tool_call_part(\n                            vendor_part_id='output',\n                            tool_name=maybe_tool_call_part.tool_name,\n                            args=maybe_tool_call_part.args_as_dict(),\n                            tool_call_id=maybe_tool_call_part.tool_call_id,\n                        )\n                else:\n                    maybe_event = self._parts_manager.handle_text_delta(vendor_part_id='content', content=text)\n                    if maybe_event is not None:  # pragma: no branch\n                        yield maybe_event\n\n            # Handle the explicit tool calls\n            for index, dtc in enumerate(choice.delta.tool_calls or []):\n                # It seems that mistral just sends full tool calls, so we just use them directly, rather than building\n                yield self._parts_manager.handle_tool_call_part(\n                    vendor_part_id=index, tool_name=dtc.function.name, args=dtc.function.arguments, tool_call_id=dtc.id\n                )\n\n    @property\n    def model_name(self) -> MistralModelName:\n        \"\"\"Get the model name of the response.\"\"\"\n        return self._model_name\n\n    @property\n    def provider_name(self) -> str:\n        \"\"\"Get the provider name.\"\"\"\n        return self._provider_name\n\n    @property\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._timestamp\n\n    @staticmethod\n    def _try_get_output_tool_from_text(text: str, output_tools: dict[str, ToolDefinition]) -> ToolCallPart | None:\n        output_json: dict[str, Any] | None = pydantic_core.from_json(text, allow_partial='trailing-strings')\n        if output_json:\n            for output_tool in output_tools.values():\n                # NOTE: Additional verification to prevent JSON validation to crash\n                # Ensures required parameters in the JSON schema are respected, especially for stream-based return types.\n                # Example with BaseModel and required fields.\n                if not MistralStreamedResponse._validate_required_json_schema(\n                    output_json, output_tool.parameters_json_schema\n                ):\n                    continue\n\n                # The following part_id will be thrown away\n                return ToolCallPart(tool_name=output_tool.name, args=output_json)\n\n    @staticmethod\n    def _validate_required_json_schema(json_dict: dict[str, Any], json_schema: dict[str, Any]) -> bool:\n        \"\"\"Validate that all required parameters in the JSON schema are present in the JSON dictionary.\"\"\"\n        required_params = json_schema.get('required', [])\n        properties = json_schema.get('properties', {})\n\n        for param in required_params:\n            if param not in json_dict:\n                return False\n\n            param_schema = properties.get(param, {})\n            param_type = param_schema.get('type')\n            param_items_type = param_schema.get('items', {}).get('type')\n\n            if param_type == 'array' and param_items_type:\n                if not isinstance(json_dict[param], list):\n                    return False\n                for item in json_dict[param]:\n                    if not isinstance(item, VALID_JSON_TYPE_MAPPING[param_items_type]):\n                        return False\n            elif param_type and not isinstance(json_dict[param], VALID_JSON_TYPE_MAPPING[param_type]):\n                return False\n\n            if isinstance(json_dict[param], dict) and 'properties' in param_schema:\n                nested_schema = param_schema\n                if not MistralStreamedResponse._validate_required_json_schema(json_dict[param], nested_schema):\n                    return False\n\n        return True\n\n```\n\n#### model_name\n\n```python\nmodel_name: MistralModelName\n\n```\n\nGet the model name of the response.\n\n#### provider_name\n\n```python\nprovider_name: str\n\n```\n\nGet the provider name.\n\n#### timestamp\n\n```python\ntimestamp: datetime\n\n```\n\nGet the timestamp of the response.",
  "content_length": 34746
}