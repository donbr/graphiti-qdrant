{
  "title": "Unit testing",
  "source_url": null,
  "content": "Writing unit tests for Pydantic AI code is just like unit tests for any other Python code.\n\nBecause for the most part they're nothing new, we have pretty well established tools and patterns for writing and running these kinds of tests.\n\nUnless you're really sure you know better, you'll probably want to follow roughly this strategy:\n\n- Use [`pytest`](https://docs.pytest.org/en/stable/) as your test harness\n- If you find yourself typing out long assertions, use [inline-snapshot](https://15r10nk.github.io/inline-snapshot/latest/)\n- Similarly, [dirty-equals](https://dirty-equals.helpmanual.io/latest/) can be useful for comparing large data structures\n- Use TestModel or FunctionModel in place of your actual model to avoid the usage, latency and variability of real LLM calls\n- Use Agent.override to replace an agent's model, dependencies, or toolsets inside your application logic\n- Set ALLOW_MODEL_REQUESTS=False globally to block any requests from being made to non-test models accidentally\n\n### Unit testing with `TestModel`\n\nThe simplest and fastest way to exercise most of your application code is using TestModel, this will (by default) call all tools in the agent, then return either plain text or a structured response depending on the return type of the agent.\n\n`TestModel` is not magic\n\nThe \"clever\" (but not too clever) part of `TestModel` is that it will attempt to generate valid structured data for [function tools](../tools/) and [output types](../output/#structured-output) based on the schema of the registered tools.\n\nThere's no ML or AI in `TestModel`, it's just plain old procedural Python code that tries to generate data that satisfies the JSON schema of a tool.\n\nThe resulting data won't look pretty or relevant, but it should pass Pydantic's validation in most cases. If you want something more sophisticated, use FunctionModel and write your own data generation logic.\n\nLet's write unit tests for the following application code:\n\n[Learn about Gateway](../gateway) weather_app.py\n\n```python\nimport asyncio\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nfrom fake_database import DatabaseConn  # (1)!\nfrom weather_service import WeatherService  # (2)!\n\nweather_agent = Agent(\n    'gateway/openai:gpt-5',\n    deps_type=WeatherService,\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\ndef weather_forecast(\n    ctx: RunContext[WeatherService], location: str, forecast_date: date\n) -> str:\n    if forecast_date < date.today():  # (3)!\n        return ctx.deps.get_historic_weather(location, forecast_date)\n    else:\n        return ctx.deps.get_forecast(location, forecast_date)\n\n\nasync def run_weather_forecast(  # (4)!\n    user_prompts: list[tuple[str, int]], conn: DatabaseConn\n):\n    \"\"\"Run weather forecast for a list of user prompts and save.\"\"\"\n    async with WeatherService() as weather_service:\n\n        async def run_forecast(prompt: str, user_id: int):\n            result = await weather_agent.run(prompt, deps=weather_service)\n            await conn.store_forecast(user_id, result.output)\n\n        # run all prompts in parallel\n        await asyncio.gather(\n            *(run_forecast(prompt, user_id) for (prompt, user_id) in user_prompts)\n        )\n\n```\n\n1. `DatabaseConn` is a class that holds a database connection\n1. `WeatherService` has methods to get weather forecasts and historic data about the weather\n1. We need to call a different endpoint depending on whether the date is in the past or the future, you'll see why this nuance is important below\n1. This function is the code we want to test, together with the agent it uses\n\nweather_app.py\n\n```python\nimport asyncio\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nfrom fake_database import DatabaseConn  # (1)!\nfrom weather_service import WeatherService  # (2)!\n\nweather_agent = Agent(\n    'openai:gpt-5',\n    deps_type=WeatherService,\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\ndef weather_forecast(\n    ctx: RunContext[WeatherService], location: str, forecast_date: date\n) -> str:\n    if forecast_date < date.today():  # (3)!\n        return ctx.deps.get_historic_weather(location, forecast_date)\n    else:\n        return ctx.deps.get_forecast(location, forecast_date)\n\n\nasync def run_weather_forecast(  # (4)!\n    user_prompts: list[tuple[str, int]], conn: DatabaseConn\n):\n    \"\"\"Run weather forecast for a list of user prompts and save.\"\"\"\n    async with WeatherService() as weather_service:\n\n        async def run_forecast(prompt: str, user_id: int):\n            result = await weather_agent.run(prompt, deps=weather_service)\n            await conn.store_forecast(user_id, result.output)\n\n        # run all prompts in parallel\n        await asyncio.gather(\n            *(run_forecast(prompt, user_id) for (prompt, user_id) in user_prompts)\n        )\n\n```\n\n1. `DatabaseConn` is a class that holds a database connection\n1. `WeatherService` has methods to get weather forecasts and historic data about the weather\n1. We need to call a different endpoint depending on whether the date is in the past or the future, you'll see why this nuance is important below\n1. This function is the code we want to test, together with the agent it uses\n\nHere we have a function that takes a list of `(user_prompt, user_id)` tuples, gets a weather forecast for each prompt, and stores the result in the database.\n\n**We want to test this code without having to mock certain objects or modify our code so we can pass test objects in.**\n\nHere's how we would write tests using TestModel:\n\ntest_weather_app.py\n\n```python\nfrom datetime import timezone\nimport pytest\n\nfrom dirty_equals import IsNow, IsStr\n\nfrom pydantic_ai import models, capture_run_messages, RequestUsage\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai import (\n    ModelResponse,\n    SystemPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    UserPromptPart,\n    ModelRequest,\n)\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio  # (1)!\nmodels.ALLOW_MODEL_REQUESTS = False  # (2)!\n\n\nasync def test_forecast():\n    conn = DatabaseConn()\n    user_id = 1\n    with capture_run_messages() as messages:\n        with weather_agent.override(model=TestModel()):  # (3)!\n            prompt = 'What will the weather be like in London on 2024-11-28?'\n            await run_weather_forecast([(prompt, user_id)], conn)  # (4)!\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == '{\"weather_forecast\":\"Sunny with a chance of rain\"}'  # (5)!\n\n    assert messages == [  # (6)!\n        ModelRequest(\n            parts=[\n                SystemPromptPart(\n                    content='Providing a weather forecast at the locations the user provides.',\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n                UserPromptPart(\n                    content='What will the weather be like in London on 2024-11-28?',\n                    timestamp=IsNow(tz=timezone.utc),  # (7)!\n                ),\n            ],\n            run_id=IsStr(),\n        ),\n        ModelResponse(\n            parts=[\n                ToolCallPart(\n                    tool_name='weather_forecast',\n                    args={\n                        'location': 'a',\n                        'forecast_date': '2024-01-01',  # (8)!\n                    },\n                    tool_call_id=IsStr(),\n                )\n            ],\n            usage=RequestUsage(\n                input_tokens=71,\n                output_tokens=7,\n            ),\n            model_name='test',\n            timestamp=IsNow(tz=timezone.utc),\n            run_id=IsStr(),\n        ),\n        ModelRequest(\n            parts=[\n                ToolReturnPart(\n                    tool_name='weather_forecast',\n                    content='Sunny with a chance of rain',\n                    tool_call_id=IsStr(),\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n            ],\n            run_id=IsStr(),\n        ),\n        ModelResponse(\n            parts=[\n                TextPart(\n                    content='{\"weather_forecast\":\"Sunny with a chance of rain\"}',\n                )\n            ],\n            usage=RequestUsage(\n                input_tokens=77,\n                output_tokens=16,\n            ),\n            model_name='test',\n            timestamp=IsNow(tz=timezone.utc),\n            run_id=IsStr(),\n        ),\n    ]\n\n```\n\n1. We're using [anyio](https://anyio.readthedocs.io/en/stable/) to run async tests.\n1. This is a safety measure to make sure we don't accidentally make real requests to the LLM while testing, see ALLOW_MODEL_REQUESTS for more details.\n1. We're using Agent.override to replace the agent's model with TestModel, the nice thing about `override` is that we can replace the model inside agent without needing access to the agent `run*` methods call site.\n1. Now we call the function we want to test inside the `override` context manager.\n1. But default, `TestModel` will return a JSON string summarising the tools calls made, and what was returned. If you wanted to customise the response to something more closely aligned with the domain, you could add custom_output_text='Sunny' when defining `TestModel`.\n1. So far we don't actually know which tools were called and with which values, we can use capture_run_messages to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected.\n1. The IsNow helper allows us to use declarative asserts even with data which will contain timestamps that change over time.\n1. `TestModel` isn't doing anything clever to extract values from the prompt, so these values are hardcoded.\n\n### Unit testing with `FunctionModel`\n\nThe above tests are a great start, but careful readers will notice that the `WeatherService.get_forecast` is never called since `TestModel` calls `weather_forecast` with a date in the past.\n\nTo fully exercise `weather_forecast`, we need to use FunctionModel to customise how the tools is called.\n\nHere's an example of using `FunctionModel` to test the `weather_forecast` tool with custom inputs\n\ntest_weather_app2.py\n\n```python\nimport re\n\nimport pytest\n\nfrom pydantic_ai import models\nfrom pydantic_ai import (\n    ModelMessage,\n    ModelResponse,\n    TextPart,\n    ToolCallPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio\nmodels.ALLOW_MODEL_REQUESTS = False\n\n\ndef call_weather_forecast(  # (1)!\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    if len(messages) == 1:\n        # first call, call the weather forecast tool\n        user_prompt = messages[0].parts[-1]\n        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n        assert m is not None\n        args = {'location': 'London', 'forecast_date': m.group()}  # (2)!\n        return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\n    else:\n        # second call, return the forecast\n        msg = messages[-1].parts[0]\n        assert msg.part_kind == 'tool-return'\n        return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\n\n\nasync def test_forecast_future():\n    conn = DatabaseConn()\n    user_id = 1\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):  # (3)!\n        prompt = 'What will the weather be like in London on 2032-01-01?'\n        await run_weather_forecast([(prompt, user_id)], conn)\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == 'The forecast is: Rainy with a chance of sun'\n\n```\n\n1. We define a function `call_weather_forecast` that will be called by `FunctionModel` in place of the LLM, this function has access to the list of ModelMessages that make up the run, and AgentInfo which contains information about the agent and the function tools and return tools.\n1. Our function is slightly intelligent in that it tries to extract a date from the prompt, but just hard codes the location.\n1. We use FunctionModel to replace the agent's model with our custom function.\n\n### Overriding model via pytest fixtures\n\nIf you're writing lots of tests that all require model to be overridden, you can use [pytest fixtures](https://docs.pytest.org/en/6.2.x/fixture.html) to override the model with TestModel or FunctionModel in a reusable way.\n\nHere's an example of a fixture that overrides the model with `TestModel`:\n\ntest_agent.py\n\n```python\nimport pytest\n\nfrom pydantic_ai.models.test import TestModel\n\nfrom weather_app import weather_agent\n\n\n@pytest.fixture\ndef override_weather_agent():\n    with weather_agent.override(model=TestModel()):\n        yield\n\n\nasync def test_forecast(override_weather_agent: None):\n    ...\n    # test code here\n\n```\n\n## Version Policy\n\nWe will not intentionally make breaking changes in minor releases of V1. V2 will be released in April 2026 at the earliest, 6 months after the release of V1 in September 2025.\n\nOnce we release V2, we'll continue to provide security fixes for V1 for another 6 months minimum, so you have time to upgrade your applications.\n\nFunctionality marked as deprecated will not be removed until V2.\n\nOf course, some apparently safe changes and bug fixes will inevitably break some users' code â€” obligatory link to [xkcd](https://xkcd.com/1172/).\n\nThe following changes will **NOT** be considered breaking changes, and may occur in minor releases:\n\n- Bug fixes that may result in existing code breaking, provided that such code was relying on undocumented features/constructs/assumptions.\n- Adding new message parts, stream events, or optional fields on existing message (part) and event types. Always code defensively when consuming message parts or event streams, and use the ModelMessagesTypeAdapter to (de)serialize message histories.\n- Changing OpenTelemetry span attributes. Because different [observability platforms](../logfire/#using-opentelemetry) support different versions of the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/), Pydantic AI lets you configure the [instrumentation version](../logfire/#configuring-data-format), but the default version may change in a minor release. Span attributes for [Pydantic Evals](../evals/) may also change as we iterate on Evals support in [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/).\n- Changing how `__repr__` behaves, even of public classes.\n\nIn all cases we will aim to minimize churn and do so only when justified by the increase of quality of Pydantic AI for users.\n\n## Beta Features\n\nAt Pydantic, we like to move quickly and innovate! To that end, minor releases may introduce beta features (indicated by a `beta` module) that are active works in progress. While in its beta phase, a feature's API and behaviors may not be stable, and it's very possible that changes made to the feature will not be backward-compatible. We aim to move beta features out of beta within a few months after initial release, once users have had a chance to provide feedback and test the feature in production.\n\n## Support for Python versions\n\nPydantic will drop support for a Python version when the following conditions are met:\n\n- The Python version has reached its [expected end of life](https://devguide.python.org/versions/).\n- less than 5% of downloads of the most recent minor release are using that version.",
  "content_length": 15629
}