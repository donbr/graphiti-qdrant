{
  "title": "Direct Model Requests",
  "source_url": null,
  "content": "The `direct` module provides low-level methods for making imperative requests to LLMs where the only abstraction is input and output schema translation, enabling you to use all models with the same API.\n\nThese methods are thin wrappers around the Model implementations, offering a simpler interface when you don't need the full functionality of an Agent.\n\nThe following functions are available:\n\n- model_request: Make a non-streamed async request to a model\n- model_request_sync: Make a non-streamed synchronous request to a model\n- model_request_stream: Make a streamed async request to a model\n- model_request_stream_sync: Make a streamed sync request to a model\n\n## Basic Example\n\nHere's a simple example demonstrating how to use the direct API to make a basic request:\n\ndirect_basic.py\n\n```python\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_sync\n\n### Make a synchronous request to the model\nmodel_response = model_request_sync(\n    'anthropic:claude-haiku-4-5',\n    [ModelRequest.user_text_prompt('What is the capital of France?')]\n)\n\nprint(model_response.parts[0].content)\n#> The capital of France is Paris.\nprint(model_response.usage)\n#> RequestUsage(input_tokens=56, output_tokens=7)\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\n## Advanced Example with Tool Calling\n\nYou can also use the direct API to work with function/tool calling.\n\nEven here we can use Pydantic to generate the JSON schema for the tool:\n\n```python\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import ModelRequest, ToolDefinition\nfrom pydantic_ai.direct import model_request\nfrom pydantic_ai.models import ModelRequestParameters\n\n\nclass Divide(BaseModel):\n    \"\"\"Divide two numbers.\"\"\"\n\n    numerator: float\n    denominator: float\n    on_inf: Literal['error', 'infinity'] = 'infinity'\n\n\nasync def main():\n    # Make a request to the model with tool access\n    model_response = await model_request(\n        'openai:gpt-5-nano',\n        [ModelRequest.user_text_prompt('What is 123 / 456?')],\n        model_request_parameters=ModelRequestParameters(\n            function_tools=[\n                ToolDefinition(\n                    name=Divide.__name__.lower(),\n                    description=Divide.__doc__,\n                    parameters_json_schema=Divide.model_json_schema(),\n                )\n            ],\n            allow_text_output=True,  # Allow model to either use tools or respond directly\n        ),\n    )\n    print(model_response)\n    \"\"\"\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='divide',\n                args={'numerator': '123', 'denominator': '456'},\n                tool_call_id='pyd_ai_2e0e396768a14fe482df90a29a78dc7b',\n            )\n        ],\n        usage=RequestUsage(input_tokens=55, output_tokens=7),\n        model_name='gpt-5-nano',\n        timestamp=datetime.datetime(...),\n    )\n    \"\"\"\n\n```\n\n*(This example is complete, it can be run \"as is\" â€” you'll need to add `asyncio.run(main())` to run `main`)*\n\n## When to Use the direct API vs Agent\n\nThe direct API is ideal when:\n\n1. You need more direct control over model interactions\n1. You want to implement custom behavior around model requests\n1. You're building your own abstractions on top of model interactions\n\nFor most application use cases, the higher-level Agent API provides a more convenient interface with additional features such as built-in tool execution, retrying, structured output parsing, and more.\n\n## OpenTelemetry or Logfire Instrumentation\n\nAs with agents, you can enable OpenTelemetry/Logfire instrumentation with just a few extra lines\n\ndirect_instrumented.py\n\n```python\nimport logfire\n\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_sync\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\n\n### Make a synchronous request to the model\nmodel_response = model_request_sync(\n    'anthropic:claude-haiku-4-5',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n)\n\nprint(model_response.parts[0].content)\n#> The capital of France is Paris.\n\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nYou can also enable OpenTelemetry on a per call basis:\n\ndirect_instrumented.py\n\n```python\nimport logfire\n\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_sync\n\nlogfire.configure()\n\n### Make a synchronous request to the model\nmodel_response = model_request_sync(\n    'anthropic:claude-haiku-4-5',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n    instrument=True\n)\n\nprint(model_response.parts[0].content)\n#> The capital of France is Paris.\n\n```\n\nSee [Debugging and Monitoring](../logfire/) for more details, including how to instrument with plain OpenTelemetry without Logfire.",
  "content_length": 4795
}