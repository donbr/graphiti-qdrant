{
  "title": "HTTP Request Retries",
  "source_url": null,
  "content": "Pydantic AI provides retry functionality for HTTP requests made by model providers through custom HTTP transports. This is particularly useful for handling transient failures like rate limits, network timeouts, or temporary server errors.\n\n## Overview\n\nThe retry functionality is built on top of the [tenacity](https://github.com/jd/tenacity) library and integrates seamlessly with httpx clients. You can configure retry behavior for any provider that accepts a custom HTTP client.\n\n## Installation\n\nTo use the retry transports, you need to install `tenacity`, which you can do via the `retries` dependency group:\n\n```bash\npip install 'pydantic-ai-slim[retries]'\n\n```\n\n```bash\nuv add 'pydantic-ai-slim[retries]'\n\n```\n\n## Usage Example\n\nHere's an example of adding retry functionality with smart retry handling:\n\nsmart_retry_example.py\n\n```python\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n\ndef create_retrying_client():\n    \"\"\"Create a client with smart retry handling for multiple error types.\"\"\"\n\n    def should_retry_status(response):\n        \"\"\"Raise exceptions for retryable HTTP status codes.\"\"\"\n        if response.status_code in (429, 502, 503, 504):\n            response.raise_for_status()  # This will raise HTTPStatusError\n\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            # Retry on HTTP errors and connection issues\n            retry=retry_if_exception_type((HTTPStatusError, ConnectionError)),\n            # Smart waiting: respects Retry-After headers, falls back to exponential backoff\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=1, max=60),\n                max_wait=300\n            ),\n            # Stop after 5 attempts\n            stop=stop_after_attempt(5),\n            # Re-raise the last exception if all retries fail\n            reraise=True\n        ),\n        validate_response=should_retry_status\n    )\n    return AsyncClient(transport=transport)\n\n### Use the retrying client with a model\nclient = create_retrying_client()\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n\n```\n\n## Wait Strategies\n\n### wait_retry_after\n\nThe `wait_retry_after` function is a smart wait strategy that automatically respects HTTP `Retry-After` headers:\n\nwait_strategy_example.py\n\n```python\nfrom tenacity import wait_exponential\n\nfrom pydantic_ai.retries import wait_retry_after\n\n### Basic usage - respects Retry-After headers, falls back to exponential backoff\nwait_strategy_1 = wait_retry_after()\n\n### Custom configuration\nwait_strategy_2 = wait_retry_after(\n    fallback_strategy=wait_exponential(multiplier=2, max=120),\n    max_wait=600  # Never wait more than 10 minutes\n)\n\n```\n\nThis wait strategy:\n\n- Automatically parses `Retry-After` headers from HTTP 429 responses\n- Supports both seconds format (`\"30\"`) and HTTP date format (`\"Wed, 21 Oct 2015 07:28:00 GMT\"`)\n- Falls back to your chosen strategy when no header is present\n- Respects the `max_wait` limit to prevent excessive delays\n\n## Transport Classes\n\n### AsyncTenacityTransport\n\nFor asynchronous HTTP clients (recommended for most use cases):\n\nasync_transport_example.py\n\n```python\nfrom httpx import AsyncClient\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig\n\n\ndef validator(response):\n    \"\"\"Treat responses with HTTP status 4xx/5xx as failures that need to be retried.\n    Without a response validator, only network errors and timeouts will result in a retry.\n    \"\"\"\n    response.raise_for_status()\n\n### Create the transport\ntransport = AsyncTenacityTransport(\n    config=RetryConfig(stop=stop_after_attempt(3), reraise=True),\n    validate_response=validator\n)\n\n### Create a client using the transport:\nclient = AsyncClient(transport=transport)\n\n```\n\n### TenacityTransport\n\nFor synchronous HTTP clients:\n\nsync_transport_example.py\n\n```python\nfrom httpx import Client\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_ai.retries import RetryConfig, TenacityTransport\n\n\ndef validator(response):\n    \"\"\"Treat responses with HTTP status 4xx/5xx as failures that need to be retried.\n    Without a response validator, only network errors and timeouts will result in a retry.\n    \"\"\"\n    response.raise_for_status()\n\n### Create the transport\ntransport = TenacityTransport(\n    config=RetryConfig(stop=stop_after_attempt(3), reraise=True),\n    validate_response=validator\n)\n\n### Create a client using the transport\nclient = Client(transport=transport)\n\n```\n\n## Common Retry Patterns\n\n### Rate Limit Handling with Retry-After Support\n\nrate_limit_handling.py\n\n```python\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n\ndef create_rate_limit_client():\n    \"\"\"Create a client that respects Retry-After headers from rate limiting responses.\"\"\"\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            retry=retry_if_exception_type(HTTPStatusError),\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=1, max=60),\n                max_wait=300  # Don't wait more than 5 minutes\n            ),\n            stop=stop_after_attempt(10),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()  # Raises HTTPStatusError for 4xx/5xx\n    )\n    return AsyncClient(transport=transport)\n\n### Example usage\nclient = create_rate_limit_client()\n### Client is now ready to use with any HTTP requests and will respect Retry-After headers\n\n```\n\nThe `wait_retry_after` function automatically detects `Retry-After` headers in 429 (rate limit) responses and waits for the specified time. If no header is present, it falls back to exponential backoff.\n\n### Network Error Handling\n\nnetwork_error_handling.py\n\n```python\nimport httpx\nfrom tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig\n\n\ndef create_network_resilient_client():\n    \"\"\"Create a client that handles network errors with retries.\"\"\"\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            retry=retry_if_exception_type((\n                httpx.TimeoutException,\n                httpx.ConnectError,\n                httpx.ReadError\n            )),\n            wait=wait_exponential(multiplier=1, max=10),\n            stop=stop_after_attempt(3),\n            reraise=True\n        )\n    )\n    return httpx.AsyncClient(transport=transport)\n\n### Example usage\nclient = create_network_resilient_client()\n### Client will now retry on timeout, connection, and read errors\n\n```\n\n### Custom Retry Logic\n\ncustom_retry_logic.py\n\n```python\nimport httpx\nfrom tenacity import retry_if_exception, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n\ndef create_custom_retry_client():\n    \"\"\"Create a client with custom retry logic.\"\"\"\n    def custom_retry_condition(exception):\n        \"\"\"Custom logic to determine if we should retry.\"\"\"\n        if isinstance(exception, httpx.HTTPStatusError):\n            # Retry on server errors but not client errors\n            return 500 <= exception.response.status_code < 600\n        return isinstance(exception, httpx.TimeoutException | httpx.ConnectError)\n\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            retry=retry_if_exception(custom_retry_condition),\n            # Use wait_retry_after for smart waiting on rate limits,\n            # with custom exponential backoff as fallback\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=2, max=30),\n                max_wait=120\n            ),\n            stop=stop_after_attempt(5),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()\n    )\n    return httpx.AsyncClient(transport=transport)\n\nclient = create_custom_retry_client()\n### Client will retry server errors (5xx) and network errors, but not client errors (4xx)\n\n```\n\n## Using with Different Providers\n\nThe retry transports work with any provider that accepts a custom HTTP client:\n\n### OpenAI\n\nopenai_with_retries.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n\n```\n\n### Anthropic\n\nanthropic_with_retries.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = AnthropicModel('claude-sonnet-4-5-20250929', provider=AnthropicProvider(http_client=client))\nagent = Agent(model)\n\n```\n\n### Any OpenAI-Compatible Provider\n\nopenai_compatible_with_retries.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIChatModel(\n    'your-model-name',  # Replace with actual model name\n    provider=OpenAIProvider(\n        base_url='https://api.example.com/v1',  # Replace with actual API URL\n        api_key='your-api-key',  # Replace with actual API key\n        http_client=client\n    )\n)\nagent = Agent(model)\n\n```\n\n## Best Practices\n\n1. **Start Conservative**: Begin with a small number of retries (3-5) and reasonable wait times.\n1. **Use Exponential Backoff**: This helps avoid overwhelming servers during outages.\n1. **Set Maximum Wait Times**: Prevent indefinite delays with reasonable maximum wait times.\n1. **Handle Rate Limits Properly**: Respect `Retry-After` headers when possible.\n1. **Log Retry Attempts**: Add logging to monitor retry behavior in production. (This will be picked up by Logfire automatically if you instrument httpx.)\n1. **Consider Circuit Breakers**: For high-traffic applications, consider implementing circuit breaker patterns.\n\n## Error Handling\n\nThe retry transports will re-raise the last exception if all retry attempts fail. Make sure to handle these appropriately in your application:\n\nerror_handling_example.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n\n```\n\n## Performance Considerations\n\n- Retries add latency to requests, especially with exponential backoff\n- Consider the total timeout for your application when configuring retry behavior\n- Monitor retry rates to detect systemic issues\n- Use async transports for better concurrency when handling multiple requests\n\nFor more advanced retry configurations, refer to the [tenacity documentation](https://tenacity.readthedocs.io/).\n\n## Provider-Specific Retry Behavior\n\n### AWS Bedrock\n\nThe AWS Bedrock provider uses boto3's built-in retry mechanisms instead of httpx. To configure retries for Bedrock, use boto3's `Config`:\n\n```python\nfrom botocore.config import Config\n\nconfig = Config(retries={'max_attempts': 5, 'mode': 'adaptive'})\n\n```\n\nSee [Bedrock: Configuring Retries](../models/bedrock/#configuring-retries) for complete examples.",
  "content_length": 12055
}