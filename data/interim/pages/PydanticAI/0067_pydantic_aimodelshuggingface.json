{
  "title": "`pydantic_ai.models.huggingface`",
  "source_url": null,
  "content": "## Setup\n\nFor details on how to set up authentication with this model, see [model configuration for Hugging Face](../../../models/huggingface/).\n\n### HuggingFaceModelSettings\n\nBases: `ModelSettings`\n\nSettings used for a Hugging Face model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/huggingface.py`\n\n```python\nclass HuggingFaceModelSettings(ModelSettings, total=False):\n    \"\"\"Settings used for a Hugging Face model request.\"\"\"\n\n```\n\n### HuggingFaceModel\n\nBases: `Model`\n\nA model that uses Hugging Face Inference Providers.\n\nInternally, this uses the [HF Python client](https://github.com/huggingface/huggingface_hub) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/huggingface.py`\n\n```python\n@dataclass(init=False)\nclass HuggingFaceModel(Model):\n    \"\"\"A model that uses Hugging Face Inference Providers.\n\n    Internally, this uses the [HF Python client](https://github.com/huggingface/huggingface_hub) to interact with the API.\n\n    Apart from `__init__`, all methods are private or match those of the base class.\n    \"\"\"\n\n    client: AsyncInferenceClient = field(repr=False)\n\n    _model_name: str = field(repr=False)\n    _provider: Provider[AsyncInferenceClient] = field(repr=False)\n\n    def __init__(\n        self,\n        model_name: str,\n        *,\n        provider: Literal['huggingface'] | Provider[AsyncInferenceClient] = 'huggingface',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize a Hugging Face model.\n\n        Args:\n            model_name: The name of the Model to use. You can browse available models [here](https://huggingface.co/models?pipeline_tag=text-generation&inference_provider=all&sort=trending).\n            provider: The provider to use for Hugging Face Inference Providers. Can be either the string 'huggingface' or an\n                instance of `Provider[AsyncInferenceClient]`. If not provided, the other parameters will be used.\n            profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n            settings: Model-specific settings that will be used as defaults for this model.\n        \"\"\"\n        self._model_name = model_name\n        if isinstance(provider, str):\n            provider = infer_provider(provider)\n        self._provider = provider\n        self.client = provider.client\n\n        super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n    @property\n    def model_name(self) -> HuggingFaceModelName:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The system / model provider.\"\"\"\n        return self._provider.name\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._completions_create(\n            messages, False, cast(HuggingFaceModelSettings, model_settings or {}), model_request_parameters\n        )\n        model_response = self._process_response(response)\n        return model_response\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._completions_create(\n            messages, True, cast(HuggingFaceModelSettings, model_settings or {}), model_request_parameters\n        )\n        yield await self._process_streamed_response(response, model_request_parameters)\n\n    @overload\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[True],\n        model_settings: HuggingFaceModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> AsyncIterable[ChatCompletionStreamOutput]: ...\n\n    @overload\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[False],\n        model_settings: HuggingFaceModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ChatCompletionOutput: ...\n\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        stream: bool,\n        model_settings: HuggingFaceModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ChatCompletionOutput | AsyncIterable[ChatCompletionStreamOutput]:\n        tools = self._get_tools(model_request_parameters)\n\n        if not tools:\n            tool_choice: Literal['none', 'required', 'auto'] | None = None\n        elif not model_request_parameters.allow_text_output:\n            tool_choice = 'required'\n        else:\n            tool_choice = 'auto'\n\n        if model_request_parameters.builtin_tools:\n            raise UserError('HuggingFace does not support built-in tools')\n\n        hf_messages = await self._map_messages(messages, model_request_parameters)\n\n        try:\n            return await self.client.chat.completions.create(  # type: ignore\n                model=self._model_name,\n                messages=hf_messages,  # type: ignore\n                tools=tools,\n                tool_choice=tool_choice or None,\n                stream=stream,\n                stop=model_settings.get('stop_sequences', None),\n                temperature=model_settings.get('temperature', None),\n                top_p=model_settings.get('top_p', None),\n                seed=model_settings.get('seed', None),\n                presence_penalty=model_settings.get('presence_penalty', None),\n                frequency_penalty=model_settings.get('frequency_penalty', None),\n                logit_bias=model_settings.get('logit_bias', None),  # type: ignore\n                logprobs=model_settings.get('logprobs', None),\n                top_logprobs=model_settings.get('top_logprobs', None),\n                extra_body=model_settings.get('extra_body'),  # type: ignore\n            )\n        except aiohttp.ClientResponseError as e:\n            raise ModelHTTPError(\n                status_code=e.status,\n                model_name=self.model_name,\n                body=e.message,\n            ) from e\n        except HfHubHTTPError as e:\n            raise ModelHTTPError(\n                status_code=e.response.status_code,\n                model_name=self.model_name,\n                body=e.response.content,\n            ) from e\n\n    def _process_response(self, response: ChatCompletionOutput) -> ModelResponse:\n        \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\n        if response.created:\n            timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)\n        else:\n            timestamp = _now_utc()\n\n        choice = response.choices[0]\n        content = choice.message.content\n        tool_calls = choice.message.tool_calls\n\n        items: list[ModelResponsePart] = []\n\n        if content:\n            items.extend(split_content_into_text_and_thinking(content, self.profile.thinking_tags))\n        if tool_calls is not None:\n            for c in tool_calls:\n                items.append(ToolCallPart(c.function.name, c.function.arguments, tool_call_id=c.id))\n\n        raw_finish_reason = choice.finish_reason\n        provider_details = {'finish_reason': raw_finish_reason}\n        finish_reason = _FINISH_REASON_MAP.get(cast(TextGenerationOutputFinishReason, raw_finish_reason), None)\n\n        return ModelResponse(\n            parts=items,\n            usage=_map_usage(response),\n            model_name=response.model,\n            timestamp=timestamp,\n            provider_response_id=response.id,\n            provider_name=self._provider.name,\n            finish_reason=finish_reason,\n            provider_details=provider_details,\n        )\n\n    async def _process_streamed_response(\n        self, response: AsyncIterable[ChatCompletionStreamOutput], model_request_parameters: ModelRequestParameters\n    ) -> StreamedResponse:\n        \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\n        peekable_response = _utils.PeekableAsyncStream(response)\n        first_chunk = await peekable_response.peek()\n        if isinstance(first_chunk, _utils.Unset):\n            raise UnexpectedModelBehavior(  # pragma: no cover\n                'Streamed response ended without content or tool calls'\n            )\n\n        return HuggingFaceStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _model_name=first_chunk.model,\n            _model_profile=self.profile,\n            _response=peekable_response,\n            _timestamp=datetime.fromtimestamp(first_chunk.created, tz=timezone.utc),\n            _provider_name=self._provider.name,\n        )\n\n    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ChatCompletionInputTool]:\n        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]\n\n    async def _map_messages(\n        self, messages: list[ModelMessage], model_request_parameters: ModelRequestParameters\n    ) -> list[ChatCompletionInputMessage | ChatCompletionOutputMessage]:\n        \"\"\"Just maps a `pydantic_ai.Message` to a `huggingface_hub.ChatCompletionInputMessage`.\"\"\"\n        hf_messages: list[ChatCompletionInputMessage | ChatCompletionOutputMessage] = []\n        for message in messages:\n            if isinstance(message, ModelRequest):\n                async for item in self._map_user_message(message):\n                    hf_messages.append(item)\n            elif isinstance(message, ModelResponse):\n                texts: list[str] = []\n                tool_calls: list[ChatCompletionInputToolCall] = []\n                for item in message.parts:\n                    if isinstance(item, TextPart):\n                        texts.append(item.content)\n                    elif isinstance(item, ToolCallPart):\n                        tool_calls.append(self._map_tool_call(item))\n                    elif isinstance(item, ThinkingPart):\n                        start_tag, end_tag = self.profile.thinking_tags\n                        texts.append('\\n'.join([start_tag, item.content, end_tag]))\n                    elif isinstance(item, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover\n                        # This is currently never returned from huggingface\n                        pass\n                    elif isinstance(item, FilePart):  # pragma: no cover\n                        # Files generated by models are not sent back to models that don't themselves generate files.\n                        pass\n                    else:\n                        assert_never(item)\n                message_param = ChatCompletionInputMessage(role='assistant')  # type: ignore\n                if texts:\n                    # Note: model responses from this model should only have one text item, so the following\n                    # shouldn't merge multiple texts into one unless you switch models between runs:\n                    message_param['content'] = '\\n\\n'.join(texts)\n                if tool_calls:\n                    message_param['tool_calls'] = tool_calls\n                hf_messages.append(message_param)\n            else:\n                assert_never(message)\n        if instructions := self._get_instructions(messages, model_request_parameters):\n            hf_messages.insert(0, ChatCompletionInputMessage(content=instructions, role='system'))  # type: ignore\n        return hf_messages\n\n    @staticmethod\n    def _map_tool_call(t: ToolCallPart) -> ChatCompletionInputToolCall:\n        return ChatCompletionInputToolCall.parse_obj_as_instance(  # type: ignore\n            {\n                'id': _guard_tool_call_id(t=t),\n                'type': 'function',\n                'function': {\n                    'name': t.tool_name,\n                    'arguments': t.args_as_json_str(),\n                },\n            }\n        )\n\n    @staticmethod\n    def _map_tool_definition(f: ToolDefinition) -> ChatCompletionInputTool:\n        tool_param: ChatCompletionInputTool = ChatCompletionInputTool.parse_obj_as_instance(  # type: ignore\n            {\n                'type': 'function',\n                'function': {\n                    'name': f.name,\n                    'description': f.description,\n                    'parameters': f.parameters_json_schema,\n                },\n            }\n        )\n        return tool_param\n\n    async def _map_user_message(\n        self, message: ModelRequest\n    ) -> AsyncIterable[ChatCompletionInputMessage | ChatCompletionOutputMessage]:\n        for part in message.parts:\n            if isinstance(part, SystemPromptPart):\n                yield ChatCompletionInputMessage.parse_obj_as_instance({'role': 'system', 'content': part.content})  # type: ignore\n            elif isinstance(part, UserPromptPart):\n                yield await self._map_user_prompt(part)\n            elif isinstance(part, ToolReturnPart):\n                yield ChatCompletionOutputMessage.parse_obj_as_instance(  # type: ignore\n                    {\n                        'role': 'tool',\n                        'tool_call_id': _guard_tool_call_id(t=part),\n                        'content': part.model_response_str(),\n                    }\n                )\n            elif isinstance(part, RetryPromptPart):\n                if part.tool_name is None:\n                    yield ChatCompletionInputMessage.parse_obj_as_instance(  # type: ignore\n                        {'role': 'user', 'content': part.model_response()}\n                    )\n                else:\n                    yield ChatCompletionInputMessage.parse_obj_as_instance(  # type: ignore\n                        {\n                            'role': 'tool',\n                            'tool_call_id': _guard_tool_call_id(t=part),\n                            'content': part.model_response(),\n                        }\n                    )\n            else:\n                assert_never(part)\n\n    @staticmethod\n    async def _map_user_prompt(part: UserPromptPart) -> ChatCompletionInputMessage:\n        content: str | list[ChatCompletionInputMessage]\n        if isinstance(part.content, str):\n            content = part.content\n        else:\n            content = []\n            for item in part.content:\n                if isinstance(item, str):\n                    content.append(ChatCompletionInputMessageChunk(type='text', text=item))  # type: ignore\n                elif isinstance(item, ImageUrl):\n                    url = ChatCompletionInputURL(url=item.url)  # type: ignore\n                    content.append(ChatCompletionInputMessageChunk(type='image_url', image_url=url))  # type: ignore\n                elif isinstance(item, BinaryContent):\n                    if item.is_image:\n                        url = ChatCompletionInputURL(url=item.data_uri)  # type: ignore\n                        content.append(ChatCompletionInputMessageChunk(type='image_url', image_url=url))  # type: ignore\n                    else:  # pragma: no cover\n                        raise RuntimeError(f'Unsupported binary content type: {item.media_type}')\n                elif isinstance(item, AudioUrl):\n                    raise NotImplementedError('AudioUrl is not supported for Hugging Face')\n                elif isinstance(item, DocumentUrl):\n                    raise NotImplementedError('DocumentUrl is not supported for Hugging Face')\n                elif isinstance(item, VideoUrl):\n                    raise NotImplementedError('VideoUrl is not supported for Hugging Face')\n                elif isinstance(item, CachePoint):\n                    # Hugging Face doesn't support prompt caching via CachePoint\n                    pass\n                else:\n                    assert_never(item)\n        return ChatCompletionInputMessage(role='user', content=content)  # type: ignore\n\n```\n\n#### __init__\n\n```python\n__init__(\n    model_name: str,\n    *,\n    provider: (\n        Literal[\"huggingface\"]\n        | Provider[AsyncInferenceClient]\n    ) = \"huggingface\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize a Hugging Face model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model_name` | `str` | The name of the Model to use. You can browse available models here. | *required* | | `provider` | `Literal['huggingface'] | Provider[AsyncInferenceClient]` | The provider to use for Hugging Face Inference Providers. Can be either the string 'huggingface' or an instance of Provider[AsyncInferenceClient]. If not provided, the other parameters will be used. | `'huggingface'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` | | `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/huggingface.py`\n\n```python\ndef __init__(\n    self,\n    model_name: str,\n    *,\n    provider: Literal['huggingface'] | Provider[AsyncInferenceClient] = 'huggingface',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize a Hugging Face model.\n\n    Args:\n        model_name: The name of the Model to use. You can browse available models [here](https://huggingface.co/models?pipeline_tag=text-generation&inference_provider=all&sort=trending).\n        provider: The provider to use for Hugging Face Inference Providers. Can be either the string 'huggingface' or an\n            instance of `Provider[AsyncInferenceClient]`. If not provided, the other parameters will be used.\n        profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n        settings: Model-specific settings that will be used as defaults for this model.\n    \"\"\"\n    self._model_name = model_name\n    if isinstance(provider, str):\n        provider = infer_provider(provider)\n    self._provider = provider\n    self.client = provider.client\n\n    super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n```\n\n#### model_name\n\n```python\nmodel_name: HuggingFaceModelName\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe system / model provider.",
  "content_length": 18988
}