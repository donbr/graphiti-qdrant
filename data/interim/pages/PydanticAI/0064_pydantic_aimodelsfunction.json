{
  "title": "`pydantic_ai.models.function`",
  "source_url": null,
  "content": "A model controlled by a local function.\n\nFunctionModel is similar to [`TestModel`](../test/), but allows greater control over the model's behavior.\n\nIts primary use case is for more advanced unit testing than is possible with `TestModel`.\n\nHere's a minimal example:\n\n[Learn about Gateway](../../../gateway) function_model_usage.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai import ModelMessage, ModelResponse, TextPart\nfrom pydantic_ai.models.function import FunctionModel, AgentInfo\n\nmy_agent = Agent('gateway/openai:gpt-5')\n\n\nasync def model_function(\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    print(messages)\n    \"\"\"\n    [\n        ModelRequest(\n            parts=[\n                UserPromptPart(\n                    content='Testing my agent...',\n                    timestamp=datetime.datetime(...),\n                )\n            ],\n            run_id='...',\n        )\n    ]\n    \"\"\"\n    print(info)\n    \"\"\"\n    AgentInfo(\n        function_tools=[],\n        allow_text_output=True,\n        output_tools=[],\n        model_settings=None,\n        model_request_parameters=ModelRequestParameters(\n            function_tools=[], builtin_tools=[], output_tools=[]\n        ),\n        instructions=None,\n    )\n    \"\"\"\n    return ModelResponse(parts=[TextPart('hello world')])\n\n\nasync def test_my_agent():\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\n    with my_agent.override(model=FunctionModel(model_function)):\n        result = await my_agent.run('Testing my agent...')\n        assert result.output == 'hello world'\n\n```\n\nfunction_model_usage.py\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai import ModelMessage, ModelResponse, TextPart\nfrom pydantic_ai.models.function import FunctionModel, AgentInfo\n\nmy_agent = Agent('openai:gpt-5')\n\n\nasync def model_function(\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    print(messages)\n    \"\"\"\n    [\n        ModelRequest(\n            parts=[\n                UserPromptPart(\n                    content='Testing my agent...',\n                    timestamp=datetime.datetime(...),\n                )\n            ],\n            run_id='...',\n        )\n    ]\n    \"\"\"\n    print(info)\n    \"\"\"\n    AgentInfo(\n        function_tools=[],\n        allow_text_output=True,\n        output_tools=[],\n        model_settings=None,\n        model_request_parameters=ModelRequestParameters(\n            function_tools=[], builtin_tools=[], output_tools=[]\n        ),\n        instructions=None,\n    )\n    \"\"\"\n    return ModelResponse(parts=[TextPart('hello world')])\n\n\nasync def test_my_agent():\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\n    with my_agent.override(model=FunctionModel(model_function)):\n        result = await my_agent.run('Testing my agent...')\n        assert result.output == 'hello world'\n\n```\n\nSee [Unit testing with `FunctionModel`](../../../testing/#unit-testing-with-functionmodel) for detailed documentation.\n\n### FunctionModel\n\nBases: `Model`\n\nA model controlled by a local function.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n```python\n@dataclass(init=False)\nclass FunctionModel(Model):\n    \"\"\"A model controlled by a local function.\n\n    Apart from `__init__`, all methods are private or match those of the base class.\n    \"\"\"\n\n    function: FunctionDef | None\n    stream_function: StreamFunctionDef | None\n\n    _model_name: str = field(repr=False)\n    _system: str = field(default='function', repr=False)\n\n    @overload\n    def __init__(\n        self,\n        function: FunctionDef,\n        *,\n        model_name: str | None = None,\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        *,\n        stream_function: StreamFunctionDef,\n        model_name: str | None = None,\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        function: FunctionDef,\n        *,\n        stream_function: StreamFunctionDef,\n        model_name: str | None = None,\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ) -> None: ...\n\n    def __init__(\n        self,\n        function: FunctionDef | None = None,\n        *,\n        stream_function: StreamFunctionDef | None = None,\n        model_name: str | None = None,\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize a `FunctionModel`.\n\n        Either `function` or `stream_function` must be provided, providing both is allowed.\n\n        Args:\n            function: The function to call for non-streamed requests.\n            stream_function: The function to call for streamed requests.\n            model_name: The name of the model. If not provided, a name is generated from the function names.\n            profile: The model profile to use.\n            settings: Model-specific settings that will be used as defaults for this model.\n        \"\"\"\n        if function is None and stream_function is None:\n            raise TypeError('Either `function` or `stream_function` must be provided')\n\n        self.function = function\n        self.stream_function = stream_function\n\n        function_name = self.function.__name__ if self.function is not None else ''\n        stream_function_name = self.stream_function.__name__ if self.stream_function is not None else ''\n        self._model_name = model_name or f'function:{function_name}:{stream_function_name}'\n\n        # Use a default profile that supports JSON schema and object output if none provided\n        if profile is None:\n            profile = ModelProfile(\n                supports_json_schema_output=True,\n                supports_json_object_output=True,\n            )\n        super().__init__(settings=settings, profile=profile)\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        agent_info = AgentInfo(\n            function_tools=model_request_parameters.function_tools,\n            allow_text_output=model_request_parameters.allow_text_output,\n            output_tools=model_request_parameters.output_tools,\n            model_settings=model_settings,\n            model_request_parameters=model_request_parameters,\n            instructions=self._get_instructions(messages, model_request_parameters),\n        )\n\n        assert self.function is not None, 'FunctionModel must receive a `function` to support non-streamed requests'\n\n        if inspect.iscoroutinefunction(self.function):\n            response = await self.function(messages, agent_info)\n        else:\n            response_ = await _utils.run_in_executor(self.function, messages, agent_info)\n            assert isinstance(response_, ModelResponse), response_\n            response = response_\n        response.model_name = self._model_name\n        # Add usage data if not already present\n        if not response.usage.has_values():  # pragma: no branch\n            response.usage = _estimate_usage(chain(messages, [response]))\n        return response\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        agent_info = AgentInfo(\n            function_tools=model_request_parameters.function_tools,\n            allow_text_output=model_request_parameters.allow_text_output,\n            output_tools=model_request_parameters.output_tools,\n            model_settings=model_settings,\n            model_request_parameters=model_request_parameters,\n            instructions=self._get_instructions(messages, model_request_parameters),\n        )\n\n        assert self.stream_function is not None, (\n            'FunctionModel must receive a `stream_function` to support streamed requests'\n        )\n\n        response_stream = PeekableAsyncStream(self.stream_function(messages, agent_info))\n\n        first = await response_stream.peek()\n        if isinstance(first, _utils.Unset):\n            raise ValueError('Stream function must return at least one item')\n\n        yield FunctionStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _model_name=self._model_name,\n            _iter=response_stream,\n        )\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The system / model provider.\"\"\"\n        return self._system\n\n```\n\n#### __init__\n\n```python\n__init__(\n    function: FunctionDef,\n    *,\n    model_name: str | None = None,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    *,\n    stream_function: StreamFunctionDef,\n    model_name: str | None = None,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    function: FunctionDef,\n    *,\n    stream_function: StreamFunctionDef,\n    model_name: str | None = None,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    function: FunctionDef | None = None,\n    *,\n    stream_function: StreamFunctionDef | None = None,\n    model_name: str | None = None,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize a `FunctionModel`.\n\nEither `function` or `stream_function` must be provided, providing both is allowed.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `function` | `FunctionDef | None` | The function to call for non-streamed requests. | `None` | | `stream_function` | `StreamFunctionDef | None` | The function to call for streamed requests. | `None` | | `model_name` | `str | None` | The name of the model. If not provided, a name is generated from the function names. | `None` | | `profile` | `ModelProfileSpec | None` | The model profile to use. | `None` | | `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n```python\ndef __init__(\n    self,\n    function: FunctionDef | None = None,\n    *,\n    stream_function: StreamFunctionDef | None = None,\n    model_name: str | None = None,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize a `FunctionModel`.\n\n    Either `function` or `stream_function` must be provided, providing both is allowed.\n\n    Args:\n        function: The function to call for non-streamed requests.\n        stream_function: The function to call for streamed requests.\n        model_name: The name of the model. If not provided, a name is generated from the function names.\n        profile: The model profile to use.\n        settings: Model-specific settings that will be used as defaults for this model.\n    \"\"\"\n    if function is None and stream_function is None:\n        raise TypeError('Either `function` or `stream_function` must be provided')\n\n    self.function = function\n    self.stream_function = stream_function\n\n    function_name = self.function.__name__ if self.function is not None else ''\n    stream_function_name = self.stream_function.__name__ if self.stream_function is not None else ''\n    self._model_name = model_name or f'function:{function_name}:{stream_function_name}'\n\n    # Use a default profile that supports JSON schema and object output if none provided\n    if profile is None:\n        profile = ModelProfile(\n            supports_json_schema_output=True,\n            supports_json_object_output=True,\n        )\n    super().__init__(settings=settings, profile=profile)\n\n```\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe system / model provider.\n\n### AgentInfo\n\nInformation about an agent.\n\nThis is passed as the second to functions used within FunctionModel.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n```python\n@dataclass(frozen=True, kw_only=True)\nclass AgentInfo:\n    \"\"\"Information about an agent.\n\n    This is passed as the second to functions used within [`FunctionModel`][pydantic_ai.models.function.FunctionModel].\n    \"\"\"\n\n    function_tools: list[ToolDefinition]\n    \"\"\"The function tools available on this agent.\n\n    These are the tools registered via the [`tool`][pydantic_ai.Agent.tool] and\n    [`tool_plain`][pydantic_ai.Agent.tool_plain] decorators.\n    \"\"\"\n    allow_text_output: bool\n    \"\"\"Whether a plain text output is allowed.\"\"\"\n    output_tools: list[ToolDefinition]\n    \"\"\"The tools that can called to produce the final output of the run.\"\"\"\n    model_settings: ModelSettings | None\n    \"\"\"The model settings passed to the run call.\"\"\"\n    model_request_parameters: ModelRequestParameters\n    \"\"\"The model request parameters passed to the run call.\"\"\"\n    instructions: str | None\n    \"\"\"The instructions passed to model.\"\"\"\n\n```\n\n#### function_tools\n\n```python\nfunction_tools: list[ToolDefinition]\n\n```\n\nThe function tools available on this agent.\n\nThese are the tools registered via the tool and tool_plain decorators.\n\n#### allow_text_output\n\n```python\nallow_text_output: bool\n\n```\n\nWhether a plain text output is allowed.\n\n#### output_tools\n\n```python\noutput_tools: list[ToolDefinition]\n\n```\n\nThe tools that can called to produce the final output of the run.\n\n#### model_settings\n\n```python\nmodel_settings: ModelSettings | None\n\n```\n\nThe model settings passed to the run call.\n\n#### model_request_parameters\n\n```python\nmodel_request_parameters: ModelRequestParameters\n\n```\n\nThe model request parameters passed to the run call.\n\n#### instructions\n\n```python\ninstructions: str | None\n\n```\n\nThe instructions passed to model.\n\n### DeltaToolCall\n\nIncremental change to a tool call.\n\nUsed to describe a chunk when streaming structured responses.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n```python\n@dataclass\nclass DeltaToolCall:\n    \"\"\"Incremental change to a tool call.\n\n    Used to describe a chunk when streaming structured responses.\n    \"\"\"\n\n    name: str | None = None\n    \"\"\"Incremental change to the name of the tool.\"\"\"\n\n    json_args: str | None = None\n    \"\"\"Incremental change to the arguments as JSON\"\"\"\n\n    _: KW_ONLY\n\n    tool_call_id: str | None = None\n    \"\"\"Incremental change to the tool call ID.\"\"\"\n\n```\n\n#### name\n\n```python\nname: str | None = None\n\n```\n\nIncremental change to the name of the tool.\n\n#### json_args\n\n```python\njson_args: str | None = None\n\n```\n\nIncremental change to the arguments as JSON\n\n#### tool_call_id\n\n```python\ntool_call_id: str | None = None\n\n```\n\nIncremental change to the tool call ID.\n\n### DeltaThinkingPart\n\nIncremental change to a thinking part.\n\nUsed to describe a chunk when streaming thinking responses.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n```python\n@dataclass(kw_only=True)\nclass DeltaThinkingPart:\n    \"\"\"Incremental change to a thinking part.\n\n    Used to describe a chunk when streaming thinking responses.\n    \"\"\"\n\n    content: str | None = None\n    \"\"\"Incremental change to the thinking content.\"\"\"\n    signature: str | None = None\n    \"\"\"Incremental change to the thinking signature.\"\"\"\n\n```\n\n#### content\n\n```python\ncontent: str | None = None\n\n```\n\nIncremental change to the thinking content.\n\n#### signature\n\n```python\nsignature: str | None = None\n\n```\n\nIncremental change to the thinking signature.\n\n### DeltaToolCalls\n\n```python\nDeltaToolCalls: TypeAlias = dict[int, DeltaToolCall]\n\n```\n\nA mapping of tool call IDs to incremental changes.\n\n### DeltaThinkingCalls\n\n```python\nDeltaThinkingCalls: TypeAlias = dict[int, DeltaThinkingPart]\n\n```\n\nA mapping of thinking call IDs to incremental changes.\n\n### FunctionDef\n\n```python\nFunctionDef: TypeAlias = Callable[\n    [list[ModelMessage], AgentInfo],\n    ModelResponse | Awaitable[ModelResponse],\n]\n\n```\n\nA function used to generate a non-streamed response.\n\n### StreamFunctionDef\n\n```python\nStreamFunctionDef: TypeAlias = Callable[\n    [list[ModelMessage], AgentInfo],\n    AsyncIterator[\n        str\n        | DeltaToolCalls\n        | DeltaThinkingCalls\n        | BuiltinToolCallsReturns\n    ],\n]\n\n```\n\nA function used to generate a streamed response.\n\nWhile this is defined as having return type of `AsyncIterator[str | DeltaToolCalls | DeltaThinkingCalls | BuiltinTools]`, it should really be considered as `AsyncIterator[str] | AsyncIterator[DeltaToolCalls] | AsyncIterator[DeltaThinkingCalls]`,\n\nE.g. you need to yield all text, all `DeltaToolCalls`, all `DeltaThinkingCalls`, or all `BuiltinToolCallsReturns`, not mix them.\n\n### FunctionStreamedResponse\n\nBases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for FunctionModel.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n```python\n@dataclass\nclass FunctionStreamedResponse(StreamedResponse):\n    \"\"\"Implementation of `StreamedResponse` for [FunctionModel][pydantic_ai.models.function.FunctionModel].\"\"\"\n\n    _model_name: str\n    _iter: AsyncIterator[str | DeltaToolCalls | DeltaThinkingCalls | BuiltinToolCallsReturns]\n    _timestamp: datetime = field(default_factory=_utils.now_utc)\n\n    def __post_init__(self):\n        self._usage += _estimate_usage([])\n\n    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:\n        async for item in self._iter:\n            if isinstance(item, str):\n                response_tokens = _estimate_string_tokens(item)\n                self._usage += usage.RequestUsage(output_tokens=response_tokens)\n                maybe_event = self._parts_manager.handle_text_delta(vendor_part_id='content', content=item)\n                if maybe_event is not None:  # pragma: no branch\n                    yield maybe_event\n            elif isinstance(item, dict) and item:\n                for dtc_index, delta in item.items():\n                    if isinstance(delta, DeltaThinkingPart):\n                        if delta.content:  # pragma: no branch\n                            response_tokens = _estimate_string_tokens(delta.content)\n                            self._usage += usage.RequestUsage(output_tokens=response_tokens)\n                        yield self._parts_manager.handle_thinking_delta(\n                            vendor_part_id=dtc_index,\n                            content=delta.content,\n                            signature=delta.signature,\n                            provider_name='function' if delta.signature else None,\n                        )\n                    elif isinstance(delta, DeltaToolCall):\n                        if delta.json_args:\n                            response_tokens = _estimate_string_tokens(delta.json_args)\n                            self._usage += usage.RequestUsage(output_tokens=response_tokens)\n                        maybe_event = self._parts_manager.handle_tool_call_delta(\n                            vendor_part_id=dtc_index,\n                            tool_name=delta.name,\n                            args=delta.json_args,\n                            tool_call_id=delta.tool_call_id,\n                        )\n                        if maybe_event is not None:  # pragma: no branch\n                            yield maybe_event\n                    elif isinstance(delta, BuiltinToolCallPart):\n                        if content := delta.args_as_json_str():  # pragma: no branch\n                            response_tokens = _estimate_string_tokens(content)\n                            self._usage += usage.RequestUsage(output_tokens=response_tokens)\n                        yield self._parts_manager.handle_part(vendor_part_id=dtc_index, part=delta)\n                    elif isinstance(delta, BuiltinToolReturnPart):\n                        if content := delta.model_response_str():  # pragma: no branch\n                            response_tokens = _estimate_string_tokens(content)\n                            self._usage += usage.RequestUsage(output_tokens=response_tokens)\n                        yield self._parts_manager.handle_part(vendor_part_id=dtc_index, part=delta)\n                    else:\n                        assert_never(delta)\n\n    @property\n    def model_name(self) -> str:\n        \"\"\"Get the model name of the response.\"\"\"\n        return self._model_name\n\n    @property\n    def provider_name(self) -> None:\n        \"\"\"Get the provider name.\"\"\"\n        return None\n\n    @property\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._timestamp\n\n```\n\n#### model_name\n\n```python\nmodel_name: str\n\n```\n\nGet the model name of the response.\n\n#### provider_name\n\n```python\nprovider_name: None\n\n```\n\nGet the provider name.\n\n#### timestamp\n\n```python\ntimestamp: datetime\n\n```\n\nGet the timestamp of the response.",
  "content_length": 21602
}