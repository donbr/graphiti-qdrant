{
  "title": "`pydantic_evals.generation`",
  "source_url": null,
  "content": "Utilities for generating example datasets for pydantic_evals.\n\nThis module provides functions for generating sample datasets for testing and examples, using LLMs to create realistic test data with proper structure.\n\n### generate_dataset\n\n```python\ngenerate_dataset(\n    *,\n    dataset_type: type[\n        Dataset[InputsT, OutputT, MetadataT]\n    ],\n    path: Path | str | None = None,\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n    model: Model | KnownModelName = \"openai:gpt-4o\",\n    n_examples: int = 3,\n    extra_instructions: str | None = None\n) -> Dataset[InputsT, OutputT, MetadataT]\n\n```\n\nUse an LLM to generate a dataset of test cases, each consisting of input, expected output, and metadata.\n\nThis function creates a properly structured dataset with the specified input, output, and metadata types. It uses an LLM to attempt to generate realistic test cases that conform to the types' schemas.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `path` | `Path | str | None` | Optional path to save the generated dataset. If provided, the dataset will be saved to this location. | `None` | | `dataset_type` | `type[Dataset[InputsT, OutputT, MetadataT]]` | The type of dataset to generate, with the desired input, output, and metadata types. | *required* | | `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Optional sequence of custom evaluator classes to include in the schema. | `()` | | `model` | `Model | KnownModelName` | The Pydantic AI model to use for generation. Defaults to 'gpt-4o'. | `'openai:gpt-4o'` | | `n_examples` | `int` | Number of examples to generate. Defaults to 3. | `3` | | `extra_instructions` | `str | None` | Optional additional instructions to provide to the LLM. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `Dataset[InputsT, OutputT, MetadataT]` | A properly structured Dataset object with generated test cases. |\n\nRaises:\n\n| Type | Description | | --- | --- | | `ValidationError` | If the LLM's response cannot be parsed as a valid dataset. |\n\nSource code in `pydantic_evals/pydantic_evals/generation.py`\n\n```python\nasync def generate_dataset(\n    *,\n    dataset_type: type[Dataset[InputsT, OutputT, MetadataT]],\n    path: Path | str | None = None,\n    custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n    model: models.Model | models.KnownModelName = 'openai:gpt-4o',\n    n_examples: int = 3,\n    extra_instructions: str | None = None,\n) -> Dataset[InputsT, OutputT, MetadataT]:\n    \"\"\"Use an LLM to generate a dataset of test cases, each consisting of input, expected output, and metadata.\n\n    This function creates a properly structured dataset with the specified input, output, and metadata types.\n    It uses an LLM to attempt to generate realistic test cases that conform to the types' schemas.\n\n    Args:\n        path: Optional path to save the generated dataset. If provided, the dataset will be saved to this location.\n        dataset_type: The type of dataset to generate, with the desired input, output, and metadata types.\n        custom_evaluator_types: Optional sequence of custom evaluator classes to include in the schema.\n        model: The Pydantic AI model to use for generation. Defaults to 'gpt-4o'.\n        n_examples: Number of examples to generate. Defaults to 3.\n        extra_instructions: Optional additional instructions to provide to the LLM.\n\n    Returns:\n        A properly structured Dataset object with generated test cases.\n\n    Raises:\n        ValidationError: If the LLM's response cannot be parsed as a valid dataset.\n    \"\"\"\n    output_schema = dataset_type.model_json_schema_with_evaluators(custom_evaluator_types)\n\n    # TODO: Use `output_type=StructuredDict(output_schema)` (and `from_dict` below) once https://github.com/pydantic/pydantic/issues/12145\n    # is fixed and `StructuredDict` no longer needs to use `InlineDefsJsonSchemaTransformer`.\n    agent = Agent(\n        model,\n        system_prompt=(\n            f'Generate an object that is in compliance with this JSON schema:\\n{output_schema}\\n\\n'\n            f'Include {n_examples} example cases.'\n            ' You must not include any characters in your response before the opening { of the JSON object, or after the closing }.'\n        ),\n        output_type=str,\n        retries=1,\n    )\n\n    result = await agent.run(extra_instructions or 'Please generate the object.')\n    output = strip_markdown_fences(result.output)\n    try:\n        result = dataset_type.from_text(output, fmt='json', custom_evaluator_types=custom_evaluator_types)\n    except ValidationError as e:  # pragma: no cover\n        print(f'Raw response from model:\\n{result.output}')\n        raise e\n    if path is not None:\n        result.to_file(path, custom_evaluator_types=custom_evaluator_types)  # pragma: no cover\n    return result\n\n```",
  "content_length": 4932
}