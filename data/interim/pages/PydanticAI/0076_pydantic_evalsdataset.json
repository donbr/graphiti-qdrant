{
  "title": "`pydantic_evals.dataset`",
  "source_url": null,
  "content": "Dataset management for pydantic evals.\n\nThis module provides functionality for creating, loading, saving, and evaluating datasets of test cases. Each case must have inputs, and can optionally have a name, expected output, metadata, and case-specific evaluators.\n\nDatasets can be loaded from and saved to YAML or JSON files, and can be evaluated against a task function to produce an evaluation report.\n\n### Case\n\nBases: `Generic[InputsT, OutputT, MetadataT]`\n\nA single row of a Dataset.\n\nEach case represents a single test scenario with inputs to test. A case may optionally specify a name, expected outputs to compare against, and arbitrary metadata.\n\nCases can also have their own specific evaluators which are run in addition to dataset-level evaluators.\n\nExample:\n\n```python\nfrom pydantic_evals import Case\n\ncase = Case(\n    name='Simple addition',\n    inputs={'a': 1, 'b': 2},\n    expected_output=3,\n    metadata={'description': 'Tests basic addition'},\n)\n\n```\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n````python\n@dataclass(init=False)\nclass Case(Generic[InputsT, OutputT, MetadataT]):\n    \"\"\"A single row of a [`Dataset`][pydantic_evals.Dataset].\n\n    Each case represents a single test scenario with inputs to test. A case may optionally specify a name, expected\n    outputs to compare against, and arbitrary metadata.\n\n    Cases can also have their own specific evaluators which are run in addition to dataset-level evaluators.\n\n    Example:\n    ```python\n    from pydantic_evals import Case\n\n    case = Case(\n        name='Simple addition',\n        inputs={'a': 1, 'b': 2},\n        expected_output=3,\n        metadata={'description': 'Tests basic addition'},\n    )\n    ```\n    \"\"\"\n\n    name: str | None\n    \"\"\"Name of the case. This is used to identify the case in the report and can be used to filter cases.\"\"\"\n    inputs: InputsT\n    \"\"\"Inputs to the task. This is the input to the task that will be evaluated.\"\"\"\n    metadata: MetadataT | None = None\n    \"\"\"Metadata to be used in the evaluation.\n\n    This can be used to provide additional information about the case to the evaluators.\n    \"\"\"\n    expected_output: OutputT | None = None\n    \"\"\"Expected output of the task. This is the expected output of the task that will be evaluated.\"\"\"\n    evaluators: list[Evaluator[InputsT, OutputT, MetadataT]] = field(default_factory=list)\n    \"\"\"Evaluators to be used just on this case.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: str | None = None,\n        inputs: InputsT,\n        metadata: MetadataT | None = None,\n        expected_output: OutputT | None = None,\n        evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (),\n    ):\n        \"\"\"Initialize a new test case.\n\n        Args:\n            name: Optional name for the case. If not provided, a generic name will be assigned when added to a dataset.\n            inputs: The inputs to the task being evaluated.\n            metadata: Optional metadata for the case, which can be used by evaluators.\n            expected_output: Optional expected output of the task, used for comparison in evaluators.\n            evaluators: Tuple of evaluators specific to this case. These are in addition to any\n                dataset-level evaluators.\n\n        \"\"\"\n        # Note: `evaluators` must be a tuple instead of Sequence due to misbehavior with pyright's generic parameter\n        # inference if it has type `Sequence`\n        self.name = name\n        self.inputs = inputs\n        self.metadata = metadata\n        self.expected_output = expected_output\n        self.evaluators = list(evaluators)\n\n````\n\n#### __init__\n\n```python\n__init__(\n    *,\n    name: str | None = None,\n    inputs: InputsT,\n    metadata: MetadataT | None = None,\n    expected_output: OutputT | None = None,\n    evaluators: tuple[\n        Evaluator[InputsT, OutputT, MetadataT], ...\n    ] = ()\n)\n\n```\n\nInitialize a new test case.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str | None` | Optional name for the case. If not provided, a generic name will be assigned when added to a dataset. | `None` | | `inputs` | `InputsT` | The inputs to the task being evaluated. | *required* | | `metadata` | `MetadataT | None` | Optional metadata for the case, which can be used by evaluators. | `None` | | `expected_output` | `OutputT | None` | Optional expected output of the task, used for comparison in evaluators. | `None` | | `evaluators` | `tuple[Evaluator[InputsT, OutputT, MetadataT], ...]` | Tuple of evaluators specific to this case. These are in addition to any dataset-level evaluators. | `()` |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    name: str | None = None,\n    inputs: InputsT,\n    metadata: MetadataT | None = None,\n    expected_output: OutputT | None = None,\n    evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (),\n):\n    \"\"\"Initialize a new test case.\n\n    Args:\n        name: Optional name for the case. If not provided, a generic name will be assigned when added to a dataset.\n        inputs: The inputs to the task being evaluated.\n        metadata: Optional metadata for the case, which can be used by evaluators.\n        expected_output: Optional expected output of the task, used for comparison in evaluators.\n        evaluators: Tuple of evaluators specific to this case. These are in addition to any\n            dataset-level evaluators.\n\n    \"\"\"\n    # Note: `evaluators` must be a tuple instead of Sequence due to misbehavior with pyright's generic parameter\n    # inference if it has type `Sequence`\n    self.name = name\n    self.inputs = inputs\n    self.metadata = metadata\n    self.expected_output = expected_output\n    self.evaluators = list(evaluators)\n\n```\n\n#### name\n\n```python\nname: str | None = name\n\n```\n\nName of the case. This is used to identify the case in the report and can be used to filter cases.\n\n#### inputs\n\n```python\ninputs: InputsT = inputs\n\n```\n\nInputs to the task. This is the input to the task that will be evaluated.\n\n#### metadata\n\n```python\nmetadata: MetadataT | None = metadata\n\n```\n\nMetadata to be used in the evaluation.\n\nThis can be used to provide additional information about the case to the evaluators.\n\n#### expected_output\n\n```python\nexpected_output: OutputT | None = expected_output\n\n```\n\nExpected output of the task. This is the expected output of the task that will be evaluated.\n\n#### evaluators\n\n```python\nevaluators: list[Evaluator[InputsT, OutputT, MetadataT]] = (\n    list(evaluators)\n)\n\n```\n\nEvaluators to be used just on this case.\n\n### Dataset\n\nBases: `BaseModel`, `Generic[InputsT, OutputT, MetadataT]`\n\nA dataset of test cases.\n\nDatasets allow you to organize a collection of test cases and evaluate them against a task function. They can be loaded from and saved to YAML or JSON files, and can have dataset-level evaluators that apply to all cases.\n\nExample:\n\n```python\n### Create a dataset with two test cases\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output\n\ndataset = Dataset(\n    cases=[\n        Case(name='test1', inputs={'text': 'Hello'}, expected_output='HELLO'),\n        Case(name='test2', inputs={'text': 'World'}, expected_output='WORLD'),\n    ],\n    evaluators=[ExactMatch()],\n)\n\n### Evaluate the dataset against a task function\nasync def uppercase(inputs: dict) -> str:\n    return inputs['text'].upper()\n\nasync def main():\n    report = await dataset.evaluate(uppercase)\n    report.print()\n'''\n   Evaluation Summary: uppercase\n┏━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID  ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ test1    │ ✔          │     10ms │\n├──────────┼────────────┼──────────┤\n│ test2    │ ✔          │     10ms │\n├──────────┼────────────┼──────────┤\n│ Averages │ 100.0% ✔   │     10ms │\n└──────────┴────────────┴──────────┘\n'''\n\n```\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n````python\nclass Dataset(BaseModel, Generic[InputsT, OutputT, MetadataT], extra='forbid', arbitrary_types_allowed=True):\n    \"\"\"A dataset of test [cases][pydantic_evals.Case].\n\n    Datasets allow you to organize a collection of test cases and evaluate them against a task function.\n    They can be loaded from and saved to YAML or JSON files, and can have dataset-level evaluators that\n    apply to all cases.\n\n    Example:\n    ```python\n    # Create a dataset with two test cases\n    from dataclasses import dataclass\n\n    from pydantic_evals import Case, Dataset\n    from pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n    @dataclass\n    class ExactMatch(Evaluator):\n        def evaluate(self, ctx: EvaluatorContext) -> bool:\n            return ctx.output == ctx.expected_output\n\n    dataset = Dataset(\n        cases=[\n            Case(name='test1', inputs={'text': 'Hello'}, expected_output='HELLO'),\n            Case(name='test2', inputs={'text': 'World'}, expected_output='WORLD'),\n        ],\n        evaluators=[ExactMatch()],\n    )\n\n    # Evaluate the dataset against a task function\n    async def uppercase(inputs: dict) -> str:\n        return inputs['text'].upper()\n\n    async def main():\n        report = await dataset.evaluate(uppercase)\n        report.print()\n    '''\n       Evaluation Summary: uppercase\n    ┏━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n    ┃ Case ID  ┃ Assertions ┃ Duration ┃\n    ┡━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n    │ test1    │ ✔          │     10ms │\n    ├──────────┼────────────┼──────────┤\n    │ test2    │ ✔          │     10ms │\n    ├──────────┼────────────┼──────────┤\n    │ Averages │ 100.0% ✔   │     10ms │\n    └──────────┴────────────┴──────────┘\n    '''\n    ```\n    \"\"\"\n\n    name: str | None = None\n    \"\"\"Optional name of the dataset.\"\"\"\n    cases: list[Case[InputsT, OutputT, MetadataT]]\n    \"\"\"List of test cases in the dataset.\"\"\"\n    evaluators: list[Evaluator[InputsT, OutputT, MetadataT]] = []\n    \"\"\"List of evaluators to be used on all cases in the dataset.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: str | None = None,\n        cases: Sequence[Case[InputsT, OutputT, MetadataT]],\n        evaluators: Sequence[Evaluator[InputsT, OutputT, MetadataT]] = (),\n    ):\n        \"\"\"Initialize a new dataset with test cases and optional evaluators.\n\n        Args:\n            name: Optional name for the dataset.\n            cases: Sequence of test cases to include in the dataset.\n            evaluators: Optional sequence of evaluators to apply to all cases in the dataset.\n        \"\"\"\n        case_names = set[str]()\n        for case in cases:\n            if case.name is None:\n                continue\n            if case.name in case_names:\n                raise ValueError(f'Duplicate case name: {case.name!r}')\n            case_names.add(case.name)\n\n        super().__init__(\n            name=name,\n            cases=cases,\n            evaluators=list(evaluators),\n        )\n\n    # TODO in v2: Make everything not required keyword-only\n    async def evaluate(\n        self,\n        task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],\n        name: str | None = None,\n        max_concurrency: int | None = None,\n        progress: bool = True,\n        retry_task: RetryConfig | None = None,\n        retry_evaluators: RetryConfig | None = None,\n        *,\n        task_name: str | None = None,\n        metadata: dict[str, Any] | None = None,\n    ) -> EvaluationReport[InputsT, OutputT, MetadataT]:\n        \"\"\"Evaluates the test cases in the dataset using the given task.\n\n        This method runs the task on each case in the dataset, applies evaluators,\n        and collects results into a report. Cases are run concurrently, limited by `max_concurrency` if specified.\n\n        Args:\n            task: The task to evaluate. This should be a callable that takes the inputs of the case\n                and returns the output.\n            name: The name of the experiment being run, this is used to identify the experiment in the report.\n                If omitted, the task_name will be used; if that is not specified, the name of the task function is used.\n            max_concurrency: The maximum number of concurrent evaluations of the task to allow.\n                If None, all cases will be evaluated concurrently.\n            progress: Whether to show a progress bar for the evaluation. Defaults to `True`.\n            retry_task: Optional retry configuration for the task execution.\n            retry_evaluators: Optional retry configuration for evaluator execution.\n            task_name: Optional override to the name of the task being executed, otherwise the name of the task\n                function will be used.\n            metadata: Optional dict of experiment metadata.\n\n        Returns:\n            A report containing the results of the evaluation.\n        \"\"\"\n        task_name = task_name or get_unwrapped_function_name(task)\n        name = name or task_name\n        total_cases = len(self.cases)\n        progress_bar = Progress() if progress else None\n\n        limiter = anyio.Semaphore(max_concurrency) if max_concurrency is not None else AsyncExitStack()\n\n        extra_attributes: dict[str, Any] = {'gen_ai.operation.name': 'experiment'}\n        if metadata is not None:\n            extra_attributes['metadata'] = metadata\n        with (\n            logfire_span(\n                'evaluate {name}',\n                name=name,\n                task_name=task_name,\n                dataset_name=self.name,\n                n_cases=len(self.cases),\n                **extra_attributes,\n            ) as eval_span,\n            progress_bar or nullcontext(),\n        ):\n            task_id = progress_bar.add_task(f'Evaluating {task_name}', total=total_cases) if progress_bar else None\n\n            async def _handle_case(case: Case[InputsT, OutputT, MetadataT], report_case_name: str):\n                async with limiter:\n                    result = await _run_task_and_evaluators(\n                        task, case, report_case_name, self.evaluators, retry_task, retry_evaluators\n                    )\n                    if progress_bar and task_id is not None:  # pragma: no branch\n                        progress_bar.update(task_id, advance=1)\n                    return result\n\n            if (context := eval_span.context) is None:  # pragma: no cover\n                trace_id = None\n                span_id = None\n            else:\n                trace_id = f'{context.trace_id:032x}'\n                span_id = f'{context.span_id:016x}'\n            cases_and_failures = await task_group_gather(\n                [\n                    lambda case=case, i=i: _handle_case(case, case.name or f'Case {i}')\n                    for i, case in enumerate(self.cases, 1)\n                ]\n            )\n            cases: list[ReportCase] = []\n            failures: list[ReportCaseFailure] = []\n            for item in cases_and_failures:\n                if isinstance(item, ReportCase):\n                    cases.append(item)\n                else:\n                    failures.append(item)\n            report = EvaluationReport(\n                name=name,\n                cases=cases,\n                failures=failures,\n                experiment_metadata=metadata,\n                span_id=span_id,\n                trace_id=trace_id,\n            )\n            full_experiment_metadata: dict[str, Any] = {'n_cases': len(self.cases)}\n            if metadata is not None:\n                full_experiment_metadata['metadata'] = metadata\n            if (averages := report.averages()) is not None:\n                full_experiment_metadata['averages'] = averages\n                if averages.assertions is not None:\n                    eval_span.set_attribute('assertion_pass_rate', averages.assertions)\n            eval_span.set_attribute('logfire.experiment.metadata', full_experiment_metadata)\n        return report\n\n    def evaluate_sync(\n        self,\n        task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],\n        name: str | None = None,\n        max_concurrency: int | None = None,\n        progress: bool = True,\n        retry_task: RetryConfig | None = None,\n        retry_evaluators: RetryConfig | None = None,\n        *,\n        task_name: str | None = None,\n        metadata: dict[str, Any] | None = None,\n    ) -> EvaluationReport[InputsT, OutputT, MetadataT]:\n        \"\"\"Evaluates the test cases in the dataset using the given task.\n\n        This is a synchronous wrapper around [`evaluate`][pydantic_evals.Dataset.evaluate] provided for convenience.\n\n        Args:\n            task: The task to evaluate. This should be a callable that takes the inputs of the case\n                and returns the output.\n            name: The name of the experiment being run, this is used to identify the experiment in the report.\n                If omitted, the task_name will be used; if that is not specified, the name of the task function is used.\n            max_concurrency: The maximum number of concurrent evaluations of the task to allow.\n                If None, all cases will be evaluated concurrently.\n            progress: Whether to show a progress bar for the evaluation. Defaults to `True`.\n            retry_task: Optional retry configuration for the task execution.\n            retry_evaluators: Optional retry configuration for evaluator execution.\n            task_name: Optional override to the name of the task being executed, otherwise the name of the task\n                function will be used.\n            metadata: Optional dict of experiment metadata.\n\n        Returns:\n            A report containing the results of the evaluation.\n        \"\"\"\n        return get_event_loop().run_until_complete(\n            self.evaluate(\n                task,\n                name=name,\n                max_concurrency=max_concurrency,\n                progress=progress,\n                retry_task=retry_task,\n                retry_evaluators=retry_evaluators,\n                task_name=task_name,\n                metadata=metadata,\n            )\n        )\n\n    def add_case(\n        self,\n        *,\n        name: str | None = None,\n        inputs: InputsT,\n        metadata: MetadataT | None = None,\n        expected_output: OutputT | None = None,\n        evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (),\n    ) -> None:\n        \"\"\"Adds a case to the dataset.\n\n        This is a convenience method for creating a [`Case`][pydantic_evals.Case] and adding it to the dataset.\n\n        Args:\n            name: Optional name for the case. If not provided, a generic name will be assigned.\n            inputs: The inputs to the task being evaluated.\n            metadata: Optional metadata for the case, which can be used by evaluators.\n            expected_output: The expected output of the task, used for comparison in evaluators.\n            evaluators: Tuple of evaluators specific to this case, in addition to dataset-level evaluators.\n        \"\"\"\n        if name in {case.name for case in self.cases}:\n            raise ValueError(f'Duplicate case name: {name!r}')\n\n        case = Case[InputsT, OutputT, MetadataT](\n            name=name,\n            inputs=inputs,\n            metadata=metadata,\n            expected_output=expected_output,\n            evaluators=evaluators,\n        )\n        self.cases.append(case)\n\n    def add_evaluator(\n        self,\n        evaluator: Evaluator[InputsT, OutputT, MetadataT],\n        specific_case: str | None = None,\n    ) -> None:\n        \"\"\"Adds an evaluator to the dataset or a specific case.\n\n        Args:\n            evaluator: The evaluator to add.\n            specific_case: If provided, the evaluator will only be added to the case with this name.\n                If None, the evaluator will be added to all cases in the dataset.\n\n        Raises:\n            ValueError: If `specific_case` is provided but no case with that name exists in the dataset.\n        \"\"\"\n        if specific_case is None:\n            self.evaluators.append(evaluator)\n        else:\n            # If this is too slow, we could try to add a case lookup dict.\n            # Note that if we do that, we'd need to make the cases list private to prevent modification.\n            added = False\n            for case in self.cases:\n                if case.name == specific_case:\n                    case.evaluators.append(evaluator)\n                    added = True\n            if not added:\n                raise ValueError(f'Case {specific_case!r} not found in the dataset')\n\n    @classmethod\n    @functools.cache\n    def _params(cls) -> tuple[type[InputsT], type[OutputT], type[MetadataT]]:\n        \"\"\"Get the type parameters for the Dataset class.\n\n        Returns:\n            A tuple of (InputsT, OutputT, MetadataT) types.\n        \"\"\"\n        for c in cls.__mro__:\n            metadata = getattr(c, '__pydantic_generic_metadata__', {})\n            if len(args := (metadata.get('args', ()) or getattr(c, '__args__', ()))) == 3:  # pragma: no branch\n                return args\n        else:  # pragma: no cover\n            warnings.warn(\n                f'Could not determine the generic parameters for {cls}; using `Any` for each.'\n                f' You should explicitly set the generic parameters via `Dataset[MyInputs, MyOutput, MyMetadata]`'\n                f' when serializing or deserializing.',\n                UserWarning,\n            )\n            return Any, Any, Any  # type: ignore\n\n    @classmethod\n    def from_file(\n        cls,\n        path: Path | str,\n        fmt: Literal['yaml', 'json'] | None = None,\n        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n    ) -> Self:\n        \"\"\"Load a dataset from a file.\n\n        Args:\n            path: Path to the file to load.\n            fmt: Format of the file. If None, the format will be inferred from the file extension.\n                Must be either 'yaml' or 'json'.\n            custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.\n                These are additional evaluators beyond the default ones.\n\n        Returns:\n            A new Dataset instance loaded from the file.\n\n        Raises:\n            ValidationError: If the file cannot be parsed as a valid dataset.\n            ValueError: If the format cannot be inferred from the file extension.\n        \"\"\"\n        path = Path(path)\n        fmt = cls._infer_fmt(path, fmt)\n\n        raw = Path(path).read_text(encoding='utf-8')\n        try:\n            return cls.from_text(raw, fmt=fmt, custom_evaluator_types=custom_evaluator_types, default_name=path.stem)\n        except ValidationError as e:  # pragma: no cover\n            raise ValueError(f'{path} contains data that does not match the schema for {cls.__name__}:\\n{e}.') from e\n\n    @classmethod\n    def from_text(\n        cls,\n        contents: str,\n        fmt: Literal['yaml', 'json'] = 'yaml',\n        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n        *,\n        default_name: str | None = None,\n    ) -> Self:\n        \"\"\"Load a dataset from a string.\n\n        Args:\n            contents: The string content to parse.\n            fmt: Format of the content. Must be either 'yaml' or 'json'.\n            custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.\n                These are additional evaluators beyond the default ones.\n            default_name: Default name of the dataset, to be used if not specified in the serialized contents.\n\n        Returns:\n            A new Dataset instance parsed from the string.\n\n        Raises:\n            ValidationError: If the content cannot be parsed as a valid dataset.\n        \"\"\"\n        if fmt == 'yaml':\n            loaded = yaml.safe_load(contents)\n            return cls.from_dict(loaded, custom_evaluator_types, default_name=default_name)\n        else:\n            dataset_model_type = cls._serialization_type()\n            dataset_model = dataset_model_type.model_validate_json(contents)\n            return cls._from_dataset_model(dataset_model, custom_evaluator_types, default_name)\n\n    @classmethod\n    def from_dict(\n        cls,\n        data: dict[str, Any],\n        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n        *,\n        default_name: str | None = None,\n    ) -> Self:\n        \"\"\"Load a dataset from a dictionary.\n\n        Args:\n            data: Dictionary representation of the dataset.\n            custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.\n                These are additional evaluators beyond the default ones.\n            default_name: Default name of the dataset, to be used if not specified in the data.\n\n        Returns:\n            A new Dataset instance created from the dictionary.\n\n        Raises:\n            ValidationError: If the dictionary cannot be converted to a valid dataset.\n        \"\"\"\n        dataset_model_type = cls._serialization_type()\n        dataset_model = dataset_model_type.model_validate(data)\n        return cls._from_dataset_model(dataset_model, custom_evaluator_types, default_name)\n\n    @classmethod\n    def _from_dataset_model(\n        cls,\n        dataset_model: _DatasetModel[InputsT, OutputT, MetadataT],\n        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n        default_name: str | None = None,\n    ) -> Self:\n        \"\"\"Create a Dataset from a _DatasetModel.\n\n        Args:\n            dataset_model: The _DatasetModel to convert.\n            custom_evaluator_types: Custom evaluator classes to register for deserialization.\n            default_name: Default name of the dataset, to be used if the value is `None` in the provided model.\n\n        Returns:\n            A new Dataset instance created from the _DatasetModel.\n        \"\"\"\n        registry = _get_registry(custom_evaluator_types)\n\n        cases: list[Case[InputsT, OutputT, MetadataT]] = []\n        errors: list[ValueError] = []\n        dataset_evaluators: list[Evaluator] = []\n        for spec in dataset_model.evaluators:\n            try:\n                dataset_evaluator = _load_evaluator_from_registry(registry, None, spec)\n            except ValueError as e:\n                errors.append(e)\n                continue\n            dataset_evaluators.append(dataset_evaluator)\n\n        for row in dataset_model.cases:\n            evaluators: list[Evaluator] = []\n            for spec in row.evaluators:\n                try:\n                    evaluator = _load_evaluator_from_registry(registry, row.name, spec)\n                except ValueError as e:\n                    errors.append(e)\n                    continue\n                evaluators.append(evaluator)\n            row = Case[InputsT, OutputT, MetadataT](\n                name=row.name,\n                inputs=row.inputs,\n                metadata=row.metadata,\n                expected_output=row.expected_output,\n            )\n            row.evaluators = evaluators\n            cases.append(row)\n        if errors:\n            raise ExceptionGroup(f'{len(errors)} error(s) loading evaluators from registry', errors[:3])\n        result = cls(name=dataset_model.name, cases=cases)\n        if result.name is None:\n            result.name = default_name\n        result.evaluators = dataset_evaluators\n        return result\n\n    def to_file(\n        self,\n        path: Path | str,\n        fmt: Literal['yaml', 'json'] | None = None,\n        schema_path: Path | str | None = DEFAULT_SCHEMA_PATH_TEMPLATE,\n        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n    ):\n        \"\"\"Save the dataset to a file.\n\n        Args:\n            path: Path to save the dataset to.\n            fmt: Format to use. If None, the format will be inferred from the file extension.\n                Must be either 'yaml' or 'json'.\n            schema_path: Path to save the JSON schema to. If None, no schema will be saved.\n                Can be a string template with {stem} which will be replaced with the dataset filename stem.\n            custom_evaluator_types: Custom evaluator classes to include in the schema.\n        \"\"\"\n        path = Path(path)\n        fmt = self._infer_fmt(path, fmt)\n\n        schema_ref: str | None = None\n        if schema_path is not None:  # pragma: no branch\n            if isinstance(schema_path, str):  # pragma: no branch\n                schema_path = Path(schema_path.format(stem=path.stem))\n\n            if not schema_path.is_absolute():\n                schema_ref = str(schema_path)\n                schema_path = path.parent / schema_path\n            elif schema_path.is_relative_to(path):  # pragma: no cover\n                schema_ref = str(_get_relative_path_reference(schema_path, path))\n            else:  # pragma: no cover\n                schema_ref = str(schema_path)\n            self._save_schema(schema_path, custom_evaluator_types)\n\n        context: dict[str, Any] = {'use_short_form': True}\n        if fmt == 'yaml':\n            dumped_data = self.model_dump(mode='json', by_alias=True, context=context)\n            content = yaml.dump(dumped_data, sort_keys=False)\n            if schema_ref:  # pragma: no branch\n                yaml_language_server_line = f'{_YAML_SCHEMA_LINE_PREFIX}{schema_ref}'\n                content = f'{yaml_language_server_line}\\n{content}'\n            path.write_text(content, encoding='utf-8')\n        else:\n            context['$schema'] = schema_ref\n            json_data = self.model_dump_json(indent=2, by_alias=True, context=context)\n            path.write_text(json_data + '\\n', encoding='utf-8')\n\n    @classmethod\n    def model_json_schema_with_evaluators(\n        cls,\n        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n    ) -> dict[str, Any]:\n        \"\"\"Generate a JSON schema for this dataset type, including evaluator details.\n\n        This is useful for generating a schema that can be used to validate YAML-format dataset files.\n\n        Args:\n            custom_evaluator_types: Custom evaluator classes to include in the schema.\n\n        Returns:\n            A dictionary representing the JSON schema.\n        \"\"\"\n        # Note: this function could maybe be simplified now that Evaluators are always dataclasses\n        registry = _get_registry(custom_evaluator_types)\n\n        evaluator_schema_types: list[Any] = []\n        for name, evaluator_class in registry.items():\n            type_hints = _typing_extra.get_function_type_hints(evaluator_class)\n            type_hints.pop('return', None)\n            required_type_hints: dict[str, Any] = {}\n\n            for p in inspect.signature(evaluator_class).parameters.values():\n                type_hints.setdefault(p.name, Any)\n                if p.default is not p.empty:\n                    type_hints[p.name] = NotRequired[type_hints[p.name]]\n                else:\n                    required_type_hints[p.name] = type_hints[p.name]\n\n            def _make_typed_dict(cls_name_prefix: str, fields: dict[str, Any]) -> Any:\n                td = TypedDict(f'{cls_name_prefix}_{name}', fields)  # pyright: ignore[reportArgumentType]\n                config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n                # TODO: Replace with pydantic.with_config once pydantic 2.11 is the min supported version\n                td.__pydantic_config__ = config  # pyright: ignore[reportAttributeAccessIssue]\n                return td\n\n            # Shortest form: just the call name\n            if len(type_hints) == 0 or not required_type_hints:\n                evaluator_schema_types.append(Literal[name])\n\n            # Short form: can be called with only one parameter\n            if len(type_hints) == 1:\n                [type_hint_type] = type_hints.values()\n                evaluator_schema_types.append(_make_typed_dict('short_evaluator', {name: type_hint_type}))\n            elif len(required_type_hints) == 1:  # pragma: no branch\n                [type_hint_type] = required_type_hints.values()\n                evaluator_schema_types.append(_make_typed_dict('short_evaluator', {name: type_hint_type}))\n\n            # Long form: multiple parameters, possibly required\n            if len(type_hints) > 1:\n                params_td = _make_typed_dict('evaluator_params', type_hints)\n                evaluator_schema_types.append(_make_typed_dict('evaluator', {name: params_td}))\n\n        in_type, out_type, meta_type = cls._params()\n\n        # Note: we shadow the `Case` and `Dataset` class names here to generate a clean JSON schema\n        class Case(BaseModel, extra='forbid'):  # pyright: ignore[reportUnusedClass]  # this _is_ used below, but pyright doesn't seem to notice..\n            name: str | None = None\n            inputs: in_type  # pyright: ignore[reportInvalidTypeForm]\n            metadata: meta_type | None = None  # pyright: ignore[reportInvalidTypeForm,reportUnknownVariableType]\n            expected_output: out_type | None = None  # pyright: ignore[reportInvalidTypeForm,reportUnknownVariableType]\n            if evaluator_schema_types:  # pragma: no branch\n                evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007\n\n        class Dataset(BaseModel, extra='forbid'):\n            name: str | None = None\n            cases: list[Case]\n            if evaluator_schema_types:  # pragma: no branch\n                evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007\n\n        json_schema = Dataset.model_json_schema()\n        # See `_add_json_schema` below, since `$schema` is added to the JSON, it has to be supported in the JSON\n        json_schema['properties']['$schema'] = {'type': 'string'}\n        return json_schema\n\n    @classmethod\n    def _save_schema(\n        cls, path: Path | str, custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = ()\n    ):\n        \"\"\"Save the JSON schema for this dataset type to a file.\n\n        Args:\n            path: Path to save the schema to.\n            custom_evaluator_types: Custom evaluator classes to include in the schema.\n        \"\"\"\n        path = Path(path)\n        json_schema = cls.model_json_schema_with_evaluators(custom_evaluator_types)\n        schema_content = to_json(json_schema, indent=2).decode() + '\\n'\n        if not path.exists() or path.read_text(encoding='utf-8') != schema_content:  # pragma: no branch\n            path.write_text(schema_content, encoding='utf-8')\n\n    @classmethod\n    @functools.cache\n    def _serialization_type(cls) -> type[_DatasetModel[InputsT, OutputT, MetadataT]]:\n        \"\"\"Get the serialization type for this dataset class.\n\n        Returns:\n            A _DatasetModel type with the same generic parameters as this Dataset class.\n        \"\"\"\n        input_type, output_type, metadata_type = cls._params()\n        return _DatasetModel[input_type, output_type, metadata_type]\n\n    @classmethod\n    def _infer_fmt(cls, path: Path, fmt: Literal['yaml', 'json'] | None) -> Literal['yaml', 'json']:\n        \"\"\"Infer the format to use for a file based on its extension.\n\n        Args:\n            path: The path to infer the format for.\n            fmt: The explicitly provided format, if any.\n\n        Returns:\n            The inferred format ('yaml' or 'json').\n\n        Raises:\n            ValueError: If the format cannot be inferred from the file extension.\n        \"\"\"\n        if fmt is not None:\n            return fmt\n        suffix = path.suffix.lower()\n        if suffix in {'.yaml', '.yml'}:\n            return 'yaml'\n        elif suffix == '.json':\n            return 'json'\n        raise ValueError(\n            f'Could not infer format for filename {path.name!r}. Use the `fmt` argument to specify the format.'\n        )\n\n    @model_serializer(mode='wrap')\n    def _add_json_schema(self, nxt: SerializerFunctionWrapHandler, info: SerializationInfo) -> dict[str, Any]:\n        \"\"\"Add the JSON schema path to the serialized output.\n\n        See <https://github.com/json-schema-org/json-schema-spec/issues/828> for context, that seems to be the nearest\n        there is to a spec for this.\n        \"\"\"\n        context = cast(dict[str, Any] | None, info.context)\n        if isinstance(context, dict) and (schema := context.get('$schema')):\n            return {'$schema': schema} | nxt(self)\n        else:\n            return nxt(self)\n\n````\n\n#### name\n\n```python\nname: str | None = None\n\n```\n\nOptional name of the dataset.\n\n#### cases\n\n```python\ncases: list[Case[InputsT, OutputT, MetadataT]]\n\n```\n\nList of test cases in the dataset.\n\n#### evaluators\n\n```python\nevaluators: list[Evaluator[InputsT, OutputT, MetadataT]] = (\n    []\n)\n\n```\n\nList of evaluators to be used on all cases in the dataset.\n\n#### __init__\n\n```python\n__init__(\n    *,\n    name: str | None = None,\n    cases: Sequence[Case[InputsT, OutputT, MetadataT]],\n    evaluators: Sequence[\n        Evaluator[InputsT, OutputT, MetadataT]\n    ] = ()\n)\n\n```\n\nInitialize a new dataset with test cases and optional evaluators.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str | None` | Optional name for the dataset. | `None` | | `cases` | `Sequence[Case[InputsT, OutputT, MetadataT]]` | Sequence of test cases to include in the dataset. | *required* | | `evaluators` | `Sequence[Evaluator[InputsT, OutputT, MetadataT]]` | Optional sequence of evaluators to apply to all cases in the dataset. | `()` |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\ndef __init__(\n    self,\n    *,\n    name: str | None = None,\n    cases: Sequence[Case[InputsT, OutputT, MetadataT]],\n    evaluators: Sequence[Evaluator[InputsT, OutputT, MetadataT]] = (),\n):\n    \"\"\"Initialize a new dataset with test cases and optional evaluators.\n\n    Args:\n        name: Optional name for the dataset.\n        cases: Sequence of test cases to include in the dataset.\n        evaluators: Optional sequence of evaluators to apply to all cases in the dataset.\n    \"\"\"\n    case_names = set[str]()\n    for case in cases:\n        if case.name is None:\n            continue\n        if case.name in case_names:\n            raise ValueError(f'Duplicate case name: {case.name!r}')\n        case_names.add(case.name)\n\n    super().__init__(\n        name=name,\n        cases=cases,\n        evaluators=list(evaluators),\n    )\n\n```\n\n#### evaluate\n\n```python\nevaluate(\n    task: (\n        Callable[[InputsT], Awaitable[OutputT]]\n        | Callable[[InputsT], OutputT]\n    ),\n    name: str | None = None,\n    max_concurrency: int | None = None,\n    progress: bool = True,\n    retry_task: RetryConfig | None = None,\n    retry_evaluators: RetryConfig | None = None,\n    *,\n    task_name: str | None = None,\n    metadata: dict[str, Any] | None = None\n) -> EvaluationReport[InputsT, OutputT, MetadataT]\n\n```\n\nEvaluates the test cases in the dataset using the given task.\n\nThis method runs the task on each case in the dataset, applies evaluators, and collects results into a report. Cases are run concurrently, limited by `max_concurrency` if specified.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `task` | `Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT]` | The task to evaluate. This should be a callable that takes the inputs of the case and returns the output. | *required* | | `name` | `str | None` | The name of the experiment being run, this is used to identify the experiment in the report. If omitted, the task_name will be used; if that is not specified, the name of the task function is used. | `None` | | `max_concurrency` | `int | None` | The maximum number of concurrent evaluations of the task to allow. If None, all cases will be evaluated concurrently. | `None` | | `progress` | `bool` | Whether to show a progress bar for the evaluation. Defaults to True. | `True` | | `retry_task` | `RetryConfig | None` | Optional retry configuration for the task execution. | `None` | | `retry_evaluators` | `RetryConfig | None` | Optional retry configuration for evaluator execution. | `None` | | `task_name` | `str | None` | Optional override to the name of the task being executed, otherwise the name of the task function will be used. | `None` | | `metadata` | `dict[str, Any] | None` | Optional dict of experiment metadata. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `EvaluationReport[InputsT, OutputT, MetadataT]` | A report containing the results of the evaluation. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\nasync def evaluate(\n    self,\n    task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],\n    name: str | None = None,\n    max_concurrency: int | None = None,\n    progress: bool = True,\n    retry_task: RetryConfig | None = None,\n    retry_evaluators: RetryConfig | None = None,\n    *,\n    task_name: str | None = None,\n    metadata: dict[str, Any] | None = None,\n) -> EvaluationReport[InputsT, OutputT, MetadataT]:\n    \"\"\"Evaluates the test cases in the dataset using the given task.\n\n    This method runs the task on each case in the dataset, applies evaluators,\n    and collects results into a report. Cases are run concurrently, limited by `max_concurrency` if specified.\n\n    Args:\n        task: The task to evaluate. This should be a callable that takes the inputs of the case\n            and returns the output.\n        name: The name of the experiment being run, this is used to identify the experiment in the report.\n            If omitted, the task_name will be used; if that is not specified, the name of the task function is used.\n        max_concurrency: The maximum number of concurrent evaluations of the task to allow.\n            If None, all cases will be evaluated concurrently.\n        progress: Whether to show a progress bar for the evaluation. Defaults to `True`.\n        retry_task: Optional retry configuration for the task execution.\n        retry_evaluators: Optional retry configuration for evaluator execution.\n        task_name: Optional override to the name of the task being executed, otherwise the name of the task\n            function will be used.\n        metadata: Optional dict of experiment metadata.\n\n    Returns:\n        A report containing the results of the evaluation.\n    \"\"\"\n    task_name = task_name or get_unwrapped_function_name(task)\n    name = name or task_name\n    total_cases = len(self.cases)\n    progress_bar = Progress() if progress else None\n\n    limiter = anyio.Semaphore(max_concurrency) if max_concurrency is not None else AsyncExitStack()\n\n    extra_attributes: dict[str, Any] = {'gen_ai.operation.name': 'experiment'}\n    if metadata is not None:\n        extra_attributes['metadata'] = metadata\n    with (\n        logfire_span(\n            'evaluate {name}',\n            name=name,\n            task_name=task_name,\n            dataset_name=self.name,\n            n_cases=len(self.cases),\n            **extra_attributes,\n        ) as eval_span,\n        progress_bar or nullcontext(),\n    ):\n        task_id = progress_bar.add_task(f'Evaluating {task_name}', total=total_cases) if progress_bar else None\n\n        async def _handle_case(case: Case[InputsT, OutputT, MetadataT], report_case_name: str):\n            async with limiter:\n                result = await _run_task_and_evaluators(\n                    task, case, report_case_name, self.evaluators, retry_task, retry_evaluators\n                )\n                if progress_bar and task_id is not None:  # pragma: no branch\n                    progress_bar.update(task_id, advance=1)\n                return result\n\n        if (context := eval_span.context) is None:  # pragma: no cover\n            trace_id = None\n            span_id = None\n        else:\n            trace_id = f'{context.trace_id:032x}'\n            span_id = f'{context.span_id:016x}'\n        cases_and_failures = await task_group_gather(\n            [\n                lambda case=case, i=i: _handle_case(case, case.name or f'Case {i}')\n                for i, case in enumerate(self.cases, 1)\n            ]\n        )\n        cases: list[ReportCase] = []\n        failures: list[ReportCaseFailure] = []\n        for item in cases_and_failures:\n            if isinstance(item, ReportCase):\n                cases.append(item)\n            else:\n                failures.append(item)\n        report = EvaluationReport(\n            name=name,\n            cases=cases,\n            failures=failures,\n            experiment_metadata=metadata,\n            span_id=span_id,\n            trace_id=trace_id,\n        )\n        full_experiment_metadata: dict[str, Any] = {'n_cases': len(self.cases)}\n        if metadata is not None:\n            full_experiment_metadata['metadata'] = metadata\n        if (averages := report.averages()) is not None:\n            full_experiment_metadata['averages'] = averages\n            if averages.assertions is not None:\n                eval_span.set_attribute('assertion_pass_rate', averages.assertions)\n        eval_span.set_attribute('logfire.experiment.metadata', full_experiment_metadata)\n    return report\n\n```\n\n#### evaluate_sync\n\n```python\nevaluate_sync(\n    task: (\n        Callable[[InputsT], Awaitable[OutputT]]\n        | Callable[[InputsT], OutputT]\n    ),\n    name: str | None = None,\n    max_concurrency: int | None = None,\n    progress: bool = True,\n    retry_task: RetryConfig | None = None,\n    retry_evaluators: RetryConfig | None = None,\n    *,\n    task_name: str | None = None,\n    metadata: dict[str, Any] | None = None\n) -> EvaluationReport[InputsT, OutputT, MetadataT]\n\n```\n\nEvaluates the test cases in the dataset using the given task.\n\nThis is a synchronous wrapper around evaluate provided for convenience.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `task` | `Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT]` | The task to evaluate. This should be a callable that takes the inputs of the case and returns the output. | *required* | | `name` | `str | None` | The name of the experiment being run, this is used to identify the experiment in the report. If omitted, the task_name will be used; if that is not specified, the name of the task function is used. | `None` | | `max_concurrency` | `int | None` | The maximum number of concurrent evaluations of the task to allow. If None, all cases will be evaluated concurrently. | `None` | | `progress` | `bool` | Whether to show a progress bar for the evaluation. Defaults to True. | `True` | | `retry_task` | `RetryConfig | None` | Optional retry configuration for the task execution. | `None` | | `retry_evaluators` | `RetryConfig | None` | Optional retry configuration for evaluator execution. | `None` | | `task_name` | `str | None` | Optional override to the name of the task being executed, otherwise the name of the task function will be used. | `None` | | `metadata` | `dict[str, Any] | None` | Optional dict of experiment metadata. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `EvaluationReport[InputsT, OutputT, MetadataT]` | A report containing the results of the evaluation. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\ndef evaluate_sync(\n    self,\n    task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],\n    name: str | None = None,\n    max_concurrency: int | None = None,\n    progress: bool = True,\n    retry_task: RetryConfig | None = None,\n    retry_evaluators: RetryConfig | None = None,\n    *,\n    task_name: str | None = None,\n    metadata: dict[str, Any] | None = None,\n) -> EvaluationReport[InputsT, OutputT, MetadataT]:\n    \"\"\"Evaluates the test cases in the dataset using the given task.\n\n    This is a synchronous wrapper around [`evaluate`][pydantic_evals.Dataset.evaluate] provided for convenience.\n\n    Args:\n        task: The task to evaluate. This should be a callable that takes the inputs of the case\n            and returns the output.\n        name: The name of the experiment being run, this is used to identify the experiment in the report.\n            If omitted, the task_name will be used; if that is not specified, the name of the task function is used.\n        max_concurrency: The maximum number of concurrent evaluations of the task to allow.\n            If None, all cases will be evaluated concurrently.\n        progress: Whether to show a progress bar for the evaluation. Defaults to `True`.\n        retry_task: Optional retry configuration for the task execution.\n        retry_evaluators: Optional retry configuration for evaluator execution.\n        task_name: Optional override to the name of the task being executed, otherwise the name of the task\n            function will be used.\n        metadata: Optional dict of experiment metadata.\n\n    Returns:\n        A report containing the results of the evaluation.\n    \"\"\"\n    return get_event_loop().run_until_complete(\n        self.evaluate(\n            task,\n            name=name,\n            max_concurrency=max_concurrency,\n            progress=progress,\n            retry_task=retry_task,\n            retry_evaluators=retry_evaluators,\n            task_name=task_name,\n            metadata=metadata,\n        )\n    )\n\n```\n\n#### add_case\n\n```python\nadd_case(\n    *,\n    name: str | None = None,\n    inputs: InputsT,\n    metadata: MetadataT | None = None,\n    expected_output: OutputT | None = None,\n    evaluators: tuple[\n        Evaluator[InputsT, OutputT, MetadataT], ...\n    ] = ()\n) -> None\n\n```\n\nAdds a case to the dataset.\n\nThis is a convenience method for creating a Case and adding it to the dataset.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str | None` | Optional name for the case. If not provided, a generic name will be assigned. | `None` | | `inputs` | `InputsT` | The inputs to the task being evaluated. | *required* | | `metadata` | `MetadataT | None` | Optional metadata for the case, which can be used by evaluators. | `None` | | `expected_output` | `OutputT | None` | The expected output of the task, used for comparison in evaluators. | `None` | | `evaluators` | `tuple[Evaluator[InputsT, OutputT, MetadataT], ...]` | Tuple of evaluators specific to this case, in addition to dataset-level evaluators. | `()` |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\ndef add_case(\n    self,\n    *,\n    name: str | None = None,\n    inputs: InputsT,\n    metadata: MetadataT | None = None,\n    expected_output: OutputT | None = None,\n    evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (),\n) -> None:\n    \"\"\"Adds a case to the dataset.\n\n    This is a convenience method for creating a [`Case`][pydantic_evals.Case] and adding it to the dataset.\n\n    Args:\n        name: Optional name for the case. If not provided, a generic name will be assigned.\n        inputs: The inputs to the task being evaluated.\n        metadata: Optional metadata for the case, which can be used by evaluators.\n        expected_output: The expected output of the task, used for comparison in evaluators.\n        evaluators: Tuple of evaluators specific to this case, in addition to dataset-level evaluators.\n    \"\"\"\n    if name in {case.name for case in self.cases}:\n        raise ValueError(f'Duplicate case name: {name!r}')\n\n    case = Case[InputsT, OutputT, MetadataT](\n        name=name,\n        inputs=inputs,\n        metadata=metadata,\n        expected_output=expected_output,\n        evaluators=evaluators,\n    )\n    self.cases.append(case)\n\n```\n\n#### add_evaluator\n\n```python\nadd_evaluator(\n    evaluator: Evaluator[InputsT, OutputT, MetadataT],\n    specific_case: str | None = None,\n) -> None\n\n```\n\nAdds an evaluator to the dataset or a specific case.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `evaluator` | `Evaluator[InputsT, OutputT, MetadataT]` | The evaluator to add. | *required* | | `specific_case` | `str | None` | If provided, the evaluator will only be added to the case with this name. If None, the evaluator will be added to all cases in the dataset. | `None` |\n\nRaises:\n\n| Type | Description | | --- | --- | | `ValueError` | If specific_case is provided but no case with that name exists in the dataset. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\ndef add_evaluator(\n    self,\n    evaluator: Evaluator[InputsT, OutputT, MetadataT],\n    specific_case: str | None = None,\n) -> None:\n    \"\"\"Adds an evaluator to the dataset or a specific case.\n\n    Args:\n        evaluator: The evaluator to add.\n        specific_case: If provided, the evaluator will only be added to the case with this name.\n            If None, the evaluator will be added to all cases in the dataset.\n\n    Raises:\n        ValueError: If `specific_case` is provided but no case with that name exists in the dataset.\n    \"\"\"\n    if specific_case is None:\n        self.evaluators.append(evaluator)\n    else:\n        # If this is too slow, we could try to add a case lookup dict.\n        # Note that if we do that, we'd need to make the cases list private to prevent modification.\n        added = False\n        for case in self.cases:\n            if case.name == specific_case:\n                case.evaluators.append(evaluator)\n                added = True\n        if not added:\n            raise ValueError(f'Case {specific_case!r} not found in the dataset')\n\n```\n\n#### from_file\n\n```python\nfrom_file(\n    path: Path | str,\n    fmt: Literal[\"yaml\", \"json\"] | None = None,\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n) -> Self\n\n```\n\nLoad a dataset from a file.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `path` | `Path | str` | Path to the file to load. | *required* | | `fmt` | `Literal['yaml', 'json'] | None` | Format of the file. If None, the format will be inferred from the file extension. Must be either 'yaml' or 'json'. | `None` | | `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to use when deserializing the dataset. These are additional evaluators beyond the default ones. | `()` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `Self` | A new Dataset instance loaded from the file. |\n\nRaises:\n\n| Type | Description | | --- | --- | | `ValidationError` | If the file cannot be parsed as a valid dataset. | | `ValueError` | If the format cannot be inferred from the file extension. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\n@classmethod\ndef from_file(\n    cls,\n    path: Path | str,\n    fmt: Literal['yaml', 'json'] | None = None,\n    custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n) -> Self:\n    \"\"\"Load a dataset from a file.\n\n    Args:\n        path: Path to the file to load.\n        fmt: Format of the file. If None, the format will be inferred from the file extension.\n            Must be either 'yaml' or 'json'.\n        custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.\n            These are additional evaluators beyond the default ones.\n\n    Returns:\n        A new Dataset instance loaded from the file.\n\n    Raises:\n        ValidationError: If the file cannot be parsed as a valid dataset.\n        ValueError: If the format cannot be inferred from the file extension.\n    \"\"\"\n    path = Path(path)\n    fmt = cls._infer_fmt(path, fmt)\n\n    raw = Path(path).read_text(encoding='utf-8')\n    try:\n        return cls.from_text(raw, fmt=fmt, custom_evaluator_types=custom_evaluator_types, default_name=path.stem)\n    except ValidationError as e:  # pragma: no cover\n        raise ValueError(f'{path} contains data that does not match the schema for {cls.__name__}:\\n{e}.') from e\n\n```\n\n#### from_text\n\n```python\nfrom_text(\n    contents: str,\n    fmt: Literal[\"yaml\", \"json\"] = \"yaml\",\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n    *,\n    default_name: str | None = None\n) -> Self\n\n```\n\nLoad a dataset from a string.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `contents` | `str` | The string content to parse. | *required* | | `fmt` | `Literal['yaml', 'json']` | Format of the content. Must be either 'yaml' or 'json'. | `'yaml'` | | `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to use when deserializing the dataset. These are additional evaluators beyond the default ones. | `()` | | `default_name` | `str | None` | Default name of the dataset, to be used if not specified in the serialized contents. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `Self` | A new Dataset instance parsed from the string. |\n\nRaises:\n\n| Type | Description | | --- | --- | | `ValidationError` | If the content cannot be parsed as a valid dataset. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\n@classmethod\ndef from_text(\n    cls,\n    contents: str,\n    fmt: Literal['yaml', 'json'] = 'yaml',\n    custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n    *,\n    default_name: str | None = None,\n) -> Self:\n    \"\"\"Load a dataset from a string.\n\n    Args:\n        contents: The string content to parse.\n        fmt: Format of the content. Must be either 'yaml' or 'json'.\n        custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.\n            These are additional evaluators beyond the default ones.\n        default_name: Default name of the dataset, to be used if not specified in the serialized contents.\n\n    Returns:\n        A new Dataset instance parsed from the string.\n\n    Raises:\n        ValidationError: If the content cannot be parsed as a valid dataset.\n    \"\"\"\n    if fmt == 'yaml':\n        loaded = yaml.safe_load(contents)\n        return cls.from_dict(loaded, custom_evaluator_types, default_name=default_name)\n    else:\n        dataset_model_type = cls._serialization_type()\n        dataset_model = dataset_model_type.model_validate_json(contents)\n        return cls._from_dataset_model(dataset_model, custom_evaluator_types, default_name)\n\n```\n\n#### from_dict\n\n```python\nfrom_dict(\n    data: dict[str, Any],\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n    *,\n    default_name: str | None = None\n) -> Self\n\n```\n\nLoad a dataset from a dictionary.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `data` | `dict[str, Any]` | Dictionary representation of the dataset. | *required* | | `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to use when deserializing the dataset. These are additional evaluators beyond the default ones. | `()` | | `default_name` | `str | None` | Default name of the dataset, to be used if not specified in the data. | `None` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `Self` | A new Dataset instance created from the dictionary. |\n\nRaises:\n\n| Type | Description | | --- | --- | | `ValidationError` | If the dictionary cannot be converted to a valid dataset. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\n@classmethod\ndef from_dict(\n    cls,\n    data: dict[str, Any],\n    custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n    *,\n    default_name: str | None = None,\n) -> Self:\n    \"\"\"Load a dataset from a dictionary.\n\n    Args:\n        data: Dictionary representation of the dataset.\n        custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.\n            These are additional evaluators beyond the default ones.\n        default_name: Default name of the dataset, to be used if not specified in the data.\n\n    Returns:\n        A new Dataset instance created from the dictionary.\n\n    Raises:\n        ValidationError: If the dictionary cannot be converted to a valid dataset.\n    \"\"\"\n    dataset_model_type = cls._serialization_type()\n    dataset_model = dataset_model_type.model_validate(data)\n    return cls._from_dataset_model(dataset_model, custom_evaluator_types, default_name)\n\n```\n\n#### to_file\n\n```python\nto_file(\n    path: Path | str,\n    fmt: Literal[\"yaml\", \"json\"] | None = None,\n    schema_path: (\n        Path | str | None\n    ) = DEFAULT_SCHEMA_PATH_TEMPLATE,\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n)\n\n```\n\nSave the dataset to a file.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `path` | `Path | str` | Path to save the dataset to. | *required* | | `fmt` | `Literal['yaml', 'json'] | None` | Format to use. If None, the format will be inferred from the file extension. Must be either 'yaml' or 'json'. | `None` | | `schema_path` | `Path | str | None` | Path to save the JSON schema to. If None, no schema will be saved. Can be a string template with {stem} which will be replaced with the dataset filename stem. | `DEFAULT_SCHEMA_PATH_TEMPLATE` | | `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to include in the schema. | `()` |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\ndef to_file(\n    self,\n    path: Path | str,\n    fmt: Literal['yaml', 'json'] | None = None,\n    schema_path: Path | str | None = DEFAULT_SCHEMA_PATH_TEMPLATE,\n    custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n):\n    \"\"\"Save the dataset to a file.\n\n    Args:\n        path: Path to save the dataset to.\n        fmt: Format to use. If None, the format will be inferred from the file extension.\n            Must be either 'yaml' or 'json'.\n        schema_path: Path to save the JSON schema to. If None, no schema will be saved.\n            Can be a string template with {stem} which will be replaced with the dataset filename stem.\n        custom_evaluator_types: Custom evaluator classes to include in the schema.\n    \"\"\"\n    path = Path(path)\n    fmt = self._infer_fmt(path, fmt)\n\n    schema_ref: str | None = None\n    if schema_path is not None:  # pragma: no branch\n        if isinstance(schema_path, str):  # pragma: no branch\n            schema_path = Path(schema_path.format(stem=path.stem))\n\n        if not schema_path.is_absolute():\n            schema_ref = str(schema_path)\n            schema_path = path.parent / schema_path\n        elif schema_path.is_relative_to(path):  # pragma: no cover\n            schema_ref = str(_get_relative_path_reference(schema_path, path))\n        else:  # pragma: no cover\n            schema_ref = str(schema_path)\n        self._save_schema(schema_path, custom_evaluator_types)\n\n    context: dict[str, Any] = {'use_short_form': True}\n    if fmt == 'yaml':\n        dumped_data = self.model_dump(mode='json', by_alias=True, context=context)\n        content = yaml.dump(dumped_data, sort_keys=False)\n        if schema_ref:  # pragma: no branch\n            yaml_language_server_line = f'{_YAML_SCHEMA_LINE_PREFIX}{schema_ref}'\n            content = f'{yaml_language_server_line}\\n{content}'\n        path.write_text(content, encoding='utf-8')\n    else:\n        context['$schema'] = schema_ref\n        json_data = self.model_dump_json(indent=2, by_alias=True, context=context)\n        path.write_text(json_data + '\\n', encoding='utf-8')\n\n```\n\n#### model_json_schema_with_evaluators\n\n```python\nmodel_json_schema_with_evaluators(\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n) -> dict[str, Any]\n\n```\n\nGenerate a JSON schema for this dataset type, including evaluator details.\n\nThis is useful for generating a schema that can be used to validate YAML-format dataset files.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to include in the schema. | `()` |\n\nReturns:\n\n| Type | Description | | --- | --- | | `dict[str, Any]` | A dictionary representing the JSON schema. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\n@classmethod\ndef model_json_schema_with_evaluators(\n    cls,\n    custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),\n) -> dict[str, Any]:\n    \"\"\"Generate a JSON schema for this dataset type, including evaluator details.\n\n    This is useful for generating a schema that can be used to validate YAML-format dataset files.\n\n    Args:\n        custom_evaluator_types: Custom evaluator classes to include in the schema.\n\n    Returns:\n        A dictionary representing the JSON schema.\n    \"\"\"\n    # Note: this function could maybe be simplified now that Evaluators are always dataclasses\n    registry = _get_registry(custom_evaluator_types)\n\n    evaluator_schema_types: list[Any] = []\n    for name, evaluator_class in registry.items():\n        type_hints = _typing_extra.get_function_type_hints(evaluator_class)\n        type_hints.pop('return', None)\n        required_type_hints: dict[str, Any] = {}\n\n        for p in inspect.signature(evaluator_class).parameters.values():\n            type_hints.setdefault(p.name, Any)\n            if p.default is not p.empty:\n                type_hints[p.name] = NotRequired[type_hints[p.name]]\n            else:\n                required_type_hints[p.name] = type_hints[p.name]\n\n        def _make_typed_dict(cls_name_prefix: str, fields: dict[str, Any]) -> Any:\n            td = TypedDict(f'{cls_name_prefix}_{name}', fields)  # pyright: ignore[reportArgumentType]\n            config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)\n            # TODO: Replace with pydantic.with_config once pydantic 2.11 is the min supported version\n            td.__pydantic_config__ = config  # pyright: ignore[reportAttributeAccessIssue]\n            return td\n\n        # Shortest form: just the call name\n        if len(type_hints) == 0 or not required_type_hints:\n            evaluator_schema_types.append(Literal[name])\n\n        # Short form: can be called with only one parameter\n        if len(type_hints) == 1:\n            [type_hint_type] = type_hints.values()\n            evaluator_schema_types.append(_make_typed_dict('short_evaluator', {name: type_hint_type}))\n        elif len(required_type_hints) == 1:  # pragma: no branch\n            [type_hint_type] = required_type_hints.values()\n            evaluator_schema_types.append(_make_typed_dict('short_evaluator', {name: type_hint_type}))\n\n        # Long form: multiple parameters, possibly required\n        if len(type_hints) > 1:\n            params_td = _make_typed_dict('evaluator_params', type_hints)\n            evaluator_schema_types.append(_make_typed_dict('evaluator', {name: params_td}))\n\n    in_type, out_type, meta_type = cls._params()\n\n    # Note: we shadow the `Case` and `Dataset` class names here to generate a clean JSON schema\n    class Case(BaseModel, extra='forbid'):  # pyright: ignore[reportUnusedClass]  # this _is_ used below, but pyright doesn't seem to notice..\n        name: str | None = None\n        inputs: in_type  # pyright: ignore[reportInvalidTypeForm]\n        metadata: meta_type | None = None  # pyright: ignore[reportInvalidTypeForm,reportUnknownVariableType]\n        expected_output: out_type | None = None  # pyright: ignore[reportInvalidTypeForm,reportUnknownVariableType]\n        if evaluator_schema_types:  # pragma: no branch\n            evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007\n\n    class Dataset(BaseModel, extra='forbid'):\n        name: str | None = None\n        cases: list[Case]\n        if evaluator_schema_types:  # pragma: no branch\n            evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007\n\n    json_schema = Dataset.model_json_schema()\n    # See `_add_json_schema` below, since `$schema` is added to the JSON, it has to be supported in the JSON\n    json_schema['properties']['$schema'] = {'type': 'string'}\n    return json_schema\n\n```\n\n### set_eval_attribute\n\n```python\nset_eval_attribute(name: str, value: Any) -> None\n\n```\n\nSet an attribute on the current task run.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str` | The name of the attribute. | *required* | | `value` | `Any` | The value of the attribute. | *required* |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\ndef set_eval_attribute(name: str, value: Any) -> None:\n    \"\"\"Set an attribute on the current task run.\n\n    Args:\n        name: The name of the attribute.\n        value: The value of the attribute.\n    \"\"\"\n    current_case = _CURRENT_TASK_RUN.get()\n    if current_case is not None:  # pragma: no branch\n        current_case.record_attribute(name, value)\n\n```\n\n### increment_eval_metric\n\n```python\nincrement_eval_metric(\n    name: str, amount: int | float\n) -> None\n\n```\n\nIncrement a metric on the current task run.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `name` | `str` | The name of the metric. | *required* | | `amount` | `int | float` | The amount to increment by. | *required* |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n```python\ndef increment_eval_metric(name: str, amount: int | float) -> None:\n    \"\"\"Increment a metric on the current task run.\n\n    Args:\n        name: The name of the metric.\n        amount: The amount to increment by.\n    \"\"\"\n    current_case = _CURRENT_TASK_RUN.get()\n    if current_case is not None:  # pragma: no branch\n        current_case.increment_metric(name, amount)\n\n```",
  "content_length": 69989
}