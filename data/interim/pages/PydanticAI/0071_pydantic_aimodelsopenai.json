{
  "title": "`pydantic_ai.models.openai`",
  "source_url": null,
  "content": "## Setup\n\nFor details on how to set up authentication with this model, see [model configuration for OpenAI](../../../models/openai/).\n\n### OpenAIModelName\n\n```python\nOpenAIModelName = str | AllModels\n\n```\n\nPossible OpenAI model names.\n\nSince OpenAI supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the OpenAI docs](https://platform.openai.com/docs/models) for a full list.\n\nUsing this more broad type for the model name instead of the ChatModel definition allows this model to be used more easily with other model types (ie, Ollama, Deepseek).\n\n### OpenAIChatModelSettings\n\nBases: `ModelSettings`\n\nSettings used for an OpenAI model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n```python\nclass OpenAIChatModelSettings(ModelSettings, total=False):\n    \"\"\"Settings used for an OpenAI model request.\"\"\"\n\n    # ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\n    openai_reasoning_effort: ReasoningEffort\n    \"\"\"Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n\n    Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n    result in faster responses and fewer tokens used on reasoning in a response.\n    \"\"\"\n\n    openai_logprobs: bool\n    \"\"\"Include log probabilities in the response.\n\n    For Chat models, these will be included in `ModelResponse.provider_details['logprobs']`.\n    For Responses models, these will be included in the response output parts `TextPart.provider_details['logprobs']`.\n    \"\"\"\n\n    openai_top_logprobs: int\n    \"\"\"Include log probabilities of the top n tokens in the response.\"\"\"\n\n    openai_user: str\n    \"\"\"A unique identifier representing the end-user, which can help OpenAI monitor and detect abuse.\n\n    See [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details.\n    \"\"\"\n\n    openai_service_tier: Literal['auto', 'default', 'flex', 'priority']\n    \"\"\"The service tier to use for the model request.\n\n    Currently supported values are `auto`, `default`, `flex`, and `priority`.\n    For more information, see [OpenAI's service tiers documentation](https://platform.openai.com/docs/api-reference/chat/object#chat/object-service_tier).\n    \"\"\"\n\n    openai_prediction: ChatCompletionPredictionContentParam\n    \"\"\"Enables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs).\n\n    This feature is currently only supported for some OpenAI models.\n    \"\"\"\n\n```\n\n#### openai_reasoning_effort\n\n```python\nopenai_reasoning_effort: ReasoningEffort\n\n```\n\nConstrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n\nCurrently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.\n\n#### openai_logprobs\n\n```python\nopenai_logprobs: bool\n\n```\n\nInclude log probabilities in the response.\n\nFor Chat models, these will be included in `ModelResponse.provider_details['logprobs']`. For Responses models, these will be included in the response output parts `TextPart.provider_details['logprobs']`.\n\n#### openai_top_logprobs\n\n```python\nopenai_top_logprobs: int\n\n```\n\nInclude log probabilities of the top n tokens in the response.\n\n#### openai_user\n\n```python\nopenai_user: str\n\n```\n\nA unique identifier representing the end-user, which can help OpenAI monitor and detect abuse.\n\nSee [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details.\n\n#### openai_service_tier\n\n```python\nopenai_service_tier: Literal[\n    \"auto\", \"default\", \"flex\", \"priority\"\n]\n\n```\n\nThe service tier to use for the model request.\n\nCurrently supported values are `auto`, `default`, `flex`, and `priority`. For more information, see [OpenAI's service tiers documentation](https://platform.openai.com/docs/api-reference/chat/object#chat/object-service_tier).\n\n#### openai_prediction\n\n```python\nopenai_prediction: ChatCompletionPredictionContentParam\n\n```\n\nEnables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs).\n\nThis feature is currently only supported for some OpenAI models.\n\n### OpenAIModelSettings\n\nBases: `OpenAIChatModelSettings`\n\nDeprecated\n\nUse `OpenAIChatModelSettings` instead.\n\nDeprecated alias for `OpenAIChatModelSettings`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n```python\n@deprecated('Use `OpenAIChatModelSettings` instead.')\nclass OpenAIModelSettings(OpenAIChatModelSettings, total=False):\n    \"\"\"Deprecated alias for `OpenAIChatModelSettings`.\"\"\"\n\n```\n\n### OpenAIResponsesModelSettings\n\nBases: `OpenAIChatModelSettings`\n\nSettings used for an OpenAI Responses model request.\n\nALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n```python\nclass OpenAIResponsesModelSettings(OpenAIChatModelSettings, total=False):\n    \"\"\"Settings used for an OpenAI Responses model request.\n\n    ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n    \"\"\"\n\n    openai_builtin_tools: Sequence[FileSearchToolParam | WebSearchToolParam | ComputerToolParam]\n    \"\"\"The provided OpenAI built-in tools to use.\n\n    See [OpenAI's built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for more details.\n    \"\"\"\n\n    openai_reasoning_generate_summary: Literal['detailed', 'concise']\n    \"\"\"Deprecated alias for `openai_reasoning_summary`.\"\"\"\n\n    openai_reasoning_summary: Literal['detailed', 'concise']\n    \"\"\"A summary of the reasoning performed by the model.\n\n    This can be useful for debugging and understanding the model's reasoning process.\n    One of `concise` or `detailed`.\n\n    Check the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries)\n    for more details.\n    \"\"\"\n\n    openai_send_reasoning_ids: bool\n    \"\"\"Whether to send the unique IDs of reasoning, text, and function call parts from the message history to the model. Enabled by default for reasoning models.\n\n    This can result in errors like `\"Item 'rs_123' of type 'reasoning' was provided without its required following item.\"`\n    if the message history you're sending does not match exactly what was received from the Responses API in a previous response,\n    for example if you're using a [history processor](../../message-history.md#processing-message-history).\n    In that case, you'll want to disable this.\n    \"\"\"\n\n    openai_truncation: Literal['disabled', 'auto']\n    \"\"\"The truncation strategy to use for the model response.\n\n    It can be either:\n    - `disabled` (default): If a model response will exceed the context window size for a model, the\n        request will fail with a 400 error.\n    - `auto`: If the context of this response and previous ones exceeds the model's context window size,\n        the model will truncate the response to fit the context window by dropping input items in the\n        middle of the conversation.\n    \"\"\"\n\n    openai_text_verbosity: Literal['low', 'medium', 'high']\n    \"\"\"Constrains the verbosity of the model's text response.\n\n    Lower values will result in more concise responses, while higher values will\n    result in more verbose responses. Currently supported values are `low`,\n    `medium`, and `high`.\n    \"\"\"\n\n    openai_previous_response_id: Literal['auto'] | str\n    \"\"\"The ID of a previous response from the model to use as the starting point for a continued conversation.\n\n    When set to `'auto'`, the request automatically uses the most recent\n    `provider_response_id` from the message history and omits earlier messages.\n\n    This enables the model to use server-side conversation state and faithfully reference previous reasoning.\n    See the [OpenAI Responses API documentation](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context)\n    for more information.\n    \"\"\"\n\n    openai_include_code_execution_outputs: bool\n    \"\"\"Whether to include the code execution results in the response.\n\n    Corresponds to the `code_interpreter_call.outputs` value of the `include` parameter in the Responses API.\n    \"\"\"\n\n    openai_include_web_search_sources: bool\n    \"\"\"Whether to include the web search results in the response.\n\n    Corresponds to the `web_search_call.action.sources` value of the `include` parameter in the Responses API.\n    \"\"\"\n\n```\n\n#### openai_builtin_tools\n\n```python\nopenai_builtin_tools: Sequence[\n    FileSearchToolParam\n    | WebSearchToolParam\n    | ComputerToolParam\n]\n\n```\n\nThe provided OpenAI built-in tools to use.\n\nSee [OpenAI's built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for more details.\n\n#### openai_reasoning_generate_summary\n\n```python\nopenai_reasoning_generate_summary: Literal[\n    \"detailed\", \"concise\"\n]\n\n```\n\nDeprecated alias for `openai_reasoning_summary`.\n\n#### openai_reasoning_summary\n\n```python\nopenai_reasoning_summary: Literal['detailed', 'concise']\n\n```\n\nA summary of the reasoning performed by the model.\n\nThis can be useful for debugging and understanding the model's reasoning process. One of `concise` or `detailed`.\n\nCheck the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries) for more details.\n\n#### openai_send_reasoning_ids\n\n```python\nopenai_send_reasoning_ids: bool\n\n```\n\nWhether to send the unique IDs of reasoning, text, and function call parts from the message history to the model. Enabled by default for reasoning models.\n\nThis can result in errors like `\"Item 'rs_123' of type 'reasoning' was provided without its required following item.\"` if the message history you're sending does not match exactly what was received from the Responses API in a previous response, for example if you're using a [history processor](../../../message-history/#processing-message-history). In that case, you'll want to disable this.\n\n#### openai_truncation\n\n```python\nopenai_truncation: Literal['disabled', 'auto']\n\n```\n\nThe truncation strategy to use for the model response.\n\nIt can be either:\n\n- `disabled` (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.\n- `auto`: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.\n\n#### openai_text_verbosity\n\n```python\nopenai_text_verbosity: Literal['low', 'medium', 'high']\n\n```\n\nConstrains the verbosity of the model's text response.\n\nLower values will result in more concise responses, while higher values will result in more verbose responses. Currently supported values are `low`, `medium`, and `high`.\n\n#### openai_previous_response_id\n\n```python\nopenai_previous_response_id: Literal['auto'] | str\n\n```\n\nThe ID of a previous response from the model to use as the starting point for a continued conversation.\n\nWhen set to `'auto'`, the request automatically uses the most recent `provider_response_id` from the message history and omits earlier messages.\n\nThis enables the model to use server-side conversation state and faithfully reference previous reasoning. See the [OpenAI Responses API documentation](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) for more information.\n\n#### openai_include_code_execution_outputs\n\n```python\nopenai_include_code_execution_outputs: bool\n\n```\n\nWhether to include the code execution results in the response.\n\nCorresponds to the `code_interpreter_call.outputs` value of the `include` parameter in the Responses API.\n\n#### openai_include_web_search_sources\n\n```python\nopenai_include_web_search_sources: bool\n\n```\n\nWhether to include the web search results in the response.\n\nCorresponds to the `web_search_call.action.sources` value of the `include` parameter in the Responses API.\n\n### OpenAIChatModel\n\nBases: `Model`\n\nA model that uses the OpenAI API.\n\nInternally, this uses the [OpenAI Python client](https://github.com/openai/openai-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n```python\n@dataclass(init=False)\nclass OpenAIChatModel(Model):\n    \"\"\"A model that uses the OpenAI API.\n\n    Internally, this uses the [OpenAI Python client](https://github.com/openai/openai-python) to interact with the API.\n\n    Apart from `__init__`, all methods are private or match those of the base class.\n    \"\"\"\n\n    client: AsyncOpenAI = field(repr=False)\n\n    _model_name: OpenAIModelName = field(repr=False)\n    _provider: Provider[AsyncOpenAI] = field(repr=False)\n\n    @overload\n    def __init__(\n        self,\n        model_name: OpenAIModelName,\n        *,\n        provider: Literal[\n            'azure',\n            'deepseek',\n            'cerebras',\n            'fireworks',\n            'github',\n            'grok',\n            'heroku',\n            'moonshotai',\n            'ollama',\n            'openai',\n            'openai-chat',\n            'openrouter',\n            'together',\n            'vercel',\n            'litellm',\n            'nebius',\n            'ovhcloud',\n            'gateway',\n        ]\n        | Provider[AsyncOpenAI] = 'openai',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ) -> None: ...\n\n    @deprecated('Set the `system_prompt_role` in the `OpenAIModelProfile` instead.')\n    @overload\n    def __init__(\n        self,\n        model_name: OpenAIModelName,\n        *,\n        provider: Literal[\n            'azure',\n            'deepseek',\n            'cerebras',\n            'fireworks',\n            'github',\n            'grok',\n            'heroku',\n            'moonshotai',\n            'ollama',\n            'openai',\n            'openai-chat',\n            'openrouter',\n            'together',\n            'vercel',\n            'litellm',\n            'nebius',\n            'ovhcloud',\n            'gateway',\n        ]\n        | Provider[AsyncOpenAI] = 'openai',\n        profile: ModelProfileSpec | None = None,\n        system_prompt_role: OpenAISystemPromptRole | None = None,\n        settings: ModelSettings | None = None,\n    ) -> None: ...\n\n    def __init__(\n        self,\n        model_name: OpenAIModelName,\n        *,\n        provider: Literal[\n            'azure',\n            'deepseek',\n            'cerebras',\n            'fireworks',\n            'github',\n            'grok',\n            'heroku',\n            'moonshotai',\n            'ollama',\n            'openai',\n            'openai-chat',\n            'openrouter',\n            'together',\n            'vercel',\n            'litellm',\n            'nebius',\n            'ovhcloud',\n            'gateway',\n        ]\n        | Provider[AsyncOpenAI] = 'openai',\n        profile: ModelProfileSpec | None = None,\n        system_prompt_role: OpenAISystemPromptRole | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize an OpenAI model.\n\n        Args:\n            model_name: The name of the OpenAI model to use. List of model names available\n                [here](https://github.com/openai/openai-python/blob/v1.54.3/src/openai/types/chat_model.py#L7)\n                (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API).\n            provider: The provider to use. Defaults to `'openai'`.\n            profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n            system_prompt_role: The role to use for the system prompt message. If not provided, defaults to `'system'`.\n                In the future, this may be inferred from the model name.\n            settings: Default model settings for this model instance.\n        \"\"\"\n        self._model_name = model_name\n\n        if isinstance(provider, str):\n            provider = infer_provider('gateway/openai' if provider == 'gateway' else provider)\n        self._provider = provider\n        self.client = provider.client\n\n        super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n        if system_prompt_role is not None:\n            self.profile = OpenAIModelProfile(openai_system_prompt_role=system_prompt_role).update(self.profile)\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.base_url)\n\n    @property\n    def model_name(self) -> OpenAIModelName:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The model provider.\"\"\"\n        return self._provider.name\n\n    @property\n    @deprecated('Set the `system_prompt_role` in the `OpenAIModelProfile` instead.')\n    def system_prompt_role(self) -> OpenAISystemPromptRole | None:\n        return OpenAIModelProfile.from_profile(self.profile).openai_system_prompt_role\n\n    async def request(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._completions_create(\n            messages, False, cast(OpenAIChatModelSettings, model_settings or {}), model_request_parameters\n        )\n        model_response = self._process_response(response)\n        return model_response\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._completions_create(\n            messages, True, cast(OpenAIChatModelSettings, model_settings or {}), model_request_parameters\n        )\n        async with response:\n            yield await self._process_streamed_response(response, model_request_parameters)\n\n    @overload\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[True],\n        model_settings: OpenAIChatModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> AsyncStream[ChatCompletionChunk]: ...\n\n    @overload\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        stream: Literal[False],\n        model_settings: OpenAIChatModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> chat.ChatCompletion: ...\n\n    async def _completions_create(\n        self,\n        messages: list[ModelMessage],\n        stream: bool,\n        model_settings: OpenAIChatModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> chat.ChatCompletion | AsyncStream[ChatCompletionChunk]:\n        tools = self._get_tools(model_request_parameters)\n        web_search_options = self._get_web_search_options(model_request_parameters)\n\n        if not tools:\n            tool_choice: Literal['none', 'required', 'auto'] | None = None\n        elif (\n            not model_request_parameters.allow_text_output\n            and OpenAIModelProfile.from_profile(self.profile).openai_supports_tool_choice_required\n        ):\n            tool_choice = 'required'\n        else:\n            tool_choice = 'auto'\n\n        openai_messages = await self._map_messages(messages, model_request_parameters)\n\n        response_format: chat.completion_create_params.ResponseFormat | None = None\n        if model_request_parameters.output_mode == 'native':\n            output_object = model_request_parameters.output_object\n            assert output_object is not None\n            response_format = self._map_json_schema(output_object)\n        elif (\n            model_request_parameters.output_mode == 'prompted' and self.profile.supports_json_object_output\n        ):  # pragma: no branch\n            response_format = {'type': 'json_object'}\n\n        unsupported_model_settings = OpenAIModelProfile.from_profile(self.profile).openai_unsupported_model_settings\n        for setting in unsupported_model_settings:\n            model_settings.pop(setting, None)\n\n        try:\n            extra_headers = model_settings.get('extra_headers', {})\n            extra_headers.setdefault('User-Agent', get_user_agent())\n            return await self.client.chat.completions.create(\n                model=self.model_name,\n                messages=openai_messages,\n                parallel_tool_calls=model_settings.get('parallel_tool_calls', OMIT),\n                tools=tools or OMIT,\n                tool_choice=tool_choice or OMIT,\n                stream=stream,\n                stream_options={'include_usage': True} if stream else OMIT,\n                stop=model_settings.get('stop_sequences', OMIT),\n                max_completion_tokens=model_settings.get('max_tokens', OMIT),\n                timeout=model_settings.get('timeout', NOT_GIVEN),\n                response_format=response_format or OMIT,\n                seed=model_settings.get('seed', OMIT),\n                reasoning_effort=model_settings.get('openai_reasoning_effort', OMIT),\n                user=model_settings.get('openai_user', OMIT),\n                web_search_options=web_search_options or OMIT,\n                service_tier=model_settings.get('openai_service_tier', OMIT),\n                prediction=model_settings.get('openai_prediction', OMIT),\n                temperature=model_settings.get('temperature', OMIT),\n                top_p=model_settings.get('top_p', OMIT),\n                presence_penalty=model_settings.get('presence_penalty', OMIT),\n                frequency_penalty=model_settings.get('frequency_penalty', OMIT),\n                logit_bias=model_settings.get('logit_bias', OMIT),\n                logprobs=model_settings.get('openai_logprobs', OMIT),\n                top_logprobs=model_settings.get('openai_top_logprobs', OMIT),\n                extra_headers=extra_headers,\n                extra_body=model_settings.get('extra_body'),\n            )\n        except APIStatusError as e:\n            if (status_code := e.status_code) >= 400:\n                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n            raise  # pragma: lax no cover\n        except APIConnectionError as e:\n            raise ModelAPIError(model_name=self.model_name, message=e.message) from e\n\n    def _validate_completion(self, response: chat.ChatCompletion) -> chat.ChatCompletion:\n        \"\"\"Hook that validates chat completions before processing.\n\n        This method may be overridden by subclasses of `OpenAIChatModel` to apply custom completion validations.\n        \"\"\"\n        return chat.ChatCompletion.model_validate(response.model_dump())\n\n    def _process_provider_details(self, response: chat.ChatCompletion) -> dict[str, Any]:\n        \"\"\"Hook that response content to provider details.\n\n        This method may be overridden by subclasses of `OpenAIChatModel` to apply custom mappings.\n        \"\"\"\n        return _map_provider_details(response.choices[0])\n\n    def _process_response(self, response: chat.ChatCompletion | str) -> ModelResponse:\n        \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\n        # Although the OpenAI SDK claims to return a Pydantic model (`ChatCompletion`) from the chat completions function:\n        # * it hasn't actually performed validation (presumably they're creating the model with `model_construct` or something?!)\n        # * if the endpoint returns plain text, the return type is a string\n        # Thus we validate it fully here.\n        if not isinstance(response, chat.ChatCompletion):\n            raise UnexpectedModelBehavior(\n                f'Invalid response from {self.system} chat completions endpoint, expected JSON data'\n            )\n\n        if response.created:\n            timestamp = number_to_datetime(response.created)\n        else:\n            timestamp = _now_utc()\n            response.created = int(timestamp.timestamp())\n\n        # Workaround for local Ollama which sometimes returns a `None` finish reason.\n        if response.choices and (choice := response.choices[0]) and choice.finish_reason is None:  # pyright: ignore[reportUnnecessaryComparison]\n            choice.finish_reason = 'stop'\n\n        try:\n            response = self._validate_completion(response)\n        except ValidationError as e:\n            raise UnexpectedModelBehavior(f'Invalid response from {self.system} chat completions endpoint: {e}') from e\n\n        choice = response.choices[0]\n        items: list[ModelResponsePart] = []\n\n        if thinking_parts := self._process_thinking(choice.message):\n            items.extend(thinking_parts)\n\n        if choice.message.content:\n            items.extend(\n                (replace(part, id='content', provider_name=self.system) if isinstance(part, ThinkingPart) else part)\n                for part in split_content_into_text_and_thinking(choice.message.content, self.profile.thinking_tags)\n            )\n        if choice.message.tool_calls is not None:\n            for c in choice.message.tool_calls:\n                if isinstance(c, ChatCompletionMessageFunctionToolCall):\n                    part = ToolCallPart(c.function.name, c.function.arguments, tool_call_id=c.id)\n                elif isinstance(c, ChatCompletionMessageCustomToolCall):  # pragma: no cover\n                    # NOTE: Custom tool calls are not supported.\n                    # See <https://github.com/pydantic/pydantic-ai/issues/2513> for more details.\n                    raise RuntimeError('Custom tool calls are not supported')\n                else:\n                    assert_never(c)\n                part.tool_call_id = _guard_tool_call_id(part)\n                items.append(part)\n\n        return ModelResponse(\n            parts=items,\n            usage=self._map_usage(response),\n            model_name=response.model,\n            timestamp=timestamp,\n            provider_details=self._process_provider_details(response),\n            provider_response_id=response.id,\n            provider_name=self._provider.name,\n            finish_reason=self._map_finish_reason(choice.finish_reason),\n        )\n\n    def _process_thinking(self, message: chat.ChatCompletionMessage) -> list[ThinkingPart] | None:\n        \"\"\"Hook that maps reasoning tokens to thinking parts.\n\n        This method may be overridden by subclasses of `OpenAIChatModel` to apply custom mappings.\n        \"\"\"\n        items: list[ThinkingPart] = []\n\n        # The `reasoning_content` field is only present in DeepSeek models.\n        # https://api-docs.deepseek.com/guides/reasoning_model\n        if reasoning_content := getattr(message, 'reasoning_content', None):\n            items.append(ThinkingPart(id='reasoning_content', content=reasoning_content, provider_name=self.system))\n\n        # The `reasoning` field is only present in gpt-oss via Ollama and OpenRouter.\n        # - https://cookbook.openai.com/articles/gpt-oss/handle-raw-cot#chat-completions-api\n        # - https://openrouter.ai/docs/use-cases/reasoning-tokens#basic-usage-with-reasoning-tokens\n        if reasoning := getattr(message, 'reasoning', None):\n            items.append(ThinkingPart(id='reasoning', content=reasoning, provider_name=self.system))\n\n        return items\n\n    async def _process_streamed_response(\n        self, response: AsyncStream[ChatCompletionChunk], model_request_parameters: ModelRequestParameters\n    ) -> OpenAIStreamedResponse:\n        \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\n        peekable_response = _utils.PeekableAsyncStream(response)\n        first_chunk = await peekable_response.peek()\n        if isinstance(first_chunk, _utils.Unset):\n            raise UnexpectedModelBehavior(  # pragma: no cover\n                'Streamed response ended without content or tool calls'\n            )\n\n        # When using Azure OpenAI and a content filter is enabled, the first chunk will contain a `''` model name,\n        # so we set it from a later chunk in `OpenAIChatStreamedResponse`.\n        model_name = first_chunk.model or self.model_name\n\n        return self._streamed_response_cls(\n            model_request_parameters=model_request_parameters,\n            _model_name=model_name,\n            _model_profile=self.profile,\n            _response=peekable_response,\n            _timestamp=number_to_datetime(first_chunk.created),\n            _provider_name=self._provider.name,\n            _provider_url=self._provider.base_url,\n        )\n\n    @property\n    def _streamed_response_cls(self) -> type[OpenAIStreamedResponse]:\n        \"\"\"Returns the `StreamedResponse` type that will be used for streamed responses.\n\n        This method may be overridden by subclasses of `OpenAIChatModel` to provide their own `StreamedResponse` type.\n        \"\"\"\n        return OpenAIStreamedResponse\n\n    def _map_usage(self, response: chat.ChatCompletion) -> usage.RequestUsage:\n        return _map_usage(response, self._provider.name, self._provider.base_url, self.model_name)\n\n    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:\n        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]\n\n    def _get_web_search_options(self, model_request_parameters: ModelRequestParameters) -> WebSearchOptions | None:\n        for tool in model_request_parameters.builtin_tools:\n            if isinstance(tool, WebSearchTool):  # pragma: no branch\n                if not OpenAIModelProfile.from_profile(self.profile).openai_chat_supports_web_search:\n                    raise UserError(\n                        f'WebSearchTool is not supported with `OpenAIChatModel` and model {self.model_name!r}. '\n                        f'Please use `OpenAIResponsesModel` instead.'\n                    )\n\n                if tool.user_location:\n                    return WebSearchOptions(\n                        search_context_size=tool.search_context_size,\n                        user_location=WebSearchOptionsUserLocation(\n                            type='approximate',\n                            approximate=WebSearchOptionsUserLocationApproximate(**tool.user_location),\n                        ),\n                    )\n                return WebSearchOptions(search_context_size=tool.search_context_size)\n            else:\n                raise UserError(\n                    f'`{tool.__class__.__name__}` is not supported by `OpenAIChatModel`. If it should be, please file an issue.'\n                )\n\n    @dataclass\n    class _MapModelResponseContext:\n        \"\"\"Context object for mapping a `ModelResponse` to OpenAI chat completion parameters.\n\n        This class is designed to be subclassed to add new fields for custom logic,\n        collecting various parts of the model response (like text and tool calls)\n        to form a single assistant message.\n        \"\"\"\n\n        _model: OpenAIChatModel\n\n        texts: list[str] = field(default_factory=list)\n        tool_calls: list[ChatCompletionMessageFunctionToolCallParam] = field(default_factory=list)\n\n        def map_assistant_message(self, message: ModelResponse) -> chat.ChatCompletionAssistantMessageParam:\n            for item in message.parts:\n                if isinstance(item, TextPart):\n                    self._map_response_text_part(item)\n                elif isinstance(item, ThinkingPart):\n                    self._map_response_thinking_part(item)\n                elif isinstance(item, ToolCallPart):\n                    self._map_response_tool_call_part(item)\n                elif isinstance(item, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover\n                    self._map_response_builtin_part(item)\n                elif isinstance(item, FilePart):  # pragma: no cover\n                    self._map_response_file_part(item)\n                else:\n                    assert_never(item)\n            return self._into_message_param()\n\n        def _into_message_param(self) -> chat.ChatCompletionAssistantMessageParam:\n            \"\"\"Converts the collected texts and tool calls into a single OpenAI `ChatCompletionAssistantMessageParam`.\n\n            This method serves as a hook that can be overridden by subclasses\n            to implement custom logic for how collected parts are transformed into the final message parameter.\n\n            Returns:\n                An OpenAI `ChatCompletionAssistantMessageParam` object representing the assistant's response.\n            \"\"\"\n            message_param = chat.ChatCompletionAssistantMessageParam(role='assistant')\n            if self.texts:\n                # Note: model responses from this model should only have one text item, so the following\n                # shouldn't merge multiple texts into one unless you switch models between runs:\n                message_param['content'] = '\\n\\n'.join(self.texts)\n            else:\n                message_param['content'] = None\n            if self.tool_calls:\n                message_param['tool_calls'] = self.tool_calls\n            return message_param\n\n        def _map_response_text_part(self, item: TextPart) -> None:\n            \"\"\"Maps a `TextPart` to the response context.\n\n            This method serves as a hook that can be overridden by subclasses\n            to implement custom logic for handling text parts.\n            \"\"\"\n            self.texts.append(item.content)\n\n        def _map_response_thinking_part(self, item: ThinkingPart) -> None:\n            \"\"\"Maps a `ThinkingPart` to the response context.\n\n            This method serves as a hook that can be overridden by subclasses\n            to implement custom logic for handling thinking parts.\n            \"\"\"\n            # NOTE: DeepSeek `reasoning_content` field should NOT be sent back per https://api-docs.deepseek.com/guides/reasoning_model,\n            # but we currently just send it in `<think>` tags anyway as we don't want DeepSeek-specific checks here.\n            # If you need this changed, please file an issue.\n            start_tag, end_tag = self._model.profile.thinking_tags\n            self.texts.append('\\n'.join([start_tag, item.content, end_tag]))\n\n        def _map_response_tool_call_part(self, item: ToolCallPart) -> None:\n            \"\"\"Maps a `ToolCallPart` to the response context.\n\n            This method serves as a hook that can be overridden by subclasses\n            to implement custom logic for handling tool call parts.\n            \"\"\"\n            self.tool_calls.append(self._model._map_tool_call(item))\n\n        def _map_response_builtin_part(self, item: BuiltinToolCallPart | BuiltinToolReturnPart) -> None:\n            \"\"\"Maps a built-in tool call or return part to the response context.\n\n            This method serves as a hook that can be overridden by subclasses\n            to implement custom logic for handling built-in tool parts.\n            \"\"\"\n            # OpenAI doesn't return built-in tool calls\n            pass\n\n        def _map_response_file_part(self, item: FilePart) -> None:\n            \"\"\"Maps a `FilePart` to the response context.\n\n            This method serves as a hook that can be overridden by subclasses\n            to implement custom logic for handling file parts.\n            \"\"\"\n            # Files generated by models are not sent back to models that don't themselves generate files.\n            pass\n\n    def _map_model_response(self, message: ModelResponse) -> chat.ChatCompletionMessageParam:\n        \"\"\"Hook that determines how `ModelResponse` is mapped into `ChatCompletionMessageParam` objects before sending.\n\n        Subclasses of `OpenAIChatModel` may override this method to provide their own mapping logic.\n        \"\"\"\n        return self._MapModelResponseContext(self).map_assistant_message(message)\n\n    def _map_finish_reason(\n        self, key: Literal['stop', 'length', 'tool_calls', 'content_filter', 'function_call']\n    ) -> FinishReason | None:\n        \"\"\"Hooks that maps a finish reason key to a [FinishReason][pydantic_ai.messages.FinishReason].\n\n        This method may be overridden by subclasses of `OpenAIChatModel` to accommodate custom keys.\n        \"\"\"\n        return _CHAT_FINISH_REASON_MAP.get(key)\n\n    async def _map_messages(\n        self, messages: list[ModelMessage], model_request_parameters: ModelRequestParameters\n    ) -> list[chat.ChatCompletionMessageParam]:\n        \"\"\"Just maps a `pydantic_ai.Message` to a `openai.types.ChatCompletionMessageParam`.\"\"\"\n        openai_messages: list[chat.ChatCompletionMessageParam] = []\n        for message in messages:\n            if isinstance(message, ModelRequest):\n                async for item in self._map_user_message(message):\n                    openai_messages.append(item)\n            elif isinstance(message, ModelResponse):\n                openai_messages.append(self._map_model_response(message))\n            else:\n                assert_never(message)\n        if instructions := self._get_instructions(messages, model_request_parameters):\n            openai_messages.insert(0, chat.ChatCompletionSystemMessageParam(content=instructions, role='system'))\n        return openai_messages\n\n    @staticmethod\n    def _map_tool_call(t: ToolCallPart) -> ChatCompletionMessageFunctionToolCallParam:\n        return ChatCompletionMessageFunctionToolCallParam(\n            id=_guard_tool_call_id(t=t),\n            type='function',\n            function={'name': t.tool_name, 'arguments': t.args_as_json_str()},\n        )\n\n    def _map_json_schema(self, o: OutputObjectDefinition) -> chat.completion_create_params.ResponseFormat:\n        response_format_param: chat.completion_create_params.ResponseFormatJSONSchema = {  # pyright: ignore[reportPrivateImportUsage]\n            'type': 'json_schema',\n            'json_schema': {'name': o.name or DEFAULT_OUTPUT_TOOL_NAME, 'schema': o.json_schema},\n        }\n        if o.description:\n            response_format_param['json_schema']['description'] = o.description\n        if OpenAIModelProfile.from_profile(self.profile).openai_supports_strict_tool_definition:  # pragma: no branch\n            response_format_param['json_schema']['strict'] = o.strict\n        return response_format_param\n\n    def _map_tool_definition(self, f: ToolDefinition) -> chat.ChatCompletionToolParam:\n        tool_param: chat.ChatCompletionToolParam = {\n            'type': 'function',\n            'function': {\n                'name': f.name,\n                'description': f.description or '',\n                'parameters': f.parameters_json_schema,\n            },\n        }\n        if f.strict and OpenAIModelProfile.from_profile(self.profile).openai_supports_strict_tool_definition:\n            tool_param['function']['strict'] = f.strict\n        return tool_param\n\n    async def _map_user_message(self, message: ModelRequest) -> AsyncIterable[chat.ChatCompletionMessageParam]:\n        for part in message.parts:\n            if isinstance(part, SystemPromptPart):\n                system_prompt_role = OpenAIModelProfile.from_profile(self.profile).openai_system_prompt_role\n                if system_prompt_role == 'developer':\n                    yield chat.ChatCompletionDeveloperMessageParam(role='developer', content=part.content)\n                elif system_prompt_role == 'user':\n                    yield chat.ChatCompletionUserMessageParam(role='user', content=part.content)\n                else:\n                    yield chat.ChatCompletionSystemMessageParam(role='system', content=part.content)\n            elif isinstance(part, UserPromptPart):\n                yield await self._map_user_prompt(part)\n            elif isinstance(part, ToolReturnPart):\n                yield chat.ChatCompletionToolMessageParam(\n                    role='tool',\n                    tool_call_id=_guard_tool_call_id(t=part),\n                    content=part.model_response_str(),\n                )\n            elif isinstance(part, RetryPromptPart):\n                if part.tool_name is None:\n                    yield chat.ChatCompletionUserMessageParam(role='user', content=part.model_response())\n                else:\n                    yield chat.ChatCompletionToolMessageParam(\n                        role='tool',\n                        tool_call_id=_guard_tool_call_id(t=part),\n                        content=part.model_response(),\n                    )\n            else:\n                assert_never(part)\n\n    async def _map_user_prompt(self, part: UserPromptPart) -> chat.ChatCompletionUserMessageParam:  # noqa: C901\n        content: str | list[ChatCompletionContentPartParam]\n        if isinstance(part.content, str):\n            content = part.content\n        else:\n            content = []\n            for item in part.content:\n                if isinstance(item, str):\n                    content.append(ChatCompletionContentPartTextParam(text=item, type='text'))\n                elif isinstance(item, ImageUrl):\n                    image_url: ImageURL = {'url': item.url}\n                    if metadata := item.vendor_metadata:\n                        image_url['detail'] = metadata.get('detail', 'auto')\n                    if item.force_download:\n                        image_content = await download_item(item, data_format='base64_uri', type_format='extension')\n                        image_url['url'] = image_content['data']\n                    content.append(ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))\n                elif isinstance(item, BinaryContent):\n                    if self._is_text_like_media_type(item.media_type):\n                        # Inline text-like binary content as a text block\n                        content.append(\n                            self._inline_text_file_part(\n                                item.data.decode('utf-8'),\n                                media_type=item.media_type,\n                                identifier=item.identifier,\n                            )\n                        )\n                    elif item.is_image:\n                        image_url = ImageURL(url=item.data_uri)\n                        if metadata := item.vendor_metadata:\n                            image_url['detail'] = metadata.get('detail', 'auto')\n                        content.append(ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))\n                    elif item.is_audio:\n                        assert item.format in ('wav', 'mp3')\n                        audio = InputAudio(data=base64.b64encode(item.data).decode('utf-8'), format=item.format)\n                        content.append(ChatCompletionContentPartInputAudioParam(input_audio=audio, type='input_audio'))\n                    elif item.is_document:\n                        content.append(\n                            File(\n                                file=FileFile(\n                                    file_data=item.data_uri,\n                                    filename=f'filename.{item.format}',\n                                ),\n                                type='file',\n                            )\n                        )\n                    else:  # pragma: no cover\n                        raise RuntimeError(f'Unsupported binary content type: {item.media_type}')\n                elif isinstance(item, AudioUrl):\n                    downloaded_item = await download_item(item, data_format='base64', type_format='extension')\n                    assert downloaded_item['data_type'] in (\n                        'wav',\n                        'mp3',\n                    ), f'Unsupported audio format: {downloaded_item[\"data_type\"]}'\n                    audio = InputAudio(data=downloaded_item['data'], format=downloaded_item['data_type'])\n                    content.append(ChatCompletionContentPartInputAudioParam(input_audio=audio, type='input_audio'))\n                elif isinstance(item, DocumentUrl):\n                    if self._is_text_like_media_type(item.media_type):\n                        downloaded_text = await download_item(item, data_format='text')\n                        content.append(\n                            self._inline_text_file_part(\n                                downloaded_text['data'],\n                                media_type=item.media_type,\n                                identifier=item.identifier,\n                            )\n                        )\n                    else:\n                        downloaded_item = await download_item(item, data_format='base64_uri', type_format='extension')\n                        content.append(\n                            File(\n                                file=FileFile(\n                                    file_data=downloaded_item['data'],\n                                    filename=f'filename.{downloaded_item[\"data_type\"]}',\n                                ),\n                                type='file',\n                            )\n                        )\n                elif isinstance(item, VideoUrl):  # pragma: no cover\n                    raise NotImplementedError('VideoUrl is not supported for OpenAI')\n                elif isinstance(item, CachePoint):\n                    # OpenAI doesn't support prompt caching via CachePoint, so we filter it out\n                    pass\n                else:\n                    assert_never(item)\n        return chat.ChatCompletionUserMessageParam(role='user', content=content)\n\n    @staticmethod\n    def _is_text_like_media_type(media_type: str) -> bool:\n        return (\n            media_type.startswith('text/')\n            or media_type == 'application/json'\n            or media_type.endswith('+json')\n            or media_type == 'application/xml'\n            or media_type.endswith('+xml')\n            or media_type in ('application/x-yaml', 'application/yaml')\n        )\n\n    @staticmethod\n    def _inline_text_file_part(text: str, *, media_type: str, identifier: str) -> ChatCompletionContentPartTextParam:\n        text = '\\n'.join(\n            [\n                f'-----BEGIN FILE id=\"{identifier}\" type=\"{media_type}\"-----',\n                text,\n                f'-----END FILE id=\"{identifier}\"-----',\n            ]\n        )\n        return ChatCompletionContentPartTextParam(text=text, type='text')\n\n```\n\n#### __init__\n\n```python\n__init__(\n    model_name: OpenAIModelName,\n    *,\n    provider: (\n        Literal[\n            \"azure\",\n            \"deepseek\",\n            \"cerebras\",\n            \"fireworks\",\n            \"github\",\n            \"grok\",\n            \"heroku\",\n            \"moonshotai\",\n            \"ollama\",\n            \"openai\",\n            \"openai-chat\",\n            \"openrouter\",\n            \"together\",\n            \"vercel\",\n            \"litellm\",\n            \"nebius\",\n            \"ovhcloud\",\n            \"gateway\",\n        ]\n        | Provider[AsyncOpenAI]\n    ) = \"openai\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    model_name: OpenAIModelName,\n    *,\n    provider: (\n        Literal[\n            \"azure\",\n            \"deepseek\",\n            \"cerebras\",\n            \"fireworks\",\n            \"github\",\n            \"grok\",\n            \"heroku\",\n            \"moonshotai\",\n            \"ollama\",\n            \"openai\",\n            \"openai-chat\",\n            \"openrouter\",\n            \"together\",\n            \"vercel\",\n            \"litellm\",\n            \"nebius\",\n            \"ovhcloud\",\n            \"gateway\",\n        ]\n        | Provider[AsyncOpenAI]\n    ) = \"openai\",\n    profile: ModelProfileSpec | None = None,\n    system_prompt_role: (\n        OpenAISystemPromptRole | None\n    ) = None,\n    settings: ModelSettings | None = None\n) -> None\n\n```\n\n```python\n__init__(\n    model_name: OpenAIModelName,\n    *,\n    provider: (\n        Literal[\n            \"azure\",\n            \"deepseek\",\n            \"cerebras\",\n            \"fireworks\",\n            \"github\",\n            \"grok\",\n            \"heroku\",\n            \"moonshotai\",\n            \"ollama\",\n            \"openai\",\n            \"openai-chat\",\n            \"openrouter\",\n            \"together\",\n            \"vercel\",\n            \"litellm\",\n            \"nebius\",\n            \"ovhcloud\",\n            \"gateway\",\n        ]\n        | Provider[AsyncOpenAI]\n    ) = \"openai\",\n    profile: ModelProfileSpec | None = None,\n    system_prompt_role: (\n        OpenAISystemPromptRole | None\n    ) = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize an OpenAI model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model_name` | `OpenAIModelName` | The name of the OpenAI model to use. List of model names available here (Unfortunately, despite being ask to do so, OpenAI do not provide .inv files for their API). | *required* | | `provider` | `Literal['azure', 'deepseek', 'cerebras', 'fireworks', 'github', 'grok', 'heroku', 'moonshotai', 'ollama', 'openai', 'openai-chat', 'openrouter', 'together', 'vercel', 'litellm', 'nebius', 'ovhcloud', 'gateway'] | Provider[AsyncOpenAI]` | The provider to use. Defaults to 'openai'. | `'openai'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` | | `system_prompt_role` | `OpenAISystemPromptRole | None` | The role to use for the system prompt message. If not provided, defaults to 'system'. In the future, this may be inferred from the model name. | `None` | | `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n```python\ndef __init__(\n    self,\n    model_name: OpenAIModelName,\n    *,\n    provider: Literal[\n        'azure',\n        'deepseek',\n        'cerebras',\n        'fireworks',\n        'github',\n        'grok',\n        'heroku',\n        'moonshotai',\n        'ollama',\n        'openai',\n        'openai-chat',\n        'openrouter',\n        'together',\n        'vercel',\n        'litellm',\n        'nebius',\n        'ovhcloud',\n        'gateway',\n    ]\n    | Provider[AsyncOpenAI] = 'openai',\n    profile: ModelProfileSpec | None = None,\n    system_prompt_role: OpenAISystemPromptRole | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize an OpenAI model.\n\n    Args:\n        model_name: The name of the OpenAI model to use. List of model names available\n            [here](https://github.com/openai/openai-python/blob/v1.54.3/src/openai/types/chat_model.py#L7)\n            (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API).\n        provider: The provider to use. Defaults to `'openai'`.\n        profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n        system_prompt_role: The role to use for the system prompt message. If not provided, defaults to `'system'`.\n            In the future, this may be inferred from the model name.\n        settings: Default model settings for this model instance.\n    \"\"\"\n    self._model_name = model_name\n\n    if isinstance(provider, str):\n        provider = infer_provider('gateway/openai' if provider == 'gateway' else provider)\n    self._provider = provider\n    self.client = provider.client\n\n    super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n    if system_prompt_role is not None:\n        self.profile = OpenAIModelProfile(openai_system_prompt_role=system_prompt_role).update(self.profile)\n\n```\n\n#### model_name\n\n```python\nmodel_name: OpenAIModelName\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe model provider.\n\n### OpenAIModel\n\nBases: `OpenAIChatModel`\n\nDeprecated\n\n`OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio.\n\nDeprecated alias for `OpenAIChatModel`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n```python\n@deprecated(\n    '`OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which '\n    \"uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or \"\n    \"require a feature that the Responses API doesn't support yet like audio.\"\n)\n@dataclass(init=False)\nclass OpenAIModel(OpenAIChatModel):\n    \"\"\"Deprecated alias for `OpenAIChatModel`.\"\"\"\n\n```\n\n### OpenAIResponsesModel\n\nBases: `Model`\n\nA model that uses the OpenAI Responses API.\n\nThe [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) is the new API for OpenAI models.\n\nIf you are interested in the differences between the Responses API and the Chat Completions API, see the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions).\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n```python\n@dataclass(init=False)\nclass OpenAIResponsesModel(Model):\n    \"\"\"A model that uses the OpenAI Responses API.\n\n    The [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) is the\n    new API for OpenAI models.\n\n    If you are interested in the differences between the Responses API and the Chat Completions API,\n    see the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions).\n    \"\"\"\n\n    client: AsyncOpenAI = field(repr=False)\n\n    _model_name: OpenAIModelName = field(repr=False)\n    _provider: Provider[AsyncOpenAI] = field(repr=False)\n\n    def __init__(\n        self,\n        model_name: OpenAIModelName,\n        *,\n        provider: Literal[\n            'openai',\n            'deepseek',\n            'azure',\n            'openrouter',\n            'grok',\n            'fireworks',\n            'together',\n            'nebius',\n            'ovhcloud',\n            'gateway',\n        ]\n        | Provider[AsyncOpenAI] = 'openai',\n        profile: ModelProfileSpec | None = None,\n        settings: ModelSettings | None = None,\n    ):\n        \"\"\"Initialize an OpenAI Responses model.\n\n        Args:\n            model_name: The name of the OpenAI model to use.\n            provider: The provider to use. Defaults to `'openai'`.\n            profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n            settings: Default model settings for this model instance.\n        \"\"\"\n        self._model_name = model_name\n\n        if isinstance(provider, str):\n            provider = infer_provider('gateway/openai' if provider == 'gateway' else provider)\n        self._provider = provider\n        self.client = provider.client\n\n        super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n    @property\n    def base_url(self) -> str:\n        return str(self.client.base_url)\n\n    @property\n    def model_name(self) -> OpenAIModelName:\n        \"\"\"The model name.\"\"\"\n        return self._model_name\n\n    @property\n    def system(self) -> str:\n        \"\"\"The model provider.\"\"\"\n        return self._provider.name\n\n    async def request(\n        self,\n        messages: list[ModelRequest | ModelResponse],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n    ) -> ModelResponse:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._responses_create(\n            messages, False, cast(OpenAIResponsesModelSettings, model_settings or {}), model_request_parameters\n        )\n        return self._process_response(response, model_request_parameters)\n\n    @asynccontextmanager\n    async def request_stream(\n        self,\n        messages: list[ModelMessage],\n        model_settings: ModelSettings | None,\n        model_request_parameters: ModelRequestParameters,\n        run_context: RunContext[Any] | None = None,\n    ) -> AsyncIterator[StreamedResponse]:\n        check_allow_model_requests()\n        model_settings, model_request_parameters = self.prepare_request(\n            model_settings,\n            model_request_parameters,\n        )\n        response = await self._responses_create(\n            messages, True, cast(OpenAIResponsesModelSettings, model_settings or {}), model_request_parameters\n        )\n        async with response:\n            yield await self._process_streamed_response(response, model_request_parameters)\n\n    def _process_response(  # noqa: C901\n        self, response: responses.Response, model_request_parameters: ModelRequestParameters\n    ) -> ModelResponse:\n        \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"\n        timestamp = number_to_datetime(response.created_at)\n        items: list[ModelResponsePart] = []\n        for item in response.output:\n            if isinstance(item, responses.ResponseReasoningItem):\n                signature = item.encrypted_content\n                if item.summary:\n                    for summary in item.summary:\n                        # We use the same id for all summaries so that we can merge them on the round trip.\n                        items.append(\n                            ThinkingPart(\n                                content=summary.text,\n                                id=item.id,\n                                signature=signature,\n                                provider_name=self.system if signature else None,\n                            )\n                        )\n                        # We only need to store the signature once.\n                        signature = None\n                elif signature:\n                    items.append(\n                        ThinkingPart(\n                            content='',\n                            id=item.id,\n                            signature=signature,\n                            provider_name=self.system,\n                        )\n                    )\n                # NOTE: We don't currently handle the raw CoT from gpt-oss `reasoning_text`: https://cookbook.openai.com/articles/gpt-oss/handle-raw-cot\n                # If you need this, please file an issue.\n            elif isinstance(item, responses.ResponseOutputMessage):\n                for content in item.content:\n                    if isinstance(content, responses.ResponseOutputText):  # pragma: no branch\n                        part_provider_details: dict[str, Any] | None = None\n                        if content.logprobs:\n                            part_provider_details = {'logprobs': _map_logprobs(content.logprobs)}\n                        items.append(TextPart(content.text, id=item.id, provider_details=part_provider_details))\n            elif isinstance(item, responses.ResponseFunctionToolCall):\n                items.append(\n                    ToolCallPart(\n                        item.name,\n                        item.arguments,\n                        tool_call_id=item.call_id,\n                        id=item.id,\n                    )\n                )\n            elif isinstance(item, responses.ResponseCodeInterpreterToolCall):\n                call_part, return_part, file_parts = _map_code_interpreter_tool_call(item, self.system)\n                items.append(call_part)\n                if file_parts:\n                    items.extend(file_parts)\n                items.append(return_part)\n            elif isinstance(item, responses.ResponseFunctionWebSearch):\n                call_part, return_part = _map_web_search_tool_call(item, self.system)\n                items.append(call_part)\n                items.append(return_part)\n            elif isinstance(item, responses.response_output_item.ImageGenerationCall):\n                call_part, return_part, file_part = _map_image_generation_tool_call(item, self.system)\n                items.append(call_part)\n                if file_part:  # pragma: no branch\n                    items.append(file_part)\n                items.append(return_part)\n            elif isinstance(item, responses.ResponseComputerToolCall):  # pragma: no cover\n                # Pydantic AI doesn't yet support the ComputerUse built-in tool\n                pass\n            elif isinstance(item, responses.ResponseCustomToolCall):  # pragma: no cover\n                # Support is being implemented in https://github.com/pydantic/pydantic-ai/pull/2572\n                pass\n            elif isinstance(item, responses.response_output_item.LocalShellCall):  # pragma: no cover\n                # Pydantic AI doesn't yet support the `codex-mini-latest` LocalShell built-in tool\n                pass\n            elif isinstance(item, responses.ResponseFileSearchToolCall):  # pragma: no cover\n                # Pydantic AI doesn't yet support the FileSearch built-in tool\n                pass\n            elif isinstance(item, responses.response_output_item.McpCall):\n                call_part, return_part = _map_mcp_call(item, self.system)\n                items.append(call_part)\n                items.append(return_part)\n            elif isinstance(item, responses.response_output_item.McpListTools):\n                call_part, return_part = _map_mcp_list_tools(item, self.system)\n                items.append(call_part)\n                items.append(return_part)\n            elif isinstance(item, responses.response_output_item.McpApprovalRequest):  # pragma: no cover\n                # Pydantic AI doesn't yet support McpApprovalRequest (explicit tool usage approval)\n                pass\n\n        finish_reason: FinishReason | None = None\n        provider_details: dict[str, Any] | None = None\n        raw_finish_reason = details.reason if (details := response.incomplete_details) else response.status\n        if raw_finish_reason:\n            provider_details = {'finish_reason': raw_finish_reason}\n            finish_reason = _RESPONSES_FINISH_REASON_MAP.get(raw_finish_reason)\n\n        return ModelResponse(\n            parts=items,\n            usage=_map_usage(response, self._provider.name, self._provider.base_url, self.model_name),\n            model_name=response.model,\n            provider_response_id=response.id,\n            timestamp=timestamp,\n            provider_name=self._provider.name,\n            finish_reason=finish_reason,\n            provider_details=provider_details,\n        )\n\n    async def _process_streamed_response(\n        self,\n        response: AsyncStream[responses.ResponseStreamEvent],\n        model_request_parameters: ModelRequestParameters,\n    ) -> OpenAIResponsesStreamedResponse:\n        \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"\n        peekable_response = _utils.PeekableAsyncStream(response)\n        first_chunk = await peekable_response.peek()\n        if isinstance(first_chunk, _utils.Unset):  # pragma: no cover\n            raise UnexpectedModelBehavior('Streamed response ended without content or tool calls')\n\n        assert isinstance(first_chunk, responses.ResponseCreatedEvent)\n        return OpenAIResponsesStreamedResponse(\n            model_request_parameters=model_request_parameters,\n            _model_name=first_chunk.response.model,\n            _response=peekable_response,\n            _timestamp=number_to_datetime(first_chunk.response.created_at),\n            _provider_name=self._provider.name,\n            _provider_url=self._provider.base_url,\n        )\n\n    @overload\n    async def _responses_create(\n        self,\n        messages: list[ModelRequest | ModelResponse],\n        stream: Literal[False],\n        model_settings: OpenAIResponsesModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> responses.Response: ...\n\n    @overload\n    async def _responses_create(\n        self,\n        messages: list[ModelRequest | ModelResponse],\n        stream: Literal[True],\n        model_settings: OpenAIResponsesModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> AsyncStream[responses.ResponseStreamEvent]: ...\n\n    async def _responses_create(  # noqa: C901\n        self,\n        messages: list[ModelRequest | ModelResponse],\n        stream: bool,\n        model_settings: OpenAIResponsesModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> responses.Response | AsyncStream[responses.ResponseStreamEvent]:\n        tools = (\n            self._get_builtin_tools(model_request_parameters)\n            + list(model_settings.get('openai_builtin_tools', []))\n            + self._get_tools(model_request_parameters)\n        )\n        profile = OpenAIModelProfile.from_profile(self.profile)\n        if not tools:\n            tool_choice: Literal['none', 'required', 'auto'] | None = None\n        elif not model_request_parameters.allow_text_output and profile.openai_supports_tool_choice_required:\n            tool_choice = 'required'\n        else:\n            tool_choice = 'auto'\n\n        previous_response_id = model_settings.get('openai_previous_response_id')\n        if previous_response_id == 'auto':\n            previous_response_id, messages = self._get_previous_response_id_and_new_messages(messages)\n\n        instructions, openai_messages = await self._map_messages(messages, model_settings, model_request_parameters)\n        reasoning = self._get_reasoning(model_settings)\n\n        text: responses.ResponseTextConfigParam | None = None\n        if model_request_parameters.output_mode == 'native':\n            output_object = model_request_parameters.output_object\n            assert output_object is not None\n            text = {'format': self._map_json_schema(output_object)}\n        elif (\n            model_request_parameters.output_mode == 'prompted' and self.profile.supports_json_object_output\n        ):  # pragma: no branch\n            text = {'format': {'type': 'json_object'}}\n\n            # Without this trick, we'd hit this error:\n            # > Response input messages must contain the word 'json' in some form to use 'text.format' of type 'json_object'.\n            # Apparently they're only checking input messages for \"JSON\", not instructions.\n            assert isinstance(instructions, str)\n            openai_messages.insert(0, responses.EasyInputMessageParam(role='system', content=instructions))\n            instructions = OMIT\n\n        if verbosity := model_settings.get('openai_text_verbosity'):\n            text = text or {}\n            text['verbosity'] = verbosity\n\n        unsupported_model_settings = profile.openai_unsupported_model_settings\n        for setting in unsupported_model_settings:\n            model_settings.pop(setting, None)\n\n        include: list[responses.ResponseIncludable] = []\n        if profile.openai_supports_encrypted_reasoning_content:\n            include.append('reasoning.encrypted_content')\n        if model_settings.get('openai_include_code_execution_outputs'):\n            include.append('code_interpreter_call.outputs')\n        if model_settings.get('openai_include_web_search_sources'):\n            include.append('web_search_call.action.sources')\n        if model_settings.get('openai_logprobs'):\n            include.append('message.output_text.logprobs')\n\n        # When there are no input messages and we're not reusing a previous response,\n        # the OpenAI API will reject a request without any input,\n        # even if there are instructions.\n        # To avoid this provide an explicit empty user message.\n        if not openai_messages and not previous_response_id:\n            openai_messages.append(\n                responses.EasyInputMessageParam(\n                    role='user',\n                    content='',\n                )\n            )\n\n        try:\n            extra_headers = model_settings.get('extra_headers', {})\n            extra_headers.setdefault('User-Agent', get_user_agent())\n            return await self.client.responses.create(\n                input=openai_messages,\n                model=self.model_name,\n                instructions=instructions,\n                parallel_tool_calls=model_settings.get('parallel_tool_calls', OMIT),\n                tools=tools or OMIT,\n                tool_choice=tool_choice or OMIT,\n                max_output_tokens=model_settings.get('max_tokens', OMIT),\n                stream=stream,\n                temperature=model_settings.get('temperature', OMIT),\n                top_p=model_settings.get('top_p', OMIT),\n                truncation=model_settings.get('openai_truncation', OMIT),\n                timeout=model_settings.get('timeout', NOT_GIVEN),\n                service_tier=model_settings.get('openai_service_tier', OMIT),\n                previous_response_id=previous_response_id or OMIT,\n                top_logprobs=model_settings.get('openai_top_logprobs', OMIT),\n                reasoning=reasoning,\n                user=model_settings.get('openai_user', OMIT),\n                text=text or OMIT,\n                include=include or OMIT,\n                extra_headers=extra_headers,\n                extra_body=model_settings.get('extra_body'),\n            )\n        except APIStatusError as e:\n            if (status_code := e.status_code) >= 400:\n                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e\n            raise  # pragma: lax no cover\n        except APIConnectionError as e:\n            raise ModelAPIError(model_name=self.model_name, message=e.message) from e\n\n    def _get_reasoning(self, model_settings: OpenAIResponsesModelSettings) -> Reasoning | Omit:\n        reasoning_effort = model_settings.get('openai_reasoning_effort', None)\n        reasoning_summary = model_settings.get('openai_reasoning_summary', None)\n        reasoning_generate_summary = model_settings.get('openai_reasoning_generate_summary', None)\n\n        if reasoning_summary and reasoning_generate_summary:  # pragma: no cover\n            raise ValueError('`openai_reasoning_summary` and `openai_reasoning_generate_summary` cannot both be set.')\n\n        if reasoning_generate_summary is not None:  # pragma: no cover\n            warnings.warn(\n                '`openai_reasoning_generate_summary` is deprecated, use `openai_reasoning_summary` instead',\n                DeprecationWarning,\n            )\n            reasoning_summary = reasoning_generate_summary\n\n        if reasoning_effort is None and reasoning_summary is None:\n            return OMIT\n        return Reasoning(effort=reasoning_effort, summary=reasoning_summary)\n\n    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[responses.FunctionToolParam]:\n        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]\n\n    def _get_builtin_tools(self, model_request_parameters: ModelRequestParameters) -> list[responses.ToolParam]:\n        tools: list[responses.ToolParam] = []\n        has_image_generating_tool = False\n        for tool in model_request_parameters.builtin_tools:\n            if isinstance(tool, WebSearchTool):\n                web_search_tool = responses.WebSearchToolParam(\n                    type='web_search', search_context_size=tool.search_context_size\n                )\n                if tool.user_location:\n                    web_search_tool['user_location'] = responses.web_search_tool_param.UserLocation(\n                        type='approximate', **tool.user_location\n                    )\n                tools.append(web_search_tool)\n            elif isinstance(tool, CodeExecutionTool):\n                has_image_generating_tool = True\n                tools.append({'type': 'code_interpreter', 'container': {'type': 'auto'}})\n            elif isinstance(tool, MCPServerTool):\n                mcp_tool = responses.tool_param.Mcp(\n                    type='mcp',\n                    server_label=tool.id,\n                    require_approval='never',\n                )\n\n                if tool.authorization_token:  # pragma: no branch\n                    mcp_tool['authorization'] = tool.authorization_token\n\n                if tool.allowed_tools is not None:  # pragma: no branch\n                    mcp_tool['allowed_tools'] = tool.allowed_tools\n\n                if tool.description:  # pragma: no branch\n                    mcp_tool['server_description'] = tool.description\n\n                if tool.headers:  # pragma: no branch\n                    mcp_tool['headers'] = tool.headers\n\n                if tool.url.startswith(MCP_SERVER_TOOL_CONNECTOR_URI_SCHEME + ':'):\n                    _, connector_id = tool.url.split(':', maxsplit=1)\n                    mcp_tool['connector_id'] = connector_id  # pyright: ignore[reportGeneralTypeIssues]\n                else:\n                    mcp_tool['server_url'] = tool.url\n\n                tools.append(mcp_tool)\n            elif isinstance(tool, ImageGenerationTool):  # pragma: no branch\n                has_image_generating_tool = True\n                tools.append(\n                    responses.tool_param.ImageGeneration(\n                        type='image_generation',\n                        background=tool.background,\n                        input_fidelity=tool.input_fidelity,\n                        moderation=tool.moderation,\n                        output_compression=tool.output_compression,\n                        output_format=tool.output_format or 'png',\n                        partial_images=tool.partial_images,\n                        quality=tool.quality,\n                        size=tool.size,\n                    )\n                )\n            else:\n                raise UserError(  # pragma: no cover\n                    f'`{tool.__class__.__name__}` is not supported by `OpenAIResponsesModel`. If it should be, please file an issue.'\n                )\n\n        if model_request_parameters.allow_image_output and not has_image_generating_tool:\n            tools.append({'type': 'image_generation'})\n        return tools\n\n    def _map_tool_definition(self, f: ToolDefinition) -> responses.FunctionToolParam:\n        return {\n            'name': f.name,\n            'parameters': f.parameters_json_schema,\n            'type': 'function',\n            'description': f.description,\n            'strict': bool(\n                f.strict and OpenAIModelProfile.from_profile(self.profile).openai_supports_strict_tool_definition\n            ),\n        }\n\n    def _get_previous_response_id_and_new_messages(\n        self, messages: list[ModelMessage]\n    ) -> tuple[str | None, list[ModelMessage]]:\n        # When `openai_previous_response_id` is set to 'auto', the most recent\n        # `provider_response_id` from the message history is selected and all\n        # earlier messages are omitted. This allows the OpenAI SDK to reuse\n        # server-side history for efficiency. The returned tuple contains the\n        # `previous_response_id` (if found) and the trimmed list of messages.\n        previous_response_id = None\n        trimmed_messages: list[ModelMessage] = []\n        for m in reversed(messages):\n            if isinstance(m, ModelResponse) and m.provider_name == self.system:\n                previous_response_id = m.provider_response_id\n                break\n            else:\n                trimmed_messages.append(m)\n\n        if previous_response_id and trimmed_messages:\n            return previous_response_id, list(reversed(trimmed_messages))\n        else:\n            return None, messages\n\n    async def _map_messages(  # noqa: C901\n        self,\n        messages: list[ModelMessage],\n        model_settings: OpenAIResponsesModelSettings,\n        model_request_parameters: ModelRequestParameters,\n    ) -> tuple[str | Omit, list[responses.ResponseInputItemParam]]:\n        \"\"\"Just maps a `pydantic_ai.Message` to a `openai.types.responses.ResponseInputParam`.\"\"\"\n        profile = OpenAIModelProfile.from_profile(self.profile)\n        send_item_ids = model_settings.get(\n            'openai_send_reasoning_ids', profile.openai_supports_encrypted_reasoning_content\n        )\n\n        openai_messages: list[responses.ResponseInputItemParam] = []\n        for message in messages:\n            if isinstance(message, ModelRequest):\n                for part in message.parts:\n                    if isinstance(part, SystemPromptPart):\n                        openai_messages.append(responses.EasyInputMessageParam(role='system', content=part.content))\n                    elif isinstance(part, UserPromptPart):\n                        openai_messages.append(await self._map_user_prompt(part))\n                    elif isinstance(part, ToolReturnPart):\n                        call_id = _guard_tool_call_id(t=part)\n                        call_id, _ = _split_combined_tool_call_id(call_id)\n                        item = FunctionCallOutput(\n                            type='function_call_output',\n                            call_id=call_id,\n                            output=part.model_response_str(),\n                        )\n                        openai_messages.append(item)\n                    elif isinstance(part, RetryPromptPart):\n                        if part.tool_name is None:\n                            openai_messages.append(\n                                Message(role='user', content=[{'type': 'input_text', 'text': part.model_response()}])\n                            )\n                        else:\n                            call_id = _guard_tool_call_id(t=part)\n                            call_id, _ = _split_combined_tool_call_id(call_id)\n                            item = FunctionCallOutput(\n                                type='function_call_output',\n                                call_id=call_id,\n                                output=part.model_response(),\n                            )\n                            openai_messages.append(item)\n                    else:\n                        assert_never(part)\n            elif isinstance(message, ModelResponse):\n                send_item_ids = send_item_ids and message.provider_name == self.system\n\n                message_item: responses.ResponseOutputMessageParam | None = None\n                reasoning_item: responses.ResponseReasoningItemParam | None = None\n                web_search_item: responses.ResponseFunctionWebSearchParam | None = None\n                code_interpreter_item: responses.ResponseCodeInterpreterToolCallParam | None = None\n                for item in message.parts:\n                    if isinstance(item, TextPart):\n                        if item.id and send_item_ids:\n                            if message_item is None or message_item['id'] != item.id:  # pragma: no branch\n                                message_item = responses.ResponseOutputMessageParam(\n                                    role='assistant',\n                                    id=item.id,\n                                    content=[],\n                                    type='message',\n                                    status='completed',\n                                )\n                                openai_messages.append(message_item)\n\n                            message_item['content'] = [\n                                *message_item['content'],\n                                responses.ResponseOutputTextParam(\n                                    text=item.content, type='output_text', annotations=[]\n                                ),\n                            ]\n                        else:\n                            openai_messages.append(\n                                responses.EasyInputMessageParam(role='assistant', content=item.content)\n                            )\n                    elif isinstance(item, ToolCallPart):\n                        call_id = _guard_tool_call_id(t=item)\n                        call_id, id = _split_combined_tool_call_id(call_id)\n                        id = id or item.id\n\n                        param = responses.ResponseFunctionToolCallParam(\n                            name=item.tool_name,\n                            arguments=item.args_as_json_str(),\n                            call_id=call_id,\n                            type='function_call',\n                        )\n                        if profile.openai_responses_requires_function_call_status_none:\n                            param['status'] = None  # type: ignore[reportGeneralTypeIssues]\n                        if id and send_item_ids:  # pragma: no branch\n                            param['id'] = id\n                        openai_messages.append(param)\n                    elif isinstance(item, BuiltinToolCallPart):\n                        if item.provider_name == self.system and send_item_ids:  # pragma: no branch\n                            if (\n                                item.tool_name == CodeExecutionTool.kind\n                                and item.tool_call_id\n                                and (args := item.args_as_dict())\n                                and (container_id := args.get('container_id'))\n                            ):\n                                code_interpreter_item = responses.ResponseCodeInterpreterToolCallParam(\n                                    id=item.tool_call_id,\n                                    code=args.get('code'),\n                                    container_id=container_id,\n                                    outputs=None,  # These can be read server-side\n                                    status='completed',\n                                    type='code_interpreter_call',\n                                )\n                                openai_messages.append(code_interpreter_item)\n                            elif (\n                                item.tool_name == WebSearchTool.kind\n                                and item.tool_call_id\n                                and (args := item.args_as_dict())\n                            ):\n                                web_search_item = responses.ResponseFunctionWebSearchParam(\n                                    id=item.tool_call_id,\n                                    action=cast(responses.response_function_web_search_param.Action, args),\n                                    status='completed',\n                                    type='web_search_call',\n                                )\n                                openai_messages.append(web_search_item)\n                            elif item.tool_name == ImageGenerationTool.kind and item.tool_call_id:\n                                # The cast is necessary because of https://github.com/openai/openai-python/issues/2648\n                                image_generation_item = cast(\n                                    responses.response_input_item_param.ImageGenerationCall,\n                                    {\n                                        'id': item.tool_call_id,\n                                        'type': 'image_generation_call',\n                                    },\n                                )\n                                openai_messages.append(image_generation_item)\n                            elif (  # pragma: no branch\n                                item.tool_name.startswith(MCPServerTool.kind)\n                                and item.tool_call_id\n                                and (server_id := item.tool_name.split(':', 1)[1])\n                                and (args := item.args_as_dict())\n                                and (action := args.get('action'))\n                            ):\n                                if action == 'list_tools':\n                                    mcp_list_tools_item = responses.response_input_item_param.McpListTools(\n                                        id=item.tool_call_id,\n                                        type='mcp_list_tools',\n                                        server_label=server_id,\n                                        tools=[],  # These can be read server-side\n                                    )\n                                    openai_messages.append(mcp_list_tools_item)\n                                elif (  # pragma: no branch\n                                    action == 'call_tool'\n                                    and (tool_name := args.get('tool_name'))\n                                    and (tool_args := args.get('tool_args'))\n                                ):\n                                    mcp_call_item = responses.response_input_item_param.McpCall(\n                                        id=item.tool_call_id,\n                                        server_label=server_id,\n                                        name=tool_name,\n                                        arguments=to_json(tool_args).decode(),\n                                        error=None,  # These can be read server-side\n                                        output=None,  # These can be read server-side\n                                        type='mcp_call',\n                                    )\n                                    openai_messages.append(mcp_call_item)\n\n                    elif isinstance(item, BuiltinToolReturnPart):\n                        if item.provider_name == self.system and send_item_ids:  # pragma: no branch\n                            if (\n                                item.tool_name == CodeExecutionTool.kind\n                                and code_interpreter_item is not None\n                                and isinstance(item.content, dict)\n                                and (content := cast(dict[str, Any], item.content))  # pyright: ignore[reportUnknownMemberType]\n                                and (status := content.get('status'))\n                            ):\n                                code_interpreter_item['status'] = status\n                            elif (\n                                item.tool_name == WebSearchTool.kind\n                                and web_search_item is not None\n                                and isinstance(item.content, dict)  # pyright: ignore[reportUnknownMemberType]\n                                and (content := cast(dict[str, Any], item.content))  # pyright: ignore[reportUnknownMemberType]\n                                and (status := content.get('status'))\n                            ):\n                                web_search_item['status'] = status\n                            elif item.tool_name == ImageGenerationTool.kind:\n                                # Image generation result does not need to be sent back, just the `id` off of `BuiltinToolCallPart`.\n                                pass\n                            elif item.tool_name.startswith(MCPServerTool.kind):  # pragma: no branch\n                                # MCP call result does not need to be sent back, just the fields off of `BuiltinToolCallPart`.\n                                pass\n                    elif isinstance(item, FilePart):\n                        # This was generated by the `ImageGenerationTool` or `CodeExecutionTool`,\n                        # and does not need to be sent back separately from the corresponding `BuiltinToolReturnPart`.\n                        # If `send_item_ids` is false, we won't send the `BuiltinToolReturnPart`, but OpenAI does not have a type for files from the assistant.\n                        pass\n                    elif isinstance(item, ThinkingPart):\n                        if item.id and send_item_ids:\n                            signature: str | None = None\n                            if (\n                                item.signature\n                                and item.provider_name == self.system\n                                and profile.openai_supports_encrypted_reasoning_content\n                            ):\n                                signature = item.signature\n\n                            if (reasoning_item is None or reasoning_item['id'] != item.id) and (\n                                signature or item.content\n                            ):  # pragma: no branch\n                                reasoning_item = responses.ResponseReasoningItemParam(\n                                    id=item.id,\n                                    summary=[],\n                                    encrypted_content=signature,\n                                    type='reasoning',\n                                )\n                                openai_messages.append(reasoning_item)\n\n                            if item.content:\n                                # The check above guarantees that `reasoning_item` is not None\n                                assert reasoning_item is not None\n                                reasoning_item['summary'] = [\n                                    *reasoning_item['summary'],\n                                    Summary(text=item.content, type='summary_text'),\n                                ]\n                        else:\n                            start_tag, end_tag = profile.thinking_tags\n                            openai_messages.append(\n                                responses.EasyInputMessageParam(\n                                    role='assistant', content='\\n'.join([start_tag, item.content, end_tag])\n                                )\n                            )\n                    else:\n                        assert_never(item)\n            else:\n                assert_never(message)\n        instructions = self._get_instructions(messages, model_request_parameters) or OMIT\n        return instructions, openai_messages\n\n    def _map_json_schema(self, o: OutputObjectDefinition) -> responses.ResponseFormatTextJSONSchemaConfigParam:\n        response_format_param: responses.ResponseFormatTextJSONSchemaConfigParam = {\n            'type': 'json_schema',\n            'name': o.name or DEFAULT_OUTPUT_TOOL_NAME,\n            'schema': o.json_schema,\n        }\n        if o.description:\n            response_format_param['description'] = o.description\n        if OpenAIModelProfile.from_profile(self.profile).openai_supports_strict_tool_definition:  # pragma: no branch\n            response_format_param['strict'] = o.strict\n        return response_format_param\n\n    @staticmethod\n    async def _map_user_prompt(part: UserPromptPart) -> responses.EasyInputMessageParam:  # noqa: C901\n        content: str | list[responses.ResponseInputContentParam]\n        if isinstance(part.content, str):\n            content = part.content\n        else:\n            content = []\n            for item in part.content:\n                if isinstance(item, str):\n                    content.append(responses.ResponseInputTextParam(text=item, type='input_text'))\n                elif isinstance(item, BinaryContent):\n                    if item.is_image:\n                        detail: Literal['auto', 'low', 'high'] = 'auto'\n                        if metadata := item.vendor_metadata:\n                            detail = cast(\n                                Literal['auto', 'low', 'high'],\n                                metadata.get('detail', 'auto'),\n                            )\n                        content.append(\n                            responses.ResponseInputImageParam(\n                                image_url=item.data_uri,\n                                type='input_image',\n                                detail=detail,\n                            )\n                        )\n                    elif item.is_document:\n                        content.append(\n                            responses.ResponseInputFileParam(\n                                type='input_file',\n                                file_data=item.data_uri,\n                                # NOTE: Type wise it's not necessary to include the filename, but it's required by the\n                                # API itself. If we add empty string, the server sends a 500 error - which OpenAI needs\n                                # to fix. In any case, we add a placeholder name.\n                                filename=f'filename.{item.format}',\n                            )\n                        )\n                    elif item.is_audio:\n                        raise NotImplementedError('Audio as binary content is not supported for OpenAI Responses API.')\n                    else:  # pragma: no cover\n                        raise RuntimeError(f'Unsupported binary content type: {item.media_type}')\n                elif isinstance(item, ImageUrl):\n                    detail: Literal['auto', 'low', 'high'] = 'auto'\n                    image_url = item.url\n                    if metadata := item.vendor_metadata:\n                        detail = cast(Literal['auto', 'low', 'high'], metadata.get('detail', 'auto'))\n                    if item.force_download:\n                        downloaded_item = await download_item(item, data_format='base64_uri', type_format='extension')\n                        image_url = downloaded_item['data']\n\n                    content.append(\n                        responses.ResponseInputImageParam(\n                            image_url=image_url,\n                            type='input_image',\n                            detail=detail,\n                        )\n                    )\n                elif isinstance(item, AudioUrl):  # pragma: no cover\n                    downloaded_item = await download_item(item, data_format='base64_uri', type_format='extension')\n                    content.append(\n                        responses.ResponseInputFileParam(\n                            type='input_file',\n                            file_data=downloaded_item['data'],\n                            filename=f'filename.{downloaded_item[\"data_type\"]}',\n                        )\n                    )\n                elif isinstance(item, DocumentUrl):\n                    downloaded_item = await download_item(item, data_format='base64_uri', type_format='extension')\n                    content.append(\n                        responses.ResponseInputFileParam(\n                            type='input_file',\n                            file_data=downloaded_item['data'],\n                            filename=f'filename.{downloaded_item[\"data_type\"]}',\n                        )\n                    )\n                elif isinstance(item, VideoUrl):  # pragma: no cover\n                    raise NotImplementedError('VideoUrl is not supported for OpenAI.')\n                elif isinstance(item, CachePoint):\n                    # OpenAI doesn't support prompt caching via CachePoint, so we filter it out\n                    pass\n                else:\n                    assert_never(item)\n        return responses.EasyInputMessageParam(role='user', content=content)\n\n```\n\n#### __init__\n\n```python\n__init__(\n    model_name: OpenAIModelName,\n    *,\n    provider: (\n        Literal[\n            \"openai\",\n            \"deepseek\",\n            \"azure\",\n            \"openrouter\",\n            \"grok\",\n            \"fireworks\",\n            \"together\",\n            \"nebius\",\n            \"ovhcloud\",\n            \"gateway\",\n        ]\n        | Provider[AsyncOpenAI]\n    ) = \"openai\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n\n```\n\nInitialize an OpenAI Responses model.\n\nParameters:\n\n| Name | Type | Description | Default | | --- | --- | --- | --- | | `model_name` | `OpenAIModelName` | The name of the OpenAI model to use. | *required* | | `provider` | `Literal['openai', 'deepseek', 'azure', 'openrouter', 'grok', 'fireworks', 'together', 'nebius', 'ovhcloud', 'gateway'] | Provider[AsyncOpenAI]` | The provider to use. Defaults to 'openai'. | `'openai'` | | `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` | | `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n```python\ndef __init__(\n    self,\n    model_name: OpenAIModelName,\n    *,\n    provider: Literal[\n        'openai',\n        'deepseek',\n        'azure',\n        'openrouter',\n        'grok',\n        'fireworks',\n        'together',\n        'nebius',\n        'ovhcloud',\n        'gateway',\n    ]\n    | Provider[AsyncOpenAI] = 'openai',\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None,\n):\n    \"\"\"Initialize an OpenAI Responses model.\n\n    Args:\n        model_name: The name of the OpenAI model to use.\n        provider: The provider to use. Defaults to `'openai'`.\n        profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n        settings: Default model settings for this model instance.\n    \"\"\"\n    self._model_name = model_name\n\n    if isinstance(provider, str):\n        provider = infer_provider('gateway/openai' if provider == 'gateway' else provider)\n    self._provider = provider\n    self.client = provider.client\n\n    super().__init__(settings=settings, profile=profile or provider.model_profile)\n\n```\n\n#### model_name\n\n```python\nmodel_name: OpenAIModelName\n\n```\n\nThe model name.\n\n#### system\n\n```python\nsystem: str\n\n```\n\nThe model provider.",
  "content_length": 97900
}