{
  "title": "Command Line Interface (CLI)",
  "source_url": null,
  "content": "**Pydantic AI** comes with a CLI, `clai` (pronounced \"clay\") which you can use to interact with various LLMs from the command line. It provides a convenient way to chat with language models and quickly get answers right in the terminal.\n\nWe originally developed this CLI for our own use, but found ourselves using it so frequently that we decided to share it as part of the Pydantic AI package.\n\nWe plan to continue adding new features, such as interaction with MCP servers, access to tools, and more.\n\n## Usage\n\nYou'll need to set an environment variable depending on the provider you intend to use.\n\nE.g. if you're using OpenAI, set the `OPENAI_API_KEY` environment variable:\n\n```bash\nexport OPENAI_API_KEY='your-api-key-here'\n\n```\n\nThen with [`uvx`](https://docs.astral.sh/uv/guides/tools/), run:\n\n```bash\nuvx clai\n\n```\n\nOr to install `clai` globally [with `uv`](https://docs.astral.sh/uv/guides/tools/#installing-tools), run:\n\n```bash\nuv tool install clai\n...\nclai\n\n```\n\nOr with `pip`, run:\n\n```bash\npip install clai\n...\nclai\n\n```\n\nEither way, running `clai` will start an interactive session where you can chat with the AI model. Special commands available in interactive mode:\n\n- `/exit`: Exit the session\n- `/markdown`: Show the last response in markdown format\n- `/multiline`: Toggle multiline input mode (use Ctrl+D to submit)\n- `/cp`: Copy the last response to clipboard\n\n### Help\n\nTo get help on the CLI, use the `--help` flag:\n\n```bash\nuvx clai --help\n\n```\n\n### Choose a model\n\nYou can specify which model to use with the `--model` flag:\n\n```bash\nuvx clai --model anthropic:claude-sonnet-4-0\n\n```\n\n(a full list of models available can be printed with `uvx clai --list-models`)\n\n### Custom Agents\n\nYou can specify a custom agent using the `--agent` flag with a module path and variable name:\n\n[Learn about Gateway](../gateway) custom_agent.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5', instructions='You always respond in Italian.')\n\n```\n\ncustom_agent.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='You always respond in Italian.')\n\n```\n\nThen run:\n\n```bash\nuvx clai --agent custom_agent:agent \"What's the weather today?\"\n\n```\n\nThe format must be `module:variable` where:\n\n- `module` is the importable Python module path\n- `variable` is the name of the Agent instance in that module\n\nAdditionally, you can directly launch CLI mode from an `Agent` instance using `Agent.to_cli_sync()`:\n\n[Learn about Gateway](../gateway) agent_to_cli_sync.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5', instructions='You always respond in Italian.')\nagent.to_cli_sync()\n\n```\n\nagent_to_cli_sync.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='You always respond in Italian.')\nagent.to_cli_sync()\n\n```\n\nYou can also use the async interface with `Agent.to_cli()`:\n\n[Learn about Gateway](../gateway) agent_to_cli.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('gateway/openai:gpt-5', instructions='You always respond in Italian.')\n\nasync def main():\n    await agent.to_cli()\n\n```\n\nagent_to_cli.py\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='You always respond in Italian.')\n\nasync def main():\n    await agent.to_cli()\n\n```\n\n*(You'll need to add `asyncio.run(main())` to run `main`)*\n\n### Message History\n\nBoth `Agent.to_cli()` and `Agent.to_cli_sync()` support a `message_history` parameter, allowing you to continue an existing conversation or provide conversation context:\n\n[Learn about Gateway](../gateway) agent_with_history.py\n\n```python\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\nagent = Agent('gateway/openai:gpt-5')\n\n### Create some conversation history\nmessage_history: list[ModelMessage] = [\n    ModelRequest([UserPromptPart(content='What is 2+2?')]),\n    ModelResponse([TextPart(content='2+2 equals 4.')])\n]\n\n### Start CLI with existing conversation context\nagent.to_cli_sync(message_history=message_history)\n\n```\n\nagent_with_history.py\n\n```python\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\nagent = Agent('openai:gpt-5')\n\n### Create some conversation history\nmessage_history: list[ModelMessage] = [\n    ModelRequest([UserPromptPart(content='What is 2+2?')]),\n    ModelResponse([TextPart(content='2+2 equals 4.')])\n]\n\n### Start CLI with existing conversation context\nagent.to_cli_sync(message_history=message_history)\n\n```\n\nThe CLI will start with the provided conversation history, allowing the agent to refer back to previous exchanges and maintain context throughout the session.\n\nWe'd love you to contribute to Pydantic AI!\n\n## Installation and Setup\n\nClone your fork and cd into the repo directory\n\n```bash\ngit clone git@github.com:<your username>/pydantic-ai.git\ncd pydantic-ai\n\n```\n\nInstall `uv` (version 0.4.30 or later), `pre-commit` and `deno`:\n\n- [`uv` install docs](https://docs.astral.sh/uv/getting-started/installation/)\n- [`pre-commit` install docs](https://pre-commit.com/#install)\n- [`deno` install docs](https://docs.deno.com/runtime/getting_started/installation/)\n\nTo install `pre-commit` you can run the following command:\n\n```bash\nuv tool install pre-commit\n\n```\n\nFor `deno`, you can run the following, or check [their documentation](https://docs.deno.com/runtime/getting_started/installation/) for alternative installation methods:\n\n```bash\ncurl -fsSL https://deno.land/install.sh | sh\n\n```\n\nInstall `pydantic-ai`, all dependencies and pre-commit hooks\n\n```bash\nmake install\n\n```\n\n## Running Tests etc.\n\nWe use `make` to manage most commands you'll need to run.\n\nFor details on available commands, run:\n\n```bash\nmake help\n\n```\n\nTo run code formatting, linting, static type checks, and tests with coverage report generation, run:\n\n```bash\nmake\n\n```\n\n## Documentation Changes\n\nTo run the documentation page locally, run:\n\n```bash\nuv run mkdocs serve\n\n```\n\n## Rules for adding new models to Pydantic AI\n\nTo avoid an excessive workload for the maintainers of Pydantic AI, we can't accept all model contributions, so we're setting the following rules for when we'll accept new models and when we won't. This should hopefully reduce the chances of disappointment and wasted work.\n\n- To add a new model with an extra dependency, that dependency needs > 500k monthly downloads from PyPI consistently over 3 months or more\n- To add a new model which uses another models logic internally and has no extra dependencies, that model's GitHub org needs > 20k stars in total\n- For any other model that's just a custom URL and API key, we're happy to add a one-paragraph description with a link and instructions on the URL to use\n- For any other model that requires more logic, we recommend you release your own Python package `pydantic-ai-xxx`, which depends on [`pydantic-ai-slim`](../install/#slim-install) and implements a model that inherits from our Model ABC\n\nIf you're unsure about adding a model, please [create an issue](https://github.com/pydantic/pydantic-ai/issues).",
  "content_length": 7165
}