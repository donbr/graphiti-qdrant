{
  "title": "Quickstart",
  "source_url": null,
  "content": "> Run end-to-end memory evaluations using Zep's evaluation framework\n\nThis guide shows you how to use Zep's evaluation harness to systematically test your memory implementation.\n\n## Why use the evaluation harness?\n\nWith this evaluation harness, you can:\n\n* **Evaluate Zep's performance for your use case**: Test how well Zep retrieves relevant information and answers questions specific to your domain and conversation patterns.\n* **Systematically experiment with Zep ontologies, search strategies, and other capabilities**: Compare different configurations to optimize retrieval accuracy and response quality.\n* **Develop a suite of tests that can be run in CI**: Continuously evaluate your application for regressions, ensuring that changes to your data model or Zep configuration don't degrade memory performance over time.\n\nThe harness provides objective metrics for context completeness and answer accuracy, enabling data-driven decisions about memory configuration and search strategies.\n\n## Steps\n\n### Clone the Zep repository\n\nClone the [Zep repository](https://github.com/getzep/zep/tree/main) that includes the evaluation harness:\n\n```bash\ngit clone https://github.com/getzep/zep.git\ncd zep/zep-eval-harness\n```\n\n### Set up your environment\n\nInstall UV package manager for macOS/Linux:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nFor other platforms, visit the [UV installation guide](https://docs.astral.sh/uv/).\n\nInstall all required dependencies using UV:\n\n```bash\nuv sync\n```\n\nSet up your API keys by copying the example file and adding your keys:\n\n```bash\ncp .env.example .env\n```\n\nEdit `.env` and add your keys:\n\n```bash\nZEP_API_KEY=your_zep_api_key_here\nOPENAI_API_KEY=your_openai_api_key_here\n```\n\nGet your Zep API key at [app.getzep.com](https://app.getzep.com) and OpenAI API key at [platform.openai.com/api-keys](https://platform.openai.com/api-keys).\n\n### Write down 3-5 example interactions\n\n<Callout intent=\"tip\">\n  **Most important step**: This is the most critical part of the evaluation process. Take time to write down 3-5 specific examples that showcase how you want your agent to behave once it has memory. These examples will be dropped into an AI prompt in the next step to automatically generate your evaluation data.\n</Callout>\n\nFor each example, simply note what the user asks and what the agent should respond with:\n\n```\n1. User: \"What is my dog's name?\"\n   Agent: \"Max\"\n\n2. User: \"When is my vet appointment?\"\n   Agent: \"Friday at Dr. Peterson's clinic\"\n\n3. User: \"What training classes did I sign up for?\"\n   Agent: \"Puppy training classes at PetSmart, Saturdays at 9am\"\n```\n\n### Use an AI coding assistant to update the test data\n\nUse Cursor, Copilot, Claude Code, or another AI coding assistant to automatically update the test files based on your examples.\n\nProvide this prompt to your AI assistant:\n\n```text\nPlease update the files in the zep-eval-harness to match the example interactions below.\n\nImportant: Use the existing user in `data/users.json` (user_id: zep_eval_test_user_001).\nDo not create or modify any users.\n\nFirst, read the README.md file in the zep-eval-harness directory to understand the\nbest practices for creating conversations and test cases.\n\nStep 1 - Generate test cases first:\nFor `data/test_cases/`:\n- Create one file: `zep_eval_test_user_001_tests.json`\n- Follow the format and structure of the existing test case files\n- Generate exactly 10 test cases based on my example interactions, expanding on them\n  with variations and related questions\n- Write clear golden_answer text that specifies what information must be present\n  in a correct response\n- Follow the best practices in the README for designing fair test cases\n\nStep 2 - Generate conversations that contain the answers:\nFor `data/conversations/`:\n- Create exactly 5 conversation files named `zep_eval_test_user_001_conv_001.json`\n  through `zep_eval_test_user_001_conv_005.json`\n- Each conversation file should contain exactly 6 messages (alternating user/assistant)\n- Follow the format and structure of the existing conversation files\n- CRITICAL: Ensure that the information needed to answer ALL test questions from\n  Step 1 is present somewhere across these 5 conversations\n- Spread the information naturally across the conversations\n- Make conversations feel natural and contextual\n- Follow the best practices in the README for conversation design\n\nHere are my 3-5 example interactions:\n\n[PASTE YOUR 3-5 EXAMPLE INTERACTIONS HERE]\n```\n\n### Run the ingestion script\n\nLoad your test conversations into Zep:\n\n```bash\nuv run zep_ingest.py\n```\n\n<Callout intent=\"info\">\n  The ingestion process creates numbered run directories (e.g., `1_20251103T123456`) containing manifest files that document created users, thread IDs, and configuration details.\n</Callout>\n\nFor ingestion with a custom ontology:\n\n```bash\nuv run zep_ingest.py --custom-ontology\n```\n\n### Wait for graph processing to complete\n\nAfter ingestion completes, the knowledge graph needs time to process all messages and extract facts, entities, and relationships. Graph processing happens sequentially to preserve the temporal sequence of events.\n\n**Processing time**: 5-10 seconds per message. With 5 conversations of 6 messages each (30 messages total), expect processing to take approximately 2.5-5 minutes.\n\nYou can monitor processing status in the Zep dashboard or wait for the recommended time before proceeding to evaluation.\n\n### Run the evaluation script\n\nExecute the evaluation pipeline:\n\n```bash\nuv run zep_evaluate.py\n```\n\nTo evaluate a specific run:\n\n```bash\nuv run zep_evaluate.py 1\n```\n\nThe script processes each test question through four automated steps:\n\n1. **Search**: Query Zep's knowledge graph using a cross-encoder reranker to retrieve relevant information\n2. **Evaluate context**: Assess whether the retrieved information is sufficient to answer the test question (produces the primary metric: COMPLETE, PARTIAL, or INSUFFICIENT)\n3. **Generate response**: Use GPT-4o-mini with the retrieved context to generate an answer\n4. **Grade answer**: Evaluate the generated response against the golden answer using GPT-4o (produces the secondary metric: CORRECT or WRONG)\n\n<Tip>\n  The context completeness evaluation (step 2) is the primary metric as it measures Zep's core capability: retrieving relevant information. The answer grading (step 4) is secondary since it also depends on the LLM's ability to use that context.\n</Tip>\n\nResults are saved to `runs/{run_number}/evaluation_results_{timestamp}.json`.\n\n### Interpret your results\n\nThe evaluation results include overall accuracy on the test questions and detailed per-test breakdown. Look at these key metrics:\n\n* **Context completeness**: Whether Zep retrieved all necessary information (COMPLETE, PARTIAL, or INSUFFICIENT). This is your primary indicator of Zep's retrieval performance.\n* **Answer accuracy**: Whether the generated answer matched your golden answer criteria (CORRECT or WRONG). This measures both retrieval and generation quality.\n* **Per-user breakdown**: Performance metrics for each user to identify patterns.\n* **Detailed test results**: Individual test case results with retrieved context, generated answers, and the LLM judge's reasoning.\n\nThe script prints overall scores and saves detailed results including which questions the agent answered correctly versus missed, along with the LLM judge's reasoning for each evaluation.\n\n### Review results and iterate\n\nLook at the evaluation results to identify any missed questions. For each incorrect answer:\n\n1. Check if the conversation data contains the necessary information\n2. Verify the golden\\_answer is clear and specific\n3. Review the retrieved context in the results JSON to understand what Zep found\n4. Adjust your conversations or test questions as needed\n\nIf context is consistently incomplete, consider adjusting your data ingestion strategy, search parameters, or graph configuration.\n\nIterate by modifying your data files, then re-run the ingestion and evaluation scripts.\n\n## Next steps\n\nOnce you have the basic evaluation working, consider these next steps:\n\n* **Add more examples and variations**: Expand your test set with additional examples and variations of existing scenarios to cover more edge cases.\n\n* **Evaluate Zep's performance with your existing agent**: Once you've validated Zep's retrieval capabilities with the evaluation harness, integrate Zep into your existing agent and evaluate end-to-end performance. Create test cases based on real user conversations from your application to reflect actual usage patterns. This helps you understand how Zep performs in your complete system, including your agent's prompt engineering, tool calling, and response generation.\n\n* **Define a custom ontology for your domain**: Create entity and edge types tailored to your specific use case for better knowledge graph structure and retrieval. Use an AI coding assistant to define custom types based on your conversation data:\n\n```text\nBased on the conversations in `data/conversations/`, help me define a custom ontology\nfor my domain in `ontology.py`.\n\nPlease create entity types (classes) and edge types (relationships) that are specific\nto my use case and conversation patterns. Follow these guidelines:\n\n- Entity types should be domain-specific (e.g., for healthcare: Patient, Diagnosis,\n  Medication; for e-commerce: Product, Order, CustomerIssue)\n- Include 1-2 key attributes per entity type using EntityText fields\n- Entity names should contain specific values for better semantic search\n- Edge types should model the key relationships in my domain\n- Add descriptive docstrings explaining when to use each type\n\nLook at the existing `ontology.py` file for the structure and format to follow.\n\nHere's a summary of my use case and domain:\n[DESCRIBE YOUR USE CASE AND DOMAIN]\n```\n\nAfter updating `ontology.py`, run ingestion with the custom ontology flag:\n\n```bash\nuv run zep_ingest.py --custom-ontology\n```\n\nLearn more about [customizing graph structure](/customizing-graph-structure).\n\n* **Add background data**: Ingest a larger dataset before your test conversations to evaluate retrieval performance when relevant information is buried in a larger knowledge graph.\n\n* **Test with JSON and unstructured data**: Add JSON documents, transcripts, or business data alongside conversations, then create test questions that require retrieving this non-conversational data. See [Adding Data to the Graph](/adding-data-to-the-graph).\n\n* **Tune search strategy and graph parameters**: Experiment with different rerankers, search scopes, and graph creation settings like [ignoring assistant messages](/adding-memory#ignore-assistant-messages) to optimize performance for your use case. You can customize the evaluation parameters in `zep_evaluate.py`:\n\n```python\n### Search limits\nFACTS_LIMIT = 20      # Number of edges to return\nENTITIES_LIMIT = 10   # Number of nodes to return\nEPISODES_LIMIT = 0    # Disabled by default\n\n### Reranker options: cross_encoder (default), rrf, or mmr\n```",
  "content_length": 11053
}