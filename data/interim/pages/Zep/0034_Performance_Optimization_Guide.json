{
  "title": "Performance Optimization Guide",
  "source_url": null,
  "content": "> Best practices for optimizing Zep performance in production\n\nThis guide covers best practices for optimizing Zep's performance in production environments.\n\n## Reuse the Zep SDK Client\n\nThe Zep SDK client maintains an HTTP connection pool that enables connection reuse, significantly reducing latency by avoiding the overhead of establishing new connections. To optimize performance:\n\n* Create a single client instance and reuse it across your application\n* Avoid creating new client instances for each request or function\n* Consider implementing a client singleton pattern in your application\n* For serverless environments, initialize the client outside the handler function\n\n## Optimizing Memory Operations\n\nThe `thread.add_messages` and `thread.get_user_context` methods are optimized for conversational messages and low-latency retrieval. For optimal performance:\n\n* Keep individual messages under 10K characters\n* Use `graph.add` for larger documents, tool outputs, or business data\n* Consider chunking large documents before adding them to the graph (the `graph.add` endpoint has a 10,000 character limit)\n* Remove unnecessary metadata or content before persistence\n* For bulk document ingestion, process documents in parallel while respecting rate limits\n\n```python\n### Recommended for conversations\nzep_client.thread.add_messages(\n    thread_id=\"thread_123\",\n    message={\n        \"role\": \"user\",\n        \"name\": \"Alice\",\n        \"content\": \"What's the weather like today?\"\n    }\n)\n\n### Recommended for large documents\nawait zep_client.graph.add(\n    data=document_content,  # Your chunked document content\n    user_id=user_id,       # Or graph_id\n    type=\"text\"            # Can be \"text\", \"message\", or \"json\"\n)\n```\n\n### Get the Context Block sooner\n\nYou can request the Context Block directly in the response to the `thread.add_messages()` call.\nThis optimization eliminates the need for a separate `thread.get_user_context()` call.\nRead more about our [Context Block](/retrieving-memory#zeps-context-block).\n\nIn this scenario you can pass in the `return_context=True` flag to the `thread.add_messages()` method.\nZep will perform a user graph search right after persisting the memory and return the context relevant to the recently added memory.\n\n<CodeBlocks>\n  ```python Python\n  memory_response = await zep_client.thread.add_messages(\n      thread_id=thread_id,\n      messages=messages,\n      return_context=True\n  )\n\n  context = memory_response.context\n  ```\n\n  ```typescript TypeScript\n  const memoryResponse = await zepClient.thread.addMessages(threadId, {\n      messages: messages,\n      returnContext: true\n  });\n\n  const context = memoryResponse.context;\n  ```\n\n  ```go Go\n  memoryResponse, err := zepClient.Thread.AddMessages(\n      context.TODO(),\n      threadId,\n      &zep.AddThreadMessagesRequest{\n          Messages: messages,\n          ReturnContext: zep.Bool(true),\n      },\n  )\n  if err != nil {\n      // handle error\n  }\n  contextBlock := memoryResponse.Context\n  ```\n</CodeBlocks>\n\n<Tip>\n  Read more in the \n\n  [Thread SDK Reference](/sdk-reference/thread/add-messages)\n</Tip>\n\n### Searching the Graph Sooner\n\nInstead of using `thread.get_user_context`, you might want to [search the graph](/searching-the-graph) directly with custom parameters and construct your own [custom context block](/cookbook/advanced-context-block-construction). When doing this, you can search the graph and add data to the graph concurrently.\n\n```python\nimport asyncio\nfrom zep_cloud.client import AsyncZep\nfrom zep_cloud.types import Message\n\nclient = AsyncZep(api_key=\"your_api_key\")\n\nasync def add_and_retrieve_from_zep(messages):\n    # Concatenate message content to create query string\n    query = \" \".join([msg.content for msg in messages])\n    \n    # Execute all operations concurrently\n    add_result, edges_result, nodes_result = await asyncio.gather(\n        client.thread.add_messages(\n            thread_id=thread_id,\n            messages=messages\n        ),\n        client.graph.search(\n            user_id=user_id,\n            query=query,\n            scope=\"edges\"\n        ),\n        client.graph.search(\n            user_id=user_id,\n            query=query,\n            scope=\"nodes\"\n        )\n    )\n    \n    return add_result, edges_result, nodes_result\n```\n\nYou would then need to construct a custom context block using the search results. Learn more about [customizing your context block](/cookbook/advanced-context-block-construction).\n\n## Optimizing Search Queries\n\nZep uses hybrid search combining semantic similarity and BM25 full-text search. For optimal performance:\n\n* Keep your queries concise. Queries are automatically truncated to 8,192 tokens (approximately 32,000 Latin characters)\n* Longer queries may not improve search quality and will increase latency\n* Consider breaking down complex searches into smaller, focused queries\n* Use specific, contextual queries rather than generic ones\n\nBest practices for search:\n\n* Keep search queries concise and specific\n* Structure queries to target relevant information\n* Use natural language queries for better semantic matching\n* Consider the scope of your search (graphs versus user graphs)\n\n```python\n### Recommended - concise query\nresults = await zep_client.graph.search(\n    user_id=user_id,  # Or graph_id\n    query=\"project requirements discussion\"\n)\n\n### Not recommended - overly long query\nresults = await zep_client.graph.search(\n    user_id=user_id,\n    query=\"very long text with multiple paragraphs...\"  # Will be truncated\n)\n```\n\n## Warming the User Cache\n\nZep has a multi-tier retrieval architecture. The highest tier is a \"hot\" cache where a user's context retrieval is fastest. After several hours of no activity, a user's data will be moved to a lower tier.\n\nYou can hint to Zep that a retrieval may be made soon, allowing Zep to move user data into cache ahead of this retrieval. A good time to do this is when a user logs in to your service or opens your app.\n\n<CodeBlocks>\n  ```python Python\n  # Warm the user's cache when they log in\n  client.user.warm(user_id=user_id)\n  ```\n\n  ```typescript TypeScript\n  // Warm the user's cache when they log in\n  await client.user.warm(userId);\n  ```\n\n  ```go Go\n  // Warm the user's cache when they log in\n  _, err := client.User.Warm(context.TODO(), userId)\n  if err != nil {\n      log.Printf(\"Error warming user cache: %v\", err)\n  }\n  ```\n</CodeBlocks>\n\n<Tip>\n  Read more in the \n\n  [User SDK Reference](/sdk-reference/user/warm)\n</Tip>\n\n## Summary\n\n* Reuse Zep SDK client instances to optimize connection management\n* Use appropriate methods for different types of content (`thread.add_messages` for conversations, `graph.add` for large documents)\n* Keep search queries focused and under the token limit for optimal performance\n* Warm the user cache when users log in or open your app for faster retrieval",
  "content_length": 6853
}