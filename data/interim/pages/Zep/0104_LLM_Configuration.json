{
  "title": "LLM Configuration",
  "source_url": null,
  "content": "> Configure Graphiti with different LLM providers\n\n<Note>\n  Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini). Using other services may result in incorrect output schemas and ingestion failures, particularly when using smaller models.\n</Note>\n\nGraphiti defaults to using OpenAI for LLM inference and embeddings, but supports multiple LLM providers including Azure OpenAI, Google Gemini, Anthropic, Groq, and local models via Ollama. This guide covers configuring Graphiti with alternative LLM providers.\n\n## Azure OpenAI\n\n<Warning>\n  **Azure OpenAI v1 API Opt-in Required for Structured Outputs**\n\n  Graphiti uses structured outputs via the `client.beta.chat.completions.parse()` method, which requires Azure OpenAI deployments to opt into the v1 API. Without this opt-in, you'll encounter 404 Resource not found errors during episode ingestion.\n\n  To enable v1 API support in your Azure OpenAI deployment, follow Microsoft's guide: [Azure OpenAI API version lifecycle](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/api-version-lifecycle?tabs=key#api-evolution).\n</Warning>\n\nAzure OpenAI deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n### Installation\n\n```bash\npip install graphiti-core\n```\n\n### Configuration\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n### Azure OpenAI configuration - use separate endpoints for different services\napi_key = \"<your-api-key>\"\napi_version = \"<your-api-version>\"\nllm_endpoint = \"<your-llm-endpoint>\"  # e.g., \"https://your-llm-resource.openai.azure.com/\"\nembedding_endpoint = \"<your-embedding-endpoint>\"  # e.g., \"https://your-embedding-resource.openai.azure.com/\"\n\n### Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n### Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=\"gpt-4.1-nano\",\n    model=\"gpt-4.1-mini\",\n)\n\n### Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=OpenAIClient(\n        config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=\"text-embedding-3-small-deployment\"  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n```\n\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names.\n\n### Environment Variables\n\nAzure OpenAI can also be configured using environment variables:\n\n* `AZURE_OPENAI_ENDPOINT` - Azure OpenAI LLM endpoint URL\n* `AZURE_OPENAI_DEPLOYMENT_NAME` - Azure OpenAI LLM deployment name\n* `AZURE_OPENAI_API_VERSION` - Azure OpenAI API version\n* `AZURE_OPENAI_EMBEDDING_API_KEY` - Azure OpenAI Embedding deployment key (if different from `OPENAI_API_KEY`)\n* `AZURE_OPENAI_EMBEDDING_ENDPOINT` - Azure OpenAI Embedding endpoint URL\n* `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME` - Azure OpenAI embedding deployment name\n* `AZURE_OPENAI_EMBEDDING_API_VERSION` - Azure OpenAI embedding API version\n* `AZURE_OPENAI_USE_MANAGED_IDENTITY` - Use Azure Managed Identities for authentication\n\n## Google Gemini\n\nGoogle's Gemini models provide excellent structured output support and can be used for LLM inference, embeddings, and cross-encoding/reranking.\n\n### Installation\n\n```bash\npip install \"graphiti-core[google-genai]\"\n```\n\n### Configuration\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n### Google API key configuration\napi_key = \"<your-google-api-key>\"\n\n### Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.0-flash\"\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=\"embedding-001\"\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.0-flash-exp\"\n        )\n    )\n)\n```\n\nThe Gemini reranker uses the `gemini-2.0-flash-exp` model by default, which is optimized for cost-effective and low-latency classification tasks.\n\n### Environment Variables\n\nGoogle Gemini can be configured using:\n\n* `GOOGLE_API_KEY` - Your Google API key\n\n## Anthropic\n\nAnthropic's Claude models can be used for LLM inference with OpenAI embeddings and reranking.\n\n<Warning>\n  When using Anthropic for LLM inference, you still need an OpenAI API key for embeddings and reranking functionality. Make sure to set both `ANTHROPIC_API_KEY` and `OPENAI_API_KEY` environment variables.\n</Warning>\n\n### Installation\n\n```bash\npip install \"graphiti-core[anthropic]\"\n```\n\n### Configuration\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.anthropic_client import AnthropicClient, LLMConfig\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n### Configure Anthropic LLM with OpenAI embeddings and reranking\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\", \n    \"password\",\n    llm_client=AnthropicClient(\n        config=LLMConfig(\n            api_key=\"<your-anthropic-api-key>\",\n            model=\"claude-sonnet-4-20250514\",\n            small_model=\"claude-3-5-haiku-20241022\"\n        )\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=\"<your-openai-api-key>\",\n            embedding_model=\"text-embedding-3-small\"\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        config=LLMConfig(\n            api_key=\"<your-openai-api-key>\",\n            model=\"gpt-4.1-nano\"  # Use a smaller model for reranking\n        )\n    )\n)\n```\n\n### Environment Variables\n\nAnthropic can be configured using:\n\n* `ANTHROPIC_API_KEY` - Your Anthropic API key\n* `OPENAI_API_KEY` - Required for embeddings and reranking\n\n## Groq\n\nGroq provides fast inference with various open-source models, using OpenAI for embeddings and reranking.\n\n<Warning>\n  When using Groq, avoid smaller models as they may not accurately extract data or output the correct JSON structures required by Graphiti. Use larger, more capable models like Llama 3.1 70B for best results.\n</Warning>\n\n### Installation\n\n```bash\npip install \"graphiti-core[groq]\"\n```\n\n### Configuration\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.groq_client import GroqClient, LLMConfig\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n### Configure Groq LLM with OpenAI embeddings and reranking\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\", \n    llm_client=GroqClient(\n        config=LLMConfig(\n            api_key=\"<your-groq-api-key>\",\n            model=\"llama-3.1-70b-versatile\",\n            small_model=\"llama-3.1-8b-instant\"\n        )\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=\"<your-openai-api-key>\",\n            embedding_model=\"text-embedding-3-small\"\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        config=LLMConfig(\n            api_key=\"<your-openai-api-key>\",\n            model=\"gpt-4.1-nano\"  # Use a smaller model for reranking\n        )\n    )\n)\n```\n\n### Environment Variables\n\nGroq can be configured using:\n\n* `GROQ_API_KEY` - Your Groq API key\n* `OPENAI_API_KEY` - Required for embeddings\n\n## Ollama (Local LLMs)\n\nOllama enables running local LLMs and embedding models via its OpenAI-compatible API, ideal for privacy-focused applications or avoiding API costs.\n\n<Warning>\n  When using Ollama, avoid smaller local models as they may not accurately extract data or output the correct JSON structures required by Graphiti. Use larger, more capable models and ensure they support structured output for reliable knowledge graph construction.\n</Warning>\n\n### Installation\n\nFirst, install and configure Ollama:\n\n```bash\n### Install Ollama (visit https://ollama.ai for installation instructions)\n### Then pull the models you want to use:\nollama pull deepseek-r1:7b     # LLM\nollama pull nomic-embed-text   # embeddings\n```\n\n### Configuration\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n### Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=\"abc\",  # Ollama doesn't require a real API key\n    model=\"deepseek-r1:7b\",\n    small_model=\"deepseek-r1:7b\",\n    base_url=\"http://localhost:11434/v1\",  # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n### Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=\"abc\",\n            embedding_model=\"nomic-embed-text\",\n            embedding_dim=768,\n            base_url=\"http://localhost:11434/v1\",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.\n\n## OpenAI Compatible Services\n\nMany LLM providers offer OpenAI-compatible APIs. Use the `OpenAIGenericClient` for these services, which ensures proper schema injection for JSON output since most providers don't support OpenAI's structured output format.\n\n<Warning>\n  When using OpenAI-compatible services, avoid smaller models as they may not accurately extract data or output the correct JSON structures required by Graphiti. Choose larger, more capable models that can handle complex reasoning and structured output.\n</Warning>\n\n### Installation\n\n```bash\npip install graphiti-core\n```\n\n### Configuration\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n### Configure OpenAI-compatible service\nllm_config = LLMConfig(\n    api_key=\"<your-api-key>\",\n    model=\"<your-main-model>\",        # e.g., \"mistral-large-latest\"\n    small_model=\"<your-small-model>\", # e.g., \"mistral-small-latest\"\n    base_url=\"<your-base-url>\",       # e.g., \"https://api.mistral.ai/v1\"\n)\n\n### Initialize Graphiti with OpenAI-compatible service\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=OpenAIGenericClient(config=llm_config),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=\"<your-api-key>\",\n            embedding_model=\"<your-embedding-model>\", # e.g., \"mistral-embed\"\n            base_url=\"<your-base-url>\",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        config=LLMConfig(\n            api_key=\"<your-api-key>\",\n            model=\"<your-small-model>\",  # Use smaller model for reranking\n            base_url=\"<your-base-url>\",\n        )\n    )\n)\n```\n\nReplace the placeholder values with your actual service credentials and model names.",
  "content_length": 12559
}