{
  "title": "LangGraph Memory Example",
  "source_url": null,
  "content": "> LangGraph is a library created by LangChain for building stateful, multi-agent applications. This example demonstrates using Zep for LangGraph agent memory.\n\n<Info>\n  A complete Notebook example of using Zep for LangGraph Memory may be found in the [Zep Python SDK Repository](https://github.com/getzep/zep/blob/main/examples/python/langgraph-agent/agent.ipynb).\n</Info>\n\nThe following example demonstrates building an agent using LangGraph. Zep is used to personalize agent responses based on information learned from prior conversations.\n\nThe agent implements:\n\n* persistance of new chat turns to Zep and recall of relevant Facts using the most recent messages.\n* an in-memory MemorySaver to maintain agent state. We use this to add recent chat history to the agent prompt. As an alternative, you could use Zep for this.\n\n<Note>\n  You should consider truncating MemorySaver's chat history as by default LangGraph state grows unbounded. We've included this in our example below. See the LangGraph documentation for insight.\n</Note>\n\n## Install dependencies\n\n```shell\npip install zep-cloud langchain-openai langgraph ipywidgets python-dotenv\n```\n\n## Configure Zep\n\nEnsure that you've configured the following API keys in your environment. We're using Zep's Async client here, but we could also use the non-async equivalent.\n\n```bash\nZEP_API_KEY=your_zep_api_key_here\nOPENAI_API_KEY=your_openai_api_key_here\n```\n\n```python\nimport os\nimport uuid\nimport logging\nfrom typing import Annotated, TypedDict\n\nfrom zep_cloud.client import AsyncZep\nfrom zep_cloud import Message\n\nfrom langchain_core.messages import AIMessage, SystemMessage, HumanMessage, trim_messages\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph, add_messages\nfrom langgraph.prebuilt import ToolNode\n\n### Optional: Load environment variables from .env file\n### from dotenv import load_dotenv\n### load_dotenv()\n\n### Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n### Initialize Zep client\nzep = AsyncZep(api_key=os.environ.get('ZEP_API_KEY'))\n```\n\n## Define State and Setup Tools\n\nFirst, define the state structure for our LangGraph agent:\n\n```python\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n    first_name: str\n    last_name: str\n    thread_id: str\n    user_name: str\n```\n\n## Using Zep's Search as a Tool\n\nThese are examples of simple Tools that search Zep for facts (from edges) or nodes. Since LangGraph tools don't automatically receive the full graph state, we create a function that returns configured tools for a specific user:\n\n```python\ndef create_zep_tools(user_name: str):\n    \"\"\"Create Zep search tools configured for a specific user.\"\"\"\n    \n    @tool\n    async def search_facts(query: str, limit: int = 5) -> list[str]:\n        \"\"\"Search for facts in all conversations had with a user.\n        \n        Args:\n            query (str): The search query.\n            limit (int): The number of results to return. Defaults to 5.\n\n        Returns:\n            list: A list of facts that match the search query.\n        \"\"\"\n        result = await zep.graph.search(\n            user_id=user_name, query=query, limit=limit, scope=\"edges\"\n        )\n        facts = [edge.fact for edge in result.edges or []]\n        if not facts:\n            return [\"No facts found for the query.\"]\n        return facts\n\n    @tool\n    async def search_nodes(query: str, limit: int = 5) -> list[str]:\n        \"\"\"Search for nodes in all conversations had with a user.\n        \n        Args:\n            query (str): The search query.\n            limit (int): The number of results to return. Defaults to 5.\n\n        Returns:\n            list: A list of node summaries for nodes that match the search query.\n        \"\"\"\n        result = await zep.graph.search(\n            user_id=user_name, query=query, limit=limit, scope=\"nodes\"\n        )\n        summaries = [node.summary for node in result.nodes or []]\n        if not summaries:\n            return [\"No nodes found for the query.\"]\n        return summaries\n    \n    return [search_facts, search_nodes]\n\n### We'll create the actual tools after we have a user_name\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n```\n\n## Chatbot Function Explanation\n\nThe chatbot uses Zep to provide context-aware responses. Here's how it works:\n\n1. **Context Retrieval**: It retrieves relevant facts for the user's current conversation (thread). Zep uses the most recent messages to determine what facts to retrieve.\n\n2. **System Message**: It constructs a system message incorporating the facts retrieved in 1., setting the context for the AI's response.\n\n3. **Message Persistence**: After generating a response, it asynchronously adds the user and assistant messages to Zep. New Facts are created and existing Facts updated using this new information.\n\n4. **Messages in State**: We use LangGraph state to store the most recent messages and add these to the Agent prompt. We limit the message list to the most recent 3 messages for demonstration purposes.\n\n<Note>\n  We could also use Zep to recall the chat history, rather than LangGraph's MemorySaver.\n\n  See [`thread.get_user_context`](/sdk-reference/thread/get-user-context) in the Zep SDK documentation.\n</Note>\n\n```python\nasync def chatbot(state: State):\n    memory = await zep.thread.get_user_context(state[\"thread_id\"])\n\n    system_message = SystemMessage(\n        content=f\"\"\"You are a compassionate mental health bot and caregiver. Review information about the user and their prior conversation below and respond accordingly.\n        Keep responses empathetic and supportive. And remember, always prioritize the user's well-being and mental health.\n\n        {memory.context}\"\"\"\n    )\n\n    messages = [system_message] + state[\"messages\"]\n\n    response = await llm.ainvoke(messages)\n\n    # Add the new chat turn to the Zep graph\n    messages_to_save = [\n        Message(\n            role=\"user\",\n            name=state[\"first_name\"] + \" \" + state[\"last_name\"],\n            content=state[\"messages\"][-1].content,\n        ),\n        Message(role=\"assistant\", content=response.content),\n    ]\n\n    await zep.thread.add_messages(\n        thread_id=state[\"thread_id\"],\n        messages=messages_to_save,\n    )\n\n    # Truncate the chat history to keep the state from growing unbounded\n    # In this example, we going to keep the state small for demonstration purposes\n    # We'll use Zep's Facts to maintain conversation context\n    state[\"messages\"] = trim_messages(\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=len,\n        max_tokens=3,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n        include_system=True,\n    )\n\n    logger.info(f\"Messages in state: {state['messages']}\")\n\n    return {\"messages\": [response]}\n```\n\n## Setting up the Agent\n\nThis function creates a complete LangGraph agent configured for a specific user. This approach allows us to properly configure the tools with the user context:\n\n```python\ndef create_agent(user_name: str):\n    \"\"\"Create a LangGraph agent configured for a specific user.\"\"\"\n    \n    # Create tools configured for this user\n    tools = create_zep_tools(user_name)\n    tool_node = ToolNode(tools)\n    llm_with_tools = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind_tools(tools)\n    \n    # Update the chatbot function to use the configured LLM\n    async def chatbot_with_tools(state: State):\n        memory = await zep.thread.get_user_context(state[\"thread_id\"])\n\n        system_message = SystemMessage(\n            content=f\"\"\"You are a compassionate mental health bot and caregiver. Review information about the user and their prior conversation below and respond accordingly.\n            Keep responses empathetic and supportive. And remember, always prioritize the user's well-being and mental health.\n\n            {memory.context}\"\"\"\n        )\n\n        messages = [system_message] + state[\"messages\"]\n\n        response = await llm_with_tools.ainvoke(messages)\n\n        # Add the new chat turn to the Zep graph\n        messages_to_save = [\n            Message(\n                role=\"user\",\n                name=state[\"first_name\"] + \" \" + state[\"last_name\"],\n                content=state[\"messages\"][-1].content,\n            ),\n            Message(role=\"assistant\", content=response.content),\n        ]\n\n        await zep.thread.add_messages(\n            thread_id=state[\"thread_id\"],\n            messages=messages_to_save,\n        )\n\n        # Truncate the chat history to keep the state from growing unbounded\n        state[\"messages\"] = trim_messages(\n            state[\"messages\"],\n            strategy=\"last\",\n            token_counter=len,\n            max_tokens=3,\n            start_on=\"human\",\n            end_on=(\"human\", \"tool\"),\n            include_system=True,\n        )\n\n        logger.info(f\"Messages in state: {state['messages']}\")\n\n        return {\"messages\": [response]}\n    \n    # Define the function that determines whether to continue or not\n    async def should_continue(state, config):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        # If there is no function call, then we finish\n        if not last_message.tool_calls:\n            return \"end\"\n        # Otherwise if there is, we continue\n        else:\n            return \"continue\"\n    \n    # Build the graph\n    graph_builder = StateGraph(State)\n    memory = MemorySaver()\n    \n    graph_builder.add_node(\"agent\", chatbot_with_tools)\n    graph_builder.add_node(\"tools\", tool_node)\n    \n    graph_builder.add_edge(START, \"agent\")\n    graph_builder.add_conditional_edges(\"agent\", should_continue, {\"continue\": \"tools\", \"end\": END})\n    graph_builder.add_edge(\"tools\", \"agent\")\n    \n    return graph_builder.compile(checkpointer=memory)\n```\n\nOur LangGraph agent graph is illustrated below.\n\n![Agent Graph](file:71e4b795-c674-47dd-add9-73242e4cb3f3)\n\n## Running the Agent\n\nWe generate a unique user name and thread id, add these to Zep, and create our configured agent:\n\n```python\nfirst_name = \"Daniel\"\nlast_name = \"Chalef\"\nuser_name = first_name + uuid.uuid4().hex[:4]\nthread_id = uuid.uuid4().hex\n\n### Create user and thread in Zep\nawait zep.user.add(user_id=user_name, first_name=first_name, last_name=last_name)\nawait zep.thread.create(thread_id=thread_id, user_id=user_name)\n\n### Create the agent configured for this user\ngraph = create_agent(user_name)\n\n\ndef extract_messages(result, user_name):\n    output = \"\"\n    for message in result[\"messages\"]:\n        if isinstance(message, AIMessage):\n            name = \"assistant\"\n        else:\n            name = user_name\n        output += f\"{name}: {message.content}\\n\"\n    return output.strip()\n\n\nasync def graph_invoke(\n    message: str,\n    first_name: str,\n    last_name: str,\n    user_name: str,\n    thread_id: str,\n    ai_response_only: bool = True,\n):\n    r = await graph.ainvoke(\n        {\n            \"messages\": [HumanMessage(content=message)],\n            \"first_name\": first_name,\n            \"last_name\": last_name,\n            \"thread_id\": thread_id,\n            \"user_name\": user_name,\n        },\n        config={\"configurable\": {\"thread_id\": thread_id}},\n    )\n\n    if ai_response_only:\n        return r[\"messages\"][-1].content\n    else:\n        return extract_messages(r, user_name)\n```\n\nLet's test the agent with a few messages:\n\n```python\nr = await graph_invoke(\n    \"Hi there?\",\n    first_name,\n    last_name,\n    user_name,\n    thread_id,\n)\n\nprint(r)\n```\n\n> Hello! How are you feeling today? I'm here to listen and support you.\n\n```python\nr = await graph_invoke(\n    \"\"\"\n    I'm fine. But have been a bit stressful lately. Mostly work related. \n    But also my dog. I'm worried about her.\n    \"\"\",\n    first_name,\n    last_name,\n    user_name,\n    thread_id,\n)\n\nprint(r)\n```\n\n> I'm sorry to hear that you've been feeling stressed. Work can be a significant source of pressure, and it sounds like your dog might be adding to that stress as well. If you feel comfortable sharing, what specifically has been causing you stress at work and with your dog? I'm here to help you through it.\n\n## Viewing The Context Value\n\n```python\nmemory = await zep.thread.get_user_context(thread_id=thread_id)\n\nprint(memory.context)\n```\n\nThe context value will look something like this:\n\n```text\nFACTS and ENTITIES represent relevant context to the current conversation.\n\n### These are the most relevant facts and their valid date ranges\n### format: FACT (Date range: from - to)\n<FACTS>\n  - Daniel99db is worried about his sick dog. (2025-01-24 02:11:54 - present)\n  - Daniel Chalef is worried about his sick dog. (2025-01-24 02:11:54 - present)\n  - The assistant asks how the user is feeling. (2025-01-24 02:11:51 - present)\n  - Daniel99db has been a bit stressful lately due to his dog. (2025-01-24 02:11:53 - present)\n  - Daniel99db has been a bit stressful lately due to work. (2025-01-24 02:11:53 - present)\n  - Daniel99db is a user. (2025-01-24 02:11:51 - present)\n  - user has the id of Daniel99db (2025-01-24 02:11:50 - present)\n  - user has the name of Daniel Chalef (2025-01-24 02:11:50 - present)\n</FACTS>\n### These are the most relevant entities\n### ENTITY_NAME: entity summary\n<ENTITIES>\n  - worried: Daniel Chalef (Daniel99db) is feeling stressed lately, primarily due to work-related issues and concerns about his sick dog, which has made him worried.\n  - Daniel99db: Daniel99db, or Daniel Chalef, is currently experiencing stress primarily due to work-related issues and concerns about his sick dog. Despite these challenges, he has shown a desire for interaction by initiating conversations, indicating his openness to communication.\n  - sick: Daniel Chalef, also known as Daniel99db, is feeling stressed lately, primarily due to work-related issues and concerns about his sick dog. He expresses worry about his dog's health.\n  - Daniel Chalef: Daniel Chalef, also known as Daniel99db, has been experiencing stress recently, primarily related to work issues and concerns about his sick dog. Despite this stress, he has been feeling generally well and has expressed a desire to connect with others, as indicated by his friendly greeting, \"Hi there?\".\n  - dog: Daniel99db, also known as Daniel Chalef, mentioned that he has been feeling a bit stressed lately, which is related to both work and his dog.\n  - work: Daniel Chalef, also known as Daniel99db, has been experiencing stress lately, primarily related to work.\n  - feeling: The assistant initiates a conversation by asking how the user is feeling today, indicating a willingness to listen and provide support.\n</ENTITIES>\n```\n\n```python\nr = await graph_invoke(\n    \"She ate my shoes which were expensive.\",\n    first_name,\n    last_name,\n    user_name,\n    thread_id,\n)\n\nprint(r)\n```\n\n> That sounds really frustrating, especially when you care so much about your belongings and your dog's health. It's tough when pets get into things they shouldn't, and it can add to your stress. How are you feeling about that situation? Are you able to focus on her health despite the shoe incident?\n\nLet's now test whether the Agent is correctly grounded with facts from the prior conversation.\n\n```python\nr = await graph_invoke(\n    \"What are we talking about?\",\n    first_name,\n    last_name,\n    user_name,\n    thread_id,\n)\n\nprint(r)\n```\n\n> We were discussing your concerns about your dog being sick and the situation with her eating your expensive shoes. It sounds like you're dealing with a lot right now, and I want to make sure we're addressing what's on your mind. If there's something else you'd like to talk about or if you want to share more about your dog, I'm here to listen.\n\nLet's go even further back to determine whether context is kept by referencing a user message that is not currently in the Agent State. Zep will retrieve Facts related to the user's job.\n\n```python\nr = await graph_invoke(\n    \"What have I said about my job?\",\n    first_name,\n    last_name,\n    user_name,\n    thread_id,\n)\n\nprint(r)\n```\n\n> You've mentioned that you've been feeling a bit stressed lately, primarily due to work-related issues. If you'd like to share more about what's been going on at work or how it's affecting you, I'm here to listen and support you.",
  "content_length": 16319
}