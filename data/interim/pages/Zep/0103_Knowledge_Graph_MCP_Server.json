{
  "title": "Knowledge Graph MCP Server",
  "source_url": null,
  "content": "> A Knowledge Graph MCP Server for AI Assistants\n\n<Card title=\"What is the Graphiti MCP Server?\" icon=\"duotone server\">\n  The Graphiti MCP Server is an experimental implementation that exposes Graphiti's key functionality through the Model Context Protocol (MCP). This enables AI assistants like Claude Desktop, Cursor, and VS Code with Copilot to interact with Graphiti's knowledge graph capabilities, providing persistent memory and contextual awareness.\n</Card>\n\nThe Graphiti MCP Server bridges AI assistants with Graphiti's temporally-aware knowledge graphs, allowing assistants to maintain persistent memory across conversations and sessions. Unlike traditional RAG methods, it continuously integrates user interactions, structured and unstructured data, and external information into a coherent, queryable graph.\n\n## Key Features\n\nThe MCP server exposes Graphiti's core capabilities:\n\n* **Episode Management**: Add, retrieve, and delete episodes (text, messages, or JSON data)\n* **Entity Management**: Search and manage entity nodes and relationships\n* **Search Capabilities**: Semantic and hybrid search for facts and node summaries\n* **Group Management**: Organize data with group\\_id filtering for multi-user scenarios\n* **Graph Maintenance**: Clear graphs and rebuild indices as needed\n* **Pre-configured Entity Types**: Structured entity extraction for domain-specific use cases\n* **Multiple Database Support**: FalkorDB (Redis-based, default) and Neo4j\n* **Flexible LLM Providers**: OpenAI, Anthropic, Gemini, Groq, and Azure OpenAI\n* **Multiple Embedding Options**: OpenAI, Voyage, Sentence Transformers, and Gemini\n\n## Quick Start\n\n<Note>\n  This quick start uses OpenAI and FalkorDB (default). The server supports multiple LLM providers (OpenAI, Anthropic, Gemini, Grogu, Azure OpenAI) and databases (FalkorDB, Neo4j). For detailed configuration options, see the [MCP Server README](https://github.com/getzep/graphiti/blob/main/mcp_server/README.md).\n</Note>\n\n### Prerequisites\n\nBefore getting started, ensure you have:\n\n1. **Python 3.10+** installed on your system\n2. **Database** - Either FalkorDB (default, Redis-based) or Neo4j (5.26+) running locally or accessible remotely\n3. **LLM API key** - For OpenAI, Anthropic, Gemini, Groq, or Azure OpenAI\n\n### Installation\n\n1. Clone the Graphiti repository:\n\n```bash\ngit clone https://github.com/getzep/graphiti.git\ncd graphiti\n```\n\n2. Navigate to the MCP server directory and install dependencies:\n\n```bash\ncd mcp_server\nuv sync\n```\n\n### Configuration\n\nConfiguration follows a precedence hierarchy: command-line arguments override environment variables, which override `config.yaml` settings.\n\nSet up your environment variables in a `.env` file:\n\n```bash\n### Required LLM Configuration\nOPENAI_API_KEY=your_openai_api_key_here\nMODEL_NAME=gpt-4o-mini\n\n### Database Configuration (FalkorDB is default, or use Neo4j)\n### For FalkorDB (Redis-based):\n### REDIS_HOST=localhost\n### REDIS_PORT=6379\n### REDIS_PASSWORD=your_redis_password\n\n### For Neo4j:\nNEO4J_URI=bolt://localhost:7687\nNEO4J_USER=neo4j\nNEO4J_PASSWORD=your_neo4j_password\n\n### Optional: Disable telemetry\n### GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n### Running the Server\n\nStart the MCP server:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nFor development with custom options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4o-mini --transport sse --group-id my-project\n```\n\n## MCP Client Integration\n\nThe MCP server supports integration with multiple AI assistants through different transport protocols.\n\n### Claude Desktop\n\nConfigure Claude Desktop to connect via the stdio transport:\n\n```json\n{\n  \"mcpServers\": {\n    \"graphiti-memory\": {\n      \"transport\": \"stdio\",\n      \"command\": \"/path/to/uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\",\n        \"/path/to/graphiti/mcp_server\",\n        \"graphiti_mcp_server.py\",\n        \"--transport\",\n        \"stdio\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your_api_key\",\n        \"MODEL_NAME\": \"gpt-4o-mini\",\n        \"NEO4J_URI\": \"bolt://localhost:7687\",\n        \"NEO4J_USER\": \"neo4j\",\n        \"NEO4J_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n### Cursor IDE\n\nFor Cursor, use the SSE transport configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"graphiti-memory\": {\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```\n\n### VS Code with Copilot\n\nVS Code with Copilot can connect to the MCP server using HTTP endpoints. Configure your VS Code settings to point to the running MCP server.\n\n## Available Tools\n\nOnce connected, AI assistants have access to these Graphiti tools:\n\n* `add_memory` - Store episodes and interactions in the knowledge graph\n* `search_facts` - Find relevant facts and relationships\n* `search_nodes` - Search for entity summaries and information\n* `get_episodes` - Retrieve recent episodes for context\n* `delete_episode` - Remove episodes from the graph\n* `clear_graph` - Reset the knowledge graph entirely\n\n## Docker Deployment\n\nFor containerized deployment, use the provided Docker Compose setup:\n\n```bash\ndocker compose up\n```\n\nThis starts both the database (FalkorDB or Neo4j) and the MCP server with SSE transport enabled. Docker Compose can launch services in unified or separate containers with sensible defaults for immediate use.\n\n## Performance and Privacy\n\n### Performance Tuning\n\nEpisode processing uses asynchronous queuing with concurrency controlled by `SEMAPHORE_LIMIT`. The MCP server README provides tier-specific guidelines for major LLM providers to prevent rate-limiting while maximizing throughput.\n\n### Telemetry\n\nThe framework includes optional anonymous telemetry collection that captures only system information. Telemetry never exposes API keys or graph content. Disable telemetry by setting:\n\n```bash\nGRAPHITI_TELEMETRY_ENABLED=false\n```\n\n## Next Steps\n\nFor comprehensive configuration options, advanced features, and troubleshooting:\n\n* **Full Documentation**: See the complete [MCP Server README](https://github.com/getzep/graphiti/blob/main/mcp_server/README.md)\n* **Integration Examples**: Explore client-specific setup guides for Claude Desktop, Cursor, and VS Code\n* **Custom Entity Types**: Configure pre-configured entity types for domain-specific extraction\n* **Multi-tenant Setup**: Use group IDs for organizing data across different contexts\n* **Alternative LLM Providers**: Configure Anthropic, Gemini, Groq, or Azure OpenAI\n* **Database Options**: Switch between FalkorDB and Neo4j based on your needs\n\n<Warning>\n  The MCP server is experimental and under active development. Features and APIs may change between releases.\n</Warning>",
  "content_length": 6589
}