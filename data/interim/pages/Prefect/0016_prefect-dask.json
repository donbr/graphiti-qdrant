{
  "title": "prefect-dask",
  "source_url": "https://docs-3.prefect.io/integrations/prefect-dask/index",
  "content": "Accelerate your workflows by running tasks in parallel with Dask\n\nDask can run your tasks in parallel and distribute them over multiple machines.\nThe `prefect-dask` integration makes it easy to accelerate your flow runs with Dask.\n\n## Getting started\n\n### Install `prefect-dask`\n\nThe following command will install a version of `prefect-dask` compatible with your installed version of `prefect`.\nIf you don't already have `prefect` installed, it will install the newest version of `prefect` as well.\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install \"prefect[dask]\"\n  ```\n\n  ```bash uv theme={null}\n  uv pip install \"prefect[dask]\"\n  ```\n</CodeGroup>\n\nUpgrade to the latest versions of `prefect` and `prefect-dask`:\n\n<CodeGroup>\n  ```bash pip theme={null}\n  pip install -U \"prefect[dask]\"\n  ```\n\n  ```bash uv theme={null}\n  uv pip install -U \"prefect[dask]\"\n  ```\n</CodeGroup>\n\n## Why use Dask?\n\nSay your flow downloads many images to train a machine learning model.\nIt takes longer than you'd like for the flow to run because it executes sequentially.\n\nTo accelerate your flow code, parallelize it with `prefect-dask` in three steps:\n\n1. Add the import: `from prefect_dask import DaskTaskRunner`\n2. Specify the task runner in the flow decorator: `@flow(task_runner=DaskTaskRunner)`\n3. Submit tasks to the flow's task runner: `a_task.submit(*args, **kwargs)`\n\nBelow is code with and without the DaskTaskRunner:\n\n<Tabs>\n  <Tab title=\"Sequential by default\">\n    ```python  theme={null}\n    # Completed in 15.2 seconds\n\n    from typing import List\n    from pathlib import Path\n\n    import httpx\n    from prefect import flow, task\n\n    URL_FORMAT = (\n        \"https://www.cpc.ncep.noaa.gov/products/NMME/archive/\"\n        \"{year:04d}{month:02d}0800/current/images/nino34.rescaling.ENSMEAN.png\"\n    )\n\n    @task\n    def download_image(year: int, month: int, directory: Path) -> Path:\n        # download image from URL\n        url = URL_FORMAT.format(year=year, month=month)\n        resp = httpx.get(url)\n\n        # save content to directory/YYYYMM.png\n        file_path = (directory / url.split(\"/\")[-1]).with_stem(f\"{year:04d}{month:02d}\")\n        file_path.write_bytes(resp.content)\n        return file_path\n\n    @flow\n    def download_nino_34_plumes_from_year(year: int) -> List[Path]:\n        # create a directory to hold images\n        directory = Path(\"data\")\n        directory.mkdir(exist_ok=True)\n\n        # download all images\n        file_paths = []\n        for month in range(1, 12 + 1):\n            file_path = download_image(year, month, directory)\n            file_paths.append(file_path)\n        return file_paths\n\n    if __name__ == \"__main__\":\n        download_nino_34_plumes_from_year(2022)\n    ```\n  </Tab>\n\n  <Tab title=\"Parallel with Dask\">\n    ```python  theme={null}\n    # Completed in 5.7 seconds\n\n    from typing import List\n    from pathlib import Path\n\n    import httpx\n    from prefect import flow, task\n    from prefect_dask import DaskTaskRunner\n\n    URL_FORMAT = (\n        \"https://www.cpc.ncep.noaa.gov/products/NMME/archive/\"\n        \"{year:04d}{month:02d}0800/current/images/nino34.rescaling.ENSMEAN.png\"\n    )\n\n    @task\n    def download_image(year: int, month: int, directory: Path) -> Path:\n        # download image from URL\n        url = URL_FORMAT.format(year=year, month=month)\n        resp = httpx.get(url)\n\n        # save content to directory/YYYYMM.png\n        file_path = (directory / url.split(\"/\")[-1]).with_stem(f\"{year:04d}{month:02d}\")\n        file_path.write_bytes(resp.content)\n        return file_path\n\n    @flow(task_runner=DaskTaskRunner(cluster_kwargs={\"processes\": False}))\n    def download_nino_34_plumes_from_year(year: int) -> List[Path]:\n        # create a directory to hold images\n        directory = Path(\"data\")\n        directory.mkdir(exist_ok=True)\n\n        # download all images\n        file_paths = []\n        for month in range(1, 12 + 1):\n            file_path = download_image.submit(year, month, directory)\n            file_paths.append(file_path)\n        return file_paths\n\n    if __name__ == \"__main__\":\n        download_nino_34_plumes_from_year(2022)\n    ```\n  </Tab>\n</Tabs>\n\nIn our tests, the flow run took 15.2 seconds to execute sequentially.\nUsing the `DaskTaskRunner` reduced the runtime to **5.7** seconds!\n\n## Run tasks on Dask\n\nThe `DaskTaskRunner` is a [task runner](/v3/develop/task-runners) that submits tasks to the [`dask.distributed`](http://distributed.dask.org/) scheduler.\nBy default, when the `DaskTaskRunner` is specified for a flow run, a temporary Dask cluster is created and used for the duration of the flow run.\n\nIf you already have a Dask cluster running, either cloud-hosted or local, you can provide the connection URL with the `address` kwarg.\n\n`DaskTaskRunner` accepts the following optional parameters:\n\n| Parameter       | Description                                                                                                                                                             |\n| --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| address         | Address of a currently running Dask scheduler.                                                                                                                          |\n| cluster\\_class  | The cluster class to use when creating a temporary Dask cluster. It can be either the full class name (for example, `\"distributed.LocalCluster\"`), or the class itself. |\n| cluster\\_kwargs | Additional kwargs to pass to the `cluster_class` when creating a temporary Dask cluster.                                                                                |\n| adapt\\_kwargs   | Additional kwargs to pass to `cluster.adapt` when creating a temporary Dask cluster. Note that adaptive scaling is only enabled if `adapt_kwargs` are provided.         |\n| client\\_kwargs  | Additional kwargs to use when creating a [`dask.distributed.Client`](https://distributed.dask.org/en/latest/api.html#client).                                           |\n\n<Warning>\n  **Multiprocessing safety**\n\n  Because the `DaskTaskRunner` uses multiprocessing, calls to flows in scripts must be guarded with `if __name__ == \"__main__\":` or you will encounter warnings and errors.\n</Warning>\n\nIf you don't provide the `address` of a Dask scheduler, Prefect creates a temporary local cluster automatically.\nThe number of workers used is based on the number of cores on your machine.\nThe default provides a mix of processes and threads that work well for most workloads.\nTo specify this explicitly, pass values for `n_workers` or `threads_per_worker` to `cluster_kwargs`:\n\n```python  theme={null}\nfrom prefect_dask import DaskTaskRunner\n\n# Use 4 worker processes, each with 2 threads\nDaskTaskRunner(\n    cluster_kwargs={\"n_workers\": 4, \"threads_per_worker\": 2}\n)\n```\n\n### Use a temporary cluster\n\nThe `DaskTaskRunner` can create a temporary cluster using any of [Dask's cluster-manager options](https://docs.dask.org/en/latest/setup.html).\nThis is useful when you want each flow run to have its own Dask cluster, allowing for per-flow adaptive scaling.\nTo configure it, provide a `cluster_class`.\nThis can be:\n\n* A string specifying the import path to the cluster class (for example, `\"dask_cloudprovider.aws.FargateCluster\"`)\n* The cluster class itself\n* A function for creating a custom cluster\n\nYou can also configure `cluster_kwargs`.\nThis takes a dictionary of keyword arguments to pass to `cluster_class` when starting the flow run.\n\nFor example, to configure a flow to use a temporary `dask_cloudprovider.aws.FargateCluster` with four workers running with an image named `my-prefect-image`:\n\n```python  theme={null}\nfrom prefect_dask import DaskTaskRunner\n\nDaskTaskRunner(\n    cluster_class=\"dask_cloudprovider.aws.FargateCluster\",\n    cluster_kwargs={\"n_workers\": 4, \"image\": \"my-prefect-image\"},\n)\n```\n\nFor larger workloads, you can accelerate execution further by distributing task runs over multiple machines.\n\n### Connect to an existing cluster\n\nMultiple Prefect flow runs can use the same existing Dask cluster.\nYou might manage a single long-running Dask cluster (for example, using the Dask [Helm Chart](https://docs.dask.org/en/latest/setup/kubernetes-helm.html)) and configure flows to connect to it during execution.\nThis has disadvantages compared to using a temporary Dask cluster:\n\n* All workers in the cluster must have dependencies installed for all flows you intend to run.\n* Multiple flow runs may compete for resources. Dask tries to do a good job\n  sharing resources between tasks, but you may still run into issues.\n\nStill, you may prefer managing a single long-running Dask cluster.\n\nTo configure a `DaskTaskRunner` to connect to an existing cluster, pass in the address of the scheduler to the `address` argument:\n\n```python  theme={null}\nfrom prefect_dask import DaskTaskRunner\n\n@flow(task_runner=DaskTaskRunner(address=\"http://my-dask-cluster\"))\ndef my_flow():\n    ...\n```\n\nSuppose you have an existing Dask client/cluster such as a `dask.dataframe.DataFrame`.\nWith `prefect-dask`, it takes just a few steps:\n\n1. Add imports\n2. Add `task` and `flow` decorators\n3. Use `get_dask_client` context manager to distribute work across Dask workers\n4. Specify the task runner and client's address in the flow decorator\n5. Submit the tasks to the flow's task runner\n\n<Tabs>\n  <Tab title=\"Without Prefect\">\n    ```python  theme={null}\n    import dask.dataframe\n    import dask.distributed\n\n\n    client = dask.distributed.Client()\n\n    def read_data(start: str, end: str) -> dask.dataframe.DataFrame:\n        df = dask.datasets.timeseries(start, end, partition_freq=\"4w\")\n        return df\n\n\n    def process_data(df: dask.dataframe.DataFrame) -> dask.dataframe.DataFrame:\n\n        df_yearly_avg = df.groupby(df.index.year).mean()\n        return df_yearly_avg.compute()\n\n\n    def dask_pipeline():\n        df = read_data(\"1988\", \"2022\")\n        df_yearly_average = process_data(df)\n        return df_yearly_average\n\n\n    if __name__ == \"__main__\":\n        dask_pipeline()\n    ```\n  </Tab>\n\n  <Tab title=\"With Prefect\">\n    ```python  theme={null}\n    import dask.dataframe\n    import dask.distributed\n    from prefect import flow, task\n    from prefect_dask import DaskTaskRunner, get_dask_client\n\n\n    client = dask.distributed.Client()\n\n    @task\n    def read_data(start: str, end: str) -> dask.dataframe.DataFrame:\n        df = dask.datasets.timeseries(start, end, partition_freq=\"4w\")\n        return df\n\n    @task\n    def process_data(df: dask.dataframe.DataFrame) -> dask.dataframe.DataFrame:\n        with get_dask_client():\n            df_yearly_avg = df.groupby(df.index.year).mean()\n            return df_yearly_avg.compute()\n\n    @flow(task_runner=DaskTaskRunner(address=client.scheduler.address))\n    def dask_pipeline():\n        df = read_data.submit(\"1988\", \"2022\")\n        df_yearly_average = process_data.submit(df)\n        return df_yearly_average\n\n\n    if __name__ == \"__main__\":\n        dask_pipeline()\n    ```\n  </Tab>\n</Tabs>\n\n### Configure adaptive scaling\n\nA key feature of using a `DaskTaskRunner` is the ability to scale adaptively to the workload.\nInstead of specifying `n_workers` as a fixed number, you can specify a minimum and maximum number of workers to use, and the Dask cluster scales up and down as needed.\n\nTo do this, pass `adapt_kwargs` to `DaskTaskRunner`.\nThis takes the following fields:\n\n* `maximum` (`int` or `None`, optional): the maximum number of workers to scale to. Set to `None` for no maximum.\n* `minimum` (`int` or `None`, optional): the minimum number of workers to scale to. Set to `None` for no minimum.\n\nFor example, this configures a flow to run on a `FargateCluster` scaling up to a maximum of 10 workers:\n\n```python  theme={null}\nfrom prefect_dask import DaskTaskRunner\n\nDaskTaskRunner(\n    cluster_class=\"dask_cloudprovider.aws.FargateCluster\",\n    adapt_kwargs={\"maximum\": 10}\n)\n```\n\n### Use Dask annotations\n\nUse Dask annotations to further control the behavior of tasks.\nFor example, set the [priority](http://distributed.dask.org/en/stable/priority.html) of tasks in the Dask scheduler:\n\n```python  theme={null}\nimport dask\nfrom prefect import flow, task\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n\n@task\ndef show(x):\n    print(x)\n\n\n@flow(task_runner=DaskTaskRunner())\ndef my_flow():\n    with dask.annotate(priority=-10):\n        future = show.submit(1)  # low priority task\n\n    with dask.annotate(priority=10):\n        future = show.submit(2)  # high priority task\n```\n\nAnother common use case is [resource](http://distributed.dask.org/en/stable/resources.html) annotations:\n\n```python  theme={null}\nimport dask\nfrom prefect import flow, task\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n\n@task\ndef show(x):\n    print(x)\n\n# Create a `LocalCluster` with some resource annotations\n# Annotations are abstract in dask and not inferred from your system.\n# Here, we claim that our system has 1 GPU and 1 process available per worker\n@flow(\n    task_runner=DaskTaskRunner(\n        cluster_kwargs={\"n_workers\": 1, \"resources\": {\"GPU\": 1, \"process\": 1}}\n    )\n)\n\ndef my_flow():\n    with dask.annotate(resources={'GPU': 1}):\n        future = show(0)  # this task requires 1 GPU resource on a worker\n\n    with dask.annotate(resources={'process': 1}):\n        # These tasks each require 1 process on a worker; because we've\n        # specified that our cluster has 1 process per worker and 1 worker,\n        # these tasks will run sequentially\n        future = show(1)\n        future = show(2)\n        future = show(3)\n\n\nif __name__ == \"__main__\":\n    my_flow()\n```\n\n## Additional Resources\n\nRefer to the `prefect-dask` [SDK documentation](https://reference.prefect.io/prefect_dask/) to explore all the capabilities of the `prefect-dask` library.\n\nFor assistance using Dask, consult the [Dask documentation](https://docs.dask.org/en/stable/)\n\n<Warning>\n  **Resolving futures in sync client**\n\n  Note, by default, `dask_collection.compute()` returns concrete values while `client.compute(dask_collection)` returns Dask Futures. Therefore, if you call `client.compute`, you must resolve all futures before exiting out of the context manager by either:\n\n  1. setting `sync=True`\n\n  ```python  theme={null}\n  with get_dask_client() as client:\n      df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n      summary_df = client.compute(df.describe(), sync=True)\n  ```\n\n  2. calling `result()`\n\n  ```python  theme={null}\n  with get_dask_client() as client:\n      df = dask.datasets.timeseries(\"2000\", \"2001\", partition_freq=\"4w\")\n      summary_df = client.compute(df.describe()).result()\n  ```\n\n  For more information, visit the docs on [Waiting on Futures](https://docs.dask.org/en/stable/futures.html#waiting-on-futures).\n</Warning>\n\nThere is also an equivalent context manager for asynchronous tasks and flows: `get_async_dask_client`. When using the async client, you must `await client.compute(dask_collection)` before exiting the context manager.\n\nNote that task submission (`.submit()`) and future resolution (`.result()`) are always synchronous operations in Prefect, even when working with async tasks and flows.",
  "content_length": 15289
}