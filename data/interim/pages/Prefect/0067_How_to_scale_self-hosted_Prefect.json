{
  "title": "How to scale self-hosted Prefect",
  "source_url": "https://docs-3.prefect.io/v3/advanced/self-hosted",
  "content": "Learn how to run multiple Prefect server instances for high availability and load distribution\n\nRunning multiple Prefect server instances enables high availability and distributes load across your infrastructure. This guide covers configuration and deployment patterns for scaling self-hosted Prefect.\n\n## Requirements\n\nMulti-server deployments require:\n\n* PostgreSQL database version 14.9 or higher (SQLite does not support multi-server synchronization)\n* Redis for event messaging\n* Load balancer for API traffic distribution\n\n## Architecture\n\nA scaled Prefect deployment typically includes:\n\n* **Multiple API server instances** - Handle UI and API requests\n* **Background services** - Runs the scheduler, automation triggers, and other loop services\n* **[PostgreSQL](https://www.postgresql.org/) database** - Stores all persistent data and synchronizes state across servers\n* **[Redis](https://redis.io/)** - Distributes events between services\n* **Load balancer** - Routes traffic to healthy API instances (e.g. [NGINX](https://www.f5.com/go/product/welcome-to-nginx) or [Traefik](https://doc.traefik.io/traefik/))\n\n```mermaid  theme={null}\n%%{\n  init: {\n    'theme': 'neutral',\n    'flowchart': {\n      'curve' : 'linear',\n      'rankSpacing': 120,\n      'nodeSpacing': 80\n    }\n  }\n}%%\n\nflowchart TB\n    %% Style definitions\n    classDef userClass fill:#ede7f6db,stroke:#4527a0db,stroke-width:2px\n    classDef lbClass fill:#e3f2fddb,stroke:#1565c0db,stroke-width:2px\n    classDef apiClass fill:#1860f2db,stroke:#1860f2db,stroke-width:2px\n    classDef bgClass fill:#7c3aeddb,stroke:#7c3aeddb,stroke-width:2px\n    classDef dataClass fill:#16a34adb,stroke:#16a34adb,stroke-width:2px\n    classDef workerClass fill:#f59e0bdb,stroke:#f59e0bdb,stroke-width:2px\n\n    %% Nodes\n    subgraph clients[Client Side]\n        direction TB\n        Users[Users / UI / API Clients]:::userClass\n        Workers[Workers poll any available API server<br/>Process / K8s / Docker / Serverless]:::workerClass\n    end\n\n    LB[Load Balancer<br/>NGINX / HAProxy / ALB<br/>Port 4200]:::lbClass\n\n    subgraph servers[Prefect Server Components]\n        direction TB\n        subgraph api[API Servers - Horizontal Scaling]\n            direction LR\n            API1[API Server 1<br/>--no-services]:::apiClass\n            API2[API Server 2<br/>--no-services]:::apiClass\n            API3[API Server N...<br/>--no-services]:::apiClass\n        end\n\n        BG[Background Services<br/>prefect server services start<br/><br/>• Event Processing<br/>• Automation Triggers<br/>• Schedule Management]:::bgClass\n    end\n\n    subgraph data[Data Layer]\n        direction LR\n        PG[(PostgreSQL<br/>• Flow/Task State<br/>• Configuration<br/>• History)]:::dataClass\n        Redis[(Redis<br/>• Events<br/>• Automations<br/>• Real-time Updates)]:::dataClass\n    end\n\n    %% Connections\n    Users --> |HTTPS| LB\n    LB --> |Round Robin| api\n\n    api --> |Read/Write| PG\n    api --> |Publish| Redis\n\n    BG --> |Read/Write| PG\n    BG --> |Subscribe| Redis\n\n    Workers -.-> |Poll Work| api\n```\n\n## Configuration\n\n### Database setup\n\nConfigure PostgreSQL as your database backend:\n\n```bash  theme={null}\nexport PREFECT_API_DATABASE_CONNECTION_URL=\"postgresql+asyncpg://user:password@host:5432/prefect\"\n```\n\n<Warning>\n  PostgreSQL version 14.9 or higher is required for multi-server deployments. SQLite does not support the features needed for state synchronization across multiple servers.\n</Warning>\n\n### Redis setup\n\nConfigure Redis as your server's message broker, cache, and lease storage:\n\n```bash  theme={null}\nexport PREFECT_MESSAGING_BROKER=\"prefect_redis.messaging\"\nexport PREFECT_MESSAGING_CACHE=\"prefect_redis.messaging\"\nexport PREFECT_SERVER_EVENTS_CAUSAL_ORDERING=\"prefect_redis.ordering\"\nexport PREFECT_SERVER_CONCURRENCY_LEASE_STORAGE=\"prefect_redis.lease_storage\"\nexport PREFECT_REDIS_MESSAGING_HOST=\"redis-host\"\nexport PREFECT_REDIS_MESSAGING_PORT=\"6379\"\nexport PREFECT_REDIS_MESSAGING_DB=\"0\"\n```\n\nIf your Redis instance requires authentication, you may configure a username and password:\n\n```bash  theme={null}\nexport PREFECT_REDIS_MESSAGING_USERNAME=\"marvin\"\nexport PREFECT_REDIS_MESSAGING_PASSWORD=\"dontpanic!\"\n```\n\nFor Redis instances that require an encrypted connection, you can enable SSL/TLS:\n\n```bash  theme={null}\nexport PREFECT_REDIS_MESSAGING_SSL=\"true\"\n```\n\n### Service separation\n\nFor optimal performance, run API servers and background services separately:\n\n**API servers** (multiple instances):\n\n```bash  theme={null}\nprefect server start --host 0.0.0.0 --port 4200 --no-services\n```\n\n**Background services**:\n\n```bash  theme={null}\nprefect server services start\n```\n\n### Database migrations\n\nDisable automatic migrations in multi-server deployments:\n\n```bash  theme={null}\nexport PREFECT_API_DATABASE_MIGRATE_ON_START=\"false\"\n```\n\nRun migrations separately before deployment:\n\n```bash  theme={null}\nprefect server database upgrade -y\n```\n\n### Load balancer configuration\n\nConfigure health checks for your load balancer:\n\n* **Health endpoint**: `/api/health`\n* **Expected response**: HTTP 200 with JSON `{\"status\": \"healthy\"}`\n* **Check interval**: 5-10 seconds\n\nExample NGINX configuration:\n\n```nginx  theme={null}\nupstream prefect_api {\n    least_conn;\n    server prefect-api-1:4200 max_fails=3 fail_timeout=30s;\n    server prefect-api-2:4200 max_fails=3 fail_timeout=30s;\n    server prefect-api-3:4200 max_fails=3 fail_timeout=30s;\n}\n\nserver {\n    listen 4200;\n\n    location /api/health {\n        proxy_pass http://prefect_api;\n        proxy_connect_timeout 1s;\n        proxy_read_timeout 1s;\n    }\n\n    location / {\n        proxy_pass http://prefect_api;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n\n### Reverse proxy configuration\n\nWhen hosting Prefect behind a reverse proxy, ensure proper header forwarding:\n\n```nginx  theme={null}\nserver {\n    listen 80;\n    server_name prefect.example.com;\n\n    location / {\n        return 301 https://$host$request_uri;\n    }\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name prefect.example.com;\n\n    ssl_certificate /path/to/ssl/certificate.pem;\n    ssl_certificate_key /path/to/ssl/certificate_key.pem;\n\n    location /api {\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Real-IP $remote_addr;\n\n        # WebSocket support\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n\n        # Authentication headers\n        proxy_set_header Authorization $http_authorization;\n        proxy_pass_header Authorization;\n\n        proxy_pass http://prefect_api;\n    }\n\n    location / {\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_pass http://prefect_api;\n    }\n}\n```\n\n#### UI proxy settings\n\nWhen self-hosting the UI behind a proxy:\n\n* `PREFECT_UI_API_URL`: Connection URL from UI to API\n* `PREFECT_UI_SERVE_BASE`: Base URL path to serve the UI\n* `PREFECT_UI_URL`: URL for clients to access the UI\n\n#### SSL certificates\n\nFor self-signed certificates:\n\n1. Add certificate to system bundle and set:\n   ```bash  theme={null}\n   export SSL_CERT_FILE=/path/to/certificate.pem\n   ```\n\n2. Or disable verification (testing only):\n   ```bash  theme={null}\n   export PREFECT_API_TLS_INSECURE_SKIP_VERIFY=True\n   ```\n\n#### Environment proxy settings\n\nPrefect respects standard proxy environment variables:\n\n```bash  theme={null}\nexport HTTPS_PROXY=http://proxy.example.com:8080\nexport HTTP_PROXY=http://proxy.example.com:8080\nexport NO_PROXY=localhost,127.0.0.1,.internal\n```\n\n## Deployment examples\n\n### Docker Compose\n\n<Accordion title=\"3 servers, 1 background service, postgres, redis\">\n  ```yaml  theme={null}\n  services:\n    postgres:\n      image: postgres:15\n      environment:\n        POSTGRES_USER: prefect\n        POSTGRES_PASSWORD: prefect\n        POSTGRES_DB: prefect\n      volumes:\n        - postgres_data:/var/lib/postgresql/data\n      healthcheck:\n        test: pg_isready -h localhost -U $$POSTGRES_USER\n        interval: 2s\n        timeout: 5s\n        retries: 15\n\n    redis:\n      image: redis:7\n\n    migrate:\n      image: prefecthq/prefect:3-latest\n      depends_on:\n        postgres:\n          condition: service_healthy\n      command: prefect server database upgrade -y\n      environment:\n        PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@postgres:5432/prefect\n\n    prefect-api:\n      image: prefecthq/prefect:3-latest\n      depends_on:\n        migrate:\n          condition: service_completed_successfully\n        postgres:\n          condition: service_healthy\n        redis:\n          condition: service_started\n      deploy:\n        replicas: 3\n      command: prefect server start --host 0.0.0.0 --no-services\n      environment:\n        PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@postgres:5432/prefect\n        PREFECT_API_DATABASE_MIGRATE_ON_START: \"false\"\n        PREFECT_MESSAGING_BROKER: prefect_redis.messaging\n        PREFECT_MESSAGING_CACHE: prefect_redis.messaging\n        PREFECT_SERVER_EVENTS_CAUSAL_ORDERING: prefect_redis.ordering\n        PREFECT_SERVER_CONCURRENCY_LEASE_STORAGE: prefect_redis.lease_storage\n        PREFECT_REDIS_MESSAGING_HOST: redis\n        PREFECT_REDIS_MESSAGING_PORT: \"6379\"\n      ports:\n        - \"4200-4202:4200\"  # Maps to different ports for each replica\n\n    prefect-background:\n      image: prefecthq/prefect:3-latest\n      depends_on:\n        migrate:\n          condition: service_completed_successfully\n        postgres:\n          condition: service_healthy\n        redis:\n          condition: service_started\n      command: prefect server services start\n      environment:\n        PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@postgres:5432/prefect\n        PREFECT_API_DATABASE_MIGRATE_ON_START: \"false\"\n        PREFECT_MESSAGING_BROKER: prefect_redis.messaging\n        PREFECT_MESSAGING_CACHE: prefect_redis.messaging\n        PREFECT_SERVER_EVENTS_CAUSAL_ORDERING: prefect_redis.ordering\n        PREFECT_SERVER_CONCURRENCY_LEASE_STORAGE: prefect_redis.lease_storage\n        PREFECT_REDIS_MESSAGING_HOST: redis\n        PREFECT_REDIS_MESSAGING_PORT: \"6379\"\n\n  volumes:\n    postgres_data:\n  ```\n</Accordion>\n\n<Tip>\n  Deploying Prefect self-hosted somehow else? Consider [opening a PR](/contribute/docs-contribute) to add your deployment pattern to this guide.\n</Tip>\n\n## Operations\n\n### Migration considerations\n\n#### Handling large databases\n\nWhen running migrations on large database instances (especially where tables like `events`, `flow_runs`, or `task_runs` can reach millions of rows), the default database timeout of 10 seconds may not be sufficient for creating indexes.\n\nIf you encounter a `TimeoutError` during migrations, increase the database timeout:\n\n```bash  theme={null}\n# Set timeout to 10 minutes (adjust based on your database size)\nexport PREFECT_API_DATABASE_TIMEOUT=600\n\n# Then run the migration\nprefect server database upgrade -y\n```\n\nFor Docker deployments:\n\n```bash  theme={null}\ndocker run -e PREFECT_API_DATABASE_TIMEOUT=600 prefecthq/prefect:latest prefect server database upgrade -y\n```\n\n<Note>\n  Index creation time scales with table size. A database with millions of events may require 30+ minutes for some migrations. If a migration fails due to timeout, you may need to manually clean up any partially created indexes before retrying.\n</Note>\n\n#### Recovering from failed migrations\n\nIf a migration times out while creating indexes, you may need to manually complete it. For example, if migration `7a73514ca2d6` fails:\n\n1. First, check which indexes were partially created:\n   ```sql  theme={null}\n   SELECT indexname FROM pg_indexes WHERE tablename = 'events' AND indexname LIKE 'ix_events%';\n   ```\n\n2. Manually create the missing indexes using `CONCURRENTLY` to avoid blocking:\n   ```sql  theme={null}\n   -- Drop any partial indexes from the failed migration\n   DROP INDEX IF EXISTS ix_events__event_related_occurred;\n   DROP INDEX IF EXISTS ix_events__related_resource_ids;\n\n   -- Create the new indexes\n   CREATE INDEX CONCURRENTLY ix_events__related_gin ON events USING gin(related);\n   CREATE INDEX CONCURRENTLY ix_events__event_occurred ON events (event, occurred);\n   CREATE INDEX CONCURRENTLY ix_events__related_resource_ids_gin ON events USING gin(related_resource_ids);\n   ```\n\n3. Mark the migration as complete:\n   ```sql  theme={null}\n   UPDATE alembic_version SET version_num = '7a73514ca2d6';\n   ```\n\n<Warning>\n  Only use manual recovery if increasing the timeout and retrying the migration doesn't work. Always verify the correct migration version and index definitions from the migration files.\n</Warning>\n\n### Monitoring\n\nMonitor your multi-server deployment:\n\n* **Database connections**: Watch for connection pool exhaustion\n* **Redis memory**: Ensure adequate memory for message queues\n* **API response times**: Track latency across different endpoints\n* **Background service lag**: Monitor time between event creation and processing\n\n### Best practices\n\n1. **Start with 2-3 API instances** and scale based on load\n2. **Use connection pooling** to manage database connections efficiently\n3. **Monitor extensively** before scaling further (e.g. [Prometheus](https://prometheus.io/) + [Grafana](https://grafana.com/) or [Logfire](https://logfire.pydantic.dev/docs/why/))\n4. **Test failover scenarios** regularly\n\n## Further reading\n\n* [Server concepts](/v3/concepts/server)\n* Deploy [Helm charts](/v3/advanced/server-helm) for Kubernetes",
  "content_length": 13789
}