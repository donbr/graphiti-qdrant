{
  "title": "How to Migrate from Airflow",
  "source_url": "https://docs-3.prefect.io/v3/how-to-guides/migrate/airflow",
  "content": "Migration from Apache Airflow to Prefect: A Comprehensive How-To Guide\n\nMigrating from Apache Airflow to Prefect simplifies orchestration, reduces overhead, and enables a more Pythonic workflow.\nPrefect's flexible **library-based approach** lets you write, test, and run workflows with regular code—without the complexity of schedulers, executors, or metadata databases.\n\nThis guide will walk you through a **step-by-step migration**, helping you transition from Airflow DAGs to Prefect flows while mapping key concepts, adapting infrastructure, and optimizing deployments. By the end, you'll have a streamlined, scalable orchestration system that lets your team focus on engineering rather than maintaining workflow infrastructure.\n\n**Airflow to Prefect Mapping**\n\nThis table provides a quick reference for migrating key Airflow concepts to their Prefect equivalents. Click on each concept to jump to a detailed explanation.\n\n| **Airflow Concept**                                        | **Prefect Equivalent**                                       | **Key Differences**                                                                           |\n| ---------------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------------------------------------------- |\n| [**DAGs**](#choose-a-dag-to-convert)                       | [**Flows**](#define-a-prefect-flow)                          | Prefect flows are standard Python functions (`@flow`). No DAG classes or `>>` dependencies.   |\n| [**Operators**](#create-equivalent-prefect-tasks)          | [**Tasks**](#create-equivalent-prefect-tasks)                | Prefect tasks (`@task`) replace Airflow Operators, removing the need for specialized classes. |\n| [**Executors**](#airflow-executors)                        | [**Work Pools & Workers**](#airflow-executors)               | Prefect decouples task execution using lightweight **workers** polling **work pools**.        |\n| [**Scheduling**](#prefect-deployment)                      | [**Deployments**](#prefect-deployment)                       | Scheduling is separate from flow code and configured externally.                              |\n| [**XComs**](#define-a-prefect-flow)                        | [**Return Values**](#define-a-prefect-flow)                  | Prefect tasks return data directly; no need for XComs or metadata storage.                    |\n| [**Hooks & Connections**](#airflow-hooks-and-integrations) | [**Blocks & Integrations**](#airflow-hooks-and-integrations) | Prefect replaces Hooks with **Blocks** for secure resource management.                        |\n| [**Sensors**](#airflow-sensors)                            | [**Triggers & Event-Driven Flows**](#airflow-sensors)        | Prefect uses external event triggers or lightweight polling flows.                            |\n| [**Airflow UI**](#observability)                           | [**Prefect UI**](#observability)                             | Prefect provides real-time monitoring, task logs, and automation features.                    |\n\nThere are also so key differences when it comes to task execution, resource control, data passing, and parallelism. We'll cover these in more detail below.\n\n| **Feature**           | **Airflow**                               | **Prefect**                                |\n| --------------------- | ----------------------------------------- | ------------------------------------------ |\n| **Task Execution**    | Tasks run as independent processes/pods   | Tasks execute in single flow runtime       |\n| **Resource Control**  | Task-level via executor settings          | Flow-level via work pools & task runners   |\n| **Data Passing**      | Requires XComs or external storage        | Direct in-memory data passing              |\n| **Parallelism**       | Managed by executor configuration         | Managed by work pools and task runners     |\n| **Task Dependencies** | Uses `>>` operators and `set_upstream()`  | Implicit via Python function calls         |\n| **DAG Parsing**       | Pre-parsed with global variable execution | Standard Python function execution         |\n| **State & Retries**   | Individual task retries, manual DAG fixes | Built-in flow & task retry handling        |\n| **Scheduling**        | Tightly coupled with DAG code             | Decoupled via deployments                  |\n| **Infrastructure**    | Requires scheduler, metadata DB, workers  | Lightweight API server with optional cloud |\n\n## Preparing for migration\n\nBefore jumping into code conversion, set the stage for a smooth migration. Preparation includes auditing your existing Airflow DAGs, setting up a Prefect environment for testing, and mapping Airflow concepts to their Prefect equivalents.\n\n**Audit your Airflow DAGs and dependencies:** Catalog all DAGs, schedules, task counts, and dependencies (databases, APIs, cloud services). Identify **high-priority pipelines** (business-critical, failure-prone, frequently updated) and **simpler DAGs** for pilot migration. Start with a small, non-critical DAG to gain confidence before tackling complex workflows.\n\n**Set up Prefect for testing:** Before fully migrating, set up a parallel Prefect environment to test your flows. Prefect provides a managed execution environment out of the box, so you can get started without configuring infrastructure.\n\n1. [**Install Prefect**](/v3/get-started/install) (`pip install prefect`).\n2. **Start a Prefect server locally** (`prefect server start`) or sign up for [**Prefect Cloud**](https://app.prefect.cloud/) to run flows immediately.\n3. **Run initial flows without infrastructure setup**: Run flows locally or using Prefect Cloud Managed Exxecution - allowing you to test without configuring work pools or Kubernetes.\n\n<Tip>Prefect Cloud provides a managed execution environment out of the box, so you can get started without configuring infrastructure.</Tip>\n\nOnce you've validated basic functionality, you can explore configuring an [**execution environment**](/v3/deploy/infrastructure-concepts/work-pools) (e.g., Docker, Kubernetes) for production, which we cover later in this tutorial.\n\nFor each Airflow DAG, you can outline its Prefect flow structure (tasks and control flow), where its schedule will live, and what execution infrastructure it needs. With preparation done, it's time to start converting code.\n\n## Converting DAGs to Prefect Flows\n\nIn this phase, you will **rewrite your Airflow DAGs as Prefect flows and tasks**. The goal is to replicate each workflow's logic in Prefect, while simplifying wherever possible.\n\n<Note>Prefect's API is quite ergonomic - many Airflow users find they can express the same logic with *less code* and *more flexibility*</Note>\n\nLet's break down the conversion process step-by-step, and walk through a concrete example.\n\n### Choose a DAG to convert\n\nStart with one of your simpler DAGs (perhaps one of those identified in the audit as an easy win). For illustration, suppose we have an Airflow DAG that runs a simple ETL: it **extracts data**, **transforms** it, and then **loads** the results. In Airflow, this might be defined as:\n\n```python  theme={null}\n# Airflow DAG example (simplified ETL)\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\n# Airflow task functions (to be used by PythonOperator)\ndef extract_fn():\n    # ... (extract data, e.g., query an API or database)\n    return data\n\ndef transform_fn(data):\n    # ... (transform the data)\n    return processed_data\n\ndef load_fn(processed_data):\n    # ... (load data to target, e.g., save to DB or file)\n\nwith DAG(\"etl_pipeline\", start_date=datetime(2023,1,1), schedule_interval=\"@daily\", catchup=False) as dag:\n    extract = PythonOperator(task_id='extract_data', python_callable=extract_fn)\n    transform = PythonOperator(task_id='transform_data', python_callable=transform_fn)\n    load = PythonOperator(task_id='load_data', python_callable=load_fn)\n\n    # Set task dependencies\n    extract >> transform >> load\n```\n\nIn this Airflow DAG, we define three tasks using `PythonOperator`, then specify that they run sequentially (`extract` then `transform` then `load`).\n\n### Create equivalent Prefect tasks\n\nIn Prefect, we'll take the core logic of `extract_fn`, `transform_fn`, `load_fn` and turn each into a `@task` decorated function. The code inside can remain largely the same (minus any Airflow-specific cruft). For example:\n\n```python  theme={null}\n# Prefect tasks for ETL\nfrom prefect import task, flow\n\n@task\ndef extract_data():\n    # ... (extract data as before)\n    return data\n\n@task\ndef transform_data(data):\n    # ... (transform data as before)\n    return processed_data\n\n@task\ndef load_data(processed_data):\n    # ... (load data as before)\n```\n\nNotice we simply applied `@task` to each function. No need for a special operator class or task IDs - the function name serves as an identifier, and Prefect will handle the orchestration.\n\n### Define a Prefect flow\n\nNow we write a `@flow` function that calls these tasks in the required order:\n\n```python  theme={null}\n@flow\ndef etl_pipeline():\n    data = extract_data()         # calls extract_data task\n    processed = transform_data(data)  # uses output of extract_data\n    load_data(processed)          # calls load_data with result of transform_data\n```\n\nThis Prefect flow function replaces the Airflow DAG. No need for `>>` dependencies or XComs. **Task results can be stored in variables that are passed directly to other tasks as arguments**. By default, tasks are automatically executed in the order they are called.\n\nUnlike Airflow, where testing often requires an Airflow context, Prefect flows run like standard Python code. You can execute `etl_pipeline()` in an interpreter, import it elsewhere, or test tasks individually (`transform_data.fn(sample_data)`).\n\n<Card title=\"Key Differences\" icon=\"file-plus-minus\">\n  * **Airflow:** Defines operators, sets dependencies (`>>`), and relies on XCom for data passing.\n  * **Prefect:** Calls tasks like functions, with execution order determined by data flow, making workflows more intuitive and testable.\n</Card>\n\n### Branching and conditional logic\n\nIn Airflow, conditional branching is typically handled using BranchPythonOperator, ShortCircuitOperator, or trigger rules, requiring explicit DAG constructs to determine execution paths. Prefect simplifies branching by leveraging standard Python if/else logic directly within flows.\n\n**Implementing Branching in Prefect**\n\nInstead of using BranchPythonOperator and dummy tasks for joining paths, you can structure conditional execution using native Python control flow:\n\n```python  theme={null}\n@flow\ndef my_flow():\n    result = extract_data()\n    if some_condition(result):\n        outcome = branch_task_a()  # a task or subflow for branch A\n    else:\n        outcome = branch_task_b()  # branch B\n    final_task(outcome)\n```\n\n**Key Differences from Airflow**\n\n| **Feature**          | **Airflow (BranchPythonOperator)**                        | **Prefect (`if/else` logic)**                                           |\n| -------------------- | --------------------------------------------------------- | ----------------------------------------------------------------------- |\n| **Branching Method** | Often uses specialized operators (`BranchPythonOperator`) | Uses native Python conditionals (`if/else`)                             |\n| **Skipped Tasks**    | Unselected branches are explicitly **skipped**            | Prefect **only runs** the executed branch—no skipping needed            |\n| **Join Behavior**    | Uses **DummyOperator** to rejoin paths                    | Downstream tasks execute **automatically** after the conditional branch |\n\n**Advantages of Prefect’s Approach**\n\n* **No special operators** — branching is simpler and more intuitive\n* **Cleaner code** — fewer unnecessary tasks like `DummyOperator`\n* **No explicit skipping required** — Prefect only executes the called tasks\n\nBy using standard Python control flow, Prefect **eliminates complexity** and makes conditional execution more **readable, maintainable, and testable**.\n\n### Retries and error handling\n\nAirflow DAGs often have retry settings either at the DAG level (`default_args`) or per task (e.g., `retries=3`). In Prefect, you can specify [retries](/v3/develop/write-flows#retries) for any task or flow.\n\nUse `@task(retries=2, retry_delay_seconds=60)` to retry a task twice on failure, or `@flow(retries=1)` to retry the entire flow once. Prefect **distinguishes flow and task retries**—flow retries rerun all tasks, while task retries rerun only the failed task. Replace Airflow-specific error handling (`on_failure_callback`, sensors) with Prefect's **Retry**, **State Handlers**, or built-in failure notifications.\n\n### Remove Airflow-specific code\n\nGo through the DAG code and strip out anything that doesn't apply in Prefect.\n\nThis includes: DAG declarations (`DAG(...)` blocks), default\\_args, Airflow imports (`from airflow...`), XCom push/pull calls (replace with return values), Jinja templating in operator arguments (you can often just compute those values in Python directly or use [Prefect parameters](/v3/deploy/index#workflow-scheduling-and-parametrization)).\n\nIf your DAG used Airflow Variables or Connections (Airflow's way to store config in the Metastore), you'll need to supply those to Prefect tasks via another means - for example, as [environment variables](/v3/develop/settings-and-profiles#environment-variables) or using [Prefect Blocks](/integrations/integrations) (like a Block for a database connection string). Essentially, your Prefect flow code should look like a regular Python script with functions, not like an Airflow DAG file.\n\nAs an illustration, here's how our example **ETL pipeline** looks after conversion:\n\n```python  theme={null}\nfrom prefect import flow, task\n\n@task(retries=1, log_prints=True)\ndef extract_data():\n    # fetch data from API (simulated)\n    data = get_data_from_api()\n    return data\n\n@task\ndef transform_data(data):\n    # process the data\n    processed = transform(data)\n    return processed\n\n@task\ndef load_data(data):\n    # load data to database\n    load_into_db(data)\n\n@flow(name=\"etl_pipeline\")\ndef etl_pipeline_flow():\n    raw = extract_data()\n    processed = transform_data(raw)\n    load_data(processed)\n\nif __name__ == \"__main__\":\n    # For local testing\n    etl_pipeline_flow()\n```\n\nKey improvements in this converted code:\n\n* **Direct execution for testing** - `if __name__ == \"__main__\": etl_pipeline_flow()` allows running the flow locally during development. In production, Prefect handles scheduling.\n* **Built-in retries and logging** - `retries=1` ensures one retry on failure, and `log_prints=True` sends `print()` output to Prefect's UI.\n* **Pure Python** - No Airflow imports or context, making the flow easy to test, debug, and run consistently across environments (IDE, CI, or production).\n\n### Validate functional equivalence\n\nOnce a DAG has been rewritten as a Prefect flow, execute the flow and compare its results with the Airflow DAG to ensure expected outcomes. If discrepancies arise, modify the flow accordingly. Keep in mind the original DAG may have depended on XComs or global variables that you will need to account for.\n\nFor each task and special case, including [subDAGs](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#concepts-subdags) and [TaskGroups](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#taskgroups), implement them as subflows or Python functions in Prefect. When transitioning from Airflow's TaskFlow API, keep in mind that Prefect's `@task` decorator serves a similar purpose but does not rely on XComs.\n\nAfter completing these steps, the Prefect flow should accurately replicate the functionality of the Airflow DAG while being more modular and testable. The migration is now complete, and the next step is to focus on deploying and optimizing the new workflows.\n\n## Infrastructure Migration Considerations\n\nMigrating your code is a big step, but ensuring your workflows run smoothly in Prefect is just as important. Prefect's **flexible execution** makes this easier, supporting Prefect managed execution, local machines, VMs, containers, and Kubernetes with less setup. This section maps Airflowss executors to **Prefect Work Pools and Workers**, while also covering sensors, hooks, logging, and state management to complete your migration.\n\n### Leveraging Prefect Managed Execution\n\n**Running Flows Without Infrastructure Setup**\n\nPrefect Cloud offers [Managed Execution](/v3/how-to-guides/deployment_infra/serverless), allowing you to run flows **without setting up infrastructure or maintaining workers**. With Prefect Managed work pools, Prefect handles compute, execution, and scheduling, eliminating the need for a cloud provider account or on-premises infrastructure.\n\n**Getting Started with Prefect Managed Execution**\n\n<Steps>\n  <Step title=\"Create a Prefect Managed Work Pool\">\n    ```bash  theme={null}\n    prefect work-pool create my-managed-pool --type prefect:managed\n    ```\n  </Step>\n\n  <Step title=\"Deploy a Flow to Managed Execution\">\n    ```python  theme={null}\n    from prefect import flow\n\n    if __name__ == \"__main__\":\n        flow.from_source(\n            source=\"https://github.com/prefecthq/demo.git\",\n            entrypoint=\"flow.py:my_flow\",\n        ).deploy(\n            name=\"test-managed-flow\",\n            work_pool_name=\"my-managed-pool\",\n        )\n    ```\n  </Step>\n\n  <Step title=\"Run the Deployment via Prefect UI or CLI\">\n    ```bash  theme={null}\n    python managed-execution.py\n    ```\n  </Step>\n</Steps>\n\nThis will allow your flow to run remotely without provisioning workers, setting up Kubernetes, or maintaining cloud infrastructure.\n\n**When to Use Prefect Managed Execution**\n\n<CardGroup cols={2}>\n  <Card title=\"Best for\" icon=\"check\" color=\"green\">\n    Ideal for testing and running flows without infrastructure setup, especially for teams that want managed execution without a cloud provider.\n  </Card>\n\n  <Card title=\"Consider self-hosted execution\" icon=\"x\" color=\"red\">\n    If you need custom images, heavy dependencies, private networking, or higher concurrency limits than Prefect's tiers allow.\n  </Card>\n</CardGroup>\n\n**Next Steps**\n\nIf you require self-hosted execution, the next sections cover how to migrate Airflow Executors to Prefect Work Pools across different infrastructure types (Kubernetes, Docker, Celery, etc.).\n\nFor full details on Prefect Managed Execution, refer to the [Managed Execution documentation](/v3/how-to-guides/deployment_infra/serverless).\n\n### Airflow Executors\n\n**Airflow Executors vs Prefect Work Pools/Workers:** Airflow's executor setting determines how tasks are distributed. Prefect's equivalent concept is the [**work pool**](/v3/deploy/infrastructure-concepts/work-pools) (with one or more [**workers**](/v3/deploy/infrastructure-concepts/workers) polling it).\n\nIn Airflow, each task executes independently, regardless of the executor used. Whether running with LocalExecutor, CeleryExecutor, or KubernetesExecutor, every task runs as an isolated process or pod. Executors control how and where these tasks are executed, but the core execution model remains task-by-task.\n\nIn contrast, Prefect executes an entire flow run within a single execution environment (e.g., a local process, Docker container, or Kubernetes pod). Tasks within a flow execute within the same runtime context, reducing fragmentation and improving performance. Prefect's execution model simplifies resource management, allowing for in-memory data passing between tasks rather than relying on external storage or metadata databases.\n\nHere's a mapping of typical setups:\n\n#### Airflow LocalExecutor\n\nWith the **Airflow LocalExecutor** tasks run as subprocesses on the same machine. In Prefect, the default behavior is similar - you can run the flow in a local Python process, and tasks will execute sequentially by default. That does not *have* to be the same machine that is running your Prefect UI and scheduler.\n\nFor parallelism on a single machine, use [**`DaskTaskRunner`**](/integrations/prefect-dask/index) to enable multi-process execution:\n\n```python  theme={null}\n@flow(task_runner=DaskTaskRunner())\n```\n\nBy default, Prefect's **Process work pool** runs flows as subprocesses. A basic **Airflow LocalExecutor** setup can be replaced with a **Prefect worker** on the same VM using a **process work pool**, eliminating the need for a separate scheduler.\n\n#### Airflow CeleryExecutor\n\n**Airflow CeleryExecutor** where distributed workers run across multiple machines, using a message broker like RabbitMQ/Redis.\n\nPrefect eliminates the need for a **message broker** or **results backend**, as its API server manages work distribution. To replicate an Airflow **CeleryExecutor** setup, deploy **multiple Prefect workers** across machines, all polling from a shared **work pool**.\n\n**Setting Up a Work Pool and Workers**\n\n1. **Create a work pool** (e.g., `\"prod-work-pool\"`):\n   ```bash  theme={null}\n   prefect work-pool create prod-work-pool --type process\n   ```\n2. **Start a worker on each node**, assigning it to the work pool:\n   ```bash  theme={null}\n   prefect worker start -p prod-work-pool\n   ```\n3. **Workers poll the work pool** and execute assigned flow runs.\n\nPrefect **work pools** function similarly to **Celery queues**, allowing multiple workers to process tasks concurrently.\n\n#### Airflow KubernetesExecutor\n\nIn Airflow, the **KubernetesExecutor** follows the per-task execution model, launching each task in its own Kubernetes pod. Prefect, instead, uses a Kubernetes Work Pool, where each flow run executes in a single Kubernetes pod. This approach reduces fragmentation, as tasks run within the same execution environment rather than spawning separate pods.\n\n**Configuring a Kubernetes Work Pool**\nFor detailed instructions, see [Prefect's Kubernetes Work Pool documentation](/v3/how-to-guides/deployment_infra/kubernetes). But the general steps to take are:\n\n1. **Create a Kubernetes work pool** with the desired pod template (e.g., image, resources):\n   ```bash  theme={null}\n   prefect work-pool create k8s-pool --type kubernetes\n   ```\n2. **Deploy a flow to the Kubernetes work pool**:\n\n```python  theme={null}\nfrom prefect import flow\n\n@flow(log_prints=True)\ndef buy():\n    print(\"Buying securities\")\n\nif __name__ == \"__main__\":\n    buy.deploy(\n        name=\"my-code-baked-into-an-image-deployment\",\n        work_pool_name=\"k8s-pool\",\n        image=\"my_registry/my_image:my_image_tag\"\n    )\n```\n\nAlternatively, you can use a [prefect.yaml](/v3/how-to-guides/deployment_infra/kubernetes#define-a-prefect-deployment) file to deploy your flow to the Kubernetes work pool.\n\n3. [**Run a Kubernetes worker in-cluster**](/v3/how-to-guides/deployment_infra/kubernetes#deploy-a-worker-using-helm) to execute flow runs.\n4. **Execution Flow**:\n\n* The worker **picks up a scheduled flow run**.\n* It **creates a new pod**, which executes the entire flow.\n* The **pod terminates automatically** after execution.\n\nThis setup eliminates the need for a long-running scheduler, reducing operational complexity while leveraging Kubernetes for **on-demand, containerized execution**.\n\n#### Airflow CeleryKubernetes\n\n**Airflow + Celery + Kubernetes (CeleryKubernetes Executor)** or other hybrid: Some Airflow deployments use Celery for distributed scheduling but run tasks in containers or on Kubernetes.\n\nPrefect's model can handle these as well by combining approaches - e.g., use a Kubernetes work pool with multiple worker processes distributed as needed. The general principle is that Prefect **work pools** can cover all these patterns (local, multi-machine, containers, serverless) via configuration, not code, and you manage them via Prefect's UI/CLI.\n\n#### Using Serverless compute\n\nPrefect supports [**serverless execution**](/v3/how-to-guides/deployment_infra/serverless) on various cloud platforms, eliminating the need for dedicated infrastructure. Instead of provisioning long-running workers, flows can be executed **on-demand** in ephemeral environments. Prefect's push-based work pools allow flows to be submitted to serverless services, where they run in isolated containers and automatically scale with demand.\n\n**Serverless Platforms**\nPrefect flows can run on:\n\n* **AWS ECS** (Fargate or EC2-backed containers)\n* **Azure Container Instances (ACI)**\n* **Google Cloud Run**\n* **Modal** (serverless compute for AI/ML workloads)\n* **Coiled** (serverless Dask clusters for parallel workloads)\n\n**Configuring a Serverless Work Pool**\nTo run flows on a serverless platform, create a **push-based work pool** and configure it to submit jobs to the desired service.\n\nExample: Creating an **ECS work pool**:\n\n```bash  theme={null}\nprefect work-pool create --type ecs:push --provision-infra my-ecs-pool\n```\n\nDeployments can then be configured to use the serverless work pool, allowing Prefect to submit flow runs without maintaining long-lived infrastructure.\n\nFor setup details, refer to [Prefect's serverless execution documentation](/v3/how-to-guides/deployment_infra/serverless).\n\n### Airflow Sensors\n\nAirflow Sensors continuously poll for external conditions, such as file availability or database changes, which can tie up resources. Prefect replaces this with an **event-driven approach**, where external systems trigger flow execution when conditions are met.\n\n**Using External Triggers**\nInstead of using an Airflow `S3KeySensor`, configure an AWS Lambda or EventBridge rule to call the Prefect API when an S3 file is uploaded. Prefect Cloud and Server provide API endpoints to start flows on demand. Prefect's **Automations** can also trigger flows based on specific conditions.\n\n**Handling Polling Scenarios**\nIf an external system lacks event-driven capabilities, implement a lightweight **polling flow** that runs on a schedule (e.g., every 5 minutes), checks the condition, and triggers the main flow if met. This approach minimizes idle resource consumption compared to Airflow's persistent sensors.\n\nPrefect's model eliminates long-running sensor tasks, making workflows **more efficient, scalable, and event-driven**.\n\n### Airflow Hooks and Integrations\n\nAirflow provides hooks and operators for interacting with external systems (e.g., **JDBC, cloud services, databases**). In Prefect, these integrations are handled through [**Prefect Integrations**](/integrations/integrations) (e.g., `prefect-snowflake`, `prefect-gcp`, `prefect-dbt`) or by directly using the relevant **Python libraries** within tasks.\n\n**Migrating Airflow Hooks to Prefect**\n\n1. **Identify Airflow hooks** used in your DAGs (e.g., `PostgresHook`, `GoogleCloudStorageHook`).\n2. **Replace them with equivalent Prefect integrations** or direct Python library calls.\n\n**Example:** Instead of\n\n```python  theme={null}\nhook = PostgresHook(postgres_conn_id=my_conn_id)\nengine = hook.get_sqlalchemy_engine()\nsession = sessionmaker(bind=engine)()\n```\n\nUse Prefect Blocks for secure credential management:\n\n```python  theme={null}\nfrom prefect_sqlalchemy import SqlAlchemyConnector\n\nSqlAlchemyConnector.load(\"BLOCK_NAME-PLACEHOLDER\")\n```\n\n3. **Use Prefect Blocks for secrets management**, similar to Airflow Connections, to separate credentials from code.\n\n**Replacing Airflow Operators with Prefect Tasks**\n\n* **Prefect tasks** can call any Python library, eliminating the need for custom Airflow operators.\n* Example: Instead of using a **BashOperator** to call an API via a shell script, install the necessary package in the flow's environment and call it directly in a task.\n\nPrefect's approach **removes unnecessary abstraction layers**, allowing direct access to the full Python ecosystem without Airflow-specific constraints.\n\nBasically: **anything done with a custom Airflow operator or hook can be replaced in Prefect with a task using the appropriate Python library.** Prefect removes Airflow's constraints, allowing direct use of the full Python ecosystem. For example, instead of using a **BashOperator** to call an API via a shell script, install the required package in your environment and call it directly from a task, eliminating unnecessary workarounds.\n\n### Observability\n\n#### State and logging\n\n**Task and Flow State Management**\nIn Airflow, task states (`success`, `failed`, `skipped`, etc.) are stored in a metadata database and displayed in the Airflow UI's DAG run view. Prefect also tracks state for **each task and flow run**, but these states are managed by the **Prefect backend** (Prefect Server or Cloud API) and can be accessed via the **Prefect UI, API, or CLI**.\n\nAfter migration, similar visibility is available in Prefect's UI, where you can track which flows and tasks succeeded or failed. Prefect also includes additional state management features such as:\n\n* Cancel a flow run (`Cancelling` state).\n* Retry a failed flow run (with manual steps).\n* **Task caching** between runs to avoid redundant computations.\n\n**Logging Differences**\nAirflow logs task execution output to files (stored on executor machines or remote storage), viewable through the UI. Prefect **captures stdout, stderr, and Python logging** from tasks and sends them to the Prefect backend, making logs accessible in the **Prefect UI, API, and CLI**.\n\nTo ensure logs appear correctly in Prefect's UI, use `@flow(log_prints=True)` or `@task(log_prints=True)`\n\nThese flags route `print()` statements to Prefect logs automatically.\n\nFor centralized logging (e.g., ElasticSearch, Stackdriver), Prefect supports [**custom logging handlers**](/v3/advanced/logging-customization) and **third-party integrations**. Logs can be forwarded similarly to how Airflow handled external logging.\n\n**Debugging and Troubleshooting**\nPrefect simplifies debugging because tasks are **standard Python functions**. Instead of analyzing scheduler or worker logs, you can:\n\n* **Re-run individual tasks or flows locally** to reproduce issues.\n* **Test flows interactively** in an IDE before deploying.\n\nThis direct execution model eliminates the need to troubleshoot failures through a scheduling system, making debugging faster and more intuitive than in Airflow.\n\n#### Monitoring\n\n**Notifications and Alerts**\nIn Airflow, monitoring is typically managed through the UI, email alerts on task failures, and external monitoring of the scheduler.\nPrefect provides similar capabilities through *Automations*, which can be configured to trigger alerts via Slack, email, or webhooks based on specific events.\n\nTo replicate Airflow's alerting (e.g., failures or SLA misses), configure [**Prefect Automations**](/v3/automate/events/automations-triggers) to:\n\n* Notify on **flow or task failures**.\n* Alert when a **flow run exceeds a specified runtime**.\n* Trigger **custom actions** based on state changes.\n\n**Service Level Agreements (SLAs)**\nPrefect Cloud supports Service Level Agreements (SLAs) to monitor and enforce performance expectations for flow runs. SLAs automatically trigger alerts when predefined thresholds are violated.\n\nSLAs can be defined via the Prefect UI, prefect.yaml, `.deploy()` method, or CLI. Violations generate `prefect.sla.violation` events, which can trigger Automations to send notifications or take corrective actions.\n\nFor full configuration details, refer to the [Measure reliability with Service Level Agreements](/v3/automate/events/slas) documentation.\n\n**Implementation Considerations**\nPrefect allows flexible logging and alerting adjustments to match existing monitoring workflows. Logging handlers can integrate with **third-party services** (e.g., ElasticSearch, Datadog), and **Prefect's API and UI provide real-time state visibility** for proactive monitoring.\n\n## Deployment & CI/CD Changes\n\nDeploying workflows in Prefect differs from Airflow's approach of “drop DAG files in a folder.” In Prefect, a **Deployment** is the unit of deployment: it associates a flow (Python function) with infrastructure (how/where to run) and optional schedule or triggers. Migrating to Prefect means adopting a new way to package and release your workflows, as well as updating any CI/CD pipelines that automated your Airflow deployments.\n\n### Prefect Deployment\n\n**From Airflow DAG schedules to Prefect Deployment:** In Airflow, deployment usually meant placing your DAG code on the Airflow scheduler (e.g., by committing to a Git repo that the scheduler reads, or copying files to the DAGs directory). There isn't a formal deployment artifact beyond the Python files. Prefect, by contrast, treats deployments as first-class objects. You will create a deployment for each flow (or for each distinct configuration of a flow you want to run). This can be done via code (calling `flow.deploy()`), via CLI (`prefect deployment`), or by writing a YAML (`prefect.yaml`) that describes the deployment.\n\nKey things a **Prefect deployment** defines:\n\n* **Target flow** (which function, and which file or import path it comes from).\n* **Infrastructure configuration**: e.g., use the “Kubernetes work pool” or “process” type, possibly the docker image to use, resource settings, etc.\n* **Storage of code**: e.g., whether the code is stored in the image, pulled from Git, etc. (Prefect can package code into a Docker image or rely on an existing image).\n* **Schedule** (optional): e.g., Cron or interval schedule for automatic runs, or you can leave it manual.\n* **Parameters** (optional): default parameter values for the flow, if any.\n\nTo migrate each Airflow DAG, you will create a Prefect deployment for its flow. For example, if we converted `etl_pipeline` DAG to `etl_pipeline_flow` in Prefect, we might write a `prefect.yaml` like:\n\n```yaml  theme={null}\n# prefect.yaml\ndeployments:\n  - name: etl-pipeline-prod\n    flow_name: etl_pipeline_flow\n    entrypoint: etl_flow.py:etl_pipeline_flow # file and function where the flow is defined\n    parameters: {}\n    schedule: \"@daily\"\n    work_pool:\n      name: prod-k8s-pool\n      # other infra settings like image, etc., if needed\n```\n\nThis YAML can define multiple deployments, but in this case we have one named “etl-pipeline-prod” which runs daily via the `prod-k8s-pool` (a Kubernetes pool perhaps). In Airflow, these details were all intertwined in the DAG file (the schedule was in code, the infrastructure maybe in the executor config or the DAG via `executor_config`). In Prefect, there is a separation of these concerns.\n\n### Automation via CI/CD\n\nMany organizations use CI/CD to deploy Airflow DAGs (for example, a Git push triggers a Jenkins job that lints DAGs and copies them to the Airflow server). With Prefect, you'll likely adjust your CI/CD to **register Prefect deployments** whenever you update the flow code. Prefect's CLI is your friend here. A common pattern is:\n\n1. On merge to main, build a Docker image with your flow code, push it to a registry\n2. Then run `prefect deployment build -n <name> -p <work_pool_name> --cron \"<schedule>\" -q default -o deployment.yaml` (or use `prefect.yaml`) and apply it.\n\nThis can all be scripted. In fact, Prefect provides guidance on using [GitHub Actions or similar tooling to do this](/v3/advanced/deploy-ci-cd). By integrating Prefect's deployment steps into CI, you ensure that any change in your flow code gets reflected in Prefect's orchestrator, much like updating DAG code in Airflow.\n\nAlternatively, if your deployment is set to pull the workflow code from your git repository each time, you only need to push the latest workflow code, and automatically next time your deployment runs it will pull the latest workflow code.\n\nThis CI pipeline approach allows versioning and automating your flows deployment, treating them similarly to application code deployments. It's a shift from Airflow where deployment could be syncing a folder - Prefect's method is more **controlled** and **atomic** (you create a deployment manifest and apply it, which registers everything with Prefect).\n\n### Prefect in Production\n\nOnce deployed, Prefect schedules and orchestrates flows based on your **deployments**. Follow these best practices to ensure a reliable production setup:\n\n* **High Availability**: If self-hosting, use PostgreSQL and consider running **multiple API replicas** behind a load balancer. [**Prefect Cloud**](https://prefect.io/cloud) handles availability automatically.\n* **Keep Workers Active**: Ensure Prefect workers are always running, whether as systemd services, Docker containers, or Kubernetes deployments.\n* **Logging & Observability**: Use Prefect's UI for logs or configure external storage (e.g., S3, Elasticsearch) for **long-term retention**.\n* **Notifications & Alerts**: Set up failure alerts via Slack, email, or Twilio using [**Prefect Automations**](/v3/automate/events/automations-triggers) to ensure timely issue resolution.\n* **CI/CD & Testing**: Validate deployment YAMLs in CI (`prefect deployment build --skip-upload`), and unit test tasks as regular Python functions.\n* **Configuration Management**: Replace Airflow Variables/Connections with [**Prefect Blocks**](/v3/develop/variables), storing secrets via CLI, UI, or version-controlled JSON.\n* **Security & Access Control**: Prefect Cloud includes built-in authentication & role-based access; self-hosted setups should secure API and workers accordingly.\n* **Decommissioning Airflow**: Once migration is complete, disable DAGs, archive the code, and shut down Airflow components to reduce operational overhead.\n\nFor more details on operating Prefect in production, see the [How-To Guides](/v3/how-to-guides).\n\n## Testing & Validation\n\nThorough testing ensures your Prefect flows perform like their Airflow equivalents. Since this is a **full migration**, validation is essential before decommissioning Airflow.\n\n**Testing Prefect Flows in Isolation**\n\n* **Unit test task logic** - Write tests for tasks as regular Python functions.\n* **Run flows locally** - Run the script that calls your flow function - just like a normal Python script.\n* **Use Prefect's local orchestration** - Start a Prefect server (`prefect server start`), register a deployment, and trigger flows via Prefect UI to mirror production behavior.\n* **Compare outputs** - Run both Airflow and Prefect for the same input and validate results (e.g., database rows, file outputs). Debug discrepancies early.\n\n**Validation Phase: Temporary Parallel Running (Shadow Mode)**\n\n* **Keep the Airflow DAG inactive** but available for testing.\n* **Manually trigger** both Airflow and Prefect flows for the same execution date.\n* **Write test outputs separately** to prevent conflicts, ensuring parity before stopping Airflow runs.\n\nFor batch jobs, this phase should be **brief**, ensuring correctness without long-term dual maintenance.\n\n**Decommissioning Airflow**\nOnce a Prefect flow is stable, **disable the corresponding Airflow DAG** to prevent accidental execution. Clearly document Prefect as the new source of truth. Avoid keeping inactive DAGs indefinitely, as they can cause confusion—**archive or remove them once the migration is complete**.\n\n### Common issues and troubleshooting\n\n* **Missing dependencies:** If a Prefect flow fails with `ImportError`, ensure all required libraries are installed in the execution environment (Docker image, VM, etc.), not just locally.\n* **Credentials & access:** Verify that Prefect workers have the same permissions as Airflow (e.g., service accounts, IAM roles). If using Kubernetes, ensure pods can access necessary databases and APIs.\n* **Scheduling differences:** Airflow schedules may trigger at the end of an interval, while Prefect runs in real-time. Align Cron schedules and time zones if needed.\n* **Concurrency & parallelism:** Configure **work pool and flow run concurrency limits** to prevent overlapping jobs. If too many tasks run in parallel, use Prefect's **tags and concurrency controls** to throttle execution.\n* **Error handling & retries:** Test retries by forcing failures. If Airflow used `trigger_rule=\"all_done\"`, implement equivalent logic in Prefect with `try/except`.\n* **Performance monitoring:** Compare Prefect vs. Airflow run times. If slower, check if tasks are running sequentially instead of in parallel (enable mapping, async, or parallel task runners). If too much parallelism, adjust concurrency settings.\n\nFor some help with troubleshooting, you can see articles on:\n\n* [Configuring logging](/v3/how-to-guides/workflows/add-logging)\n* [Tracking activity](/v3/concepts/events)\n\nThroughout testing, keep an eye on the Prefect UI's **Flow Run and Task Run views** - they will show you the execution steps, logs, and any exceptions. The UI can be very helpful for pinpointing where a flow failed or hung. It's analogous to Airflow's Graph view and log view but with the benefit of real-time state updates (no need to refresh for state changes).\n\nYou might also consider joining the [Prefect Slack community](https://prefect.io/slack) to get help from the community and Prefect team.\n\n**Debugging tips:**\n\n* If a flow run gets stuck, you can cancel it via UI/CLI.\n* Utilize the fact that you can re-run a Prefect flow easily. For example, if a specific task fails consistently, you can add some debug `print` statements, re-deploy (which is quick with Prefect CLI), and re-run to see output.\n* Leverage Prefect's task state inspection. In the UI, you can often see the exception message and stack trace for a failed task, which helps identify the problem in code.\n* Read the results from MarvinAI's analysis of your code to help identify potential issues.\n\n<Frame caption=\"MarvinAI is a tool that can help you debug your Prefect flows.\">\n  <img src=\"https://mintcdn.com/prefect-bd373955/dwD6EJObIjtIzwSC/v3/img/ui/marvin-ai.png?fit=max&auto=format&n=dwD6EJObIjtIzwSC&q=85&s=fe0535a244c49db9646518e953ef0965\" alt=\"MarvinAI\" data-og-width=\"626\" width=\"626\" data-og-height=\"155\" height=\"155\" data-path=\"v3/img/ui/marvin-ai.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/prefect-bd373955/dwD6EJObIjtIzwSC/v3/img/ui/marvin-ai.png?w=280&fit=max&auto=format&n=dwD6EJObIjtIzwSC&q=85&s=c6a13bd1975b54e61969dcb4be55e6b9 280w, https://mintcdn.com/prefect-bd373955/dwD6EJObIjtIzwSC/v3/img/ui/marvin-ai.png?w=560&fit=max&auto=format&n=dwD6EJObIjtIzwSC&q=85&s=425c3d02d50950b743e223093391210f 560w, https://mintcdn.com/prefect-bd373955/dwD6EJObIjtIzwSC/v3/img/ui/marvin-ai.png?w=840&fit=max&auto=format&n=dwD6EJObIjtIzwSC&q=85&s=d35d5ab21cdf2218b1348dbe94d2bf49 840w, https://mintcdn.com/prefect-bd373955/dwD6EJObIjtIzwSC/v3/img/ui/marvin-ai.png?w=1100&fit=max&auto=format&n=dwD6EJObIjtIzwSC&q=85&s=4cf11d36b9973571407f84a99152a4c5 1100w, https://mintcdn.com/prefect-bd373955/dwD6EJObIjtIzwSC/v3/img/ui/marvin-ai.png?w=1650&fit=max&auto=format&n=dwD6EJObIjtIzwSC&q=85&s=63da3788dea45d11ccb0c7ef596e3557 1650w, https://mintcdn.com/prefect-bd373955/dwD6EJObIjtIzwSC/v3/img/ui/marvin-ai.png?w=2500&fit=max&auto=format&n=dwD6EJObIjtIzwSC&q=85&s=0c6ca79596e39e53060dd69343e402db 2500w\" />\n</Frame>\n\nAs you systematically validate each migrated workflow, you'll build confidence in the new system. When all tests pass and the outputs match the old system's, you can declare the migration a success for that workflow. After migrating a few, you'll also develop a playbook for the rest, and the process may speed up.\n\n## Post-Migration\n\n### Optimizing & Scaling Prefect Workflows\n\nWith your workflows running in Prefect, it's time to optimize, scale, and take full advantage of its capabilities. This section covers best practices for streamlining flows, monitoring performance, and ensuring long-term reliability.\n\n**Simplify and Enhance Your Workflows**\n\n* **Remove unnecessary complexity**: If your Airflow DAGs used workarounds (e.g., database intermediaries for data passing), replace them with direct Prefect task returns.\n* **Use nested flows for modularity**: Instead of chaining DAGs, use **nested flows** to orchestrate dependencies within a single flow.\n* **Optimize async convenience**: Use **dynamic task mapping** (`task.map(items)`) to process large datasets efficiently.\n* **Leverage caching**: Enable **result persistence** to skip redundant computations.\n* **Ensure idempotency**: Prevent duplicate processing by parameterizing flows and validating execution logic.\n\n**Monitor and Maintain Your Prefect System**\n\n* **Track performance**: Use Prefect UI and analytics to monitor run durations, failure rates, and bottlenecks.\n* **Set up alerts**: Automate failure notifications via Slack, email, or other integrations.\n* **Improve debugging**: Use UI logs, parameterized re-runs, and version control for better issue resolution.\n* **Version control deployments**: Treat flows like code, using PRs and staging environments before production deployment.\n* **Update documentation**: Ensure internal runbooks reflect Prefect's CLI/UI for managing schedules, failures, and retries.\n\nTo scale and optimize for cost:\n\n| Technique                 | Description                                                                                                                  |\n| ------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n| *Scale efficiently*       | Prefect makes it simple to distribute workloads across work pools and workers, eliminating Airflow's scheduler bottlenecks.  |\n| *Optimize infrastructure* | Adjust worker capacity based on usage, scaling vertically (more resources per worker) or horizontally (adding more workers). |\n| *Reduce costs*            | Consider *serverless work pools* (AWS ECS, GCP Cloud Run) to avoid idle infrastructure costs.                                |\n\nTo set yourself up for future success:\n\n| Technique                       | Description                                                                                                                     |\n| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |\n| *Share best practices*          | Conduct a team retrospective to refine workflows and establish templates for new flows.                                         |\n| *Embrace Prefect's flexibility* | Now that scheduling and execution are handled seamlessly, focus on building better data workflows, not managing infrastructure. |\n\n### Conclusion\n\nBy completing this migration, you've moved to a more scalable, efficient orchestration system.\nPrefect allows your team to focus on engineering—iterating faster, improving reliability, and scaling seamlessly.\n\n**Next steps:**\n\n* [Learn more about Prefect](/v3/get-started/index)\n* [See more Prefect examples](/v3/examples/index)\n* [Join the community](https://prefect.io/slack)\n* [Dig into Prefect's Integrations](/integrations/integrations)\n* [Learn more about Prefect Cloud](https://prefect.io/cloud)\n* [Visit Prefect's GitHub](https://github.com/PrefectHQ/prefect)",
  "content_length": 47146
}