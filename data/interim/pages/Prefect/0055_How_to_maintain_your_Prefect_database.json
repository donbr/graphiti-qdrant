{
  "title": "How to maintain your Prefect database",
  "source_url": "https://docs-3.prefect.io/v3/advanced/database-maintenance",
  "content": "Monitor and maintain your PostgreSQL database for self-hosted Prefect deployments\n\nSelf-hosted Prefect deployments require database maintenance to ensure optimal performance and manage disk usage. This guide provides monitoring queries and maintenance strategies for PostgreSQL databases.\n\n<Warning>\n  This guide is for advanced users managing production deployments. Always test maintenance operations in a non-production environment first, if possible.\n\n  Exact numbers included in this guide will vary based on your workload and installation.\n</Warning>\n\n## Quick reference\n\n**Daily tasks:**\n\n* Check disk space and table sizes\n* Monitor bloat levels (> 50% requires action)\n* Run retention policies for old flow runs\n\n**Weekly tasks:**\n\n* Review autovacuum performance\n* Check index usage and bloat\n* Analyze high-traffic tables\n\n**Red flags requiring immediate action:**\n\n* Disk usage > 80%\n* Table bloat > 100%\n* Connection count approaching limit\n* Autovacuum hasn't run in 24+ hours\n\n## Database growth monitoring\n\nPrefect stores entities like events, flow runs, task runs, and logs that accumulate over time. Monitor your database regularly to understand growth patterns specific to your usage.\n\n### Check table sizes\n\n```sql  theme={null}\n-- Total database size\nSELECT pg_size_pretty(pg_database_size('prefect')) AS database_size;\n\n-- Table sizes with row counts\nSELECT \n    schemaname,\n    relname AS tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||relname)) AS total_size,\n    to_char(n_live_tup, 'FM999,999,999') AS row_count\nFROM pg_stat_user_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||relname) DESC\nLIMIT 20;\n```\n\n### Monitor disk space\n\nTrack overall disk usage to prevent outages:\n\n```sql  theme={null}\n-- Check database disk usage\nSELECT \n    current_setting('data_directory') AS data_directory,\n    pg_size_pretty(pg_database_size('prefect')) AS database_size,\n    pg_size_pretty(pg_total_relation_size('public.events')) AS events_table_size,\n    pg_size_pretty(pg_total_relation_size('public.log')) AS log_table_size;\n\n-- Check available disk space (requires pg_stat_disk extension or shell access)\n-- Run from shell: df -h /path/to/postgresql/data\n```\n\nCommon large tables in Prefect databases:\n\n* `events` - Automatically generated for all state changes (often the largest table)\n* `log` - Flow and task run logs\n* `flow_run` and `task_run` - Execution records\n* `flow_run_state` and `task_run_state` - State history\n\n### Monitor table bloat\n\nPostgreSQL tables can accumulate \"dead tuples\" from updates and deletes. Monitor bloat percentage to identify tables needing maintenance:\n\n```sql  theme={null}\nSELECT\n    schemaname,\n    relname AS tablename,\n    n_live_tup AS live_tuples,\n    n_dead_tup AS dead_tuples,\n    CASE WHEN n_live_tup > 0 \n        THEN round(100.0 * n_dead_tup / n_live_tup, 2)\n        ELSE 0\n    END AS bloat_percent,\n    last_vacuum,\n    last_autovacuum\nFROM pg_stat_user_tables\nWHERE schemaname = 'public' \n    AND n_dead_tup > 1000\nORDER BY bloat_percent DESC;\n```\n\n### Monitor index bloat\n\nIndexes can also bloat and impact performance:\n\n```sql  theme={null}\n-- Check index sizes and bloat\nSELECT \n    schemaname,\n    relname AS tablename,\n    indexrelname AS indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,\n    idx_scan AS index_scans,\n    idx_tup_read AS tuples_read,\n    idx_tup_fetch AS tuples_fetched\nFROM pg_stat_user_indexes\nWHERE schemaname = 'public'\nORDER BY pg_relation_size(indexrelid) DESC\nLIMIT 20;\n```\n\n## PostgreSQL VACUUM\n\nVACUUM reclaims storage occupied by dead tuples. While PostgreSQL runs autovacuum automatically, you may need manual intervention for heavily updated tables.\n\n### Manual VACUUM\n\nFor tables with high bloat percentages:\n\n```sql  theme={null}\n-- Standard VACUUM (doesn't lock table)\nVACUUM ANALYZE flow_run;\nVACUUM ANALYZE task_run;\nVACUUM ANALYZE log;\n\n-- VACUUM FULL (rebuilds table, requires exclusive lock)\n-- WARNING: This COMPLETELY LOCKS the table - no reads or writes!\n-- Can take HOURS on large tables. Only use as last resort.\nVACUUM FULL flow_run;\n\n-- Better alternative: pg_repack (if installed)\n-- Rebuilds tables online without blocking\n-- pg_repack -t flow_run -d prefect\n```\n\n### Monitor autovacuum\n\nCheck if autovacuum is keeping up with your workload:\n\n```sql  theme={null}\n-- Show autovacuum settings\nSHOW autovacuum;\nSHOW autovacuum_vacuum_scale_factor;\nSHOW autovacuum_vacuum_threshold;\n\n-- Check when tables were last vacuumed\nSELECT \n    schemaname,\n    relname AS tablename,\n    last_vacuum,\n    last_autovacuum,\n    vacuum_count,\n    autovacuum_count\nFROM pg_stat_user_tables\nWHERE schemaname = 'public'\nORDER BY last_autovacuum NULLS FIRST;\n```\n\n### Tune autovacuum for Prefect workloads\n\nDepending on your workload, your write patterns may require more aggressive autovacuum settings than defaults:\n\n```sql  theme={null}\n-- For high-volume events table (INSERT/DELETE heavy)\nALTER TABLE events SET (\n    autovacuum_vacuum_scale_factor = 0.05,  -- Default is 0.2\n    autovacuum_vacuum_threshold = 1000,\n    autovacuum_analyze_scale_factor = 0.02  -- Keep stats current\n);\n\n-- For state tables (INSERT-heavy)\nALTER TABLE flow_run_state SET (\n    autovacuum_vacuum_scale_factor = 0.1,\n    autovacuum_analyze_scale_factor = 0.05\n);\n\n-- For frequently updated tables\nALTER TABLE flow_run SET (\n    autovacuum_vacuum_scale_factor = 0.1,\n    autovacuum_vacuum_threshold = 500\n);\n```\n\n### When to take action\n\n**Bloat thresholds:**\n\n* **\\< 20% bloat**: Normal, autovacuum should handle\n* **20-50% bloat**: Monitor closely, consider manual VACUUM\n* **> 50% bloat**: Manual VACUUM recommended\n* **> 100% bloat**: Significant performance impact, urgent action needed\n\n**Warning signs:**\n\n* Autovacuum hasn't run in > 24 hours on active tables\n* Query performance degrading over time\n* Disk space usage growing faster than data volume\n\n## Data retention\n\nImplement data retention policies to manage database growth. The following example shows a Prefect flow that safely deletes old flow runs using the Prefect API:\n\n<Note>\n  Using the Prefect API ensures proper cleanup of all related data, including logs and artifacts. The API handles cascade deletions and triggers necessary background tasks.\n</Note>\n\n```python  theme={null}\nimport asyncio\nfrom datetime import datetime, timedelta, timezone\nfrom prefect import flow, task, get_run_logger\nfrom prefect.client.orchestration import get_client\nfrom prefect.client.schemas.filters import FlowRunFilter, FlowRunFilterState, FlowRunFilterStateType, FlowRunFilterStartTime\nfrom prefect.client.schemas.objects import StateType\n\n@task\nasync def delete_old_flow_runs(\n    days_to_keep: int = 30,\n    batch_size: int = 100\n):\n    \"\"\"Delete completed flow runs older than specified days.\"\"\"\n    logger = get_run_logger()\n    \n    async with get_client() as client:\n        cutoff = datetime.now(timezone.utc) - timedelta(days=days_to_keep)\n        \n        # Create filter for old completed flow runs\n        # Note: Using start_time because created time filtering is not available\n        flow_run_filter = FlowRunFilter(\n            start_time=FlowRunFilterStartTime(before_=cutoff),\n            state=FlowRunFilterState(\n                type=FlowRunFilterStateType(\n                    any_=[StateType.COMPLETED, StateType.FAILED, StateType.CANCELLED]\n                )\n            )\n        )\n        \n        # Get flow runs to delete\n        flow_runs = await client.read_flow_runs(\n            flow_run_filter=flow_run_filter,\n            limit=batch_size\n        )\n        \n        deleted_total = 0\n        \n        while flow_runs:\n            batch_deleted = 0\n            failed_deletes = []\n            \n            # Delete each flow run through the API\n            for flow_run in flow_runs:\n                try:\n                    await client.delete_flow_run(flow_run.id)\n                    deleted_total += 1\n                    batch_deleted += 1\n                except Exception as e:\n                    logger.warning(f\"Failed to delete flow run {flow_run.id}: {e}\")\n                    failed_deletes.append(flow_run.id)\n                    \n                # Rate limiting - adjust based on your API capacity\n                if batch_deleted % 10 == 0:\n                    await asyncio.sleep(0.5)\n                    \n            logger.info(f\"Deleted {batch_deleted}/{len(flow_runs)} flow runs (total: {deleted_total})\")\n            if failed_deletes:\n                logger.warning(f\"Failed to delete {len(failed_deletes)} flow runs\")\n            \n            # Get next batch\n            flow_runs = await client.read_flow_runs(\n                flow_run_filter=flow_run_filter,\n                limit=batch_size\n            )\n            \n            # Delay between batches to avoid overwhelming the API\n            await asyncio.sleep(1.0)\n        \n        logger.info(f\"Retention complete. Total deleted: {deleted_total}\")\n\n@flow(name=\"database-retention\")\nasync def retention_flow():\n    \"\"\"Run database retention tasks.\"\"\"\n    await delete_old_flow_runs(\n        days_to_keep=30,\n        batch_size=100\n    )\n```\n\n### Direct SQL approach\n\nIn some cases, you may need to use direct SQL for performance reasons or when the API is unavailable. Be aware that direct deletion bypasses application-level cascade logic:\n\n```python  theme={null}\n# Direct SQL only deletes what's defined by database foreign keys\n# Logs and artifacts may be orphaned without proper cleanup\nasync with asyncpg.connect(connection_url) as conn:\n    await conn.execute(\"\"\"\n        DELETE FROM flow_run \n        WHERE created < $1 \n        AND state_type IN ('COMPLETED', 'FAILED', 'CANCELLED')\n    \"\"\", cutoff)\n```\n\n### Important considerations\n\n1. **Filtering limitation**: The current API filters by `start_time` (when the flow run began execution), not `created` time (when the flow run was created in the database). This means flows that were created but never started won't be deleted.\n\n2. **Test first**: Run with `SELECT` instead of `DELETE` to preview what will be removed\n\n3. **Start conservative**: Begin with longer retention periods and adjust based on needs\n\n4. **Monitor performance**: Large deletes can impact database performance\n\n5. **Backup**: Always backup before major cleanup operations\n\n## Event retention\n\nEvents are automatically generated for all state changes in Prefect and can quickly become the largest table in your database. Prefect includes built-in event retention that automatically removes old events.\n\n### Configure event retention\n\nThe default retention period is 7 days. For high-volume deployments, consider reducing this:\n\n```bash  theme={null}\n# Set retention to 2 days (as environment variable)\nexport PREFECT_EVENTS_RETENTION_PERIOD=\"2d\"\n\n# Or in your prefect configuration\nprefect config set PREFECT_EVENTS_RETENTION_PERIOD=\"2d\"\n```\n\n### Check event table size\n\nMonitor your event table growth:\n\n```sql  theme={null}\n-- Event table size and row count\nSELECT \n    pg_size_pretty(pg_total_relation_size('public.events')) AS total_size,\n    to_char(count(*), 'FM999,999,999') AS row_count,\n    min(occurred) AS oldest_event,\n    max(occurred) AS newest_event\nFROM events;\n```\n\n<Note>\n  Events are used for automations and triggers. Ensure your retention period keeps events long enough for your automation needs.\n</Note>\n\n## Connection monitoring\n\nMonitor connection usage to prevent exhaustion:\n\n```sql  theme={null}\nSELECT \n    count(*) AS total_connections,\n    count(*) FILTER (WHERE state = 'active') AS active,\n    count(*) FILTER (WHERE state = 'idle') AS idle,\n    (SELECT setting::int FROM pg_settings WHERE name = 'max_connections') AS max_connections\nFROM pg_stat_activity;\n```\n\n## Automating database maintenance\n\n### Schedule maintenance tasks\n\nSchedule the retention flow to run automatically. See [how to create deployments](/v3/how-to-guides/deployments/create-deployments) for creating scheduled deployments.\n\nFor example, you could run the retention flow daily at 2 AM to clean up old flow runs.\n\n### Recommended maintenance schedule\n\n* **Hourly**: Monitor disk space and connection count\n* **Daily**: Run retention policies, check bloat levels\n* **Weekly**: Analyze tables, review autovacuum performance\n* **Monthly**: REINDEX heavily used indexes, full database backup\n\n## Troubleshooting common issues\n\n### \"VACUUM is taking forever\"\n\n* Check for long-running transactions blocking VACUUM:\n  ```sql  theme={null}\n  SELECT pid, age(clock_timestamp(), query_start), usename, query \n  FROM pg_stat_activity \n  WHERE state <> 'idle' AND query NOT ILIKE '%vacuum%' \n  ORDER BY age DESC;\n  ```\n* Consider using `pg_repack` instead of `VACUUM FULL`\n* Run during low-traffic periods\n\n### \"Database is growing despite retention policies\"\n\n* Verify event retention is configured: `prefect config view | grep EVENTS_RETENTION`\n* Check if autovacuum is running on the events table\n* Ensure retention flow is actually executing (check flow run history)\n\n### \"Queries are getting slower over time\"\n\n* Update table statistics: `ANALYZE;`\n* Check for missing indexes using `pg_stat_user_tables`\n* Review query plans with `EXPLAIN ANALYZE`\n\n### \"Connection limit reached\"\n\n* Implement connection pooling immediately\n* Check for connection leaks: connections in 'idle' state for hours\n* Reduce Prefect worker/agent connection counts\n\n## Further reading\n\n* [PostgreSQL documentation on VACUUM](https://www.postgresql.org/docs/current/sql-vacuum.html)\n* [PostgreSQL routine maintenance](https://www.postgresql.org/docs/current/routine-vacuuming.html)\n* [Monitoring PostgreSQL](https://www.postgresql.org/docs/current/monitoring-stats.html)\n* [pg\\_repack extension](https://github.com/reorg/pg_repack)",
  "content_length": 13778
}