{
  "title": "API-sourced ETL",
  "source_url": "https://docs-3.prefect.io/v3/examples/run-api-sourced-etl",
  "content": "Build a small ETL pipeline that fetches JSON from a public API, transforms it with pandas, and writes a CSV â€“ all orchestrated by Prefect.\n\n<a href=\"https://github.com/PrefectHQ/prefect/blob/main/examples/run_api_sourced_etl.py\" target=\"_blank\">View on GitHub</a>\n\nPrefect turns everyday Python into production-grade workflows with **zero boilerplate**.\n\nWhen you pair Prefect with pandas you get a **versatile ETL toolkit**:\n\n* **Python** supplies a rich ecosystem of connectors and libraries for virtually every data source and destination.\n* **pandas** gives you lightning-fast, expressive transforms that turn raw bits into tidy DataFrames.\n* **Prefect** wraps the whole thing in battle-tested orchestration: automatic [retries](https://docs.prefect.io/v3/develop/write-tasks#retries), [scheduling](https://docs.prefect.io/v3/deploy/index#workflow-scheduling-and-parametrization), and [observability](https://docs.prefect.io/v3/develop/logging#prefect-loggers) , so you don't have to write reams of defensive code.\n\nThe result? You spend your time thinking about *what* you want to build, not *how* to keep it alive. Point this trio at any API, database, or file system and it will move the data where you need it while handling the messy details for you.\n\nIn this article you will:\n\n1. **Extract** JSON from the public [Dev.to REST API](https://dev.to/api).\n2. **Transform** it into an analytics-friendly pandas `DataFrame`.\n3. **Load** the result to a CSV â€“ ready for your BI tool of choice.\n\nThis example demonstrates these Prefect features:\n\n* [`@task`](https://docs.prefect.io/v3/develop/write-tasks#write-and-run-tasks) â€“ wrap any function in retries & observability.\n* [`log_prints`](https://docs.prefect.io/v3/develop/logging#configure-logging) â€“ surface `print()` logs automatically.\n* Automatic [**retries**](https://docs.prefect.io/v3/develop/write-tasks#retries) with back-off, no extra code.\n\n### Rapid analytics from a public API\n\nYour data team wants engagement metrics from Dev.to articles, daily. You need a quick,\nreliable pipeline that anyone can run locally and later schedule in Prefect Cloud.\n\n### The Solution\n\nWrite three small Python functions (extract, transform, load), add two decorators, and\nlet Prefect handle [retries](https://docs.prefect.io/v3/develop/write-tasks#retries), [concurrency](https://docs.prefect.io/v3/develop/task-runners#configure-a-task-runner), and [logging](https://docs.prefect.io/v3/develop/logging#prefect-loggers). No framework-specific hoops, just\nPython the way you already write it.\n\n*For more background on Prefect's design philosophy, check out our blog post: [Built to Fail: Design Patterns for Resilient Data Pipelines](https://www.prefect.io/blog/built-to-fail-design-patterns-for-resilient-data-pipelines)*\n\nWatch as Prefect orchestrates the ETL pipeline with automatic retries and logging. The flow fetches multiple pages of articles, transforms them into a structured DataFrame, and saves the results to CSV. This pattern is highly adaptable - use it to build pipelines that move data between any sources and destinations:\n\n* APIs â†’ Databases (Postgres, MySQL, etc.)\n* APIs â†’ Cloud Storage (S3, GCS, Azure)\n* APIs â†’ Data Warehouses (Snowflake, BigQuery, Redshift, etc.)\n* And many more combinations\n\n## Code walkthrough\n\n1. **Imports** â€“ Standard libraries for HTTP + pandas.\n2. **`fetch_page` task** â€“ Downloads a single page with retries.\n3. **`to_dataframe` task** â€“ Normalises JSON to a pandas DataFrame.\n4. **`save_csv` task** â€“ Persists the DataFrame and logs a peek.\n5. **`etl` flow** â€“ Orchestrates the tasks sequentially for clarity.\n6. **Execution** â€“ A friendly `if __name__ == \"__main__\"` with some basic configurations kicks things off.\n\n```python  theme={null}\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Any\n\nimport httpx\nimport pandas as pd\n\nfrom prefect import flow, task\n\n```\n\n***\n\n## Extract â€“ fetch a single page of articles\n\n```python  theme={null}\n@task(retries=3, retry_delay_seconds=[2, 5, 15])\ndef fetch_page(page: int, api_base: str, per_page: int) -> list[dict[str, Any]]:\n    \"\"\"Return a list of article dicts for a given page number.\"\"\"\n    url = f\"{api_base}/articles\"\n    params = {\"page\": page, \"per_page\": per_page}\n    print(f\"Fetching page {page} â€¦\")\n    response = httpx.get(url, params=params, timeout=30)\n    response.raise_for_status()\n    return response.json()\n\n\n```\n\n***\n\n## Transform â€“ convert list\\[dict] âžœ pandas DataFrame\n\n```python  theme={null}\n@task\ndef to_dataframe(raw_articles: list[list[dict[str, Any]]]) -> pd.DataFrame:\n    \"\"\"Flatten & normalise JSON into a tidy DataFrame.\"\"\"\n    # Combine pages, then select fields we care about\n    records = [article for page in raw_articles for article in page]\n    df = pd.json_normalize(records)[\n        [\n            \"id\",\n            \"title\",\n            \"published_at\",\n            \"url\",\n            \"comments_count\",\n            \"positive_reactions_count\",\n            \"tag_list\",\n            \"user.username\",\n        ]\n    ]\n    return df\n\n\n```\n\n***\n\n## Load â€“ save DataFrame to CSV (or print preview)\n\n```python  theme={null}\n@task\ndef save_csv(df: pd.DataFrame, path: Path) -> None:\n    \"\"\"Persist DataFrame to disk then log a preview.\"\"\"\n    df.to_csv(path, index=False)\n    print(f\"Saved {len(df)} rows âžœ {path}\\n\\nPreview:\\n{df.head()}\\n\")\n\n\n```\n\n***\n\n## Flow â€“ orchestrate the ETL with optional concurrency\n\n```python  theme={null}\n@flow(name=\"devto_etl\", log_prints=True)\ndef etl(api_base: str, pages: int, per_page: int, output_file: Path) -> None:\n    \"\"\"Run the end-to-end ETL for *pages* of articles.\"\"\"\n\n    # Extract â€“ simple loop for clarity\n    raw_pages: list[list[dict[str, Any]]] = []\n    for page_number in range(1, pages + 1):\n        raw_pages.append(fetch_page(page_number, api_base, per_page))\n\n    # Transform\n    df = to_dataframe(raw_pages)\n\n    # Load\n    save_csv(df, output_file)\n\n\n```\n\n## Run it!\n\n```bash  theme={null}\npython 01_getting_started/03_run_api_sourced_etl.py\n```\n\n```python  theme={null}\nif __name__ == \"__main__\":\n    # Configuration â€“ tweak to taste\n    api_base = \"https://dev.to/api\"\n    pages = 3  # Number of pages to fetch\n    per_page = 30  # Articles per page (max 30 per API docs)\n    output_file = Path(\"devto_articles.csv\")\n\n    etl(api_base=api_base, pages=pages, per_page=per_page, output_file=output_file)\n\n```\n\n## What just happened?\n\n1. Prefect registered a *flow run* and three *task runs* (`fetch_page`, `to_dataframe`, `save_csv`).\n2. Each `fetch_page` call downloaded a page and, if it failed, would automatically retry.\n3. The raw JSON pages were combined into a single pandas DataFrame.\n4. The CSV was written to disk and a preview printed locally (the flow's `log_prints=True` flag logs messages inside the flow body; prints inside tasks are displayed in the console).\n5. You can view run details, timings, and logs in the Prefect UI.\n\n## Key Takeaways\n\n* **Pure Python, powered-up** â€“ Decorators add retries and logging without changing your logic.\n* **Observability first** â€“ Each task run (including every page fetch) is logged and can be viewed in the UI if you have a Prefect Cloud account or a local Prefect server running.\n* **Composable** â€“ Swap `save_csv` for a database loader or S3 upload with one small change.\n* **Reusable** â€“ Import the `etl` flow and run it with different parameters from another flow.\n\nPrefect lets you focus on *data*, not orchestration plumbing â€“ happy ETL-ing! ðŸŽ‰",
  "content_length": 7474
}