{
  "title": "How to submit flows directly to dynamic infrastructure",
  "source_url": "https://docs-3.prefect.io/v3/advanced/submit-flows-directly-to-dynamic-infrastructure",
  "content": "Submit flows directly to different infrastructure types without a deployment\n\n<Warning>\n  **Beta Feature**\n\n  This feature is currently in beta. While we encourage you to try it out and provide feedback, please be aware that the API may change in future releases, potentially including breaking changes.\n</Warning>\n\nPrefect allows you to submit workflows directly to different infrastructure types without requiring a deployment. This enables you to dynamically choose where your workflows run based on their requirements, such as:\n\n* Training machine learning models that require GPUs\n* Processing large datasets that need significant memory\n* Running lightweight tasks that can use minimal resources\n\n## Benefits\n\nSubmitting workflows directly to dynamic infrastructure provides several advantages:\n\n* **Dynamic resource allocation**: Choose infrastructure based on workflow requirements at runtime\n* **Cost efficiency**: Use expensive infrastructure only when needed\n* **Consistency**: Ensure workflows always run on the appropriate infrastructure type\n* **Simplified workflow management**: No need to create and maintain deployments for different infrastructure types\n\n## Supported infrastructure\n\nDirect submission of workflows is currently supported for the following infrastructures:\n\n| Infrastructure            | Required Package     | Decorator                   |\n| ------------------------- | -------------------- | --------------------------- |\n| Docker                    | `prefect-docker`     | `@docker`                   |\n| Kubernetes                | `prefect-kubernetes` | `@kubernetes`               |\n| AWS ECS                   | `prefect-aws`        | `@ecs`                      |\n| Google Cloud Run          | `prefect-gcp`        | `@cloud_run`                |\n| Google Vertex AI          | `prefect-gcp`        | `@vertex_ai`                |\n| Azure Container Instances | `prefect-azure`      | `@azure_container_instance` |\n\nEach package can be installed using pip, for example:\n\n```bash  theme={null}\npip install prefect-docker\n```\n\n## Prerequisites\n\nBefore submitting workflows to specific infrastructure, you'll need:\n\n1. A work pool for each infrastructure type you want to use\n2. Object storage to associate with your work pool(s)\n\n## Setting up work pools and storage\n\n### Creating a work pool\n\nCreate work pools for each infrastructure type using the Prefect CLI:\n\n```bash  theme={null}\nprefect work-pool create NAME --type WORK_POOL_TYPE\n```\n\nFor detailed information on creating and configuring work pools, refer to the [work pools documentation](/v3/deploy/infrastructure-concepts/work-pools).\n\n### Configuring work pool storage\n\nTo enable Prefect to run workflows in remote infrastructure, work pools need an associated storage location to store serialized versions of submitted workflows and results from workflow runs.\n\nConfigure storage for your work pools using one of the supported storage types:\n\n<CodeGroup>\n  ```bash S3 theme={null}\n  prefect work-pool storage configure s3 WORK_POOL_NAME \\\n      --bucket BUCKET_NAME \\\n      --aws-credentials-block-name BLOCK_NAME\n  ```\n\n  ```bash Google Cloud Storage theme={null}\n  prefect work-pool storage configure gcs WORK_POOL_NAME \\\n      --bucket BUCKET_NAME \\\n      --gcp-credentials-block-name BLOCK_NAME\n  ```\n\n  ```base Azure Blob Storage theme={null}\n  prefect work-pool storage configure azure-blob-storage WORK_POOL_NAME \\\n      --container CONTAINER_NAME \\\n      --azure-blob-storage-credentials-block-name BLOCK_NAME\n  ```\n</CodeGroup>\n\nTo allow Prefect to upload and download serialized workflows, you can [create a block](/v3/develop/blocks) containing credentials with permission to access your configured storage location.\n\nIf a credentials block is not provided, Prefect will use the default credentials (e.g., a local profile or an IAM role) as determined by the corresponding cloud provider.\n\nYou can inspect your storage configuration using:\n\n```bash  theme={null}\nprefect work-pool storage inspect WORK_POOL_NAME\n```\n\n<Note>\n  **Local storage for `@docker`**\n\n  When using the `@docker` decorator with a local Docker engine, you can use volume mounts to share data between your Docker container and host machine.\n\n  Here's an example:\n\n  ```python  theme={null}\n  from prefect import flow\n  from prefect.filesystems import LocalFileSystem\n  from prefect_docker.experimental import docker\n\n\n  result_storage = LocalFileSystem(basepath=\"/tmp/results\")\n  result_storage.save(\"result-storage\", overwrite=True)\n\n\n  @docker(\n      work_pool=\"above-ground\",\n      volumes=[\"/tmp/results:/tmp/results\"],\n  )\n  @flow(result_storage=result_storage)\n  def run_in_docker(name: str):\n      return(f\"Hello, {name}!\")\n\n\n  print(run_in_docker(\"world\")) # prints \"Hello, world!\"\n  ```\n\n  To use local storage, ensure that:\n\n  1. The volume mount path is identical on both the host and container side\n  2. The `LocalFileSystem` block's `basepath` matches the path specified in the volume mount\n</Note>\n\n## Submitting workflows to specific infrastructure\n\nTo submit a flow to specific infrastructure, use the appropriate decorator for that infrastructure type.\n\nHere's an example using `@kubernetes`:\n\n```python  theme={null}\nfrom prefect import flow\nfrom prefect_kubernetes.experimental.decorators import kubernetes\n\n\n# Submit `my_remote_flow` to run in a Kubernetes job\n@kubernetes(work_pool=\"olympic\")\n@flow\ndef my_remote_flow(name: str):\n    print(f\"Hello {name}!\")\n\n@flow\ndef my_flow():\n    my_remote_flow(\"Marvin\")\n\n# Run the flow\nmy_flow()\n```\n\nWhen you run this code on your machine, `my_flow` will execute locally, while `my_remote_flow` will be submitted to run in a Kubernetes job.\n\n<Note>\n  **Parameters must be serializable**\n\n  Parameters passed to infrastructure-bound flows are serialized with `cloudpickle` to allow them to be transported to the destination infrastructure.\n\n  Most Python objects can be serialized with `cloudpickle`, but objects like database connections cannot be serialized. For parameters that cannot be serialized, you'll need to create the object inside your infrastructure-bound workflow.\n</Note>\n\n## Customizing infrastructure configuration\n\nYou can override the default configuration by providing additional kwargs to the infrastructure decorator:\n\n```python  theme={null}\nfrom prefect import flow\nfrom prefect_kubernetes.experimental.decorators import kubernetes\n\n\n@kubernetes(\n    work_pool=\"my-kubernetes-pool\",\n    namespace=\"custom-namespace\"\n)\n@flow\ndef custom_namespace_flow():\n    pass\n```\n\nAny kwargs passed to the infrastructure decorator will override the corresponding default value in the [base job template](/v3/how-to-guides/deployment_infra/manage-work-pools#base-job-template) for the specified work pool.\n\n## Further reading\n\n* [Work pools](/v3/concepts/work-pools) concept page",
  "content_length": 6833
}