{
  "title": "How to customize caching behavior",
  "source_url": "https://docs-3.prefect.io/v3/advanced/caching",
  "content": "### Separate cache key storage from result storage\n\nTo store cache records separately from the cached value, you can configure a cache policy to use a custom storage location.\n\nHere's an example of a cache policy configured to store cache records in a local directory:\n\n```python  theme={null}\nfrom prefect import task\nfrom prefect.cache_policies import TASK_SOURCE, INPUTS\n\ncache_policy = (TASK_SOURCE + INPUTS).configure(key_storage=\"/path/to/cache/storage\")\n\n@task(cache_policy=cache_policy)\ndef my_cached_task(x: int):\n    return x + 42\n```\n\nCache records will be stored in the specified directory while the persisted results will continue to be stored in `~/prefect/storage`.\n\nTo store cache records in a remote object store such as S3, pass a storage block instead:\n\n```python  theme={null}\nfrom prefect import task\nfrom prefect.cache_policies import TASK_SOURCE, INPUTS\n\nfrom prefect_aws import S3Bucket, AwsCredentials\n\ns3_bucket = S3Bucket(\n    credentials=AwsCredentials(\n        aws_access_key_id=\"my-access-key-id\",\n        aws_secret_access_key=\"my-secret-access-key\",\n    ),\n    bucket_name=\"my-bucket\",\n)\n# save the block to ensure it is available across machines\ns3_bucket.save(\"my-cache-records-bucket\")\n\ncache_policy = (TASK_SOURCE + INPUTS).configure(key_storage=s3_bucket)\n\n@task(cache_policy=cache_policy)\ndef my_cached_task(x: int):\n    return x + 42\n```\n\nStoring cache records in a remote object store allows you to share cache records across multiple machines.\n\n### Isolate cache access\n\nYou can control concurrent access to cache records by setting the `isolation_level` parameter on the cache policy. Prefect supports two isolation levels: `READ_COMMITTED` and `SERIALIZABLE`.\n\nBy default, cache records operate with a `READ_COMMITTED` isolation level. This guarantees that reading a cache record will see the latest committed cache value,\nbut allows multiple executions of the same task to occur simultaneously.\n\nConsider the following example:\n\n```python  theme={null}\nfrom prefect import task\nfrom prefect.cache_policies import INPUTS\nimport threading\n\n\ncache_policy = INPUTS\n\n@task(cache_policy=cache_policy)\ndef my_task_version_1(x: int):\n    print(\"my_task_version_1 running\")\n    return x + 42\n\n@task(cache_policy=cache_policy)\ndef my_task_version_2(x: int):\n    print(\"my_task_version_2 running\")\n    return x + 43\n\nif __name__ == \"__main__\":\n    thread_1 = threading.Thread(target=my_task_version_1, args=(1,))\n    thread_2 = threading.Thread(target=my_task_version_2, args=(1,))\n\n    thread_1.start()\n    thread_2.start()\n\n    thread_1.join()\n    thread_2.join()\n```\n\nWhen running this script, both tasks will execute in parallel and perform work despite both tasks using the same cache key.\n\nFor stricter isolation, you can use the `SERIALIZABLE` isolation level. This ensures that only one execution of a task occurs at a time for a given cache\nrecord via a locking mechanism.\n\nWhen setting `isolation_level` to `SERIALIZABLE`, you must also provide a `lock_manager` that implements locking logic for your system.\n\nHere's an updated version of the previous example that uses `SERIALIZABLE` isolation:\n\n```python  theme={null}\nimport threading\n\nfrom prefect import task\nfrom prefect.cache_policies import INPUTS\nfrom prefect.locking.memory import MemoryLockManager\nfrom prefect.transactions import IsolationLevel\n\ncache_policy = INPUTS.configure(\n    isolation_level=IsolationLevel.SERIALIZABLE,\n    lock_manager=MemoryLockManager(),\n)\n\n\n@task(cache_policy=cache_policy)\ndef my_task_version_1(x: int):\n    print(\"my_task_version_1 running\")\n    return x + 42\n\n\n@task(cache_policy=cache_policy)\ndef my_task_version_2(x: int):\n    print(\"my_task_version_2 running\")\n    return x + 43\n\n\nif __name__ == \"__main__\":\n    thread_1 = threading.Thread(target=my_task_version_1, args=(2,))\n    thread_2 = threading.Thread(target=my_task_version_2, args=(2,))\n\n    thread_1.start()\n    thread_2.start()\n\n    thread_1.join()\n    thread_2.join()\n```\n\nIn this example, only one of the tasks will run and the other will use the cached value.\n\n<Note>\n  **Locking in a distributed setting**\n\n  To manage locks in a distributed setting, you will need to use a storage system for locks that is accessible by all of your execution infrastructure.\n\n  We recommend using the `RedisLockManager` provided by `prefect-redis` in conjunction with a shared Redis instance:\n\n  ```python  theme={null}\n  from prefect import task\n  from prefect.cache_policies import TASK_SOURCE, INPUTS\n  from prefect.transactions import IsolationLevel\n\n  from prefect_redis import RedisLockManager\n\n  cache_policy = (INPUTS + TASK_SOURCE).configure(\n      isolation_level=IsolationLevel.SERIALIZABLE,\n      lock_manager=RedisLockManager(host=\"my-redis-host\"),\n  )\n\n  @task(cache_policy=cache_policy)\n  def my_cached_task(x: int):\n      return x + 42\n  ```\n</Note>\n\n### Coordinate caching across multiple tasks\n\nTo coordinate cache writes across tasks, you can run multiple tasks within a single [*transaction*](/v3/develop/transactions).\n\n```python  theme={null}\nfrom prefect import task, flow\nfrom prefect.transactions import transaction\n\n\n@task(cache_key_fn=lambda *args, **kwargs: \"static-key-1\")\ndef load_data():\n    return \"some-data\"\n\n\n@task(cache_key_fn=lambda *args, **kwargs: \"static-key-2\")\ndef process_data(data, fail):\n    if fail:\n        raise RuntimeError(\"Error! Abort!\")\n\n    return len(data)\n\n\n@flow\ndef multi_task_cache(fail: bool = True):\n    with transaction():\n        data = load_data()\n        process_data(data=data, fail=fail)\n```\n\nWhen this flow is run with the default parameter values it will fail on the `process_data` task after the `load_data` task has succeeded.\n\nHowever, because caches are only written to when a transaction is *committed*, the `load_data` task will *not* write a result to its cache key location until\nthe `process_data` task succeeds as well.\n\nOn a subsequent run with `fail=False`, both tasks will be re-executed and the results will be cached.\n\n### Handling Non-Serializable Objects\n\nYou may have task inputs that can't (or shouldn't) be serialized as part of the cache key. There are two direct approaches to handle this, both of which based on the same idea.\n\nYou can **adjust the serialization logic** to only serialize certain properties of an input:\n\n1. Using a custom cache key function:\n\n```python  theme={null}\nfrom prefect import flow, task\nfrom prefect.cache_policies import CacheKeyFnPolicy, RUN_ID\nfrom prefect.context import TaskRunContext\nfrom pydantic import BaseModel, ConfigDict\n\nclass NotSerializable:\n    def __getstate__(self):\n        raise TypeError(\"NooOoOOo! I will not be serialized!\")\n\nclass ContainsNonSerializableObject(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str\n    bad_object: NotSerializable\n\ndef custom_cache_key_fn(context: TaskRunContext, parameters: dict) -> str:\n    return parameters[\"some_object\"].name\n\n@task(cache_policy=CacheKeyFnPolicy(cache_key_fn=custom_cache_key_fn) + RUN_ID)\ndef use_object(some_object: ContainsNonSerializableObject) -> str:\n    return f\"Used {some_object.name}\"\n\n\n@flow\ndef demo_flow():\n    obj = ContainsNonSerializableObject(name=\"test\", bad_object=NotSerializable())\n    state = use_object(obj, return_state=True) # Not cached!\n    assert state.name == \"Completed\"\n    other_state = use_object(obj, return_state=True) # Cached!\n    assert other_state.name == \"Cached\"\n    assert state.result() == other_state.result()\n```\n\n2. Using Pydantic's [custom serialization](https://docs.pydantic.dev/latest/concepts/serialization/#custom-serializers) on your input types:\n\n```python  theme={null}\nfrom pydantic import BaseModel, ConfigDict, model_serializer\nfrom prefect import flow, task\nfrom prefect.cache_policies import INPUTS, RUN_ID\n\nclass NotSerializable:\n    def __getstate__(self):\n        raise TypeError(\"NooOoOOo! I will not be serialized!\")\n\nclass ContainsNonSerializableObject(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str\n    bad_object: NotSerializable\n\n    @model_serializer\n    def ser_model(self) -> dict:\n        \"\"\"Only serialize the name, not the problematic object\"\"\"\n        return {\"name\": self.name}\n\n@task(cache_policy=INPUTS + RUN_ID)\ndef use_object(some_object: ContainsNonSerializableObject) -> str:\n    return f\"Used {some_object.name}\"\n\n@flow\ndef demo_flow():\n    some_object = ContainsNonSerializableObject(\n        name=\"test\",\n        bad_object=NotSerializable()\n    )\n    state = use_object(some_object, return_state=True) # Not cached!\n    assert state.name == \"Completed\"\n    other_state = use_object(some_object, return_state=True) # Cached!\n    assert other_state.name == \"Cached\"\n    assert state.result() == other_state.result()\n```\n\nChoose the approach that best fits your needs:\n\n* Use Pydantic models when you want consistent serialization across your application\n* Use custom cache key functions when you need different caching logic for different tasks",
  "content_length": 8992
}