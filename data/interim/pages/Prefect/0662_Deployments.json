{
  "title": "Deployments",
  "source_url": "https://docs-3.prefect.io/v3/concepts/deployments",
  "content": "Learn how to use deployments to trigger flow runs remotely.\n\nDeployments allow you to run flows on a [schedule](/v3/concepts/schedules) and trigger runs based on [events](/v3/how-to-guides/automations/creating-deployment-triggers/).\n\nDeployments are server-side representations of flows.\nThey store the crucial metadata for remote orchestration including when, where, and how a workflow should run.\n\nIn addition to manually triggering and managing flow runs, deploying a flow exposes an API and UI that allow you to:\n\n* trigger new runs, [cancel active runs](/v3/how-to-guides/workflows/write-and-run#cancel-a-flow-run), pause scheduled runs, [customize parameters](/v3/concepts/flows#specify-flow-parameters), and more\n* remotely configure [schedules](/v3/concepts/schedules) and [automation rules](/v3/how-to-guides/automations/creating-deployment-triggers)\n* dynamically provision infrastructure with [work pools](/v3/deploy/infrastructure-concepts/work-pools) - optionally with templated guardrails for other users\n\nIn Prefect Cloud, deployment configuration is versioned, and a new [deployment version](/v3/how-to-guides/deployments/versioning) is created each time a deployment is updated.\n\n### Work pools\n\n[Work pools](/v3/concepts/work-pools) allow you to switch between different types of infrastructure and to create a template for deployments.\nData platform teams find work pools especially useful for managing infrastructure configuration across teams of data professionals.\n\nCommon work pool types include [Docker](/v3/how-to-guides/deployment_infra/docker), [Kubernetes](/v3/how-to-guides/deployment_infra/kubernetes), and serverless options such as [AWS ECS](/integrations/prefect-aws/ecs_guide#ecs-worker-guide), [Azure ACI](/integrations/prefect-azure/aci_worker), [GCP Vertex AI](/integrations/prefect-gcp/index#run-flows-on-google-cloud-run-or-vertex-ai), or [GCP Google Cloud Run](/integrations/prefect-gcp/gcp-worker-guide).\n\n### Work pool-based deployment requirements\n\nDeployments created through the Python SDK that use a work pool require a `name`.\nThis value becomes the deployment name.\nA `work_pool_name` is also required.\n\nYour flow code location can be specified in a few ways:\n\n1. Bake it into your Docker image (for work-pools that use Docker images).\n   As shown in the example above,Prefect facilitates this as the default method for deployments created with the Python SDK.\n   This method requires that you specify the `image` argument in the `deploy` method.\n2. Call `from_source` on a flow and specify one of the following:\n   1. the git-based cloud provider location (for example, GitHub)\n   2. the cloud provider storage location (for example, AWS S3)\n   3. the local path (an option for Process work pools)\n\nSee the [Retrieve code from storage docs](/v3/how-to-guides/deployments/store-flow-code) for more information about flow code storage.\n\n## Run a deployment\n\nYou can set a deployment to run manually, on a [schedule](/v3/how-to-guides/deployments/create-schedules), or [in response to an event](/v3/how-to-guides/automations/creating-deployment-triggers).\n\nThe deployment inherits the infrastructure configuration from the work pool, and can be overridden at deployment creation time or at runtime.\n\n### Work pools that require a worker\n\nTo run a deployment with a hybrid work pool type, such as Docker or Kubernetes, you must start a [worker](/v3/concepts/workers/).\n\nA [Prefect worker](/v3/concepts/workers) is a client-side process that checks for scheduled flow runs in the work pool that it matches.\nWhen a scheduled run is found, the worker kicks off a flow run on the specified infrastructure and monitors the flow run until completion.\n\n### Work pools that don't require a worker\n\nPrefect Cloud offers [push work pools](/v3/how-to-guides/deployment_infra/serverless#automatically-create-a-new-push-work-pool-and-provision-infrastructure) that run flows on Cloud provider serverless infrastructure without a worker and that can be set up quickly.\n\nPrefect Cloud also provides the option to run work flows on Prefect's infrastructure through a [Prefect Managed work pool](/v3/how-to-guides/deployment_infra/managed).\n\nThese work pool types do not require a worker to run flows.\nHowever, they do require sharing a bit more information with Prefect, which can be a challenge depending upon the security posture of your organization.\n\n## Static vs. dynamic infrastructure\n\nYou can deploy your flows on long-lived static infrastructure or on dynamic infrastructure that is able to scale horizontally.\nThe best choice depends on your use case.\n\n### Static infrastructure\n\nWhen you have several flows running regularly, [the `serve` method](/v3/how-to-guides/deployment_infra/run-flows-in-local-processes#serve-a-flow)\nof the `Flow` object or [the `serve` utility](/v3/how-to-guides/deployment_infra/run-flows-in-local-processes#serve-multiple-flows-at-once)\nis a great option for managing multiple flows simultaneously.\n\nOnce you have authored your flow and decided on its deployment settings, run this long-running\nprocess in a location of your choosing.\nThe process stays in communication with the Prefect API, monitoring for work and submitting each run\nwithin an individual subprocess.\nBecause runs are submitted to subprocesses, any external infrastructure configuration\nmust be set up beforehand and kept associated with this process.\n\nBenefits to this approach include:\n\n* Users are in complete control of their infrastructure, and anywhere the \"serve\" Python process can\n  run is a suitable deployment environment.\n* It is simple to reason about.\n* Creating deployments requires a minimal set of decisions.\n* Iteration speed is fast.\n\n### Dynamic infrastructure\n\nConsider running flows on dynamically provisioned infrastructure with work pools when you have any of the following:\n\n* Flows that require expensive infrastructure due to the long-running process.\n* Flows with heterogeneous infrastructure needs across runs.\n* Large volumes of deployments.\n* An internal organizational structure in which deployment authors or runners are not members of\n  the team that manages the infrastructure.\n\n[Work pools](/v3/concepts/work-pools/) allow Prefect to exercise greater control\nof the infrastructure on which flows run.\nOptions for [serverless work pools](/v3/how-to-guides/deployment_infra/serverless/) allow you to\nscale to zero when workflows aren't running.\nPrefect even provides you with the ability to\n[provision cloud infrastructure via a single CLI command](/v3/how-to-guides/deployment_infra/serverless/#automatically-create-a-new-push-work-pool-and-provision-infrastructure),\nif you use a Prefect Cloud push work pool option.\n\nWith work pools:\n\n* You can configure and monitor infrastructure configuration within the Prefect UI.\n* Infrastructure is ephemeral and dynamically provisioned.\n* Prefect is more infrastructure-aware and collects more event data from your infrastructure by default.\n* Highly decoupled setups are possible.\n\n<Note>\n  **You don't have to commit to one approach**\n\n  You can mix and match approaches based on the needs of each flow. You can also change the\n  deployment approach for a particular flow as its needs evolve.\n  For example, you might use workers for your expensive machine learning pipelines,\n  but use the serve mechanics for smaller, more frequent file-processing pipelines.\n</Note>\n\n## Deployment schema\n\n```python  theme={null}\nclass Deployment:\n    \"\"\"\n    Structure of the schema defining a deployment\n    \"\"\"\n\n    # required defining data\n    name: str\n    flow_id: UUID\n    entrypoint: str\n    path: str | None = None\n\n    # workflow scheduling and parametrization\n    parameters: dict[str, Any] | None = None\n    parameter_openapi_schema: dict[str, Any] | None = None\n    schedules: list[Schedule] | None = None\n    paused: bool = False\n    trigger: Trigger | None = None\n\n    # concurrency limiting\n    concurrency_limit: int | None = None\n    concurrency_options: ConcurrencyOptions(collision_strategy=Literal['ENQUEUE', 'CANCEL_NEW']) | None = None\n\n    # metadata for bookkeeping\n    version: str | None = None\n    version_type: VersionType | None = None\n    description: str | None = None\n    tags: list | None = None\n\n    # worker-specific fields\n    work_pool_name: str | None = None\n    work_queue_name: str | None = None\n    job_variables: dict[str, Any] | None = None\n    pull_steps: dict[str, Any] | None = None\n```\n\nAll methods for creating Prefect deployments are interfaces for populating this schema.\n\n### Required defining data\n\nDeployments require a `name` and a reference to an underlying `Flow`.\n\nThe deployment name is not required to be unique across all deployments, but is required to be unique\nfor a given flow ID. This means you will often see references to the deployment's unique identifying name\n`{FLOW_NAME}/{DEPLOYMENT_NAME}`.\n\nYou can trigger deployment runs in multiple ways. For a complete guide, see [Run deployments](/v3/how-to-guides/deployments/run-deployments).\n\nQuick examples:\n\nFrom the CLI:\n\n```bash  theme={null}\nprefect deployment run my-first-flow/my-first-deployment\n```\n\nFrom Python:\n\n```python  theme={null}\nfrom prefect.deployments import run_deployment\n\nrun_deployment(name=\"my-first-flow/my-first-deployment\")\n```\n\nThe other two fields are:\n\n* **`path`**: think of the path as the runtime working directory for the flow.\n  For example, if a deployment references a workflow defined within a Docker image, the `path` is the\n  absolute path to the parent directory where that workflow will run anytime the deployment is triggered.\n  This interpretation is more subtle in the case of flows defined in remote filesystems.\n* **`entrypoint`**: the entrypoint of a deployment is a relative reference to a function decorated as a\n  flow that exists on some filesystem. It is always specified relative to the `path`.\n  Entrypoints use Python's standard path-to-object syntax\n  (for example, `path/to/file.py:function_name` or simply `path:object`).\n\nThe entrypoint must reference the same flow as the flow ID.\n\nPrefect requires that deployments reference flows defined *within Python files*.\nFlows defined within interactive REPLs or notebooks cannot currently be deployed as such.\nThey are still valid flows that will be monitored by the API and observable in the UI whenever they are run,\nbut Prefect cannot trigger them.\n\n<Note>\n  **Deployments do not contain code definitions**\n\n  Deployment metadata references code that exists in potentially diverse locations within your environment.\n  This separation means that your flow code stays within your storage and execution\n  infrastructure.\n\n  This is key to the Prefect hybrid model: there's a boundary between your proprietary assets,\n  such as your flow code, and the Prefect backend (including [Prefect Cloud](/v3/how-to-guides/cloud/connect-to-cloud)).\n</Note>\n\n### Workflow scheduling and parametrization\n\nOne of the primary motivations for creating deployments of flows is to remotely *schedule* and *trigger* them.\nJust as you can call flows as functions with different input values, deployments can be triggered or\nscheduled with different values through parameters.\n\nThese are the fields to capture the required metadata for those actions:\n\n* **`schedules`**: a list of [schedule objects](/v3/concepts/schedules).\n  Most of the convenient interfaces for creating deployments allow users to avoid creating this object themselves.\n  For example, when [updating a deployment schedule in the UI](/v3/concepts/schedules)\n  basic information such as a cron string or interval is all that's required.\n* **`parameter_openapi_schema`**: an [OpenAPI compatible schema](https://swagger.io/specification/) that defines\n  the types and defaults for the flow's parameters.\n  This is used by the UI and the backend to expose options for creating manual runs as well as type validation.\n* **`parameters`**: default values of flow parameters that this deployment will pass on each run.\n  These can be overwritten through a trigger or when manually creating a custom run.\n* **`enforce_parameter_schema`**: a boolean flag that determines whether the API should validate the parameters\n  passed to a flow run against the schema defined by `parameter_openapi_schema`.\n\n<Tip>\n  **Scheduling is asynchronous and decoupled**\n\n  Pausing a schedule, updating your deployment, and other actions reset your auto-scheduled runs.\n</Tip>\n\n### Concurrency limiting\n\nPrefect supports managing concurrency at the deployment level to enable limiting how many runs of a\ndeployment can be active at once. To enable this behavior, deployments have the following fields:\n\n* **`concurrency_limit`**: an integer that sets the maximum number of concurrent flow runs for the deployment.\n* **`collision_strategy`**: configure the behavior for runs once the concurrency limit is reached.\n  Falls back to `ENQUEUE` if unset.\n  * `ENQUEUE`: new runs transition to `AwaitingConcurrencySlot` and execute as slots become available.\n  * `CANCEL_NEW`: new runs are canceled until a slot becomes available.\n\n<CodeGroup>\n  ```sh prefect deploy theme={null}\n  prefect deploy ... --concurrency-limit 3 --collision-strategy ENQUEUE\n  ```\n\n  ```python flow.deploy() theme={null}\n  from prefect.client.schemas.objects import (\n      ConcurrencyLimitConfig,\n      ConcurrencyLimitStrategy\n  )\n\n  my_flow.deploy(..., concurrency_limit=3)\n\n  my_flow.deploy(\n      ...,\n      concurrency_limit=ConcurrencyLimitConfig(\n          limit=3, collision_strategy=ConcurrencyLimitStrategy.CANCEL_NEW\n      ),\n  )\n  ```\n\n  ```python flow.serve() theme={null}\n  from prefect.client.schemas.objects import (\n      ConcurrencyLimitConfig,\n      ConcurrencyLimitStrategy\n  )\n\n  my_flow.serve(..., global_limit=3)\n\n  my_flow.serve(\n      ...,\n      global_limit=ConcurrencyLimitConfig(\n          limit=3, collision_strategy=ConcurrencyLimitStrategy.CANCEL_NEW\n      ),\n  )\n  ```\n</CodeGroup>\n\n### Metadata for bookkeeping\n\nImportant information for the versions, descriptions, and tags fields:\n\n* **`version`**: versions are always set by the client and can be any arbitrary string.\n  We recommend tightly coupling this field on your deployments to your software development lifecycle and choosing human-readable version strings.\n  If left unset, the version field will be automatically populated in one of two ways:\n  * If deploying from a directory inside a Git repository or from a CI environment on a supported version control provider, `version` will be the first eight characters of your commit hash.\n  * In all other circumstances, `version` will be your flow's version, which if not assigned in the flow decorator (`@flow(version=\"my-version\")`) will be a hash of the file the flow is defined in.\n* **`version_type`**: When a deployment is created or updated, Prefect will attempt to infer version information from your environment.\n  Providing a `version_type` instructs Prefect to only attempt version information collection from an environment of that type.\n  The following version types are available: `vcs:github`, `vcs:gitlab`, `vcs:bitbucket`, `vcs:azuredevops`, `vcs:git`, or `prefect:simple`.\n  `vcs:git` offers similar versioning detail to officially supported version control platforms, but does not support direct linking to commits from the Prefect Cloud UI.\n  It is meant as a fallback option in case your version control platform is not supported.\n  `prefect:simple` is for any deployment version created where no Git context is available.\n  If left unset, Prefect will automatically select the appropriate `version_type` based on the detected environment.\n* **`description`**: provide reference material such as intended use and parameter documentation.\n  Markdown is accepted. The docstring of your flow function is the default value.\n* **`tags`**: group related work together across a diverse set of objects.\n  Tags set on a deployment are inherited by that deployment's flow runs. Filter, customize views, and\n  searching by tag.\n\n<Tip>\n  **Everything has a version**\n\n  Deployments have a version attached; and flows and tasks also have\n  versions set through their respective decorators. These versions are sent to the API\n  anytime the flow or task runs, allowing you to audit changes.\n</Tip>\n\n### Worker-specific fields\n\n[Work pools](/v3/concepts/work-pools/) and [workers](/v3/concepts/workers/) are an advanced deployment pattern that\nallow you to dynamically provision infrastructure for each flow run.\nThe work pool job template interface allows users to create and govern opinionated interfaces\nto their workflow infrastructure.\nTo do this, a deployment using workers needs the following fields:\n\n* **`work_pool_name`**: the name of the work pool this deployment is associated with.\n  Work pool types mirror infrastructure types, which means this field impacts the options available\n  for the other fields.\n* **`work_queue_name`**: if you are using work queues to either manage priority or concurrency, you can\n  associate a deployment with a specific queue within a work pool using this field.\n* **`job_variables`**: this field allows deployment authors to customize whatever infrastructure\n  options have been exposed on this work pool.\n  This field is often used for Docker image names, Kubernetes annotations and limits,\n  and environment variables.\n* **`pull_steps`**: a JSON description of steps that retrieves flow code or\n  configuration, and prepares the runtime environment for workflow execution.\n\nPull steps allow users to highly decouple their workflow architecture.\nFor example, a common use of pull steps is to dynamically pull code from remote filesystems such as\nGitHub with each run of their deployment.",
  "content_length": 17692
}